[
  {
    "id": 1,
    "text": "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most cost-optimal and resource-efficient solution. Amazon CloudWatch alarms can directly trigger an EC2 reboot action upon Instance Health Check failure. This approach leverages built-in AWS functionality, avoiding the need for custom Lambda functions and EventBridge rules, which incur additional costs and complexity. The EC2 Reboot CloudWatch Alarm Action is specifically designed for this type of scenario and provides a simple, direct, and cost-effective way to automatically recover from instance health check failures.\n\n**Why option 0 is incorrect:**\nis incorrect because using Amazon EventBridge to trigger a Lambda function every 5 minutes to reboot the instance is not efficient. It's an unnecessary overhead to reboot the instance every 5 minutes regardless of its health. This approach is also more costly than using CloudWatch alarms with built-in reboot actions. It also doesn't address the core issue of detecting a failure before rebooting.\n\n**Why option 2 is incorrect:**\nis incorrect because while it correctly identifies the need to check the instance status and reboot on failure, it introduces unnecessary complexity and cost. Using a Lambda function triggered by EventBridge to check the instance status and then use the EC2 API to reboot is more complex and expensive than using the built-in CloudWatch alarm action. CloudWatch alarms can directly trigger the reboot action without the need for custom code.\n\n**Why option 3 is incorrect:**\nis incorrect because it adds unnecessary complexity and cost by introducing SNS as an intermediary between the CloudWatch alarm and the Lambda function. Publishing to SNS and then triggering a Lambda function adds latency and increases the operational overhead. The direct EC2 Reboot CloudWatch Alarm Action is a simpler and more efficient solution.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 2,
    "text": "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance. How will you resolve the issue and make sure it doesn't happen again?",
    "options": [
      {
        "id": 0,
        "text": "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Disable the Termination from the Auto Scaling Group any time a user reports an issue",
        "correct": false
      },
      {
        "id": 3,
        "text": "Make a snapshot of the Amazon EC2 instance just before it gets terminated",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nInstalling an Amazon CloudWatch Logs agent on the EC2 instances allows the logs to be streamed to CloudWatch Logs in real-time. This ensures that the logs are persisted even after the EC2 instance is terminated. CloudWatch Logs provides a centralized and durable storage solution for logs, making them accessible for debugging and analysis even after the instance is gone. This approach aligns with best practices for logging in cloud environments, ensuring that logs are not tied to the lifecycle of individual instances.\n\n**Why option 1 is incorrect:**\nUsing AWS Lambda to SSH into the EC2 instances and copy log files to S3 is a complex and inefficient solution. It requires managing SSH keys, dealing with potential network connectivity issues, and introduces unnecessary overhead. It also adds a security risk by requiring Lambda to have SSH access to the instances. This approach is not scalable or reliable compared to using CloudWatch Logs.\n\n**Why option 2 is incorrect:**\nDisabling termination from the Auto Scaling Group manually is not a scalable or practical solution. It requires manual intervention every time an issue is reported, which is error-prone and time-consuming. It also defeats the purpose of having an Auto Scaling Group, which is to automatically manage the capacity of the application. This approach is not suitable for a production environment.\n\n**Why option 3 is incorrect:**\nMaking a snapshot of the EC2 instance just before termination is also not a scalable or efficient solution. It requires implementing a mechanism to detect instance termination and trigger the snapshot creation. Snapshots can be large and time-consuming to create, especially for instances with large amounts of data. It also adds complexity to the infrastructure and is not as efficient as streaming logs to CloudWatch Logs.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement. Can you identify the correct solution leveraging the capabilities of AWS WAF?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS WAF can be deployed on Amazon CloudFront. By creating a CloudFront distribution in front of the EC2 instances and then deploying AWS WAF on CloudFront, all incoming requests are inspected by WAF before they reach the EC2 instances. This allows WAF to filter out malicious traffic and protect the application from various attacks, such as SQL injection, cross-site scripting (XSS), and other common web exploits. CloudFront also provides caching capabilities, which can improve performance and reduce the load on the EC2 instances.\n\n**Why option 0 is incorrect:**\nis incorrect because while it suggests using CloudFront, it incorrectly states that AWS WAF cannot be directly configured on ALB. AWS WAF *can* be directly configured on an ALB. The suggestion to distribute from ALB to CloudFront is backwards and less efficient for this scenario. The correct approach is CloudFront in front of the ALB (or EC2 directly).\n\n**Why option 1 is incorrect:**\nis incorrect because AWS WAF cannot be directly configured on Amazon EC2 instances. AWS WAF is a service that integrates with other AWS services like CloudFront, ALB, and API Gateway. It doesn't run directly on EC2 instances.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down. As a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
    "options": [
      {
        "id": 0,
        "text": "The application will be down until the primary database has recovered itself",
        "correct": false
      },
      {
        "id": 1,
        "text": "The URL to access the database will change to the standby database",
        "correct": false
      },
      {
        "id": 2,
        "text": "An email will be sent to the System Administrator asking for manual intervention",
        "correct": false
      },
      {
        "id": 3,
        "text": "The CNAME record will be updated to point to the standby database",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Amazon RDS Multi-AZ deployments are designed for automatic failover. When the primary database instance fails, RDS automatically promotes the standby instance to become the new primary. To ensure applications continue to connect to the database without interruption, RDS updates the CNAME record (Canonical Name record) in DNS to point to the new primary instance (formerly the standby). This DNS update is transparent to the application, allowing it to continue operating with minimal downtime. The application uses the same endpoint, and RDS handles the redirection behind the scenes.\n\n**Why option 0 is incorrect:**\nis incorrect because Multi-AZ deployments are specifically designed to avoid application downtime during primary instance failures. The automatic failover mechanism ensures that the standby instance takes over, minimizing the impact on the application. The application is not down until the primary database recovers itself.\n\n**Why option 1 is incorrect:**\nis incorrect because the URL (or endpoint) to access the database does *not* change. The application continues to use the same DNS name. RDS manages the failover by updating the DNS record to point to the new primary instance. Changing the URL would require application code changes, which defeats the purpose of automatic failover.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A healthcare startup runs a lightweight reporting application on a single Amazon EC2 On-Demand instance. The application is designed to be stateless, fault-tolerant, and optimized for fast rendering of analytics dashboards. During major health events or news cycles, the team observes latency issues and occasional 5xx errors due to traffic spikes. To meet growing demand without over-provisioning resources during off-peak hours, the company wants to implement a cost-effective, scalable solution that ensures consistent performance even under unpredictable load. Which approach best meets the requirements while minimizing costs?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%",
        "correct": false
      },
      {
        "id": 1,
        "text": "Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load",
        "correct": false
      },
      {
        "id": 3,
        "text": "Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it leverages Auto Scaling with Spot Instances and an Application Load Balancer (ALB). Creating an AMI allows for consistent deployments. The Auto Scaling group dynamically adjusts the number of EC2 instances based on demand, providing scalability. Using Spot Instances significantly reduces costs compared to On-Demand instances, especially during off-peak hours. The ALB distributes traffic across the instances, ensuring high availability and consistent performance. The launch template ensures that all instances are configured identically.\n\n**Why option 0 is incorrect:**\nis incorrect because redeploying the application to a different Availability Zone (AZ) upon high CPU utilization is not a scalable solution. It doesn't address the underlying issue of insufficient resources to handle the load. Redeploying is also a slow process and would likely exacerbate the latency issues. It also doesn't provide a mechanism for scaling *out* the application, only moving it.\n\n**Why option 1 is incorrect:**\nis incorrect because simply cloning the EC2 instance and adding another On-Demand instance behind an ALB provides limited scalability and is not cost-effective. It only doubles the capacity and doesn't automatically adjust to changing demand. On-Demand instances are more expensive than Spot Instances, especially when the application doesn't require constant high capacity. It also requires manual intervention to scale beyond two instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway. Which conditions should be met for internet connectivity to be established? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic",
        "correct": true
      },
      {
        "id": 1,
        "text": "The subnet has been configured to be public and has no access to the internet",
        "correct": false
      },
      {
        "id": 2,
        "text": "The instance's subnet is not associated with any route table",
        "correct": false
      },
      {
        "id": 3,
        "text": "The route table in the instance’s subnet should have a route to an Internet Gateway",
        "correct": true
      },
      {
        "id": 4,
        "text": "The instance's subnet is associated with multiple route tables with conflicting configurations",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent the fundamental requirements for an EC2 instance to access the internet via an Internet Gateway.\n\n*   **Option 0:** Network ACLs act as a firewall at the subnet level. To allow internet traffic, the Network ACL associated with the subnet must have rules that explicitly allow both inbound (from the internet to the instance) and outbound (from the instance to the internet) traffic. Without these rules, traffic will be blocked, preventing internet connectivity. The default Network ACL is permissive, allowing all traffic, but custom ACLs are often used and may restrict traffic.\n\n*   **Option 3:** The route table associated with the subnet must have a route that directs traffic destined for the internet (0.0.0.0/0) to the Internet Gateway. This route tells the VPC how to handle traffic intended for destinations outside the VPC. Without this route, the VPC won't know where to send internet-bound traffic, and the instance won't be able to connect to the internet.\n\n**Why option 3 is correct:**\nThese are correct because they represent the fundamental requirements for an EC2 instance to access the internet via an Internet Gateway.\n\n*   **Option 0:** Network ACLs act as a firewall at the subnet level. To allow internet traffic, the Network ACL associated with the subnet must have rules that explicitly allow both inbound (from the internet to the instance) and outbound (from the instance to the internet) traffic. Without these rules, traffic will be blocked, preventing internet connectivity. The default Network ACL is permissive, allowing all traffic, but custom ACLs are often used and may restrict traffic.\n\n*   **Option 3:** The route table associated with the subnet must have a route that directs traffic destined for the internet (0.0.0.0/0) to the Internet Gateway. This route tells the VPC how to handle traffic intended for destinations outside the VPC. Without this route, the VPC won't know where to send internet-bound traffic, and the instance won't be able to connect to the internet.\n\n**Why option 1 is incorrect:**\nis incorrect because a subnet configured as public *should* have access to the internet, not the opposite. A public subnet is defined by having a route to an Internet Gateway in its associated route table. The statement contradicts the definition of a public subnet.\n\n**Why option 2 is incorrect:**\nis incorrect because a subnet *must* be associated with a route table. If no route table is explicitly associated, the subnet uses the VPC's main route table. If the instance's subnet is not associated with any route table, it will use the main route table. The problem is not the absence of a route table, but the absence of a route to the Internet Gateway within the route table.\n\n**Why option 4 is incorrect:**\nis incorrect because a subnet can only be associated with *one* route table. If multiple route tables are present, only one can be actively associated with the subnet. The problem is not multiple route tables, but the configuration of the single route table associated with the subnet.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A digital content production company has transitioned all of its media assets to Amazon S3 in an effort to reduce storage costs. However, the rendering engine used in production continues to run in an on-premises data center and requires frequent and low-latency access to large media files. The company wants to implement a storage solution that maintains application performance while keeping costs low. Which approach should the company choose to meet these requirements in the most cost-effective way?",
    "options": [
      {
        "id": 0,
        "text": "Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon S3 File Gateway to provide storage for the on-premises application",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nsetting up an Amazon S3 File Gateway, is the most suitable solution. S3 File Gateway provides a local cache for frequently accessed data, enabling low-latency access for the on-premises rendering engine. It efficiently manages data transfer between the on-premises environment and S3, ensuring that the rendering engine has quick access to the required media files. File Gateway also handles the data transfer and storage management, reducing the operational overhead. It's more cost-effective than options involving dedicated file systems or custom solutions.\n\n**Why option 0 is incorrect:**\nusing Mountpoint for Amazon S3, while providing access to S3 objects, doesn't inherently provide low-latency access for frequently accessed data. Mountpoint is designed for high-throughput access to large objects, but it doesn't have a local caching mechanism like File Gateway. This means that every access would require a network round trip to S3, which would not meet the low-latency requirement. Also, Mountpoint is more suitable for applications that can tolerate eventual consistency and don't require POSIX semantics.\n\n**Why option 2 is incorrect:**\ndeploying Amazon FSx for Lustre and using DataSync, is a viable solution for low-latency access, but it's significantly more expensive than using S3 File Gateway. FSx for Lustre is a high-performance file system designed for compute-intensive workloads. While it would provide excellent performance, the cost of running and managing FSx for Lustre, along with the DataSync transfers, would be higher than necessary for this scenario. The question emphasizes cost-effectiveness, making this option less desirable.\n\n**Why option 3 is incorrect:**\nsetting up a dedicated on-premises storage array with a custom application, is the least efficient and most complex solution. It requires significant upfront investment in hardware, ongoing maintenance, and the development of a custom application for data synchronization. This approach is also more prone to errors and requires more operational overhead compared to using a managed service like S3 File Gateway. It also doesn't leverage the benefits of AWS managed services for data management and security.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required. Which solution is the MOST cost-effective for the given use case?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most cost-effective solution. It suggests moving the files from Amazon S3 Standard to Amazon S3 Standard-IA after 30 days. Standard-IA is designed for infrequently accessed data but offers rapid access when needed. This aligns perfectly with the requirement of immediate accessibility and the infrequent access pattern after the initial 30 days. The lifecycle policy then deletes the files after 5 years, fulfilling the retention requirement. Standard-IA provides lower storage costs compared to Standard for infrequently accessed data, making it more cost-effective.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon S3 Glacier Flexible Retrieval (formerly Glacier) is designed for archival data where retrieval times of minutes to hours are acceptable. The requirement states that immediate accessibility is always required, which Glacier Flexible Retrieval cannot guarantee. While it's cheaper than Standard-IA for storage, the retrieval time makes it unsuitable.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon S3 One Zone-IA stores data in a single Availability Zone. While it offers lower storage costs than Standard-IA, it comes with a higher risk of data loss because if the Availability Zone is destroyed, the data is lost. The question states that the files contain critical business data that is not easy to reproduce, making One Zone-IA an unacceptable risk. The reduced cost does not outweigh the data durability concerns.\n\n**Why option 3 is incorrect:**\nis incorrect because it suggests archiving the files to Amazon S3 Glacier Deep Archive after 5 years. The requirement is to *delete* the files after 5 years, not archive them. While Glacier Deep Archive is the cheapest storage option, it involves longer retrieval times (hours), and the question explicitly states that immediate accessibility is always required. Furthermore, the lifecycle policy should delete the files after 5 years, not archive them.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A digital media startup allows users to submit images through its web portal. These images are uploaded directly into an Amazon S3 bucket. On average, around 200 images are uploaded daily. The company wants to automatically generate a smaller preview version (thumbnail) of each new image and store the resulting thumbnails in a separate Amazon S3 bucket. The team prefers a design that is low-cost, requires minimal infrastructure management, and automatically reacts to new uploads. Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most cost-effective and efficient solution. Configuring the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded creates an event-driven architecture. Lambda functions are serverless, meaning there's no infrastructure to manage, and you only pay for the compute time used when the function is triggered. The Lambda function can then process the image, create a thumbnail, and store it in the second S3 bucket. This approach directly addresses all requirements: low cost (pay-per-use Lambda), minimal infrastructure management (serverless Lambda), and automatic reaction to new uploads (S3 event trigger).\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon S3 Access Analyzer is designed to identify bucket and access point configurations that could lead to unintended access to your S3 data. It is not designed to trigger events based on object uploads. While you could potentially integrate Access Analyzer with other services to achieve the desired outcome, it would be significantly more complex and expensive than using S3 event notifications directly. Also, Access Analyzer is not the appropriate service for this task.\n\n**Why option 2 is incorrect:**\nis incorrect because using AWS Glue jobs on a regular interval is less efficient and more costly than an event-driven approach. Glue jobs are designed for ETL (Extract, Transform, Load) operations and are not ideal for real-time image processing. Polling the S3 bucket at regular intervals means that you'll be paying for compute time even when there are no new images to process. This is less cost-effective than using S3 event notifications, which only trigger the Lambda function when a new image is uploaded. Furthermore, Glue is more complex to configure and manage than a simple Lambda function.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying a containerized application on AWS Fargate to poll the S3 bucket is the least cost-effective and most complex solution. Fargate requires managing containers and infrastructure, which increases operational overhead. Polling the S3 bucket every minute means that you'll be paying for compute time even when there are no new images to process. This is significantly more expensive than using S3 event notifications and Lambda, which are pay-per-use and event-driven. Fargate is also an overkill for this simple image processing task.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "A digital publishing platform stores large volumes of media assets (such as images and documents) in an Amazon S3 bucket. These assets are accessed frequently during business hours by internal editors and content delivery tools. The company has strict encryption policies and currently uses AWS KMS to handle server-side encryption. The cloud operations team notices that AWS KMS request costs are increasing significantly due to the high frequency of object uploads and accesses. The team is now looking for a way to maintain the same encryption method but reduce the cost of KMS usage, especially for frequent access patterns. Which solution meets the company's encryption and cost optimization goals?",
    "options": [
      {
        "id": 0,
        "text": "Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object",
        "correct": true
      },
      {
        "id": 1,
        "text": "Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each object’s metadata for decryption",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nenabling S3 Bucket Keys for SSE-KMS, is the correct solution. S3 Bucket Keys reduce the cost of SSE-KMS by decreasing the number of requests to AWS KMS. When you enable S3 Bucket Keys, S3 generates a bucket-level key that is used to encrypt objects in the bucket. This reduces the number of KMS requests because S3 reuses the bucket-level key for a period of time, rather than requesting a new KMS data key for each object. This directly addresses the problem of high KMS request costs due to frequent object uploads and accesses while maintaining the desired SSE-KMS encryption.\n\n**Why option 1 is incorrect:**\nswitching to SSE-S3, eliminates KMS-related charges but does not maintain the same level of encryption control. With SSE-S3, Amazon S3 manages the encryption keys, which might not meet the company's strict encryption policies that require KMS usage. The question specifically states the need to maintain the same encryption method, which is SSE-KMS.\n\n**Why option 2 is incorrect:**\nconfiguring a VPC endpoint for S3 and restricting access, does not directly reduce KMS charges. VPC endpoints control network access to S3 but do not affect the encryption process or the number of KMS requests made. KMS charges are incurred during the encryption and decryption of objects, regardless of how the S3 bucket is accessed.\n\n**Why option 3 is incorrect:**\nusing client-side encryption, involves encrypting the data before uploading it to S3. While this provides encryption, it requires managing encryption keys on the client-side, which adds complexity and overhead. More importantly, it doesn't leverage SSE-KMS as required by the problem statement and introduces a completely different encryption approach.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages. Which of the following options represent the root cause for this issue?",
    "options": [
      {
        "id": 0,
        "text": "The database user credentials (username and password) configured for the application are incorrect",
        "correct": false
      },
      {
        "id": 1,
        "text": "The database user credentials (username and password) configured for the application do not have the required privilege for the given database",
        "correct": false
      },
      {
        "id": 2,
        "text": "The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers",
        "correct": true
      },
      {
        "id": 3,
        "text": "The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because the application servers are unable to connect to the RDS instance. This strongly suggests a security group issue. The security group associated with the RDS instance needs an inbound rule that allows traffic from the application servers' security group (or specific IP addresses if applicable) on the MySQL port (typically 3306). Without this rule, the RDS instance will reject connection attempts from the application servers, leading to the 'connection timed out' error. The bastion host working is irrelevant to the application servers' ability to connect.\n\n**Why option 0 is incorrect:**\nis incorrect because incorrect database credentials would typically result in an 'access denied' or 'invalid login' error, not a 'connection timed out' error. A timeout indicates a network connectivity issue, preventing the application from even reaching the database to authenticate.\n\n**Why option 1 is incorrect:**\nis incorrect because insufficient database privileges would also result in an 'access denied' error after a successful connection, not a 'connection timed out' error. The application is failing to establish a connection in the first place, not failing to execute a query due to lack of permissions.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A biomedical research firm operates a file exchange system for external research partners to upload and download experimental data. Currently, the system runs on two Amazon EC2 Linux instances, each configured with Elastic IP addresses to allow access from trusted IPs. File transfers use the SFTP protocol, and Linux user accounts are manually provisioned to enforce file-level access control. Data is stored on a shared file system mounted to both EC2 instances. The firm wants to modernize the solution to a fully managed, serverless model with high IOPS, fine-grained user permission control, and strict IP-based access restrictions. They also want to reduce operational overhead without sacrificing performance or security. Which solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the best solution. Amazon EFS provides a scalable, serverless file system with high IOPS capabilities. Creating an AWS Transfer Family SFTP endpoint within a VPC ensures private access. Using Elastic IP addresses allows for static, known IPs for external partners to connect to. Security groups restrict access to only the trusted IPs. POSIX identity mappings and IAM policies provide fine-grained user-level access control. EFS is fully managed, reducing operational overhead.\n\n**Why option 0 is incorrect:**\nis incorrect because while it uses S3 (serverless), it doesn't directly address the high IOPS requirement as effectively as EFS or FSx. S3 is object storage, not a file system, and requires application changes. Also, while IAM role-based access mappings provide user-level permissions, they don't directly map to existing POSIX identities, which might be a requirement for the firm. S3 is not designed for high IOPS file system operations.\n\n**Why option 1 is incorrect:**\nis incorrect because using a public endpoint for AWS Transfer Family exposes the system to the internet, which contradicts the requirement for strict IP-based access restrictions. While IAM policies can manage user access, the public endpoint creates a security risk. FSx for Lustre is a good choice for high IOPS, but the public endpoint makes this option less secure.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "An organization operates a legacy reporting tool hosted on an Amazon EC2 instance located within a public subnet of a VPC. This tool aggregates scanned PDF reports from field devices and temporarily stores them on an attached Amazon EBS volume. At the end of each day, the tool transfers the accumulated files to an Amazon S3 bucket for archival. A solutions architect identifies that the files are being uploaded over the internet using S3's public endpoint. To improve security and avoid exposing data traffic to the public internet, the architect needs to reconfigure the setup so that uploads to Amazon S3 occur privately without using the public S3 endpoint. Which solution will fulfill these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instance’s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint",
        "correct": true
      },
      {
        "id": 2,
        "text": "Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because a gateway VPC endpoint for S3 allows the EC2 instance to access S3 without traversing the public internet. The gateway endpoint creates a route within the VPC that directs S3-bound traffic to Amazon's network instead of the internet. Updating the subnet's route table to use the endpoint ensures that all S3 traffic from the EC2 instance is routed through the endpoint. IAM policies are necessary to control which resources (like the EC2 instance) can use the endpoint and which S3 buckets they can access. This solution provides a secure and private connection to S3.\n\n**Why option 0 is incorrect:**\nis incorrect because while S3 access points provide granular access control to S3 buckets, they do not inherently provide private connectivity. The traffic would still traverse the public internet unless combined with other solutions like VPC endpoints.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because a NAT gateway allows instances in a private subnet to initiate outbound traffic to the internet, but it doesn't provide private connectivity to S3. The traffic would still go through the internet, albeit through a NAT gateway. This doesn't meet the requirement of avoiding the public internet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A fintech company currently operates a real-time search and analytics platform on-premises. This platform ingests streaming data from multiple data-producing systems and provides immediate search capabilities and interactive visualizations for end users. As part of its cloud migration strategy, the company wants to rearchitect the solution using AWS-native services. Which of the following represents the most efficient solution?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshift’s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most efficient solution because it leverages purpose-built AWS services for each part of the problem. Amazon Kinesis Data Streams is designed for ingesting and processing real-time streaming data. Amazon OpenSearch Service (successor to Elasticsearch Service) is a managed service specifically designed for indexing and searching large volumes of data in near real-time. Amazon QuickSight is a business intelligence service that allows for creating interactive dashboards and visualizations. This combination provides a scalable, cost-effective, and fully managed solution that directly addresses the requirements.\n\n**Why option 0 is incorrect:**\nis incorrect because while AWS Glue can handle streaming ETL, Redshift is primarily a data warehouse and not optimized for real-time search. Redshift's full-text search capabilities are limited compared to a dedicated search engine like OpenSearch. Using Redshift for this purpose would likely be less performant and more expensive than using OpenSearch.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves managing EC2 instances, which adds operational overhead and complexity. While Athena can query data in S3, it is not designed for real-time search. Athena is better suited for ad-hoc queries on large datasets. Also, using EC2 instances for ingestion and processing is less efficient and scalable than using a managed streaming service like Kinesis Data Streams. Amazon Managed Grafana is a good visualization tool, but the overall architecture is not as efficient as option 3.\n\n**Why option 2 is incorrect:**\nis incorrect because DynamoDB, while fast for key-value lookups, is not designed for full-text search. Implementing full-text search on DynamoDB would require significant custom development and would likely be less performant and more complex than using OpenSearch. Amazon CloudWatch is primarily for monitoring and logging, not for creating interactive dashboards for end users. ECS with Fargate is a good container orchestration solution, but it's not the best choice for this specific use case compared to Kinesis for streaming data ingestion.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 15,
    "text": "A global e-commerce platform currently operates its order processing system in a single on-premises data center located in Europe. As the company grows its customer base across Asia and North America, it plans to deploy the application across multiple AWS Regions to improve availability and reduce latency. The company requires that updates to the central order database be completed in under one second with global consistency. The application layer will be deployed separately in each Region, but the order management data must remain centrally managed and globally synchronized. Which solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nmigrating the order data to Amazon DynamoDB and creating a global table, is the correct solution. DynamoDB global tables provide multi-region, multi-active database capabilities, replicating data across multiple AWS Regions. This ensures that any updates made in one Region are automatically propagated to all other Regions, providing global consistency. DynamoDB offers low-latency reads and writes, making it suitable for the sub-second update requirement. Each application instance in each region can connect to the local DynamoDB replica for low-latency access, improving the user experience.\n\n**Why option 0 is incorrect:**\nusing Amazon Aurora with MySQL engine and read-only replicas, is incorrect because while Aurora provides read replicas, it doesn't inherently guarantee global consistency with sub-second replication. Promoting a read replica to a writer in case of failure is a manual process and will not meet the sub-second requirement. Also, all writes would be routed to the central region, potentially increasing latency for users in other regions.\n\n**Why option 1 is incorrect:**\nusing Amazon Neptune, is incorrect because Neptune is a graph database service, best suited for relationship-heavy data. While it can be deployed in multiple regions, replicating changes using custom-built Lambda functions and SQS is complex, prone to errors, and unlikely to meet the sub-second update latency requirement. Furthermore, order data is typically relational, not graph-based, making Neptune an unsuitable choice.\n\n**Why option 2 is incorrect:**\nusing Amazon RDS for MySQL with a cross-Region read replica, is incorrect because while RDS provides read replicas, it doesn't inherently guarantee global consistency with sub-second replication. Promoting a read replica to a writer in case of failure is a manual process and will not meet the sub-second requirement. Also, all writes would be routed to the primary region, potentially increasing latency for users in other regions.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose",
        "correct": false
      },
      {
        "id": 1,
        "text": "Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications",
        "correct": false
      },
      {
        "id": 2,
        "text": "Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications",
        "correct": false
      },
      {
        "id": 3,
        "text": "Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it uses Kinesis Data Streams for ingesting the high volume of streaming transactions in near real-time. Kinesis Data Streams allows for custom processing of each record. AWS Lambda is then used to remove sensitive data from each transaction. This approach provides the necessary transformation before storing the cleansed data. Finally, the cleansed transactions are stored in Amazon DynamoDB, a NoSQL database, which provides low-latency retrieval. The internal applications can consume the raw transactions off the Kinesis Data Stream. This architecture satisfies all the requirements of the question: scalability, near-real-time processing, data cleansing, low-latency retrieval, and data sharing.\n\n**Why option 0 is incorrect:**\nis incorrect because Kinesis Data Firehose is designed for loading streaming data into data lakes, data warehouses, and analytics services. It's not ideal for near-real-time processing and transformation of individual records like removing sensitive data using Lambda on each record. While Firehose *can* invoke Lambda, it's designed for batching and buffering, which introduces latency. Also, sharing the *raw* transactions with internal applications is a security risk since the sensitive data hasn't been removed.\n\n**Why option 1 is incorrect:**\nis incorrect because batch processing with S3 and S3 events introduces significant latency, which violates the near-real-time requirement. While DynamoDB Streams can share data, the initial processing step is too slow. Storing the raw transactions in S3 before cleansing also poses a security risk.\n\n**Why option 2 is incorrect:**\nis incorrect because modifying data directly within DynamoDB using a rule is not a standard or recommended practice for data cleansing. DynamoDB rules are not designed for complex data transformations. While DynamoDB Streams can share data, the cleansing process is inefficient and potentially error-prone. Persisting raw transactions directly into DynamoDB before cleansing poses a security risk.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website. Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it utilizes Route 53's active-passive failover routing policy. This policy allows Route 53 to monitor the health of the ALB endpoint using a health check. If the health check determines that the ALB is unhealthy (meaning the primary website is unavailable), Route 53 automatically redirects traffic to the specified backup endpoint, which in this case is the S3 bucket hosting the static error page. This approach minimizes changes as it leverages Route 53's built-in failover capabilities and doesn't require complex configuration or code changes. The active-passive setup ensures that the S3 bucket is only used when the primary ALB endpoint is unhealthy.\n\n**Why option 0 is incorrect:**\nis incorrect because Latency-based routing directs traffic to the endpoint with the lowest latency for the user. While it can improve performance, it doesn't provide a failover mechanism in case of unavailability. It would not guarantee that users are directed to the error page if the primary website is down. Latency routing is primarily for performance optimization, not disaster recovery.\n\n**Why option 1 is incorrect:**\nis incorrect because Weighted routing distributes traffic across multiple resources based on assigned weights. While you could assign a very low weight to the S3 bucket, it doesn't guarantee that traffic will be routed to the error page *only* when the primary website is unavailable. Some traffic might still be routed to the error page even when the primary website is healthy, which is not the desired behavior. Weighted routing is more suitable for A/B testing or gradual deployments, not for failover scenarios.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A retail enterprise is expanding its hybrid IT infrastructure and plans to securely connect its on-premises corporate network to its AWS environment. The company wants to ensure that all data exchanged between on-premises systems and AWS is encrypted at both the network and session layers. Additionally, the solution must incorporate granular security controls that restrict unnecessary or unauthorized access between the cloud and on-premises environments. A solutions architect must recommend a scalable and secure approach that supports these goals. Which solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems",
        "correct": true
      },
      {
        "id": 1,
        "text": "Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses AWS Site-to-Site VPN, which provides encrypted connectivity between the on-premises network and the AWS VPC. Site-to-Site VPN uses IPsec, which encrypts data in transit at the network layer. Route tables allow for managing traffic flow between the networks. Security groups and Network ACLs (NACLs) provide granular security controls by allowing or denying traffic based on source/destination IP addresses, ports, and protocols. This addresses the requirements of encryption, traffic management, and granular access control.\n\n**Why option 1 is incorrect:**\nis incorrect because while Direct Connect provides a dedicated network connection, it doesn't inherently provide encryption. While you can encrypt data at the application layer, the question specifically requires encryption at both the network and session layers. Direct Connect by itself does not fulfill the network layer encryption requirement. Furthermore, while route tables, security groups, and NACLs provide granular access control, the lack of built-in network layer encryption makes this option less suitable than Site-to-Site VPN.\n\n**Why option 2 is incorrect:**\nis incorrect because AWS Client VPN is designed for individual users connecting to the VPC, not for connecting an entire on-premises network. It doesn't address the requirement of connecting the corporate network as a whole. While security groups and IAM policies provide access control, this solution is not scalable or appropriate for a site-to-site connection.\n\n**Why option 3 is incorrect:**\nis incorrect because using a bastion host provides SSH access, which is suitable for administrative tasks but doesn't encrypt all traffic between the on-premises network and AWS. It also doesn't scale well for general application traffic and doesn't provide the required network-level encryption. It only encrypts the SSH session to the bastion host, not all traffic between the on-premises network and the AWS environment.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume. As a solutions architect, which of the following configurations would you suggest to the engineering team?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type, is the correct answer. Provisioned IOPS SSD (io1) volumes are designed for I/O-intensive workloads, such as large relational databases like PostgreSQL. They allow you to specify a consistent IOPS rate, ensuring predictable and high performance. This directly addresses the requirement for consistent performance and high IOPS. While io2 volumes offer even better performance and durability, io1 is generally sufficient and more cost-effective unless extremely high IOPS are specifically needed.\n\n**Why option 1 is incorrect:**\nAmazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type, is incorrect. Throughput Optimized HDD (st1) volumes are designed for frequently accessed, throughput-intensive workloads with large datasets and sequential I/O patterns, such as big data, data warehouses, and log processing. They are not suitable for transactional database workloads that require high IOPS and low latency.\n\n**Why option 2 is incorrect:**\nAmazon EC2 with Amazon EBS volume of cold HDD (sc1) type, is incorrect. Cold HDD (sc1) volumes are the lowest cost EBS volume type and are designed for infrequently accessed data. They are not suitable for database workloads that require high IOPS and low latency.\n\n**Why option 3 is incorrect:**\nAmazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type, is incorrect. General Purpose SSD (gp2) volumes provide a balance of price and performance and are suitable for a wide variety of workloads. However, for a massive PostgreSQL database requiring consistent performance and high IOPS, Provisioned IOPS SSD (io1) volumes are a better choice because they allow you to provision a specific IOPS rate, ensuring predictable performance. While gp3 is an improvement over gp2 and can be suitable for some database workloads, io1 still provides the most consistent high IOPS performance.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 20,
    "text": "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs. As a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon S3 Intelligent-Tiering, is the correct answer. S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on changing access patterns. This eliminates the need for manual lifecycle policy management and minimizes IT involvement. It automatically optimizes storage costs by placing data in the most cost-effective tier without impacting performance. This aligns perfectly with the requirement to reduce storage costs with minimal IT intervention.\n\n**Why option 0 is incorrect:**\nconfiguring Amazon EFS, is incorrect. Amazon EFS is a network file system designed for shared access by multiple EC2 instances. It is not a direct replacement for S3 for object storage and is not designed for cost optimization of existing S3 data. It also requires significant IT involvement to set up and manage, contradicting the question's requirements.\n\n**Why option 1 is incorrect:**\nusing Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect. While S3 One Zone-IA is cheaper than S3 Standard or Standard-IA, it has lower availability and durability as data is stored in a single Availability Zone. This is not suitable for critical data, especially in a regulated industry like medical devices. Furthermore, it requires manual configuration or lifecycle policies to move data to this tier, increasing IT involvement and not addressing the root cause of the problem (sub-optimal lifecycle policies).\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 21,
    "text": "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 instances with Instance Store as the storage option",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Instance Store, is the most performant option for temporary storage. Instance store provides block-level storage that is physically attached to the host computer. This proximity results in very low latency and high IOPS. Since the data is temporary (before uploading to S3), the ephemeral nature of instance store is not a concern. While EBS io1 can provide high IOPS, it is more expensive than instance store. Instance store is also cost-effective because you are not paying extra for the storage itself; it's included with the EC2 instance cost. It's ideal for temporary data that doesn't need to persist if the instance stops or terminates.\n\n**Why option 0 is incorrect:**\nusing Amazon EBS General Purpose SSD (gp2), is a good general-purpose storage option, but it doesn't offer the same level of performance (IOPS) as Instance Store, especially for demanding workloads. While gp2 is cost-effective, it's not the *most* performant option. Also, EBS volumes are persistent, which is not necessary for temporary storage.\n\n**Why option 1 is incorrect:**\nusing Amazon EBS Throughput Optimized HDD (st1), is designed for large, sequential workloads with high throughput, such as log processing or data warehousing. It is not optimized for high IOPS, which is the primary requirement in this scenario. It's also not suitable for temporary storage that requires low latency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 22,
    "text": "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS transit gateway to interconnect the VPCs",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an internet gateway to interconnect the VPCs",
        "correct": false
      },
      {
        "id": 2,
        "text": "Establish VPC peering connections between all VPCs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a VPC endpoint to interconnect the VPCs",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Transit Gateway, is the correct answer. Transit Gateway simplifies network topology by providing a central hub to connect multiple VPCs and on-premises networks. It eliminates the need for complex peering relationships and supports transitive routing, allowing any connected VPC to communicate with any other connected VPC. This is both more resource-efficient (less management overhead compared to full mesh peering) and more scalable (easier to add more VPCs in the future) than other options. Transit Gateway also offers features like route tables and security policies for centralized network management.\n\n**Why option 1 is incorrect:**\nusing an internet gateway, is incorrect. Internet Gateways allow VPCs to connect to the internet. While this would allow each VPC to communicate with the internet, it doesn't directly facilitate private communication between the VPCs themselves. Furthermore, routing traffic through the public internet introduces security risks and latency, and it's not resource-efficient for internal VPC communication. This option also doesn't address the requirement of interconnecting the VPCs privately and securely.\n\n**Why option 2 is incorrect:**\nestablishing VPC peering connections between all VPCs, is incorrect. While this would eventually allow all VPCs to communicate with each other, it would require a full mesh configuration (n*(n-1)/2 peering connections). In this case, with 5 VPCs, that would be 5*(5-1)/2 = 10 peering connections. This becomes increasingly complex and difficult to manage as the number of VPCs grows, making it less scalable and less resource-efficient than using a Transit Gateway. The management overhead of maintaining numerous peering connections is significant.\n\n**Why option 3 is incorrect:**\nusing a VPC endpoint, is incorrect. VPC endpoints allow VPCs to privately connect to supported AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. VPC endpoints are designed for accessing AWS services, not for interconnecting VPCs with each other. They do not provide a general-purpose solution for enabling communication between multiple VPCs.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A logistics company runs a two-step job handling process on AWS. The first step quickly receives job submissions from clients, while the second step requires longer processing time to complete each job. Currently, both steps run on separate Amazon EC2 Auto Scaling groups. However, during high-demand hours, the job processing stage falls behind, and there is concern that jobs may be lost due to instance termination during scaling events. A solutions architect needs to design a more scalable and reliable architecture that preserves job data and accommodates fluctuating demand in both stages. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon SQS to decouple the job intake and processing stages. Two separate SQS queues allow for independent scaling of each stage based on its specific workload. SQS provides durable storage for jobs, ensuring that no jobs are lost even if EC2 instances are terminated during scaling events. The EC2 instances in each Auto Scaling group poll their respective SQS queue for work. Scaling the Auto Scaling groups based on the number of messages in each queue allows the system to dynamically adjust capacity to meet demand in each stage independently.\n\n**Why option 1 is incorrect:**\nis incorrect because using a single SQS queue for both job intake and processing does not allow for independent scaling of each stage. The processing stage might be overwhelmed while the intake stage is idle, or vice versa. This defeats the purpose of decoupling and independent scaling to address the bottleneck in the processing stage.\n\n**Why option 2 is incorrect:**\nis incorrect because it focuses on maintaining a fixed minimum capacity, which might lead to over-provisioning and increased costs during low-demand periods. Monitoring CPU utilization alone is not sufficient to address the job backlog issue. It doesn't guarantee that jobs will not be lost during scaling events. It also doesn't address the decoupling requirement.\n\n**Why option 3 is incorrect:**\nis incorrect because while it correctly uses two SQS queues for decoupling, scaling based on notifications from the queue is not the standard or most effective way to scale. Scaling based on the number of messages in the queue provides a more granular and responsive scaling mechanism. Notifications might be delayed or missed, leading to inaccurate scaling decisions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 24,
    "text": "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data. As a Solutions Architect, which set of services will you recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it recommends Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage. \n\n*   **Amazon EC2 Instance Store:** Instance store provides block-level storage that is physically attached to the host computer. This offers the highest possible I/O performance, which is ideal for processing large video files. However, data stored on instance store is ephemeral and will be lost if the instance is stopped or terminated. This is acceptable for temporary processing workloads.\n*   **Amazon S3:** Amazon S3 is a highly durable, scalable, and available object storage service. It's suitable for storing media content that needs to be readily accessible. S3 offers various storage classes with different cost and retrieval characteristics, allowing the company to optimize costs based on access frequency.\n*   **Amazon S3 Glacier:** Amazon S3 Glacier is a low-cost archival storage service designed for infrequently accessed data. It's ideal for storing legacy data that needs to be retained for compliance or other reasons but is not frequently accessed. Glacier provides different retrieval options with varying retrieval times and costs.\n\n**Why option 0 is incorrect:**\nis incorrect because AWS Storage Gateway is primarily used to integrate on-premises storage with AWS. While it can be used for durable data access, it doesn't directly address the need for 450 TB of durable storage in the cloud. Also, while Glacier Deep Archive is a good option for archival, using Storage Gateway as the primary durable storage solution is not efficient or cost-effective in this scenario, as the company is moving its IT infrastructure to AWS.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon EBS, while providing persistent block storage for EC2 instances, does not offer the same level of I/O performance as instance store. EBS is also more expensive than instance store for temporary, high-performance workloads. While EBS can be used for durable storage, S3 is a better choice for storing large amounts of media content due to its scalability, durability, and cost-effectiveness.\n\n**Why option 3 is incorrect:**\nis incorrect because while S3 standard storage can provide good performance, it is not optimized for the *maximum possible* I/O performance required for processing large videos. Instance store is the best option for this. S3 Intelligent-Tiering is a good option for cost optimization based on access patterns, but it doesn't directly address the need for a large amount of durable storage. While Glacier Deep Archive is suitable for archival, S3 Standard is not the best choice for the initial high-performance processing requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "A global enterprise is modernizing its hybrid IT infrastructure to improve both availability and network performance. The company operates a TCP-based application hosted on Amazon EC2 instances that are deployed across multiple AWS Regions, while a secondary UDP-based component of the application is hosted in its on-premises data centers. These application components must be accessed by customers around the world with minimal latency and consistent uptime. Which combination of options should a solutions architect implement for the given use case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups",
        "correct": true
      },
      {
        "id": 4,
        "text": "Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 3 together provide the best solution. Option 0, configuring AWS Global Accelerator with the TCP-based EC2 workloads behind load balancers, addresses the global accessibility and minimal latency requirements for the TCP component. Global Accelerator uses the AWS global network to route traffic to the closest healthy endpoint, improving performance and availability. Option 3, creating Network Load Balancers (NLBs) in each Region to handle the EC2-based TCP traffic and configuring NLBs in each Region to route to the on-premises UDP endpoints via IP-based target groups, provides regional failover and load balancing for both the TCP and UDP components. NLBs are suitable for UDP traffic and can route to IP addresses, allowing connectivity to the on-premises endpoints. The combination ensures high availability and low latency for both application components.\n\n**Why option 3 is correct:**\nOptions 0 and 3 together provide the best solution. Option 0, configuring AWS Global Accelerator with the TCP-based EC2 workloads behind load balancers, addresses the global accessibility and minimal latency requirements for the TCP component. Global Accelerator uses the AWS global network to route traffic to the closest healthy endpoint, improving performance and availability. Option 3, creating Network Load Balancers (NLBs) in each Region to handle the EC2-based TCP traffic and configuring NLBs in each Region to route to the on-premises UDP endpoints via IP-based target groups, provides regional failover and load balancing for both the TCP and UDP components. NLBs are suitable for UDP traffic and can route to IP addresses, allowing connectivity to the on-premises endpoints. The combination ensures high availability and low latency for both application components.\n\n**Why option 1 is incorrect:**\nis incorrect because it suggests using Application Load Balancers (ALBs) for UDP traffic to on-premises endpoints. ALBs only support HTTP/HTTPS traffic and are not suitable for UDP. While NLBs are used for the TCP traffic, the use of ALBs for UDP is a fundamental flaw.\n\n**Why option 2 is incorrect:**\nis incorrect because routing all traffic through a single Region introduces a single point of failure and increases latency for users located far from that Region. This contradicts the requirements for minimal latency and consistent uptime. Furthermore, relying solely on static routes and BGP failover is less dynamic and resilient than using Global Accelerator or regional load balancers.\n\n**Why option 4 is incorrect:**\nis incorrect because while PrivateLink provides secure private connectivity, it doesn't inherently address the global accessibility and low latency requirements. PrivateLink is more focused on secure access to services within a VPC, not necessarily optimizing global traffic routing. Also, deploying PrivateLink to *each* on-premises UDP workload is an unusual and likely unnecessary architecture.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A SaaS analytics company is deploying a microservices-based application on Amazon ECS using the Fargate launch type. The application requires access to a shared, POSIX-compliant file system that is available across multiple Availability Zones for redundancy and availability. To meet compliance requirements, the system must support regional backups and cross-Region data recovery with a recovery point objective (RPO) of no more than 8 hours. A backup strategy will be implemented using AWS Backup to automate replication across Regions. As the lead cloud architect, you are evaluating file storage solutions that align with these requirements. Which option best meets the application’s availability, durability, and RPO objectives?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon EFS with the Standard storage class and configuring AWS Backup to create cross-Region backups on a scheduled basis, is the best solution. EFS provides a shared, POSIX-compliant file system accessible across multiple Availability Zones, ensuring high availability and durability. The Standard storage class is suitable for frequently accessed data. AWS Backup can be configured to create cross-Region backups, meeting the RPO requirement of 8 hours. EFS is designed for use with compute services like ECS and provides the necessary file system semantics for the microservices application.\n\n**Why option 0 is incorrect:**\nusing Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and relying on its native high availability and AWS Backup integration to replicate the file system to another Region automatically, is not the best option. While FSx for NetApp ONTAP provides a robust file system with high availability and AWS Backup integration, it is more complex and expensive than EFS for this use case. The question doesn't explicitly require the advanced features of ONTAP, making EFS a more cost-effective and simpler solution. Also, the automatic replication mentioned might not guarantee the 8-hour RPO without proper configuration and monitoring.\n\n**Why option 1 is incorrect:**\ndeploying Amazon FSx for Lustre and configuring a backup plan using AWS Backup for cross-Region replication of the file system metadata, is incorrect. FSx for Lustre is designed for high-performance computing workloads and is not suitable for general-purpose file sharing required by the microservices application. Backing up only the metadata would not be sufficient for a complete disaster recovery scenario, as the actual data would be lost. Lustre is also not inherently multi-AZ, requiring additional configuration for high availability.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection",
        "correct": false
      },
      {
        "id": 3,
        "text": "Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 4,
        "text": "Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 4 are correct. \n\n*   **Option 0: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket.** CloudFront caches the video files at edge locations closer to the users, significantly reducing latency for downloads. While CloudFront primarily focuses on caching content for faster delivery (downloads), it also improves upload performance by leveraging optimized network paths to the origin (S3 bucket). When a user uploads a file, it's first uploaded to the nearest CloudFront edge location, which then efficiently transfers the file to the S3 bucket. This avoids traversing the public internet directly from the user's location to the S3 bucket, resulting in faster uploads.\n*   **Option 4: Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket.** S3 Transfer Acceleration utilizes CloudFront's globally distributed edge locations to optimize the upload process. When a user uploads a file, it's first routed to the nearest CloudFront edge location, which then uses optimized network paths to transfer the file to the S3 bucket. This avoids traversing the public internet directly from the user's location to the S3 bucket, resulting in faster uploads. S3TA is specifically designed to improve upload performance for geographically dispersed users.\n\n**Why option 4 is correct:**\nOptions 0 and 4 are correct. \n\n*   **Option 0: Use Amazon CloudFront distribution with origin as the Amazon S3 bucket.** CloudFront caches the video files at edge locations closer to the users, significantly reducing latency for downloads. While CloudFront primarily focuses on caching content for faster delivery (downloads), it also improves upload performance by leveraging optimized network paths to the origin (S3 bucket). When a user uploads a file, it's first uploaded to the nearest CloudFront edge location, which then efficiently transfers the file to the S3 bucket. This avoids traversing the public internet directly from the user's location to the S3 bucket, resulting in faster uploads.\n*   **Option 4: Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket.** S3 Transfer Acceleration utilizes CloudFront's globally distributed edge locations to optimize the upload process. When a user uploads a file, it's first routed to the nearest CloudFront edge location, which then uses optimized network paths to transfer the file to the S3 bucket. This avoids traversing the public internet directly from the user's location to the S3 bucket, resulting in faster uploads. S3TA is specifically designed to improve upload performance for geographically dispersed users.\n\n**Why option 1 is incorrect:**\nis incorrect because creating new S3 buckets in every region introduces data management complexity and potential data consistency issues. It requires replicating data across multiple buckets and managing access control for each bucket. This approach is not serverless in the sense that it introduces operational overhead for managing multiple buckets and data replication. While it might improve latency for users in those regions, it's not the most efficient or scalable solution.\n\n**Why option 2 is incorrect:**\nis incorrect because moving data to Amazon EFS and connecting via inter-region VPC peering is complex and expensive. EFS is designed for shared file storage within a VPC, not for global content delivery. Inter-region VPC peering can introduce latency and complexity in network management. This solution is not serverless and introduces significant operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because spinning up EC2 instances in each region and transferring data daily is a complex, expensive, and non-serverless solution. It requires managing EC2 instances, EBS volumes, and data transfer jobs. This approach introduces significant operational overhead and is not a scalable or cost-effective solution for improving latency.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A financial auditing firm uses Amazon S3 to store sensitive client records that are subject to write-once-read-many (WORM) regulations to prevent alteration or deletion of records for a specific retention period. The firm wants to enforce immutable storage, such that even administrators cannot overwrite or delete the records during the lock duration. They also need audit-friendly enforcement to prevent accidental or malicious deletion. Which configuration of S3 Object Lock will ensure that the retention policy is strictly enforced, and no user (including root or administrators) can override or delete protected objects during the lock period?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because S3 Object Lock in Compliance Mode is designed to enforce retention policies strictly. Once an object is locked in Compliance Mode, the retention settings cannot be changed, and the object cannot be deleted by any user, including the root user or administrators, during the specified retention period. This mode provides the strongest level of protection against object deletion and modification, ensuring compliance with WORM requirements. It is specifically designed for regulatory compliance where immutability is paramount.\n\n**Why option 1 is incorrect:**\nis incorrect because while Glacier Deep Archive provides long-term, low-cost storage, it does not inherently enforce immutability in the same way as S3 Object Lock. S3 Lifecycle Policies can transition data to Glacier, but the data can still be deleted from S3 before the transition or from Glacier with appropriate permissions. It doesn't provide the WORM guarantee required by the question.\n\n**Why option 2 is incorrect:**\nis incorrect because while S3 Versioning protects against accidental deletion by allowing you to recover previous versions of an object, it does not prevent intentional deletion by users with sufficient permissions. A bucket policy denying `s3:DeleteObject` can be bypassed by users with the `s3:BypassGovernanceRetention` permission (if Governance Mode was enabled) or by the root user. Furthermore, versioning alone doesn't guarantee immutability for a specific duration as required by WORM regulations.\n\n**Why option 3 is incorrect:**\nis incorrect because S3 Object Lock in Governance Mode allows users with elevated permissions (specifically, the `s3:BypassGovernanceRetention` permission) to override or remove retention settings. This contradicts the requirement that no user, including administrators, should be able to override or delete protected objects during the lock period. Governance Mode is intended for situations where some level of flexibility is needed, but it does not provide the strict immutability required for WORM compliance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Data at rest inside the volume is NOT encrypted",
        "correct": false
      },
      {
        "id": 1,
        "text": "Any snapshot created from the volume is NOT encrypted",
        "correct": false
      },
      {
        "id": 2,
        "text": "Data moving between the volume and the instance is encrypted",
        "correct": true
      },
      {
        "id": 3,
        "text": "Any snapshot created from the volume is encrypted",
        "correct": true
      },
      {
        "id": 4,
        "text": "Data moving between the volume and the instance is NOT encrypted",
        "correct": false
      },
      {
        "id": 5,
        "text": "Data at rest inside the volume is encrypted",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      3,
      5
    ],
    "explanation": "**Why option 2 is correct:**\nThese are correct because they accurately describe the capabilities of encrypted EBS volumes. Option 5 states that data at rest inside the volume is encrypted, which is a fundamental feature of EBS encryption. Option 2 states that data moving between the volume and the instance is encrypted. This is also correct; when you attach an encrypted EBS volume to an EC2 instance, the data transfer between the instance and the volume is encrypted. Option 3 states that any snapshot created from the volume is encrypted. This is also correct; snapshots of encrypted EBS volumes are automatically encrypted, ensuring data protection even in backups.\n\n**Why option 3 is correct:**\nThese are correct because they accurately describe the capabilities of encrypted EBS volumes. Option 5 states that data at rest inside the volume is encrypted, which is a fundamental feature of EBS encryption. Option 2 states that data moving between the volume and the instance is encrypted. This is also correct; when you attach an encrypted EBS volume to an EC2 instance, the data transfer between the instance and the volume is encrypted. Option 3 states that any snapshot created from the volume is encrypted. This is also correct; snapshots of encrypted EBS volumes are automatically encrypted, ensuring data protection even in backups.\n\n**Why option 5 is correct:**\nThese are correct because they accurately describe the capabilities of encrypted EBS volumes. Option 5 states that data at rest inside the volume is encrypted, which is a fundamental feature of EBS encryption. Option 2 states that data moving between the volume and the instance is encrypted. This is also correct; when you attach an encrypted EBS volume to an EC2 instance, the data transfer between the instance and the volume is encrypted. Option 3 states that any snapshot created from the volume is encrypted. This is also correct; snapshots of encrypted EBS volumes are automatically encrypted, ensuring data protection even in backups.\n\n**Why option 0 is incorrect:**\nis incorrect because encrypted EBS volumes *do* encrypt data at rest. This is a core feature of EBS encryption.\n\n**Why option 1 is incorrect:**\nis incorrect because snapshots created from encrypted EBS volumes are *also* encrypted. The encryption is inherited by the snapshot.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it deploys the visualization tool in the same AWS region as the data warehouse and accesses it over a Direct Connect connection. Data transfer within the same AWS region is generally free. Furthermore, using Direct Connect for traffic *leaving* AWS to the on-premises location is cheaper than using the public internet. The 60MB query results are transferred to the visualization tool within the AWS region (no egress cost). The 600KB webpage data is transferred from the visualization tool to the user's location via Direct Connect, which is more cost-effective than transferring it over the public internet.\n\n**Why option 0 is incorrect:**\nis incorrect because while the visualization tool is in the same region as the data warehouse (eliminating egress costs for the 60MB query results), accessing the visualization tool over the internet incurs data transfer egress costs for the 600KB webpages. Data transfer over the public internet is generally more expensive than using Direct Connect.\n\n**Why option 2 is incorrect:**\nis incorrect because querying the data warehouse directly from on-premises over Direct Connect means the 60MB query results are transferred from AWS to the on-premises location via Direct Connect. While Direct Connect is cheaper than the internet, it still incurs egress costs. The correct answer avoids this egress cost for the large query results by keeping the visualization tool in the same AWS region.\n\n**Why option 3 is incorrect:**\nis incorrect because querying the data warehouse over the internet incurs significant data transfer egress costs for the 60MB query results. This is the most expensive option because it uses the public internet for the large data transfers.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "An enterprise is developing an internal compliance framework for its cloud infrastructure hosted on AWS. The enterprise uses AWS Organizations to group accounts under various organizational units (OUs) based on departmental function. As part of its governance controls, the security team mandates that all Amazon EC2 instances must be tagged to indicate the level of data classification — either 'confidential' or 'public'. Additionally, the organization must ensure that IAM users cannot launch EC2 instances without assigning a classification tag, nor should they be able to remove the tag from running instances. A solutions architect must design a solution to meet these compliance controls while minimizing operational overhead. Which combination of steps will meet these requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected",
        "correct": false
      },
      {
        "id": 2,
        "text": "Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 2 are the correct answers.\n\n*   **Option 0:** Using Service Control Policies (SCPs) is the most effective way to enforce mandatory tagging at the AWS Organizations level. The first SCP denies the `ec2:RunInstances` API call if the `dataClassification` tag is not present in the request. This prevents users from launching instances without the required tag. The second SCP denies the `ec2:DeleteTags` API call for EC2 resources, preventing users from removing the tag from running instances. SCPs operate at the organization level, ensuring consistent enforcement across all accounts within the OU.\n*   **Option 2:** Tag Policies in AWS Organizations allow you to define a standard for tagging AWS resources. By defining a tag policy that enforces the `dataClassification` key and restricts the values to 'confidential' and 'public', you ensure that all EC2 instances within the OU must have this tag with one of the allowed values. This simplifies tag management and ensures consistency across the organization. Tag policies work in conjunction with SCPs to provide a comprehensive tagging solution.\n\n**Why option 2 is correct:**\nOptions 0 and 2 are the correct answers.\n\n*   **Option 0:** Using Service Control Policies (SCPs) is the most effective way to enforce mandatory tagging at the AWS Organizations level. The first SCP denies the `ec2:RunInstances` API call if the `dataClassification` tag is not present in the request. This prevents users from launching instances without the required tag. The second SCP denies the `ec2:DeleteTags` API call for EC2 resources, preventing users from removing the tag from running instances. SCPs operate at the organization level, ensuring consistent enforcement across all accounts within the OU.\n*   **Option 2:** Tag Policies in AWS Organizations allow you to define a standard for tagging AWS resources. By defining a tag policy that enforces the `dataClassification` key and restricts the values to 'confidential' and 'public', you ensure that all EC2 instances within the OU must have this tag with one of the allowed values. This simplifies tag management and ensures consistency across the organization. Tag policies work in conjunction with SCPs to provide a comprehensive tagging solution.\n\n**Why option 1 is incorrect:**\nis incorrect because while AWS Config can detect non-compliant instances and trigger remediation, it's a reactive approach. It doesn't prevent the creation of non-compliant resources in the first place. Also, using Systems Manager Automation runbooks adds operational overhead for remediation.\n\n**Why option 3 is incorrect:**\nis incorrect because IAM permission boundaries are applied to IAM roles and users within a single account. While they can enforce tagging requirements, they don't provide centralized enforcement across multiple accounts in an AWS Organization. Applying permission boundaries to all IAM roles would be a complex and error-prone task, increasing operational overhead. Also, permission boundaries are not as effective as SCPs in preventing actions at the AWS Organizations level.\n\n**Why option 4 is incorrect:**\nis incorrect because using a Lambda function to periodically check for missing tags is a reactive approach and adds operational overhead. It doesn't prevent the creation of non-compliant resources and requires ongoing maintenance of the Lambda function.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A financial data processing company runs a workload on Amazon EC2 instances that fetch and process real-time transaction batches from an Amazon SQS queue. The application needs to scale based on unpredictable message volume, which fluctuates significantly throughout the day. The system must process messages with minimal delay and no downtime, even during peak spikes. The company is seeking a solution that balances cost-efficiency with availability and elasticity. Which EC2 purchasing strategy best meets these requirements in the most cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume",
        "correct": true
      },
      {
        "id": 2,
        "text": "Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the correct answer. Using Reserved Instances for the baseline workload provides a cost-effective solution for the predictable portion of the traffic. Reserved Instances offer a significant discount compared to On-Demand Instances. Configuring EC2 Auto Scaling with Spot Instances to handle spikes in message volume allows the application to scale elastically and cost-effectively during peak periods. Spot Instances offer substantial discounts compared to On-Demand Instances, and Auto Scaling ensures that instances are launched and terminated as needed, matching the message volume fluctuations. This approach balances cost-efficiency with availability and elasticity by leveraging the benefits of both Reserved Instances and Spot Instances.\n\n**Why option 0 is incorrect:**\nis incorrect because while it uses Reserved Instances for the baseline, it uses On-Demand Instances for spikes. On-Demand Instances are more expensive than Spot Instances. Using Spot Instances for handling unpredictable spikes is more cost-effective.\n\n**Why option 2 is incorrect:**\nis incorrect because purchasing Reserved Instances to match peak capacity is not cost-effective. The company would be paying for unused capacity during periods of low traffic. This approach does not optimize for cost-efficiency.\n\n**Why option 3 is incorrect:**\nis incorrect because relying solely on Spot Instances can lead to interruptions if the Spot price exceeds the bid price. While Spot Instances are cost-effective, they are not suitable for the entire workload, especially the baseline, as they can be terminated, leading to potential downtime and impacting the requirement of no downtime.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB for distributed in-memory cache based session management",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elasticache for distributed in-memory cache based session management",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon RDS for distributed in-memory cache based session management",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Application Load Balancer sticky sessions",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon ElastiCache, is the correct solution. ElastiCache is a fully managed, in-memory data store and caching service by AWS. It supports both Redis and Memcached engines, which are commonly used for session management. By storing session data in ElastiCache, the application can avoid losing session data when an EC2 instance becomes unhealthy. ElastiCache provides a distributed, in-memory cache that is highly available and scalable, which perfectly addresses the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon DynamoDB, is incorrect. While DynamoDB is a distributed database, it's not an in-memory cache. It's a NoSQL database that stores data on disk. Using DynamoDB for session management would introduce latency and wouldn't provide the performance benefits of an in-memory cache. It is also not optimized for the rapid read/write operations typically associated with session management.\n\n**Why option 2 is incorrect:**\nusing Amazon RDS, is incorrect. RDS is a relational database service, not an in-memory cache. Similar to DynamoDB, using RDS for session management would introduce latency and wouldn't provide the performance benefits of an in-memory cache. RDS is designed for persistent data storage, not the temporary, volatile nature of session data.\n\n**Why option 3 is incorrect:**\nusing Application Load Balancer sticky sessions, is incorrect. While sticky sessions can help maintain session affinity to a specific EC2 instance, they don't solve the underlying problem of session data loss when an instance becomes unhealthy. If the instance the user is 'stuck' to fails, the session data is still lost, and the user will be forced to re-authenticate. Sticky sessions are also not a distributed in-memory cache.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss. What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon Route 53 geoproximity routing policy to route traffic",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nsetting up AWS Global Accelerator and adding endpoints in different geographic locations, is the correct solution. AWS Global Accelerator provides static IP addresses that serve as a single entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on user location, improving performance and reducing latency. In case of a regional failure, Global Accelerator automatically reroutes traffic to the next available healthy endpoint in another region, providing automatic failover without manual intervention. This directly addresses the requirements of low latency and automatic regional failover.\n\n**Why option 1 is incorrect:**\nsetting up AWS Direct Connect as the backbone for each AWS Region, is incorrect. AWS Direct Connect establishes a dedicated network connection from on-premises infrastructure to AWS. While it can improve network performance and reduce latency compared to public internet, it doesn't provide automatic failover across AWS Regions in the event of a regional ELB failure. It's primarily for connecting on-premises infrastructure, not for regional failover within AWS.\n\n**Why option 2 is incorrect:**\ncreating Amazon S3 buckets in different AWS Regions and configuring Amazon CloudFront to pick the nearest edge location, is incorrect. While CloudFront can improve content delivery performance by caching content at edge locations closer to users, it doesn't address the application's ELB failure scenario. CloudFront primarily serves static content, and the question implies a dynamic application behind an ELB. S3 replication across regions is useful for data redundancy, but doesn't solve the application-level failover issue.\n\n**Why option 3 is incorrect:**\nsetting up an Amazon Route 53 geoproximity routing policy, is incorrect. Route 53 geoproximity routing routes traffic based on the geographic proximity of users to AWS resources. While it can help reduce latency, it doesn't provide automatic failover in the event of a regional failure as effectively as Global Accelerator. Route 53 requires DNS propagation time for failover, which can be slower than Global Accelerator's near-instantaneous failover. Also, geoproximity routing is based on distance, not necessarily health, so it might route traffic to a failing region if it's geographically closer.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 35,
    "text": "A tech company runs a web application that includes multiple internal services deployed across Amazon EC2 instances within a VPC. These services require communication with a third-party SaaS provider's API for analytics and billing, which is also hosted on the AWS infrastructure. The company is concerned about minimizing public internet exposure while maintaining secure and reliable connectivity. The solution must ensure private access without allowing unsolicited incoming traffic from the SaaS provider. Which solution will best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS PrivateLink to create a private endpoint within the application’s VPC that connects securely to the SaaS provider’s VPC",
        "correct": true
      },
      {
        "id": 1,
        "text": "Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up VPC peering between the application VPC and the SaaS provider’s VPC to allow direct communication",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS CloudFront to route requests from the application’s internal services to the SaaS provider through edge locations",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS PrivateLink, is the best solution. AWS PrivateLink allows you to privately connect your VPC to supported AWS services, services hosted by other AWS accounts (the SaaS provider in this case), and AWS Marketplace partner services. It provides private connectivity without exposing your traffic to the public internet. The SaaS provider creates a Network Load Balancer (NLB) in their VPC and exposes it as a PrivateLink service. The tech company then creates a VPC endpoint in their VPC, which connects to the SaaS provider's NLB. This establishes a private connection, ensuring that all traffic between the internal services and the SaaS provider remains within the AWS network and is not exposed to the public internet. Importantly, PrivateLink only allows the tech company to initiate connections to the SaaS provider, preventing unsolicited incoming traffic from the SaaS provider.\n\n**Why option 1 is incorrect:**\nestablishing a VPN connection using AWS Site-to-Site VPN, creates a secure tunnel, but it's more complex to set up and manage than PrivateLink. While it provides secure communication, it doesn't inherently prevent unsolicited incoming traffic from the SaaS provider. You would need to configure firewall rules to achieve that. Also, VPN connections involve more overhead and management compared to PrivateLink.\n\n**Why option 2 is incorrect:**\nsetting up VPC peering, allows direct communication between the two VPCs, but it doesn't inherently prevent unsolicited incoming traffic from the SaaS provider. VPC peering establishes a network connection between two VPCs, allowing traffic to flow between them. However, it requires careful management of security groups and network ACLs to control traffic flow and prevent unwanted access. It also doesn't provide the same level of isolation and control as PrivateLink. Furthermore, VPC peering can become complex to manage as the number of peered VPCs grows.\n\n**Why option 3 is incorrect:**\nusing AWS CloudFront, is designed for content delivery and caching, not for establishing private connections between internal services and APIs. CloudFront is primarily used to distribute content to users globally, caching it at edge locations to improve performance. It's not suitable for establishing a secure and private connection between internal services and a third-party API. It would also expose the traffic to the public internet, which contradicts the requirement of minimizing public internet exposure.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases. As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
    "options": [
      {
        "id": 0,
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6",
        "correct": false
      },
      {
        "id": 1,
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone",
        "correct": false
      },
      {
        "id": 2,
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6",
        "correct": false
      },
      {
        "id": 3,
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it configures the Auto Scaling group with a minimum capacity of 4 instances distributed across two different Availability Zones. This ensures high availability, as the application can continue to function even if one Availability Zone experiences an outage. Setting the minimum capacity to 4 ensures that there are always enough instances to handle the baseline workload. The maximum capacity of 6 allows the Auto Scaling group to scale up to handle traffic spikes. Distributing instances across multiple Availability Zones is a key best practice for high availability in AWS.\n\n**Why option 0 is incorrect:**\nis incorrect because while it distributes instances across two Availability Zones, the minimum capacity of 2 is insufficient. If one instance fails, the application's capacity is halved, potentially impacting performance and availability. The question states the workload can be managed on 2 instances, but this doesn't provide redundancy for failures.\n\n**Why option 1 is incorrect:**\nis incorrect because it places all instances in a single Availability Zone. This creates a single point of failure. If that Availability Zone experiences an outage, the entire application will become unavailable. This violates the reliability requirement.\n\n**Why option 2 is incorrect:**\nis incorrect because it distributes instances across two different AWS Regions. While this provides even greater fault tolerance than Availability Zones, it is overkill for the stated requirements and introduces unnecessary complexity and latency. Regions are geographically isolated and are used for disaster recovery scenarios, which are not explicitly mentioned in the question. Also, the minimum capacity of 4 across two regions might be more expensive than necessary for the baseline workload.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A mobile-based e-learning platform is migrating its backend storage layer to Amazon DynamoDB to support a rapidly increasing number of student users and learning transactions. The platform must ensure seamless availability and minimal disruption for a global user base. The DynamoDB design must provide low-latency performance, high availability, and automatic fault tolerance across geographies with the lowest possible operational overhead and cost. Which solution will fulfill these needs in the most cost-efficient manner?",
    "options": [
      {
        "id": 0,
        "text": "Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing DynamoDB global tables with provisioned capacity mode and auto scaling, is the most suitable solution. DynamoDB global tables provide automatic multi-Region replication, ensuring high availability and fault tolerance across geographies. This eliminates the need for custom replication mechanisms, reducing operational overhead. Provisioned capacity mode with auto scaling allows for cost optimization by dynamically adjusting capacity based on demand. This approach directly addresses the requirements for seamless availability, low latency (due to local reads in each region), automatic fault tolerance, and cost efficiency.\n\n**Why option 0 is incorrect:**\nis incorrect because implementing a custom cross-Region replication mechanism using DynamoDB Streams and Lambda functions is complex and adds significant operational overhead. It requires managing the stream processing, error handling, and conflict resolution, which is less efficient and more prone to errors compared to using DynamoDB global tables. While on-demand capacity mode simplifies capacity management, it can be more expensive than provisioned capacity mode with auto scaling for predictable workloads.\n\n**Why option 1 is incorrect:**\nis incorrect because DAX is a read-through/write-through cache for DynamoDB. While it can improve read performance in a single region, it does not provide multi-Region replication or high availability across geographies. Using scheduled Lambda functions to replicate data is not a reliable or efficient solution for maintaining data consistency across regions, especially with a rapidly increasing number of transactions. It also introduces significant operational overhead and potential data loss during replication failures.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries. As a Solutions Architect, which of the following solutions would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages Amazon Transcribe to convert audio files to text, which is the first necessary step. Then, it utilizes Amazon Athena to perform SQL-based analysis on the transcribed text. Athena allows querying data directly from S3 using standard SQL, making it ideal for ad-hoc analysis of the sentiment data. This solution directly addresses the requirements of converting audio to text, performing sentiment analysis (implicitly through SQL queries on the transcribed text, perhaps using keyword analysis or integrating with a sentiment analysis library), and enabling ad-hoc SQL queries.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Streams can ingest audio, using Alexa for transcription is not a standard or scalable solution for this use case. Alexa is designed for interactive voice experiences, not batch audio processing. Furthermore, Kinesis Data Analytics is more suited for real-time streaming data processing, which is not the primary requirement here. While Quicksight is a good visualization tool, the initial data processing steps are not optimal.\n\n**Why option 2 is incorrect:**\nis incorrect because while it correctly uses Amazon Transcribe for audio-to-text conversion, it incorrectly uses Amazon Quicksight to perform SQL-based analysis. Amazon Quicksight is a business intelligence service for data visualization and dashboarding, not a SQL query engine. Athena is the correct service for SQL-based analysis on data stored in S3.\n\n**Why option 3 is incorrect:**\nis incorrect because while Kinesis Data Streams can ingest audio, relying on generic 'machine learning (ML) algorithms' for both transcription and sentiment analysis is vague and less efficient than using a dedicated service like Amazon Transcribe. Transcribe is specifically designed for audio transcription and provides accurate and cost-effective results. Also, this option doesn't mention how to perform SQL based analysis.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance. What could explain this anomaly?",
    "options": [
      {
        "id": 0,
        "text": "Both the Auto Scaling group and Application Load Balancer are using ALB based health check",
        "correct": false
      },
      {
        "id": 1,
        "text": "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check",
        "correct": false
      },
      {
        "id": 2,
        "text": "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check",
        "correct": false
      },
      {
        "id": 3,
        "text": "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because if the Auto Scaling group is using EC2-based health checks, it only monitors the EC2 instance's status (system and instance status checks). If the ALB detects an issue with the application running on the instance (e.g., the application is not responding to HTTP requests), it will mark the instance as unhealthy and remove it from the target group. However, the EC2 instance itself might still be running and passing the EC2 status checks. Therefore, the Auto Scaling group will not be aware of the application-level issue and will not replace the instance. The ALB is using ALB-based health checks which are more granular and can detect application-level issues.\n\n**Why option 0 is incorrect:**\nis incorrect because if both the ASG and ALB used ALB-based health checks, the ASG would be aware of the ALB's health status. When the ALB marks an instance as unhealthy, the ASG would also detect this and trigger a replacement. The problem described in the question would not occur.\n\n**Why option 1 is incorrect:**\nis incorrect because if both the ASG and ALB used EC2-based health checks, the ASG would only monitor the EC2 instance's status. While the ALB might remove the instance from its target group due to application-level issues, the ASG would only replace the instance if the EC2 instance itself failed the EC2 status checks. However, the question states that the ALB is removing the instance, implying a more granular health check than just EC2 status. This option doesn't fully explain why the ASG isn't replacing the instance when the ALB detects an issue.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 40,
    "text": "A fintech company recently conducted a security audit and discovered that some IAM roles and Amazon S3 buckets might be unintentionally shared with external accounts or publicly accessible. The security team wants to identify these overly permissive resources and ensure that only intended principals (within their AWS Organization or specific AWS accounts) have access. They need a solution that can analyze IAM policies and resource policies to detect unintended access paths to AWS resources such as S3 buckets, IAM roles, KMS keys, and SNS topics. Which solution should the team use to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Identity and Access Management (IAM) Access Analyzer is specifically designed to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization. It analyzes access paths to resources such as S3 buckets, IAM roles, KMS keys, and SNS topics, and it can detect unintended access from external entities. It provides findings that highlight resources accessible from outside the defined zone of trust (AWS account or organization). This directly addresses the problem described in the question.\n\n**Why option 0 is incorrect:**\nAmazon Inspector is primarily a vulnerability management service that assesses the security posture of EC2 instances and container images. While it can identify security vulnerabilities, it does not specifically analyze IAM policies and resource policies to detect unintended access paths to AWS resources in the same way as IAM Access Analyzer. Inspector focuses on software vulnerabilities and deviations from security best practices within the operating system and application layers, not on policy analysis for external access.\n\n**Why option 2 is incorrect:**\nIAM Access Advisor provides information about the last time IAM users and roles used AWS services and resources. It helps you refine your IAM policies by identifying unused permissions. While it can provide insights into IAM usage, it doesn't directly analyze resource-based policies (like S3 bucket policies) to determine which principals outside the organization have access. It's more focused on right-sizing IAM permissions based on actual usage, not identifying external access paths.\n\n**Why option 3 is incorrect:**\nAWS Config tracks configuration changes and assesses compliance against defined rules. While it can be used to detect changes to IAM policies and resource policies, it doesn't inherently provide the same level of analysis as IAM Access Analyzer to identify unintended access paths to resources. Inferring resource-sharing behavior through compliance rules would be a more complex and less direct approach compared to using IAM Access Analyzer, which is specifically built for this purpose.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "An enterprise runs a critical Oracle database workload in its on-premises environment. The company now plans to replicate both existing records and continuous transactional changes to a managed Oracle environment in AWS. The target database will run on Amazon RDS for Oracle. Data transfer volume is expected to fluctuate throughout the day, and the team wants the solution to provision compute resources automatically based on actual workload requirements. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because AWS DMS Serverless is designed specifically for data replication tasks and automatically scales compute resources based on workload demands. It handles both the initial data load (historical data) and ongoing changes (transactional changes) seamlessly. DMS Serverless eliminates the need for manual provisioning and scaling of replication instances, directly addressing the requirement for automatic resource management. It also simplifies the setup and management of the replication process.\n\n**Why option 0 is incorrect:**\nis incorrect because AWS Glue is primarily an ETL (Extract, Transform, Load) service, not a real-time data replication tool. While Glue can extract data from Oracle, it's not designed for continuous replication of transactional changes. Triggering Glue on demand based on change detection would be complex and inefficient, leading to significant latency and potential data inconsistencies. Glue is better suited for batch processing and data transformation, not real-time replication.\n\n**Why option 1 is incorrect:**\nis incorrect because while deploying DMS on EC2 and using Auto Scaling is *possible*, it's not the best practice or most efficient solution. It requires manual configuration of custom scripts to monitor CPU usage and resize the instance, adding complexity and overhead. DMS Serverless provides a fully managed solution that handles scaling automatically, making it a simpler and more cost-effective option. This option also introduces operational overhead for managing the EC2 instance and Auto Scaling group.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective. Which of the following features will help you in disaster recovery of the database? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use cross-Region Read Replicas",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use the database cloning feature of the Amazon RDS Database cluster",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they directly address disaster recovery requirements.\n\n*   **Option 0: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions:** RDS automated backups provide point-in-time recovery. Backups stored in a different region provide a copy of the data in a geographically separate location. A Multi-AZ deployment enhances availability within a region, but the cross-region backup is crucial for DR.\n*   **Option 3: Use cross-Region Read Replicas:** Cross-Region Read Replicas create a copy of the data in a different AWS Region. In a disaster scenario, the Read Replica can be promoted to a standalone database instance, allowing operations to continue in the secondary region. This provides a low Recovery Point Objective (RPO) and Recovery Time Objective (RTO) compared to restoring from backups alone.\n\n**Why option 3 is correct:**\nThese are correct because they directly address disaster recovery requirements.\n\n*   **Option 0: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions:** RDS automated backups provide point-in-time recovery. Backups stored in a different region provide a copy of the data in a geographically separate location. A Multi-AZ deployment enhances availability within a region, but the cross-region backup is crucial for DR.\n*   **Option 3: Use cross-Region Read Replicas:** Cross-Region Read Replicas create a copy of the data in a different AWS Region. In a disaster scenario, the Read Replica can be promoted to a standalone database instance, allowing operations to continue in the secondary region. This provides a low Recovery Point Objective (RPO) and Recovery Time Objective (RTO) compared to restoring from backups alone.\n\n**Why option 1 is incorrect:**\nOption 1: Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region: While a Multi-AZ deployment improves availability within a single region, it does not protect against a regional disaster. Backups within the same region are also vulnerable to the same regional event. Therefore, this option does not address the disaster recovery requirement.\n\n**Why option 2 is incorrect:**\nOption 2: Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage: Provisioned IOPS storage improves database performance, but it does not contribute to disaster recovery. It only affects the speed of I/O operations within the database instance and does not provide any data replication or failover capabilities.\n\n**Why option 4 is incorrect:**\nOption 4: Use the database cloning feature of the Amazon RDS Database cluster: Database cloning creates a copy of the database within the same region. While useful for development and testing, it does not provide protection against regional disasters. The clone resides in the same region as the source database and is therefore susceptible to the same failure events.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A DevOps team is tasked with enabling secure and temporary SSH access to Amazon EC2 instances for developers during deployments. The team wants to avoid distributing long-term SSH key pairs and instead prefers ephemeral access that can be audited and revoked immediately after the session ends. The team wants direct access via the AWS Management Console. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instance’s public IP address",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because EC2 Instance Connect is designed for this exact purpose. It allows injecting a temporary public key into the EC2 instance's metadata upon connection request. This key is valid only for a short period (60 seconds by default) and is automatically removed after the session ends. This provides ephemeral access, eliminates the need for managing long-term SSH keys, and allows for auditing through CloudTrail logs. The connection is established using the instance's public IP address, which aligns with the requirement of direct access via the AWS Management Console.\n\n**Why option 1 is incorrect:**\nis incorrect because disabling the Systems Manager Agent would prevent EC2 Instance Connect from functioning correctly. EC2 Instance Connect relies on the Systems Manager Agent to inject the temporary public key. Also, connecting via private IP using an internal proxy endpoint contradicts the requirement for direct access via the AWS Management Console, as it introduces an intermediary.\n\n**Why option 2 is incorrect:**\nis incorrect because EC2 Instance Connect Endpoint is not required when the EC2 instances already have public IP addresses. The endpoint is used when instances are in private subnets and need to be accessed without exposing them directly to the internet. The question states the instances have public IPs, making the endpoint unnecessary.\n\n**Why option 3 is incorrect:**\nis incorrect because EC2 Instance Connect injects a *temporary* public key, not a static one. Using a static key defeats the purpose of ephemeral access and introduces the same security risks as traditional SSH key management. Also, connecting via the instance's private IP address directly from the internet is generally not possible without a VPN or other network configuration, and it contradicts the secure architecture principles.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF). As a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Application Load Balancer geo match statement listing the countries that you want to block",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS WAF geo match statement listing the countries that you want to block",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nThese are correct because they leverage AWS WAF's capabilities to address the requirements. Option 4, 'Use AWS WAF geo match statement listing the countries that you want to block,' allows blocking traffic based on the originating country. Option 2, 'Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through,' allows creating an exception for the remote development team by whitelisting their IP addresses. By combining these two, the company can block traffic from specific countries while allowing the development team's access.\n\n**Why option 4 is correct:**\nThese are correct because they leverage AWS WAF's capabilities to address the requirements. Option 4, 'Use AWS WAF geo match statement listing the countries that you want to block,' allows blocking traffic based on the originating country. Option 2, 'Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through,' allows creating an exception for the remote development team by whitelisting their IP addresses. By combining these two, the company can block traffic from specific countries while allowing the development team's access.\n\n**Why option 0 is incorrect:**\n'Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through,' is incorrect because ALBs do not directly support IP set statements for whitelisting. While ALBs can forward traffic based on source IP, they don't have the advanced rule-based filtering capabilities of WAF. Using ALB alone would not allow for geo-based blocking.\n\n**Why option 1 is incorrect:**\n'Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances,' is incorrect because network ACLs operate at the subnet level and are stateless. While they can block traffic based on IP addresses, they cannot directly block based on geographic location. Also, managing ACLs for multiple EC2 instances can become complex and error-prone. Furthermore, ACLs are a blunt instrument and less flexible than WAF rules.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A digital design company has migrated its project archiving platform to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group that spans multiple Availability Zones. Designers upload and retrieve high-resolution image files from a shared file system, which is currently configured to use Amazon EFS Standard-IA. Metadata for these files is stored and indexed in an Amazon RDS for PostgreSQL database. The company's cloud engineering team has been asked to optimize storage costs for the image archive without compromising reliability. They are open to refactoring the application to use managed AWS services when necessary. Which solution offers the most cost-effective architecture?",
    "options": [
      {
        "id": 0,
        "text": "Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\ncreating an Amazon S3 bucket with Intelligent-Tiering enabled and updating the application to use the Amazon S3 API, is the most cost-effective solution. Amazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on changing access patterns. This eliminates the need for manual tiering and optimizes costs without impacting performance. S3 offers high durability and availability, ensuring reliability. Refactoring the application to use the S3 API is a one-time effort that yields long-term cost savings and improved scalability.\n\n**Why option 0 is incorrect:**\nreplacing EFS with FSx for NetApp ONTAP and using volume tiering, is more complex and expensive than using S3 Intelligent-Tiering. FSx for NetApp ONTAP is a fully managed service that provides a rich set of data management capabilities, but it's generally more suitable for workloads that require specific NetApp features or compatibility. The added complexity and cost of managing FSx for NetApp ONTAP and its volume tiering outweigh the benefits in this scenario, where simple archival is the primary requirement. Also, it requires application changes to use the ONTAP mount path, similar to the correct answer, but with a more complex and expensive underlying infrastructure.\n\n**Why option 1 is incorrect:**\nreplacing EFS with FSx for Lustre, is designed for high-performance computing workloads and is not cost-effective for archival purposes. FSx for Lustre is optimized for speed and low latency, making it unsuitable for infrequently accessed archive data. It is also more expensive than S3 Intelligent-Tiering. While it might reduce access latency, the primary goal is cost optimization, and FSx for Lustre is not a cost-effective solution for archival storage.\n\n**Why option 2 is incorrect:**\nusing AWS Backup to export EFS files daily to S3 and retaining the EFS file system for occasional access, is less efficient and potentially more expensive than using S3 Intelligent-Tiering directly. While AWS Backup provides data protection, it introduces unnecessary complexity and cost for archival purposes. Retaining the EFS file system in Standard-IA for occasional access adds to the overall storage cost. S3 Intelligent-Tiering automatically handles the tiering based on access patterns, eliminating the need for daily backups and a separate EFS file system.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
    "options": [
      {
        "id": 0,
        "text": "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Enhanced Fanout feature of Amazon Kinesis Data Streams",
        "correct": true
      },
      {
        "id": 2,
        "text": "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues",
        "correct": false
      },
      {
        "id": 3,
        "text": "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing the Enhanced Fanout feature of Amazon Kinesis Data Streams, is the correct solution. Enhanced Fanout allows consumers to subscribe to a Kinesis data stream and receive their own dedicated throughput. This eliminates the contention that occurs when multiple consumers share the same shard's read capacity. Each consumer gets its own dedicated connection and throughput, resulting in significantly improved performance and lower latency for data delivery. This directly addresses the performance lag issue described in the question.\n\n**Why option 0 is incorrect:**\nswapping out Amazon Kinesis Data Streams with Amazon SQS FIFO queues, is incorrect. While SQS FIFO queues guarantee message ordering, they are not designed for high-throughput, real-time data streaming like Kinesis Data Streams. SQS is better suited for decoupling applications and handling asynchronous tasks, not for the continuous ingestion and processing of streaming data from IoT devices. Also, SQS does not inherently support fan-out to multiple consumers in the same way as Kinesis.\n\n**Why option 2 is incorrect:**\nswapping out Amazon Kinesis Data Streams with Amazon SQS Standard queues, is incorrect. SQS Standard queues do not guarantee message ordering and are not designed for high-throughput, real-time data streaming like Kinesis Data Streams. Similar to FIFO queues, SQS is better suited for decoupling applications and handling asynchronous tasks, not for the continuous ingestion and processing of streaming data from IoT devices. Replacing Kinesis with SQS would fundamentally change the architecture and likely introduce significant performance bottlenecks and data loss.\n\n**Why option 3 is incorrect:**\nswapping out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose, is incorrect. Kinesis Data Firehose is designed for loading streaming data into data lakes, data warehouses, and analytics services. It's primarily used for data transformation and delivery to destinations like S3, Redshift, and Elasticsearch. While Firehose can handle high volumes of data, it's not intended for real-time consumption by multiple applications that require low latency. It's more of a data delivery mechanism than a data processing platform. Replacing Kinesis Data Streams with Firehose would not solve the performance lag issue for multiple consumers needing real-time access to the data.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A data analytics team at a global media firm is building a new analytics platform to process large volumes of both historical and real-time data. This data is stored in Amazon S3. The team wants to implement a serverless solution that allows them to query the data directly using SQL. Additionally, the solution must ensure that all data is encrypted at rest and automatically replicated to another AWS Region to support business continuity. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it directly addresses all requirements with minimal operational overhead. Creating an S3 bucket with SSE-KMS using multi-region keys ensures encryption at rest using KMS, which provides more control and security compared to SSE-S3. Enabling CRR automatically replicates the data to another region for business continuity. Athena allows querying the data directly in S3 using SQL in a serverless manner, eliminating the need for managing any infrastructure. Multi-region keys are essential for CRR with KMS encryption, as the replicated data needs to be decrypted in the destination region.\n\n**Why option 0 is incorrect:**\nis incorrect because while it uses CRR and Athena, it implies that the bucket already exists and then CRR is enabled. While this is possible, option 1 is better as it configures the bucket with encryption from the start. Also, the order of operations in option 1 is generally preferred for clarity and consistency.\n\n**Why option 2 is incorrect:**\nis incorrect because it uses SSE-S3 for encryption. While SSE-S3 is simpler to implement, it offers less control and security compared to SSE-KMS. In a scenario requiring encryption at rest, SSE-KMS is generally preferred, especially when combined with CRR, as it provides better key management and compliance capabilities. Furthermore, SSE-S3 does not support multi-region keys, which are necessary for CRR with KMS encryption.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Redshift Spectrum instead of Athena. While Redshift Spectrum can query data in S3, it is generally more complex to set up and manage than Athena, adding operational overhead. Athena is designed specifically for serverless querying of data in S3 and is a better fit for the 'least operational overhead' requirement. Also, using SSE-S3 is less secure than SSE-KMS.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages Amazon S3 Glacier vaults and vault lock policies. S3 Glacier vaults provide a container for storing archives. Vault lock policies are specifically designed to enforce compliance controls by allowing you to define immutable policies that govern access to the vault. Once a vault lock policy is locked, it cannot be changed, ensuring that the compliance controls are consistently enforced. This is crucial for meeting regulatory requirements in the healthcare industry.\n\n**Why option 0 is incorrect:**\nis incorrect because while S3 lifecycle policies can move data to S3 Glacier, they primarily manage the transition of data between storage classes. They do not provide the immutable, compliance-focused controls offered by Glacier vault lock policies. Lifecycle policies are about cost optimization and data management, not strict compliance enforcement.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Access Control Lists (ACLs) are a legacy access control mechanism and are not the recommended approach for managing access to S3 Glacier vaults. While ACLs can grant basic permissions, they lack the granularity and immutability required for robust compliance controls. Vault lock policies are the preferred and more effective method for enforcing compliance in S3 Glacier.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company hosts a Microsoft SQL Server database on Amazon EC2 instances with attached Amazon EBS volumes. The operations team takes daily snapshots of these EBS volumes as backups. However, a recent incident occurred in which an automated script designed to clean up expired snapshots accidentally deleted all available snapshots, leading to potential data loss. The company wants to improve the backup strategy to avoid permanent data loss while still ensuring that old snapshots are eventually removed to optimize cost. A solutions architect needs to implement a mechanism that prevents immediate and irreversible deletion of snapshots. Which solution will best meet these requirements with the least development effort?",
    "options": [
      {
        "id": 0,
        "text": "Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up the IAM policy of the user to deny EBS snapshot deletion",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because Recycle Bin provides a simple and effective way to protect against accidental deletion of EBS snapshots. By setting up a retention rule, deleted snapshots are retained in the Recycle Bin for a specified period (in this case, 7 days). During this period, the snapshots can be easily restored, preventing permanent data loss. After the retention period expires, the snapshots are permanently deleted. This solution requires minimal development effort as Recycle Bin is a built-in AWS feature that can be configured through the console or CLI.\n\n**Why option 1 is incorrect:**\nis incorrect because while denying EBS snapshot deletion through IAM policies would prevent accidental deletion by the specified user, it also prevents legitimate deletion when snapshots are no longer needed. This does not address the requirement of eventual removal for cost optimization. Additionally, it's not a comprehensive solution as other users with different IAM roles might still have the permission to delete snapshots.\n\n**Why option 2 is incorrect:**\nis incorrect because implementing a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive is a complex solution that requires significant development effort. While it provides long-term recovery, it's overkill for the stated requirement of preventing immediate and irreversible deletion. The question specifically asks for a solution with the least development effort. Also, Glacier Deep Archive is intended for very long-term archival, not for short-term protection against accidental deletion.\n\n**Why option 3 is incorrect:**\nis incorrect because while AWS Backup Vault Lock can enforce deletion protection, it's a more complex solution than using Recycle Bin. Vault Lock is typically used to enforce compliance requirements and prevent any deletion of backups, even by authorized users, for a specified period. This is more restrictive than the requirement of preventing *immediate* and irreversible deletion while still allowing for eventual removal for cost optimization. It also requires more configuration and understanding of AWS Backup service.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Site-to-Site VPN, is the correct answer. Site-to-Site VPN provides a relatively quick and easy way to establish a secure, encrypted connection between an on-premises network and AWS. It uses IPsec to encrypt the traffic in transit, fulfilling the encryption requirement. It's faster to set up than Direct Connect, which involves physical connections and longer lead times.\n\n**Why option 0 is incorrect:**\nusing AWS Secrets Manager, is incorrect. Secrets Manager is used for managing secrets (like passwords, API keys, etc.), not for establishing network connectivity. It doesn't provide a mechanism for creating a secure tunnel between on-premises and AWS.\n\n**Why option 1 is incorrect:**\nusing AWS DataSync, is incorrect. DataSync is a data transfer service used to move large amounts of data between on-premises storage and AWS storage services. While DataSync encrypts data in transit, it's not primarily designed for establishing general-purpose network connectivity. It's focused on data migration and synchronization, not creating a persistent, encrypted network link.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
    "options": [
      {
        "id": 0,
        "text": "Use long polling to handle message processing failures",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a dead-letter queue to handle message processing failures",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a temporary queue to handle message processing failures",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use short polling to handle message processing failures",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing a dead-letter queue (DLQ), is the correct solution. A DLQ is a special SQS queue that is used to store messages that cannot be processed successfully after a certain number of attempts. This prevents messages from being retried indefinitely and potentially blocking the processing of other messages. By configuring a DLQ, the engineering team can analyze the failed messages, identify the root cause of the failures, and take corrective actions without impacting the overall system performance. The DLQ allows for asynchronous failure handling and provides a mechanism for auditing and debugging message processing issues.\n\n**Why option 0 is incorrect:**\nusing long polling, is incorrect. Long polling is a technique to reduce the number of empty responses from SQS when no messages are available. It does not directly address message processing failures. While long polling can improve efficiency, it doesn't provide a mechanism for handling messages that fail to be processed after multiple attempts.\n\n**Why option 2 is incorrect:**\nusing a temporary queue, is incorrect. Temporary queues are short-lived queues typically used for specific, transient tasks. They are not designed for handling persistent message failures. Using a temporary queue for failed messages would likely result in data loss and would not provide a reliable mechanism for analyzing and addressing the root cause of the failures.\n\n**Why option 3 is incorrect:**\nusing short polling, is incorrect. Short polling is the default polling method for SQS and involves querying the queue for messages immediately. Like long polling, it doesn't address the core problem of handling message processing failures. It only affects how frequently the queue is checked for new messages.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting aProvisionedThroughputExceededExceptionexception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate. Which of the following options will help with the exception while keeping costs at a minimum?",
    "options": [
      {
        "id": 0,
        "text": "Decrease the Stream retention duration",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use batch messages",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the number of shards",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Exponential Backoff",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing batch messages is the most cost-effective solution. Kinesis Data Streams charges based on the number of PUT operations. By batching multiple messages into a single PUT operation, the number of PUT requests is significantly reduced, thereby reducing the likelihood of exceeding the shard's write capacity (1 MB/s or 1000 records per second). This directly addresses the `ProvisionedThroughputExceededException` without incurring the cost of adding more shards.\n\n**Why option 0 is incorrect:**\nDecreasing the stream retention duration will not solve the `ProvisionedThroughputExceededException`. Retention duration affects how long data is stored in the stream, not the rate at which data can be ingested. It doesn't address the immediate problem of exceeding the shard's write capacity.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nUsing Exponential Backoff is a good practice for handling transient errors, but it doesn't solve the underlying problem of exceeding the shard's throughput. It only retries the failed requests, potentially exacerbating the issue if the throughput limit is consistently exceeded. While helpful for resilience, it's not the primary solution for this specific problem. Increasing the number of shards is more effective than exponential backoff alone.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB. Which of the following steps would you combine to address the given scenario? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target",
        "correct": false
      },
      {
        "id": 1,
        "text": "Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target",
        "correct": true
      },
      {
        "id": 3,
        "text": "Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 2 are the correct answers. Option 1 suggests handling read operations by connecting to the reader endpoint of the Aurora cluster. This offloads read traffic from the primary instance, reducing CPU and memory consumption on the primary instance which is responsible for writes. Aurora automatically distributes read load across Aurora Replicas when using the reader endpoint. Option 2 suggests creating an Aurora Replica in another Availability Zone. This improves availability because the replica can serve as a failover target in case the primary instance fails. Aurora promotes one of the replicas to be the primary in case of failure. This also helps with read scalability.\n\n**Why option 2 is correct:**\nOptions 1 and 2 are the correct answers. Option 1 suggests handling read operations by connecting to the reader endpoint of the Aurora cluster. This offloads read traffic from the primary instance, reducing CPU and memory consumption on the primary instance which is responsible for writes. Aurora automatically distributes read load across Aurora Replicas when using the reader endpoint. Option 2 suggests creating an Aurora Replica in another Availability Zone. This improves availability because the replica can serve as a failover target in case the primary instance fails. Aurora promotes one of the replicas to be the primary in case of failure. This also helps with read scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because creating a standby Aurora instance in another Availability Zone, while improving availability, doesn't directly address the performance issue of high CPU and memory consumption during write operations. Aurora Replicas are the preferred method for read scaling and failover.\n\n**Why option 3 is incorrect:**\nis incorrect because increasing the concurrency of the Lambda function will likely exacerbate the problem. More concurrent Lambda executions will lead to more write requests to the database, further increasing CPU and memory consumption and potentially leading to more missed writes. The database is already overloaded, so increasing the load is counterproductive.\n\n**Why option 4 is incorrect:**\nis incorrect because introducing EC2 instances behind an Application Load Balancer to write data to Aurora doesn't inherently solve the database's performance bottleneck. It adds complexity without addressing the root cause of the high CPU and memory consumption on the Aurora instance. It also introduces network latency between the EC2 instances and the Aurora DB.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 54,
    "text": "An enterprise SaaS provider is currently operating a legacy web application hosted on a single Amazon EC2 instance within a public subnet. The same instance also hosts a MySQL database. DNS records for the application are configured through Amazon Route 53. As part of a modernization initiative, the company wants to rearchitect this application for high availability and scalability. In addition, the company wants to improve read performance on the database layer to handle increasing user traffic. Which combination of solutions will meet these requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions",
        "correct": false
      },
      {
        "id": 4,
        "text": "Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because using an Auto Scaling group across multiple Availability Zones within a single region provides high availability and scalability for the web application. The Application Load Balancer distributes traffic evenly across the EC2 instances. Option 4 is correct because migrating the MySQL database to an Amazon Aurora MySQL cluster with read replicas improves read performance. Aurora's architecture is designed for high availability and scalability, and read replicas allow offloading read traffic from the primary database instance.\n\n**Why option 4 is correct:**\nThis is correct because using an Auto Scaling group across multiple Availability Zones within a single region provides high availability and scalability for the web application. The Application Load Balancer distributes traffic evenly across the EC2 instances. Option 4 is correct because migrating the MySQL database to an Amazon Aurora MySQL cluster with read replicas improves read performance. Aurora's architecture is designed for high availability and scalability, and read replicas allow offloading read traffic from the primary database instance.\n\n**Why option 0 is incorrect:**\nis incorrect because deploying EC2 instances across multiple AWS Regions introduces unnecessary complexity for this scenario. While multi-region deployments can provide disaster recovery capabilities, the question primarily focuses on high availability and scalability within a single region. The added latency and complexity of cross-region replication and data synchronization are not justified by the requirements. Also, the question does not mention disaster recovery as a requirement.\n\n**Why option 1 is incorrect:**\nis incorrect because using a failover routing policy in Route 53 provides disaster recovery, not high availability. High availability requires automatic failover within a region, which this option does not provide. It also doesn't address the scalability requirement for the web application.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
    "options": [
      {
        "id": 0,
        "text": "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Athena to run SQL based analytics against Amazon S3 data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks",
        "correct": false
      },
      {
        "id": 3,
        "text": "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon Athena, is the most suitable solution. Athena is a serverless query service that allows you to analyze data directly in Amazon S3 using standard SQL. It is cost-effective because you only pay for the queries you run. It is also easy to maintain because it is serverless, meaning there is no infrastructure to manage. Athena integrates seamlessly with S3 and supports various data formats commonly used in data lakes, such as Parquet, ORC, CSV, and JSON. This eliminates the need for data loading or transformation before running the sanity checks.\n\n**Why option 0 is incorrect:**\nloading data into Amazon RDS, is incorrect because RDS is a relational database service designed for transactional workloads, not analytical queries against large datasets. Loading incremental data hourly would be inefficient and costly, especially considering the volume of clickstream data. RDS is also not optimized for the schema-on-read approach that is typical for data lakes.\n\n**Why option 2 is incorrect:**\nloading data into Amazon Redshift, is incorrect because while Redshift is a data warehouse service suitable for analytical workloads, it involves more overhead and cost than Athena for this specific use case. Loading incremental data hourly into Redshift would require ETL processes and cluster management, increasing complexity and cost. Athena is more cost-effective for ad-hoc queries and data exploration on S3 data.\n\n**Why option 3 is incorrect:**\nloading data into an Amazon EMR-based Spark cluster, is incorrect because it introduces significant complexity and operational overhead. Setting up and managing an EMR cluster, even for hourly data sanity checks, is more complex and expensive than using Athena. While SparkSQL can perform SQL-based analytics, it requires cluster configuration, job submission, and monitoring, making it less easy to maintain than Athena's serverless approach.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "A media streaming company expects a major increase in user activity during the launch of a highly anticipated live event. The streaming platform is deployed on AWS and uses Amazon EC2 instances for the application layer and Amazon RDS for persistent storage. The operations team needs to proactively monitor system performance to ensure a smooth user experience during the event. Their monitoring setup must provide data visibility with intervals of no more than 2 minutes, and the team prefers a solution that is quick to implement and low-maintenance. Which solution should the team implement?",
    "options": [
      {
        "id": 0,
        "text": "Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics",
        "correct": false
      },
      {
        "id": 2,
        "text": "Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling detailed monitoring on EC2 instances provides metrics at a 1-minute interval, which satisfies the requirement of data visibility with intervals of no more than 2 minutes. CloudWatch metrics are readily available and require minimal setup compared to other options. It's a low-maintenance solution as AWS manages the underlying infrastructure for CloudWatch. Detailed monitoring provides CPU utilization, disk I/O, and network traffic metrics, which are crucial for monitoring EC2 performance during a high-traffic event. While RDS performance is important, the question focuses on the application layer (EC2 instances) during the event.\n\n**Why option 0 is incorrect:**\nis incorrect because installing and configuring the CloudWatch agent on each EC2 instance is more time-consuming than enabling detailed monitoring. Streaming to CloudWatch Logs and then using Athena for analysis adds complexity and latency, making it less suitable for real-time monitoring during a critical event. While flexible, it's not the quickest or lowest-maintenance option.\n\n**Why option 1 is incorrect:**\nis incorrect because EventBridge is primarily used for event-driven architectures and reacting to state changes. It doesn't provide the granular performance metrics required for proactive monitoring of CPU, memory, disk I/O, and network traffic. Using SNS and a monitoring dashboard might provide some visibility into EC2 state, but it's not designed for detailed performance monitoring at the required frequency. It's also not the most efficient way to monitor EC2 performance metrics.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for ongoing updates",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most performant solution. AWS DataSync is designed for high-speed, secure data transfer between on-premises storage and AWS storage services like S3. It uses a purpose-built protocol and parallel data transfer to maximize throughput. File Gateway, a configuration of AWS Storage Gateway, provides a local cache of the S3 data on-premises, allowing on-premises applications to access and update the data as if it were stored locally. File Gateway efficiently handles the synchronization of changes between the on-premises cache and S3, ensuring data consistency.\n\n**Why option 1 is incorrect:**\nis incorrect because while File Gateway can migrate data to S3, it's not optimized for the initial large-scale migration of hundreds of terabytes. DataSync is significantly faster for this purpose. Also, S3 Transfer Acceleration is primarily for accelerating uploads to S3 over the public internet, not for ongoing updates from on-premises applications within a potentially private network connection. File Gateway provides a more suitable and efficient mechanism for ongoing updates.\n\n**Why option 2 is incorrect:**\nis incorrect because while DataSync is excellent for the initial migration, it's not designed for ongoing, low-latency access and updates from on-premises applications. Using DataSync for ongoing updates would involve repeatedly transferring data, which is inefficient and costly. File Gateway provides a persistent, cached access point for on-premises applications.\n\n**Why option 3 is incorrect:**\nis incorrect because S3 Transfer Acceleration is primarily for accelerating uploads to S3 over the public internet, and is not the ideal tool for migrating hundreds of terabytes of data from an on-premises data center. DataSync is specifically designed for this type of migration. Furthermore, using DataSync for ongoing updates is less efficient than using File Gateway, which provides a local cache for low-latency access.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
    "options": [
      {
        "id": 0,
        "text": "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier",
        "correct": true
      },
      {
        "id": 1,
        "text": "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "correct": false
      },
      {
        "id": 2,
        "text": "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier",
        "correct": false
      },
      {
        "id": 3,
        "text": "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by using AWS Snowball Edge Storage Optimized devices for the initial data transfer. Snowball Edge is designed for transferring large amounts of data physically, which is often faster and more cost-effective than transferring over a network connection, especially for 5 petabytes. After the data is transferred to S3, a lifecycle policy is created to transition the data into Amazon S3 Glacier, which is the most cost-effective storage option for long-term archival. This combination of Snowball Edge for initial transfer and S3 Glacier for long-term storage provides the most cost-optimal solution.\n\n**Why option 1 is incorrect:**\nWhile AWS Direct Connect provides a dedicated network connection, it can be expensive to set up and maintain, especially for a one-time data migration. For a 5 PB initial migration, the cost of Direct Connect bandwidth and the time required to transfer the data over the network would likely be higher than using Snowball Edge. Direct Connect is more suitable for ongoing, high-bandwidth data transfer needs, not a single large migration. Also, the question emphasizes cost-optimization, and Direct Connect is generally not the most cost-effective option for this scenario.\n\n**Why option 2 is incorrect:**\nThis option is missing a crucial step: transitioning the data to S3 Glacier using a lifecycle policy. While using Snowball Edge to transfer the data to S3 is a good start, storing 5PB of infrequently accessed data in standard S3 would be significantly more expensive than storing it in S3 Glacier. The question specifically asks for the *most* cost-optimal solution, and without the lifecycle policy to Glacier, this option is not the best.\n\n**Why option 3 is incorrect:**\nSetting up a Site-to-Site VPN connection is generally slower and less reliable than using AWS Snowball Edge for transferring large amounts of data. The bandwidth limitations and potential network congestion associated with a VPN connection would make the data transfer process significantly longer and potentially more costly due to the extended transfer time. For a 5 PB migration, the time and cost associated with transferring data over a VPN connection would likely be much higher than using Snowball Edge. Also, VPN connections are not designed for such large initial data migrations.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A streaming service provider collects user experience feedback through embedded feedback forms in their mobile and web apps. Feedback submissions frequently spike to thousands per hour during content launches or service outages. Currently, the feedback is sent via email to the operations team for manual review. The company now wants to automate feedback collection and sentiment analysis so that insights can be generated quickly and stored for a full year for trend analysis. Which solution provides the most scalable and automated approach to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item",
        "correct": true
      },
      {
        "id": 1,
        "text": "Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months",
        "correct": false
      },
      {
        "id": 3,
        "text": "Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution is correct because it utilizes a highly scalable and decoupled architecture. Amazon API Gateway provides a managed and scalable front-end for receiving feedback data. Amazon SQS acts as a buffer, decoupling the API from the processing logic and handling potential spikes in traffic. AWS Lambda provides a serverless compute environment to process messages from the SQS queue, perform sentiment analysis using Amazon Comprehend, and store the results in DynamoDB. DynamoDB's TTL feature automatically removes items after 365 days, simplifying data retention management.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because using EC2 for receiving and processing feedback data introduces operational overhead related to managing the EC2 instance, including scaling, patching, and monitoring. While DynamoDB with TTL is a good choice for storage, EC2 is not the most scalable or cost-effective option for handling potentially large spikes in feedback submissions. It lacks the inherent scalability of API Gateway and SQS.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it introduces unnecessary complexity and cost. While EventBridge can capture events, Step Functions and Transcribe are not necessary for this use case. Transcribing text to audio for archival adds significant overhead and cost without providing any clear benefit. Amazon RDS, while a valid database, is not as well-suited for this type of unstructured data and high write throughput as DynamoDB. Also, lifecycle policies in RDS are more complex to manage than DynamoDB TTL.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because while Kinesis Data Streams can handle high-volume data, it's more suitable for real-time analytics and continuous data processing. For this use case, where sentiment analysis is performed on individual feedback submissions, SQS is a more appropriate choice as it provides message queuing and decoupling. Amazon Translate is unnecessary as the requirement is sentiment analysis, not language translation. OpenSearch Service is a good choice for search and analytics, but DynamoDB is more suitable for storing individual records with a simple TTL-based retention policy.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application. As a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Route 53 with Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Firewall Manager with CloudFront distribution",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Security Hub with Amazon CloudFront distribution",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the most effective solution because AWS WAF is a web application firewall that helps protect web applications from common web exploits and bots that may affect availability, compromise security, or consume excessive resources. It allows you to define customizable web security rules to control which traffic is allowed or blocked, providing protection against SQL injection and XSS attacks. Integrating AWS WAF with CloudFront ensures that malicious requests are filtered before they reach the origin (ALB and EC2 instances), thus protecting the application at the edge.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Route 53 is a DNS service. While it can be used in conjunction with CloudFront for routing traffic, it does not provide any protection against SQL injection or XSS attacks. It primarily handles domain name resolution and traffic management, not web application security.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your AWS accounts and applications. While it can be used to manage AWS WAF rules, it doesn't inherently provide the protection against SQL injection and XSS attacks. Firewall Manager needs to be configured with WAF rules to provide that protection. Using WAF directly is more straightforward and effective for this specific scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A financial analytics firm runs performance-intensive modeling software on Amazon EC2 instances backed by Amazon EBS volumes. The production data resides on EBS volumes attached to EC2 instances in the same AWS Region where the testing environment is hosted. To maintain data integrity, any changes made during testing must not affect production data. The development team needs to frequently create clones of this production data for simulations. The modeling software requires high and consistent I/O performance, and the firm wants to minimize the time required to provision test data. Which solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirements effectively. Taking snapshots provides a point-in-time copy of the data, ensuring data integrity. Enabling EBS fast snapshot restore significantly reduces the time required to create new volumes from the snapshots, meeting the requirement to minimize provisioning time. Creating new volumes from these snapshots and attaching them to test EC2 instances provides the test environment with the required data while isolating it from the production environment.\n\n**Why option 1 is incorrect:**\nThis option violates the data integrity requirement. Attaching the same EBS volume to multiple EC2 instances (even with Multi-Attach) would mean that any changes made in the test environment would directly affect the production data. This is unacceptable as it could corrupt or compromise the production data. While io2 volumes offer high IOPS, the risk to data integrity outweighs the performance benefit.\n\n**Why option 2 is incorrect:**\nWhile AWS Backup provides a mechanism for backing up and restoring EBS volumes, it is generally slower than using EBS snapshots with fast snapshot restore. The restore process from AWS Backup can take longer, especially for large volumes, which contradicts the requirement to minimize the time required to provision test data. Also, it adds an additional layer of complexity compared to using snapshots directly.\n\n**Why option 3 is incorrect:**\nCreating AMIs from the production EC2 instances would include the operating system and installed software, which is not the primary focus. The main requirement is to clone the data on the EBS volumes. While this approach could work, it's less efficient and more time-consuming than directly cloning the EBS volumes. Also, using EC2 instance store volumes for temporary simulation data is not ideal because instance store volumes are ephemeral and data is lost when the instance is stopped or terminated. This would require reloading the data for each simulation, further increasing the time required.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 62,
    "text": "A multinational logistics company operates its shipment tracking platform from Amazon EC2 instances deployed in the AWS us-west-2 Region. The platform exposes a set of APIs over HTTPS, which are used by logistics partners and customers around the world to retrieve real-time tracking data. The company has observed that users from Europe and Asia experience latency issues and inconsistent API response times when accessing the service. As a cloud architect, you have been tasked to propose the most cost-effective solution to improve performance for these international users without migrating the application. Which solution should you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the latency issues by leveraging AWS Global Accelerator. Global Accelerator provides static entry points that route user traffic to the nearest healthy endpoint. By creating endpoint groups for Europe and Asia and adding the existing us-west-2 EC2 endpoint to these groups, Global Accelerator intelligently routes traffic from those regions to the application in us-west-2, taking advantage of the AWS global network to minimize latency. This approach avoids the need to deploy the application in multiple regions, fulfilling the 'no migration' requirement and providing a cost-effective solution compared to deploying and maintaining infrastructure in multiple regions.\n\n**Why option 1 is incorrect:**\nThis solution involves deploying API Gateway and Lambda in multiple regions, which is more complex and expensive than necessary. While it would improve latency, it requires significant changes to the architecture and involves deploying and managing Lambda functions as proxies, increasing operational overhead and cost. The question specifically asks for the *most cost-effective* solution and to avoid migrating the application, which this option violates.\n\n**Why option 2 is incorrect:**\nThis solution requires deploying copies of the EC2 API in multiple regions, which contradicts the requirement to avoid migrating the application. It also introduces the complexity of managing and synchronizing multiple deployments. While Route 53 latency-based routing would direct users to the closest region, the cost and effort of maintaining multiple deployments make this a less desirable solution than using Global Accelerator.\n\n**Why option 3 is incorrect:**\nWhile CloudFront can improve performance by caching content closer to users, it is primarily designed for static content or content that can be cached. APIs that return real-time tracking data are often dynamic and not suitable for caching. Applying the CachingOptimized managed policy may not be effective for this use case and might lead to stale data being served. CloudFront alone does not address the underlying network latency issues as effectively as Global Accelerator, which optimizes the network path.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A media company operates a web application that enables users to upload photos. These uploads are stored in an Amazon S3 bucket located in the eu-west-2 Region. To enhance performance and provide secure access under a custom domain name, the company wants to integrate Amazon CloudFront for uploads to the S3 bucket. The architecture must support secure HTTPS connections using a custom domain, and the upload process must ensure optimal speed and security. Which combination of actions will fulfill these requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC)",
        "correct": true
      },
      {
        "id": 4,
        "text": "Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Origin Access Control (OAC) is the recommended way to allow CloudFront to securely access S3 buckets. OAC creates a CloudFront origin access identity (OAI) that is granted permission to read and write objects in the S3 bucket. This ensures that only CloudFront can access the S3 bucket directly, enhancing security. Using OAC is more secure than using signed URLs alone, especially for uploads, as it prevents direct access to the S3 bucket from outside the CloudFront distribution.\n\n**Why option 4 is correct:**\nThis is correct because AWS Certificate Manager (ACM) certificates used with CloudFront must be requested in the us-east-1 Region (North Virginia). This is a specific requirement of CloudFront. The certificate is used to enable HTTPS connections for the custom domain name associated with the CloudFront distribution. If the certificate is not in us-east-1, CloudFront will not be able to associate it with the distribution, and HTTPS will not work correctly.\n\n**Why option 0 is incorrect:**\nThis is incorrect because ACM certificates for CloudFront distributions must be requested in the us-east-1 Region, not the region where the S3 bucket resides (eu-west-2 in this case). CloudFront only looks for certificates in us-east-1.\n\n**Why option 1 is incorrect:**\nThis is incorrect because using an S3 static website endpoint as the origin for CloudFront is not the best practice for uploads. S3 static website endpoints are primarily designed for serving static content, not for handling upload operations. Uploads to a static website endpoint are generally not as efficient or secure as using the standard S3 REST API with appropriate access controls. Furthermore, enabling upload operations directly through a static website endpoint can expose the S3 bucket to potential security vulnerabilities.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A digital media company runs its content rendering service on Amazon EC2 instances that are registered with an Application Load Balancer (ALB) using IP-based target groups. The company relies on AWS Systems Manager to manage and patch these instances regularly. According to new compliance requirements, EC2 instances must be safely removed from production traffic during patching to prevent user disruption and maintain application integrity. However, during the most recent patch cycle, the operations team noticed application failures and API timeouts, even though patching succeeded on the instances. You are asked to suggest a reliable and scalable way to ensure safe patching while preserving service availability. Which solution will best meet the new compliance and operational requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window",
        "correct": true
      },
      {
        "id": 1,
        "text": "Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Systems Manager Maintenance Windows allow you to schedule patching activities and integrate them with other actions. You can configure the Maintenance Window to first deregister the target from the ALB target group before patching begins, and then re-register it after patching is complete. This ensures that no traffic is routed to the instance while it's being patched, preventing disruptions. The Maintenance Window provides a controlled and automated way to manage the patching process, meeting both compliance and operational requirements.\n\n**Why option 2 is correct:**\nThis is correct because AWS Systems Manager Automation, specifically using the AWSEC2-PatchLoadBalancerInstance document, is designed to handle patching EC2 instances behind an ALB. This document automates the process of deregistering the instance from the ALB target group, patching the instance, and then re-registering it with the ALB target group after patching. This ensures a safe and automated patching process that minimizes downtime and maintains application availability. It directly addresses the requirement of safely removing instances from production traffic during patching.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while instance ID-based target groups can be useful in some scenarios, they don't directly solve the problem of safely removing instances from the ALB during patching. Switching to instance ID-based target groups doesn't automatically integrate with Systems Manager patching to deregister and re-register instances. Systems Manager doesn't directly communicate with instance metadata to deregister/register instances from the ALB. The core issue is the need for an automated process to manage the instance's lifecycle within the ALB during patching, which this option doesn't provide.\n\n**Why option 3 is incorrect:**\nThis is incorrect because disabling the network interface of an EC2 instance is a disruptive and potentially unreliable way to remove it from the ALB. It can lead to connection errors and unexpected behavior. The ALB health checks might not respond correctly, and the instance might be considered unhealthy even after patching is complete. A more graceful approach is to deregister the instance from the target group, allowing existing connections to drain before patching begins. This option also requires a custom Lambda function and EventBridge rule, adding unnecessary complexity compared to using Systems Manager Maintenance Windows or Automation documents specifically designed for this purpose.\n\n**Why option 4 is incorrect:**\nThis is incorrect because relying on manual adjustments to ALB target group registrations based on CloudWatch Logs Insights is not a scalable or reliable solution. It introduces the potential for human error and delays, which can lead to application downtime. The requirement is for an automated and scalable solution, and manual intervention defeats that purpose. CloudWatch Logs Insights is useful for monitoring and troubleshooting, but not for automating the patching process.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 65,
    "text": "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data. As a solutions architect, which of the following solutions will you suggest?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB Accelerator (DAX)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon ElastiCache for Memcached",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Redis",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct solution because Amazon ElastiCache for Redis offers built-in geospatial commands and data structures. These features allow for efficient storage, indexing, and querying of geospatial data, which is essential for the weather tracking company's needs. Redis's geospatial capabilities include finding locations within a radius, calculating distances between locations, and other location-based operations. This makes it a suitable caching solution for applications dealing with location data.\n\n**Why option 0 is incorrect:**\nAmazon DynamoDB Accelerator (DAX) is an in-memory cache specifically designed for DynamoDB. It does not support relational databases or geospatial data. Therefore, it is not an appropriate solution for this scenario.\n\n**Why option 1 is incorrect:**\nAmazon ElastiCache for Memcached is a distributed memory object caching system. While it can improve performance, it does not natively support geospatial data types or operations. It would require complex workarounds to handle geospatial data, making it less efficient and more difficult to manage compared to Redis.\n\n**Why option 2 is incorrect:**\nAWS Global Accelerator improves the performance of applications by directing user traffic to the optimal AWS endpoint. It does not provide caching capabilities for databases or support geospatial data. It is primarily focused on improving network performance and availability, not database performance.",
    "domain": "Design High-Performing Architectures"
  }
]