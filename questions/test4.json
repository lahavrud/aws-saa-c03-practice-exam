[
  {
    "id": 1,
    "text": "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events. Which of the following addresses the given use case?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it focuses on the ECS service's CPU utilization. The ECS service represents the running tasks (containers) that are serving the application. Monitoring the CPU utilization of the ECS service directly reflects the load on the application itself. When the service's CPU utilization rises above a threshold, it indicates that the existing tasks are becoming overloaded and that more tasks (containers) are needed to handle the increased traffic. Auto Scaling can then be configured to increase the desired count of tasks in the ECS service, effectively scaling out the application. This directly addresses the need for an automated solution to respond to traffic spikes and maintain availability.\n\n**Why option 0 is incorrect:**\nis incorrect because while the Application Load Balancer's target group CPU utilization is a valid metric, it's an *indirect* indicator of the load on the ECS service. The target group CPU utilization reflects the CPU usage of the instances registered with the target group, which are the ECS container instances. Scaling based on the ECS service's CPU utilization is more precise and directly tied to the application's performance. The target group CPU utilization could be high due to other processes running on the container instances, not necessarily the ECS tasks. Also, scaling based on target group CPU utilization might not accurately reflect the load on individual tasks within the ECS service.\n\n**Why option 1 is incorrect:**\nis incorrect because it suggests scaling based on the CloudWatch alarm's CPU utilization. CloudWatch alarms are *triggered* by metrics, but they don't *have* CPU utilization themselves. The alarm is likely monitoring the CPU utilization of the ECS container instances, similar to option 0. Scaling based directly on the ECS service's CPU utilization is a more direct and effective approach. The CloudWatch alarm is just a notification mechanism, not the trigger for Auto Scaling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Storage Gateway - Cached Volume, is the correct solution. A Cached Volume stores the entire dataset on Amazon S3 and caches the most frequently accessed data locally. This directly addresses the requirement of having frequently accessed logs available locally for low latency while ensuring all logs are backed up in S3. The Storage Gateway handles the data transfer and caching automatically.\n\n**Why option 0 is incorrect:**\nusing AWS Direct Connect, is incorrect because Direct Connect provides a dedicated network connection between the on-premises data center and AWS, but it doesn't provide a caching mechanism or automatic backup to S3. While Direct Connect can improve network performance, it doesn't solve the core problem of caching frequently accessed logs and backing up all logs.\n\n**Why option 2 is incorrect:**\nusing AWS Storage Gateway - Stored Volume, is incorrect because a Stored Volume stores the entire dataset locally and asynchronously backs it up to Amazon S3. This means the primary copy of the data resides on-premises, which is not ideal for the scenario where the requirement is to have all logs backed up to S3 and only frequently accessed logs cached locally. Stored Volumes are more suitable when the primary data storage is on-premises and S3 is used for backup or disaster recovery.\n\n**Why option 3 is incorrect:**\nusing AWS Snowball Edge Storage Optimized, is incorrect because while Snowball Edge can provide local storage and data transfer to S3, it's primarily designed for large-scale data migration or edge computing scenarios where network connectivity is limited or unavailable. It's not a suitable solution for continuous web log archival and caching, as it requires manual data transfer and doesn't provide automatic caching like Storage Gateway.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon Kinesis with Amazon Simple Notification Service (Amazon SNS), is the correct answer. Amazon Kinesis is suitable for real-time data streaming and analytics. After processing the IoT data stream with Kinesis Data Analytics (or other Kinesis services), Amazon SNS can be used to send push notifications to mobile applications. SNS supports sending notifications to various platforms, including iOS, Android, and Fire OS, making it ideal for delivering notifications to IoT device owners' mobile apps.\n\n**Why option 0 is incorrect:**\nAmazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS), is incorrect. While Amazon SNS can be used for sending notifications, Amazon SQS is a queuing service designed for decoupling components and asynchronous processing. It's not directly related to the real-time data ingestion and analytics aspect of the problem. SQS is not typically used for real-time analytics or direct notification triggering based on streaming data analysis.\n\n**Why option 1 is incorrect:**\nAmazon Kinesis with Amazon Simple Email Service (Amazon SES), is incorrect. While Amazon Kinesis is suitable for real-time data streaming and analytics, Amazon SES is designed for sending emails, not push notifications to mobile applications. SES is not the appropriate service for delivering notifications to mobile devices in this scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nThese are correct because they establish bidirectional DNS resolution. Option 2 creates an inbound endpoint in Route 53 Resolver. This allows the on-premises DNS resolvers to forward queries for AWS resources to Route 53 Resolver. Option 3 creates an outbound endpoint in Route 53 Resolver. This allows Route 53 Resolver to conditionally forward queries for on-premises resources to the on-premises DNS resolvers. The combination of inbound and outbound endpoints enables the desired bidirectional DNS resolution.\n\n**Why option 3 is correct:**\nThese are correct because they establish bidirectional DNS resolution. Option 2 creates an inbound endpoint in Route 53 Resolver. This allows the on-premises DNS resolvers to forward queries for AWS resources to Route 53 Resolver. Option 3 creates an outbound endpoint in Route 53 Resolver. This allows Route 53 Resolver to conditionally forward queries for on-premises resources to the on-premises DNS resolvers. The combination of inbound and outbound endpoints enables the desired bidirectional DNS resolution.\n\n**Why option 0 is incorrect:**\nis incorrect because there is no concept of a 'universal endpoint' in Route 53 Resolver. Route 53 Resolver uses inbound and outbound endpoints for hybrid DNS resolution.\n\n**Why option 1 is incorrect:**\nis incorrect because it only addresses one direction of DNS resolution. While it allows on-premises resolvers to forward queries to Route 53 Resolver (which is the function of an inbound endpoint, not outbound), it doesn't enable Route 53 Resolver to resolve on-premises DNS names. Therefore, it doesn't provide a complete solution for bidirectional DNS resolution.\n\n**Why option 4 is incorrect:**\nis incorrect because it describes a scenario where Route 53 Resolver conditionally forwards queries to itself. An inbound endpoint allows on-premises resolvers to forward queries *to* Route 53 Resolver, not the other way around. Conditional forwarding from Route 53 Resolver to on-premises resolvers requires an outbound endpoint.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Config, is the correct answer. AWS Config is specifically designed for configuration management and compliance. It allows you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config rules, you can define the desired configuration state of your resources and AWS Config automatically checks whether your resources comply with these rules. It also maintains a configuration history, allowing you to track changes over time and troubleshoot issues. This directly addresses both requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon CloudWatch, is incorrect. CloudWatch is primarily a monitoring service for metrics and logs. While it can monitor the performance and health of resources, it doesn't directly provide the functionality to review resource configurations for compliance or maintain a detailed history of configuration changes in the same way as AWS Config. CloudWatch can be used to trigger alarms based on configuration changes detected through other services, but it's not the primary tool for configuration management.\n\n**Why option 1 is incorrect:**\nusing AWS Systems Manager, is incorrect. AWS Systems Manager (SSM) provides a unified interface to manage your AWS resources. While SSM can be used to manage the configuration of instances and applications, it's not primarily designed for continuous compliance monitoring and maintaining a detailed configuration history across all AWS resource types. SSM State Manager can help with desired state configuration, but it lacks the comprehensive auditing and compliance features of AWS Config. SSM Inventory can collect information about instances, but it's not the same as tracking configuration changes of all AWS resources for compliance purposes.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte. As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EFS with Bursting Throughput mode",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EFS with Provisioned Throughput mode",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon DynamoDB table that is accessible by all ECS cluster instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon EFS with Provisioned Throughput mode, is the most suitable solution. EFS is a network file system that can be mounted to multiple EC2 instances simultaneously, making it ideal for sharing data between ECS tasks running on different instances. Provisioned Throughput mode allows you to specify the throughput your application requires, ensuring consistent performance even under high load. While EFS has a Bursting Throughput mode, the sustained high-frequency read/write requirements of hundreds of concurrent tasks would likely exhaust the burst credits quickly, leading to performance degradation. Provisioned Throughput guarantees the necessary performance for the application.\n\n**Why option 0 is incorrect:**\nusing Amazon EFS with Bursting Throughput mode, is incorrect because the workload involves high-frequency reading and writing from hundreds of tasks concurrently. The burst credits for EFS Bursting Throughput mode would likely be exhausted quickly, leading to significant performance degradation. Bursting is suitable for infrequent or spiky workloads, not sustained high I/O.\n\n**Why option 2 is incorrect:**\nusing an Amazon DynamoDB table, is incorrect because DynamoDB is a NoSQL database designed for key-value or document storage, not for storing file-based output data. While DynamoDB can handle high read/write throughput, it's not an appropriate storage solution for the type of data described in the scenario (20MB output files). Storing files in DynamoDB would be inefficient and costly.\n\n**Why option 3 is incorrect:**\nusing an Amazon EBS volume mounted to the Amazon ECS cluster instances, is incorrect because EBS volumes are block storage devices that are attached to a single EC2 instance. This would create a bottleneck as multiple ECS tasks running on different instances would need to access the same EBS volume, leading to contention and performance issues. Furthermore, managing data consistency and sharing across multiple instances would be complex.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead. What solution would best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nsetting up an Amazon FSx for ONTAP instance and migrating the data to it, is the best solution. FSx for ONTAP provides native support for both SMB and NFS protocols, fulfilling the protocol requirement. It also offers data tiering capabilities, allowing infrequently accessed data to be automatically moved to a lower-cost storage tier within the FSx for ONTAP volume, optimizing costs. Furthermore, FSx for ONTAP is a fully managed service, minimizing operational overhead. The 'root volume' mention is a bit misleading, as you would configure the file system on the FSx for ONTAP instance, not specifically on the root volume in a traditional sense. The key is that FSx for ONTAP handles the underlying storage management.\n\n**Why option 1 is incorrect:**\nusing Amazon EFS with EFS Infrequent Access and AWS DataSync, is not the best solution. While EFS supports NFS and offers an Infrequent Access tier for cost optimization, it does *not* natively support SMB. Therefore, it cannot directly serve Windows clients using the SMB protocol. DataSync is a good tool for migrating data, but it doesn't address the protocol incompatibility.\n\n**Why option 2 is incorrect:**\nusing Amazon FSx for OpenZFS, is not the best solution. While FSx for OpenZFS is a powerful file system, it primarily supports NFS. While it can support SMB via configuration and integration, it is not its primary strength and adds complexity. FSx for ONTAP is a better fit because it natively supports both protocols. Also, the question emphasizes minimizing operational overhead, and FSx for ONTAP is generally considered easier to manage for mixed protocol environments.\n\n**Why option 3 is incorrect:**\nusing Amazon EFS with EFS Intelligent-Tiering and AWS DataSync, is not the best solution. Similar to option 1, EFS supports NFS and offers Intelligent-Tiering for cost optimization, but it lacks native SMB support. Intelligent-Tiering automatically moves data between the Standard and Infrequent Access tiers based on access patterns. While useful, the lack of SMB support makes it unsuitable for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 8,
    "text": "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously. Which of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon ElastiCache to improve the performance of compute-intensive workloads",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache to run highly complex JOIN queries",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because ElastiCache is primarily used to improve latency and throughput for read-heavy workloads. The leaderboard, accessed by millions of users, will generate a significant number of read requests. Caching the leaderboard data in ElastiCache reduces the load on the underlying database and provides faster access to the data, thereby improving latency and throughput.\n\nOption 1 is correct because ElastiCache can improve the performance of compute-intensive workloads. The question mentions a 'complex proprietary algorithm' to identify the top users. Instead of running this algorithm repeatedly for every request, the results can be cached in ElastiCache. Subsequent requests can then retrieve the results from the cache, significantly reducing the computational load and improving performance. This is especially beneficial given the real-time requirement.\n\n**Why option 1 is correct:**\nThis is correct because ElastiCache is primarily used to improve latency and throughput for read-heavy workloads. The leaderboard, accessed by millions of users, will generate a significant number of read requests. Caching the leaderboard data in ElastiCache reduces the load on the underlying database and provides faster access to the data, thereby improving latency and throughput.\n\nOption 1 is correct because ElastiCache can improve the performance of compute-intensive workloads. The question mentions a 'complex proprietary algorithm' to identify the top users. Instead of running this algorithm repeatedly for every request, the results can be cached in ElastiCache. Subsequent requests can then retrieve the results from the cache, significantly reducing the computational load and improving performance. This is especially beneficial given the real-time requirement.\n\n**Why option 2 is incorrect:**\nis incorrect because while ElastiCache can handle writes, it's not its primary strength. Its main benefit is in improving read performance. In this scenario, the focus is on serving the leaderboard data to millions of users, which is a read-heavy operation. While updates to the leaderboard will occur, the read volume will be significantly higher.\n\n**Why option 3 is incorrect:**\nis incorrect because ElastiCache is not designed for running complex JOIN queries. It is an in-memory data store, not a relational database. JOIN queries are typically performed by a relational database like Amazon RDS or Amazon Aurora.\n\n**Why option 4 is incorrect:**\nis incorrect because ElastiCache is not typically used for ETL workloads. ETL processes involve extracting, transforming, and loading data, often into a data warehouse. While ElastiCache can be used to cache data during the transformation phase, it is not the primary service for ETL operations. Services like AWS Glue, AWS Data Pipeline, or AWS Lambda are more suitable for ETL tasks.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
    "options": [
      {
        "id": 0,
        "text": "You can use an IP address as the custom source for the inbound rule",
        "correct": false
      },
      {
        "id": 1,
        "text": "You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule",
        "correct": false
      },
      {
        "id": 2,
        "text": "You can use an Internet Gateway ID as the custom source for the inbound rule",
        "correct": true
      },
      {
        "id": 3,
        "text": "You can use a security group as the custom source for the inbound rule",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because you cannot use an Internet Gateway ID as the source for an inbound security group rule. Security groups control traffic at the instance level, and the source of traffic must be definable in terms of IP addresses, CIDR blocks, or other security groups. An Internet Gateway is a VPC component that enables communication between instances in your VPC and the internet. It doesn't represent a source of traffic in the context of security group rules.\n\n**Why option 0 is incorrect:**\nis incorrect because you can specify a single IP address as the source for an inbound security group rule. This allows traffic from a specific IP to reach the instance associated with the security group.\n\n**Why option 1 is incorrect:**\nis incorrect because you can specify a range of IP addresses in CIDR block notation as the source for an inbound security group rule. This is a common practice to allow traffic from a defined network range.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A national logistics company has a dedicated AWS Direct Connect connection from its corporate data center to AWS. Within its AWS account, the company operates 25 Amazon VPCs in the same Region, each supporting different regional distribution services. The VPCs were configured with non-overlapping CIDR blocks and currently use private VIFs for Direct Connect access to on-premises resources. As the architecture scales, the company wants to enable communication across all VPCs and the on-premises environment. The solution must scale efficiently, support full-mesh connectivity, and reduce the complexity of maintaining separate private VIFs for each VPC. Which combination of solutions will best fulfill these requirements with the least amount of operational overhead? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs",
        "correct": false
      },
      {
        "id": 1,
        "text": "Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 provide the most efficient and scalable solution. Option 3, creating an AWS Transit Gateway and attaching all 25 VPCs, centralizes routing. Enabling route propagation automates the routing between VPCs, fulfilling the full-mesh connectivity requirement. Option 2, creating a transit VIF and associating it with the Transit Gateway, allows the on-premises environment to communicate with the VPCs connected to the Transit Gateway. This combination provides a scalable and manageable solution for inter-VPC and on-premises connectivity with minimal operational overhead. The Transit Gateway simplifies routing configuration and management compared to other options.\n\n**Why option 3 is correct:**\nOptions 2 and 3 provide the most efficient and scalable solution. Option 3, creating an AWS Transit Gateway and attaching all 25 VPCs, centralizes routing. Enabling route propagation automates the routing between VPCs, fulfilling the full-mesh connectivity requirement. Option 2, creating a transit VIF and associating it with the Transit Gateway, allows the on-premises environment to communicate with the VPCs connected to the Transit Gateway. This combination provides a scalable and manageable solution for inter-VPC and on-premises connectivity with minimal operational overhead. The Transit Gateway simplifies routing configuration and management compared to other options.\n\n**Why option 0 is incorrect:**\nis incorrect because converting private VIFs to Direct Connect gateway associations and manually configuring routing between VGWs is complex and doesn't scale well. Manually managing routes between 25 VGWs would be operationally burdensome and prone to errors. Direct Connect Gateway is designed for connecting to multiple Regions, not necessarily for intra-region VPC connectivity.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS PrivateLink endpoints for inter-VPC communication is not designed for full-mesh connectivity between all VPCs and on-premises. PrivateLink is best suited for providing access to specific services in a VPC, not for general network connectivity. Setting up and managing PrivateLink endpoints for all VPCs would add significant complexity and overhead. Furthermore, it doesn't directly address the on-premises connectivity requirement.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet. As an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it suggests using a VPC Gateway Endpoint for S3. VPC Gateway Endpoints allow direct, private connectivity to S3 within the same region without traversing the internet or using a NAT Gateway. This significantly reduces data transfer costs and improves security by keeping traffic within the AWS network. The endpoint policy allows you to control which S3 buckets can be accessed through the endpoint, enhancing security. Updating the route table to direct S3-bound traffic to the VPC endpoint ensures that the traffic uses the optimized path.\n\n**Why option 0 is incorrect:**\nis incorrect because an egress-only internet gateway is designed for IPv6 traffic leaving a VPC. The scenario specifies IPv4. Also, while an egress-only internet gateway provides outbound-only internet access for IPv6 instances, it doesn't help in optimizing S3 access costs. The traffic would still traverse the internet, incurring data transfer charges.\n\n**Why option 1 is incorrect:**\nis incorrect because provisioning an internet gateway and routing traffic through it would expose the EC2 instances to the public internet, which is not desirable for instances in a private subnet. It also doesn't address the cost optimization requirement for S3 access. Using an internet gateway for S3 access would incur data transfer charges, which can be avoided by using a VPC Endpoint.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 12,
    "text": "A regional transportation authority operates a high-traffic public information portal hosted on AWS. The backend consists of Amazon EC2 instances behind an Application Load Balancer (ALB). In recent weeks, the operations team has observed intermittent slowdowns and performance issues. After investigation, the team suspect the application is being targeted by distributed denial-of-service (DDoS) attacks coming from a wide range of IP addresses. The team needs a solution that provides DDoS mitigation, detailed logs for audit purposes, and requires minimal changes to the existing architecture. Which solution best addresses these needs?",
    "options": [
      {
        "id": 0,
        "text": "Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly",
        "correct": false
      },
      {
        "id": 1,
        "text": "Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nsubscribing to AWS Shield Advanced, is the best solution. Shield Advanced provides proactive DDoS protection specifically designed to protect applications running on AWS. It offers enhanced detection and mitigation capabilities compared to the standard Shield protection included with all AWS services. The AWS DDoS Response Team (DRT) can analyze traffic patterns and apply custom mitigations tailored to the specific attack. Shield Advanced also provides detailed logging and reporting, which satisfies the audit requirement. Importantly, Shield Advanced integrates well with existing AWS infrastructure, minimizing the need for significant architectural changes. It protects resources like EC2, ALB, and CloudFront.\n\n**Why option 0 is incorrect:**\nenabling Amazon Inspector, is incorrect because Inspector is primarily a vulnerability assessment service. While it can identify potential vulnerabilities that *could* be exploited in a DDoS attack, it doesn't directly mitigate DDoS attacks. Patching vulnerabilities is important for overall security, but it's not a real-time DDoS mitigation strategy. It also doesn't provide the detailed logging needed for audit purposes.\n\n**Why option 2 is incorrect:**\ndeploying Amazon GuardDuty, is incorrect because GuardDuty is a threat detection service that analyzes CloudTrail logs, VPC Flow Logs, and DNS logs to identify malicious activity. While GuardDuty can detect suspicious activity that *might* be related to a DDoS attack, it doesn't provide automatic DDoS mitigation. Manually blocking IP addresses based on GuardDuty findings is a reactive approach and would be difficult to scale and maintain during a large-scale DDoS attack. It also requires manual intervention, which is not ideal for a high-traffic public portal.\n\n**Why option 3 is incorrect:**\ncreating a CloudFront distribution with AWS WAF, is a good solution for DDoS mitigation, but it requires more significant architectural changes than Shield Advanced. While WAF can filter malicious traffic based on custom rules, it requires configuration and ongoing maintenance to define and update those rules. CloudFront access logs can be used for analysis, but the initial setup and configuration are more involved than simply subscribing to Shield Advanced. Also, while CloudFront offers caching benefits, the primary concern here is DDoS mitigation, and Shield Advanced offers more specialized DDoS protection capabilities.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience. As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most optimal solution. Amazon S3 is a highly scalable, durable, and cost-effective object storage service, ideal for storing large video files. Amazon S3 Transfer Acceleration (S3TA) leverages the globally distributed AWS edge locations to accelerate data transfers to and from S3. When a user uploads or downloads a file, the data is routed through the nearest edge location, which then uses optimized network paths to transfer the data to or from the S3 bucket. This significantly reduces latency for users in geographically dispersed locations, especially for large files. S3 provides the storage and S3TA provides the low latency global access.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon S3 is a good choice for storage, using Amazon EC2 for hosting the web application adds unnecessary operational overhead and cost compared to hosting a static website directly from S3. ElastiCache is primarily for caching frequently accessed data to improve application performance, but it doesn't directly address the latency issues associated with transferring large files across geographic regions. While ElastiCache can improve web application performance, it doesn't accelerate the transfer of large video files like S3 Transfer Acceleration does.\n\n**Why option 1 is incorrect:**\nis partially correct. Amazon S3 can be used for hosting static websites, which could be a cost-effective way to serve the web application's static content. Amazon CloudFront is a CDN that caches content at edge locations to reduce latency for users accessing the website's static assets. However, CloudFront primarily caches static content and doesn't accelerate the upload of large files to S3. While CloudFront would improve the initial loading of the web application, it wouldn't directly address the latency issues associated with uploading and downloading large video files from geographically dispersed locations. S3 Transfer Acceleration is specifically designed for this purpose.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A healthcare startup is modernizing its monolithic Python-based analytics application by transitioning to a microservices architecture on AWS. As a pilot, the team wants to refactor one module into a standalone microservice that can handle hundreds of requests per second. They are seeking an AWS-native solution that supports Python, scales automatically with traffic, and requires minimal infrastructure management and operational overhead to build, test, and deploy the service efficiently. Which AWS solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because AWS Lambda is a serverless compute service that natively supports Python. Integrating it with Amazon API Gateway provides HTTP access, and enabling provisioned concurrency ensures low latency and predictable performance during peak loads. Lambda handles automatic scaling, eliminating the need for manual infrastructure management. This approach aligns with the requirement for minimal operational overhead and efficient build/test/deploy cycles. API Gateway provides the necessary endpoint for the microservice to be accessed.\n\n**Why option 0 is incorrect:**\nis incorrect because while EC2 Auto Scaling can handle scaling, it requires significant infrastructure management, including patching, security updates, and OS maintenance. Installing dependencies at instance startup adds to the startup time and complexity. Spot Instances are also not ideal for production workloads requiring consistent performance due to their potential for interruption. This option does not minimize operational overhead.\n\n**Why option 2 is incorrect:**\nis a strong contender, but AWS App Runner, while simplifying deployment from a repository, might not offer the same level of granular control over concurrency and performance tuning as Lambda with provisioned concurrency. Also, the question highlights the need for a solution that can handle hundreds of requests per second. Lambda with provisioned concurrency is better suited for predictable performance at scale. App Runner is a good option but not as optimized for this specific scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while ECS with Fargate reduces infrastructure management compared to EC2, it still involves managing Docker containers, defining task definitions, and configuring ECS Service Auto Scaling. This adds operational overhead compared to Lambda. While Fargate is a good option for containerized applications, Lambda is a better fit for a simple microservice requiring minimal management and automatic scaling.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 15,
    "text": "A company has its application servers in the public subnet that connect to the database instances in the private subnet. For regular maintenance, the database instances need patch fixes that need to be downloaded from the internet. Considering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Egress-only internet gateway for the resources in the private subnet of the VPC",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nconfiguring a NAT gateway in the public subnet, is the correct solution. A NAT gateway allows instances in the private subnet to initiate outbound traffic to the internet without allowing inbound traffic from the internet. It's a fully managed service, eliminating the operational overhead of managing a NAT instance. The NAT gateway resides in the public subnet and uses the Internet Gateway to access the internet. The route table for the private subnet is configured to route internet-bound traffic to the NAT gateway.\n\n**Why option 0 is incorrect:**\nconfiguring a NAT instance, is incorrect because while it provides the required functionality, it is not a fully managed service. Managing a NAT instance involves patching, scaling, and ensuring high availability, which adds operational overhead. The question specifically asks for a fully managed service.\n\n**Why option 1 is incorrect:**\nconfiguring the Internet Gateway to be accessible to the private subnet resources, is incorrect and a security risk. An Internet Gateway allows bidirectional communication between the VPC and the internet. Directly associating the private subnet with the Internet Gateway would expose the database instances to the internet, which is undesirable and violates security best practices. This would allow inbound traffic from the internet to the database instances, which is not the goal.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "General Purpose SSD (gp2)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provisioned IOPS SSD (io1)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Throughput Optimized HDD (st1)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Cold HDD (sc1)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nProvisioned IOPS SSD (io1) volumes are designed for I/O-intensive workloads that require sustained high performance. They allow you to specify the IOPS you need, and AWS guarantees that level of performance. Since the requirement is to support up to 25,000 IOPS per volume, io1 is the most suitable option. io2 volumes are also an option for high IOPS, but io1 is generally sufficient unless higher durability or IOPS per GB are needed.\n\n**Why option 0 is incorrect:**\nGeneral Purpose SSD (gp2) volumes provide a balance of price and performance for a wide variety of workloads. While they can burst to higher IOPS, they are not designed to sustain a specific high IOPS level like 25,000. gp2 volumes are suitable for boot volumes, development and test environments, and interactive applications, but not for demanding database workloads with specific IOPS requirements.\n\n**Why option 2 is incorrect:**\nThroughput Optimized HDD (st1) volumes are designed for frequently accessed, throughput-intensive workloads with large block sizes, such as big data, data warehouses, and log processing. They are not optimized for I/O-intensive database workloads requiring high IOPS. Their performance is measured in MB/s (throughput) rather than IOPS.\n\n**Why option 3 is incorrect:**\nCold HDD (sc1) volumes are designed for infrequently accessed data and offer the lowest cost per GB. They are suitable for archival storage and backups, but not for real-time database workloads requiring high IOPS or throughput. They are also measured in MB/s (throughput) rather than IOPS.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": true
      },
      {
        "id": 1,
        "text": "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": false
      },
      {
        "id": 2,
        "text": "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": false
      },
      {
        "id": 3,
        "text": "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Launch Templates offer more flexibility and features compared to Launch Configurations. Specifically, Launch Templates allow you to specify multiple instance types and purchasing options (On-Demand and Spot) within a single template. This enables the Auto Scaling group to diversify its instance selection, increasing the likelihood of fulfilling capacity requests and optimizing costs by leveraging Spot Instances when available and falling back to On-Demand Instances when Spot prices are too high or capacity is unavailable. Launch Templates also support versioning, making it easier to manage and update configurations over time. Launch Templates are the recommended approach for new Auto Scaling groups.\n\n**Why option 1 is incorrect:**\nis incorrect because Launch Templates *can* be used to provision capacity across multiple instance types using both On-Demand and Spot Instances. This is a key feature of Launch Templates and a common use case for cost optimization.\n\n**Why option 2 is incorrect:**\nis incorrect because while Launch Templates can be used, Launch Configurations are an older technology and do not offer the same level of flexibility and features, particularly the ability to easily specify multiple instance types and purchasing options within a single configuration for diversified instance selection. Launch Configurations are essentially immutable after creation, making updates more difficult.\n\n**Why option 3 is incorrect:**\nis incorrect because Launch Configurations lack the functionality to easily specify multiple instance types and purchasing options (On-Demand and Spot) within a single configuration for diversified instance selection. Launch Templates are the recommended approach for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 18,
    "text": "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations. The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Elastic IPs for each of the Application Load Balancers in each Region",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nlaunching AWS Global Accelerator and creating endpoints for all the Regions, then registering the Application Load Balancers of each Region to the corresponding endpoints, is the correct solution. AWS Global Accelerator provides a static entry point (two static IP addresses) for applications deployed in multiple AWS Regions. This significantly reduces the number of IP addresses that need to be managed in the firewall. Global Accelerator intelligently routes traffic to the nearest healthy endpoint (ALB) based on user location and health checks. This simplifies traffic management and improves application availability and performance. It also provides DDoS protection by default.\n\n**Why option 0 is incorrect:**\nsetting up a Network Load Balancer (NLB) with an elastic IP address and registering the private IPs of all the Application Load Balancers as targets, is incorrect. While an NLB can provide a single entry point with a static IP, it would require the NLB to be in a single region. Routing traffic across regions would still require managing the NLB's IP address in the firewall, and it doesn't inherently simplify the management of the ALBs themselves. Also, using private IPs as targets for an NLB that is intended to be publicly accessible is not a standard or recommended practice.\n\n**Why option 1 is incorrect:**\nassigning an Elastic IP to an Auto Scaling Group (ASG) and setting up multiple Amazon EC2 instances to run behind the Auto Scaling Groups for each of the Regions, is incorrect. This option completely bypasses the existing Application Load Balancers, which are already in place and presumably serving a purpose. It also introduces a new layer of complexity by requiring the company to manage EC2 instances and Auto Scaling Groups. This does not address the core problem of simplifying the management of the existing ALB infrastructure and reducing the number of IP addresses.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account. Which of the following solutions should a solutions architect configure for the necessary permissions?",
    "options": [
      {
        "id": 0,
        "text": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it leverages IAM roles and instance profiles. An IAM role is created with the necessary permissions to access the DynamoDB table. An instance profile is then used to associate this role with the EC2 instance. When the application running on the EC2 instance makes a request to DynamoDB, the AWS SDK automatically uses the credentials provided by the instance metadata service, which are derived from the assigned IAM role. This eliminates the need to store credentials directly on the instance or in the application code, enhancing security and simplifying credential management. The EC2 instance doesn't need to be explicitly added to the trust relationship policy document; the instance profile handles that implicitly.\n\n**Why option 0 is incorrect:**\nis incorrect because storing credentials in an S3 bucket, even if encrypted, and then retrieving them from the application code is a less secure and more complex approach than using IAM roles. It introduces the risk of the S3 bucket being compromised or the credentials being accidentally exposed. It also adds unnecessary complexity to the application code.\n\n**Why option 1 is incorrect:**\nis incorrect because storing credentials directly on the EC2 instance's local storage is a highly insecure practice. If the instance is compromised, the credentials would be easily accessible, leading to potential unauthorized access to the DynamoDB table. This violates the principle of least privilege and is a major security risk.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53. Which Amazon Route 53 record should you create?",
    "options": [
      {
        "id": 0,
        "text": "Create a CNAME record",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an A record",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a PTR record",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Alias Record",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nA CNAME (Canonical Name) record maps an alias domain name to another domain name. In this case, we want to map www.your-domain.com to yourapp.provider.com. When a user queries www.your-domain.com, Route 53 will return yourapp.provider.com, and the user's browser will then resolve yourapp.provider.com to the actual IP address. This is the correct approach when the target is another domain name and not a specific AWS resource.\n\n**Why option 1 is incorrect:**\nAn A record maps a domain name to an IPv4 address. Since the application is hosted on yourapp.provider.com, we don't have a static IP address to point to directly. The IP address associated with yourapp.provider.com could change, making the A record invalid. Also, using an A record would require us to constantly monitor and update the IP address if it changes, which is not ideal.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nAn Alias record is used to map a domain name to specific AWS resources like Elastic Load Balancers, CloudFront distributions, S3 buckets configured for website hosting, or other Route 53 records *within the same Route 53 hosted zone*. It cannot be used to point to an external domain like yourapp.provider.com. Alias records also offer health checks and automatic updates when the underlying AWS resource changes, which are not relevant in this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages. Which of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon CloudFront with Amazon S3, is the correct solution. Amazon S3 provides highly scalable and durable storage for static assets. CloudFront, a content delivery network (CDN), caches these assets at edge locations globally, reducing latency for users and offloading traffic from the origin server (S3). This combination is cost-effective because S3 is pay-as-you-go for storage and data transfer, and CloudFront's pricing is based on usage. It avoids the need to provision and manage servers, fulfilling the question's requirements.\n\n**Why option 0 is incorrect:**\nusing Amazon CloudFront with Amazon DynamoDB, is incorrect. DynamoDB is a NoSQL database designed for high-performance, low-latency access to data, but it's not suitable for storing large static assets like leaderboard reports and game tactics. Storing these assets in DynamoDB would be significantly more expensive and less efficient than using S3. While CloudFront is a good choice for caching, DynamoDB is the wrong storage service for this scenario.\n\n**Why option 2 is incorrect:**\nconfiguring AWS Lambda with an Amazon RDS database, is incorrect. Lambda is a serverless compute service, but using it with RDS to serve static assets is an overly complex and expensive solution. RDS is a relational database service, designed for structured data, not for storing and serving static files. While Lambda can serve static content, it's not optimized for this purpose, and using RDS as the data source adds unnecessary overhead and cost. This approach also requires more configuration and management than using S3 and CloudFront.\n\n**Why option 3 is incorrect:**\nusing AWS Lambda with Amazon ElastiCache and Amazon RDS, is incorrect. This option is even more complex and costly than option 2. ElastiCache is an in-memory caching service, which could be useful for caching dynamic content, but it's not the primary solution for serving static assets. RDS is still unsuitable for storing static files, and Lambda adds unnecessary complexity. This combination is not cost-optimal and requires significant configuration and management.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 22,
    "text": "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
    "options": [
      {
        "id": 0,
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each",
        "correct": false
      },
      {
        "id": 1,
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each",
        "correct": false
      },
      {
        "id": 2,
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each",
        "correct": false
      },
      {
        "id": 3,
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis correct because: \n\n*   **Cross-Zone Load Balancing Enabled:** When cross-zone load balancing is enabled, the ELB distributes traffic evenly across all registered instances, regardless of their AZ. With a total of 5 instances (1 in AZ A and 4 in AZ B), each instance receives approximately 20% of the traffic (100% / 5 instances = 20%).\n*   **Cross-Zone Load Balancing Disabled:** When cross-zone load balancing is disabled, each load balancer node only distributes traffic to instances within its own AZ. Assuming the load balancer nodes are evenly distributed across the AZs, half of the traffic will be handled by the load balancer nodes in AZ A and the other half by the load balancer nodes in AZ B. The single instance in AZ A will receive 50% of the total traffic (since it's the only instance in that AZ), and each of the four instances in AZ B will receive 12.5% of the total traffic (50% / 4 instances = 12.5%).\n\n**Why option 0 is incorrect:**\nis incorrect because it misrepresents the traffic distribution in both scenarios. With cross-zone load balancing enabled, all instances receive traffic. With cross-zone load balancing disabled, the instance in AZ A receives traffic, not zero traffic.\n\n**Why option 1 is incorrect:**\nis incorrect because it misrepresents the traffic distribution when cross-zone load balancing is disabled. The single instance in AZ A will receive a significantly higher percentage of traffic than the instances in AZ B when cross-zone load balancing is disabled.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A financial services firm runs a containerized risk analytics tool in its on-premises data center using Docker. The tool depends on persistent data storage for maintaining customer simulation results and operates on a single host machine where the volume is locally mounted. The infrastructure team is looking to replatform the tool to AWS using a fully managed service because they want to avoid managing EC2 instances, volumes, or underlying servers. Which AWS solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon ECS with Fargate and Amazon EFS, is the best solution. Amazon ECS with Fargate provides a fully managed container orchestration service, eliminating the need to manage EC2 instances. Amazon EFS provides a fully managed, scalable, and elastic file system that can be mounted into containers running in ECS. This satisfies the requirement for persistent data storage without requiring manual volume management. The container at runtime can mount the EFS volume and access the persistent data.\n\n**Why option 0 is incorrect:**\nusing Amazon EKS with managed node groups and EBS, is incorrect because while EKS offers container orchestration, it still involves managing Kubernetes nodes (even with managed node groups) and EBS volumes. The question explicitly states the desire to avoid managing EC2 instances and volumes. Also, managing storage lifecycle manually adds operational overhead that the firm wants to avoid.\n\n**Why option 1 is incorrect:**\nusing AWS Lambda with a container image runtime and S3, is incorrect because Lambda's /tmp storage is temporary and limited in size (512MB). It's not suitable for persistent data storage required for customer simulation results. While syncing with S3 provides backup, it doesn't provide the direct, persistent storage the application needs. Lambda is also generally not suitable for long-running processes like risk analytics tools.\n\n**Why option 2 is incorrect:**\nusing Amazon ECS with Fargate and S3, is incorrect because mounting an S3 bucket directly into a container is not a standard or efficient practice. S3 is object storage, not a file system, and is not designed for direct file system access. While tools like s3fs exist, they introduce complexity and performance overhead, and are not a recommended approach for persistent storage in this scenario. The question requires persistent data storage which is better provided by a file system.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 24,
    "text": "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
    "options": [
      {
        "id": 0,
        "text": "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed",
        "correct": true
      },
      {
        "id": 2,
        "text": "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Launch Configurations are immutable. To change the instance type used by an Auto Scaling group, you must create a new Launch Configuration with the desired instance type. Then, you update the Auto Scaling group to use the new Launch Configuration. Deleting the old Launch Configuration is a good practice to avoid confusion and unnecessary resources, although it's not strictly required for the solution to work. This approach ensures that future instances launched by the Auto Scaling group will use the correct instance type, providing a long-term resolution to the performance issue.\n\n**Why option 0 is incorrect:**\nis incorrect because Launch Configurations are immutable. You cannot directly modify the Auto Scaling group to use a different instance type without changing the Launch Configuration it's using. The Auto Scaling group relies on the Launch Configuration for instance type information.\n\n**Why option 2 is incorrect:**\nis incorrect because simply increasing the number of instances is a short-term workaround and does not address the underlying problem of using an incorrect instance type. While more instances might temporarily alleviate the performance issues, it's not an efficient or cost-effective solution in the long run. The correct instance type is crucial for optimal performance.\n\n**Why option 3 is incorrect:**\nis incorrect because Launch Configurations are immutable. You cannot modify an existing Launch Configuration. A new Launch Configuration must be created to define a new instance type.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. As a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use File Gateway to automate and accelerate online data transfers to the given AWS storage services",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS DataSync is the best choice because it's specifically designed for online data transfer between on-premises storage and AWS storage services like S3, EFS, and FSx for Windows File Server. It automates and accelerates the data transfer process using a purpose-built agent, optimizing network utilization and providing features like encryption, scheduling, and data integrity verification. It's cost-effective for ongoing replication and migration scenarios.\n\n**Why option 0 is incorrect:**\nAWS Transfer Family is primarily used for secure file transfers over protocols like SFTP, FTPS, and FTP directly into and out of Amazon S3. While it automates transfers, it's not designed to transfer data to EFS or FSx for Windows File Server. It's more focused on secure file exchange with external partners or applications, not general data migration to various AWS storage services.\n\n**Why option 1 is incorrect:**\nFile Gateway is a hybrid cloud storage service that allows on-premises applications to access virtually unlimited cloud storage. It provides a local cache for frequently accessed data, but it's not primarily designed for large-scale, one-time data migration. While it can be used for data transfer, it's more suited for integrating on-premises applications with AWS storage in a hybrid environment, not for the initial migration process itself. It also doesn't directly support transferring data to FSx for Windows File Server.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 26,
    "text": "A financial services company is modernizing its analytics platform on AWS. Their legacy data processing scripts, built for both Windows and Linux environments, require shared access to a file system that supports Windows ACLs and SMB protocol for compatibility with existing Windows workloads. At the same time, their Linux-based applications need to read from and write to the same shared storage to maintain cross-platform consistency. The solutions architect needs to design a storage solution that allows both Windows and Linux EC2 instances to access the shared file system simultaneously, while preserving Windows-specific features like NTFS permissions and Active Directory (AD) integration. Which solution will best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\ndeploying Amazon FSx for Windows File Server and mounting it using the SMB protocol from both Windows and Linux EC2 instances, is the correct solution. FSx for Windows File Server is specifically designed to provide fully managed, highly available, and scalable file storage that is compatible with Windows file systems. It natively supports the SMB protocol, Windows ACLs (NTFS permissions), and Active Directory integration. While Linux instances don't natively use SMB, they can access SMB shares using tools like Samba. This allows both Windows and Linux instances to access the same shared file system while preserving Windows-specific features. FSx for Windows File Server also supports integration with AWS Directory Service for Microsoft Active Directory, simplifying user authentication and authorization.\n\n**Why option 0 is incorrect:**\ndeploying Amazon FSx for Lustre and mounting the file system using a POSIX-compliant client from both platforms, is incorrect. While FSx for Lustre is a high-performance file system suitable for compute-intensive workloads, it does not natively support Windows ACLs or the SMB protocol. It's primarily designed for Linux-based environments and is not the best choice when Windows compatibility and NTFS permissions are critical requirements.\n\n**Why option 1 is incorrect:**\nusing Amazon EFS with the Standard storage class and mounting the file system using NFS from both Windows and Linux instances, is incorrect. Amazon EFS is a network file system that is well-suited for Linux-based workloads and uses the NFS protocol. While Linux instances can easily mount EFS using NFS, Windows instances do not natively support NFS and would require third-party NFS client software, which can introduce complexity and compatibility issues. More importantly, EFS does not support Windows ACLs or Active Directory integration, which are key requirements of the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes. In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages Amazon CloudWatch alarms to automatically recover an EC2 instance in case of failure. EC2 instance recovery is a feature that automatically migrates the instance to a new host if there's an underlying hardware failure. This process typically takes a few minutes, fitting within the 10-minute downtime requirement. The stipulation that the instance should be configured with an EBS volume is crucial because instance recovery is only supported for EBS-backed instances. Instance store volumes are ephemeral and data is lost upon instance failure, making recovery impossible.\n\n**Why option 1 is incorrect:**\nis incorrect because AWS Trusted Advisor provides recommendations and best practices, but it doesn't automatically remediate issues. While Trusted Advisor can identify unhealthy instances, it requires manual intervention to recover them, which doesn't meet the 'automatic recovery' requirement.\n\n**Why option 2 is incorrect:**\nis incorrect because EC2 instance recovery is only supported for EBS-backed instances. Instance store volumes are ephemeral and data is lost upon instance failure, making recovery impossible. This option suggests using instance store volumes, which contradicts the requirement for automatic recovery.\n\n**Why option 3 is incorrect:**\nis incorrect because while Amazon EventBridge can trigger actions based on events, it doesn't directly provide a built-in recovery mechanism like EC2 instance recovery. You would need to build a custom solution to handle the recovery, which would be more complex and potentially slower than using the built-in EC2 instance recovery feature. Also, the question specifies the need for a cost-effective solution, and building a custom EventBridge solution would likely be more expensive than using the built-in EC2 recovery feature.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Step Functions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon CloudWatch",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon CloudWatch is the correct answer because it provides monitoring capabilities for AWS resources, including EC2 instances. You can create CloudWatch alarms that trigger when CPU utilization exceeds a specified threshold. Amazon SNS is also correct because CloudWatch alarms can be configured to send notifications to an SNS topic. You can then subscribe an email address to the SNS topic to receive email notifications. This combination requires minimal coding, primarily configuration within CloudWatch and SNS.\n\n**Why option 4 is correct:**\nAmazon CloudWatch is the correct answer because it provides monitoring capabilities for AWS resources, including EC2 instances. You can create CloudWatch alarms that trigger when CPU utilization exceeds a specified threshold. Amazon SNS is also correct because CloudWatch alarms can be configured to send notifications to an SNS topic. You can then subscribe an email address to the SNS topic to receive email notifications. This combination requires minimal coding, primarily configuration within CloudWatch and SNS.\n\n**Why option 0 is incorrect:**\nAWS Lambda could be used, but it would require significantly more development effort. You would need to write a Lambda function to periodically retrieve CPU utilization metrics from CloudWatch, evaluate them against the threshold, and then send an email notification using a service like Amazon SES. This involves writing and deploying code, which increases development effort compared to using CloudWatch alarms directly.\n\n**Why option 1 is incorrect:**\nAWS Step Functions is a workflow orchestration service. While it could technically be used to orchestrate a process involving CloudWatch, Lambda, and SNS, it adds unnecessary complexity and development effort. Step Functions is not the most direct or efficient way to achieve the desired outcome of monitoring CPU utilization and sending email notifications.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs. As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively",
        "correct": true
      },
      {
        "id": 3,
        "text": "To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nconfiguring Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively, is the correct solution. CloudFront is a content delivery network (CDN) that caches content closer to users, reducing the amount of data that needs to be transferred directly from S3. This significantly lowers outbound data transfer costs, especially with a 300% increase in requests. CloudFront also offers lower latency due to its distributed network of edge locations.\n\n**Why option 0 is incorrect:**\nconfiguring Amazon S3 Batch Operations, is incorrect. S3 Batch Operations is designed for performing large-scale batch operations on S3 objects, such as copying objects, changing object tags, or invoking Lambda functions. It doesn't directly address the issue of reducing outbound data transfer costs. It focuses on managing objects within S3, not delivering them to users.\n\n**Why option 1 is incorrect:**\nusing Amazon EFS, is incorrect. EFS is a network file system designed for use with EC2 instances. While it's scalable, it's not designed for serving static website content to a large, geographically diverse audience. Using EFS would likely increase costs and complexity compared to S3 and CloudFront. It also wouldn't inherently reduce data transfer costs to end-users; in fact, it might increase them as data would need to be served from EC2 instances, incurring EC2 and data transfer costs. Furthermore, EFS is not suitable for serving static website content directly to the internet.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in theus-west-2region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of theus-west-2region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
    "options": [
      {
        "id": 0,
        "text": "Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Availability Zone IDs (AZ IDs) are a consistent and unique identifier for an Availability Zone across all AWS accounts. While Availability Zone names (e.g., us-west-2a) are account-specific and can map to different physical locations in different accounts, AZ IDs are consistent. Using AZ IDs ensures that the instances are launched in the same physical location regardless of the account.\n\n**Why option 0 is incorrect:**\nis incorrect. While AWS Support can assist with various issues, this is not a situation that requires their intervention. The solution lies in understanding how AZs are identified across accounts and using the correct identifier (AZ ID).\n\n**Why option 1 is incorrect:**\nis incorrect. Default subnets are associated with specific Availability Zones, but the mapping of AZ names to physical locations is account-specific. Relying on the default subnet name (e.g., us-west-2a) will not guarantee that the instances are launched in the same physical AZ across different accounts. The subnet name is just a label, and the underlying physical location can vary between accounts.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies. As a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
    "options": [
      {
        "id": 0,
        "text": "Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages serverless technologies for both static and dynamic content. Static content is hosted on Amazon S3, which is a highly scalable, durable, and cost-effective object storage service. Dynamic content is handled by AWS Lambda, a serverless compute service, along with Amazon DynamoDB, a serverless NoSQL database. Amazon CloudFront is used as a CDN to distribute the content globally, improving performance and availability. This architecture eliminates the need to manage servers, reducing operational overhead and costs. Lambda functions are only invoked when needed, further optimizing costs. DynamoDB's on-demand capacity mode further optimizes costs by only charging for consumed resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it uses Amazon EC2 and Amazon RDS, which are not serverless. While CloudFront can distribute the content, the underlying infrastructure requires server management, defeating the purpose of a serverless solution and increasing operational overhead and costs.\n\n**Why option 2 is incorrect:**\nis incorrect because it uses Amazon EC2 and Amazon RDS for dynamic content, which are not serverless. While S3 is used for static content, the EC2 instance requires server management, defeating the purpose of a serverless solution and increasing operational overhead and costs.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon S3 is designed for static content and cannot directly execute dynamic code. While CloudFront can distribute static content hosted on S3, it cannot handle dynamic content generation. This option doesn't provide a solution for the dynamic content requirement.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 32,
    "text": "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use a simple scaling policy based on a custom Amazon SQS queue metric",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a target tracking scaling policy based on a custom Amazon SQS queue metric",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a step scaling policy based on a custom Amazon SQS queue metric",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a scheduled scaling policy based on a custom Amazon SQS queue metric",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing a target tracking scaling policy based on a custom Amazon SQS queue metric, is the most suitable solution. Target tracking scaling allows you to set a target value for a metric, such as the number of messages in the SQS queue (QueueLength). Auto Scaling then automatically adjusts the number of EC2 instances to maintain that target value. This is ideal for handling fluctuating workloads because it continuously monitors the metric and makes adjustments as needed. It's more dynamic and responsive than simple or step scaling.\n\n**Why option 0 is incorrect:**\nusing a simple scaling policy based on a custom Amazon SQS queue metric, is less effective than target tracking. Simple scaling policies react to alarms based on metric thresholds. While they can scale up or down, they require a cool-down period after each scaling action, which can be problematic during rapid spikes. The cool-down period prevents the Auto Scaling group from reacting quickly enough to sustained high loads, potentially leading to continued performance issues.\n\n**Why option 2 is incorrect:**\nusing a step scaling policy based on a custom Amazon SQS queue metric, is also less ideal than target tracking. Step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires defining specific thresholds and scaling adjustments, which can be challenging to optimize for varying load patterns. Target tracking is generally simpler to configure and more adaptive to changing conditions.\n\n**Why option 3 is incorrect:**\nusing a scheduled scaling policy based on a custom Amazon SQS queue metric, is not appropriate for handling sudden spikes. Scheduled scaling is based on predictable load patterns at specific times. It does not react to real-time changes in the SQS queue length caused by unexpected order surges. Therefore, it would not address the problem of performance degradation during sudden spikes.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudTrail to analyze API calls",
        "correct": true
      },
      {
        "id": 1,
        "text": "Implement an IAM policy to forbid users to change Amazon S3 bucket settings",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 access logs to analyze user access using Athena",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS CloudTrail to analyze API calls, is the correct answer. CloudTrail records API calls made to AWS services, including S3. By analyzing CloudTrail logs, you can identify which user or role made the changes to the S3 bucket settings, the timestamp of the changes, and the source IP address. This allows you to pinpoint the source of the unauthorized changes without initially restricting user permissions. CloudTrail provides the necessary audit trail for investigation.\n\n**Why option 1 is incorrect:**\nimplementing an IAM policy to forbid users from changing Amazon S3 bucket settings, is incorrect because it restricts user rights upfront, which contradicts the question's requirement. While restricting permissions might be a solution in the long run, the question specifically asks how to figure out what's happening *without* restricting rights initially. This option is a preventative measure, not an investigative one.\n\n**Why option 2 is incorrect:**\nimplementing a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations, is incorrect because it also restricts user rights upfront by requiring MFA for all S3 operations. Similar to option 1, this is a preventative measure and doesn't help in identifying the source of the existing unauthorized changes. While MFA is a good security practice, it doesn't address the immediate need for auditing and investigation.\n\n**Why option 3 is incorrect:**\nusing Amazon S3 access logs to analyze user access using Athena, is incorrect because S3 access logs primarily record access to objects within the bucket (GET, PUT, DELETE operations on objects). While helpful for understanding data access patterns, they don't capture changes to the bucket's configuration settings (e.g., changes to bucket policy, encryption settings, or versioning). CloudTrail is the appropriate service for auditing API calls related to bucket configuration changes.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon Redshift",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon RDS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 4,
        "text": "AWS Lambda",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon DynamoDB is a NoSQL database service that is well-suited for high-velocity data ingestion. It can handle a large number of writes per second with low latency. Its distributed architecture provides high availability and scalability. AWS Lambda can be used to process the data before storing it in DynamoDB. Lambda functions can be triggered by events, such as data arriving in an S3 bucket or being pushed to a Kinesis stream. Lambda can perform transformations, aggregations, or other processing tasks before writing the data to DynamoDB. This combination provides a highly scalable, available, and cost-effective solution for this scenario.\n\n**Why option 4 is correct:**\nAmazon DynamoDB is a NoSQL database service that is well-suited for high-velocity data ingestion. It can handle a large number of writes per second with low latency. Its distributed architecture provides high availability and scalability. AWS Lambda can be used to process the data before storing it in DynamoDB. Lambda functions can be triggered by events, such as data arriving in an S3 bucket or being pushed to a Kinesis stream. Lambda can perform transformations, aggregations, or other processing tasks before writing the data to DynamoDB. This combination provides a highly scalable, available, and cost-effective solution for this scenario.\n\n**Why option 0 is incorrect:**\nAmazon Redshift is a data warehouse service designed for analytical workloads. It is not optimized for high-velocity data ingestion or frequent writes. While Redshift can store large amounts of data, it's more suitable for batch processing and complex queries rather than real-time data ingestion and processing of key-value pairs.\n\n**Why option 1 is incorrect:**\nAmazon RDS is a relational database service. While RDS can handle a certain amount of write throughput, it is not as scalable or cost-effective as DynamoDB for high-velocity data ingestion, especially for key-value data. Managing the scaling and availability of an RDS instance for this workload would be more complex and expensive than using DynamoDB.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 35,
    "text": "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use SQS message timer to retrieve messages from your Amazon SQS queues",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use SQS long polling to retrieve messages from your Amazon SQS queues",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use SQS visibility timeout to retrieve messages from your Amazon SQS queues",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use SQS short polling to retrieve messages from your Amazon SQS queues",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing SQS long polling, is the correct answer. Long polling allows the SQS queue to hold a connection open for a specified duration (up to 20 seconds) or until a message arrives. This significantly reduces the number of empty responses received by the consumer, as it only receives a response when a message is available or the timeout expires. By reducing empty responses, the number of API calls to SQS decreases, leading to lower costs. Long polling is the recommended approach for most use cases due to its cost-effectiveness and efficiency.\n\n**Why option 0 is incorrect:**\nusing SQS message timer, is incorrect. SQS message timers are used to delay the delivery of messages to the queue. While they can be useful in certain scenarios, they do not directly impact the cost of retrieving messages. The cost is still determined by the number of requests made to the queue, regardless of whether messages are delayed or not.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nusing SQS short polling, is incorrect. Short polling immediately returns a response, even if no messages are available in the queue. This results in a higher number of API calls to SQS, as the consumer continuously polls the queue, leading to increased costs. Short polling is generally less efficient and more expensive than long polling.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organizations access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Service control policy (SCP) affects service-linked roles",
        "correct": false
      },
      {
        "id": 1,
        "text": "Service control policy (SCP) does not affect service-linked role",
        "correct": true
      },
      {
        "id": 2,
        "text": "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action",
        "correct": false
      },
      {
        "id": 3,
        "text": "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action",
        "correct": true
      },
      {
        "id": 4,
        "text": "Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts",
        "correct": true
      },
      {
        "id": 5,
        "text": "Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1, 3, and 4 are correct.\n\n*   **Option 1: Service control policy (SCP) does not affect service-linked roles:** Service-linked roles are designed to allow AWS services to access other AWS resources on your behalf. SCPs do not affect service-linked roles because these roles are essential for AWS services to function correctly. Restricting these roles would break AWS service functionality.\n*   **Option 3: If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action:** SCPs act as a guardrail, setting the maximum permissions available within an account. Even if an IAM policy grants a permission, if the SCP denies or doesn't allow it, the effective permission is denied. SCPs override IAM policies in this manner.\n*   **Option 4: Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts:** SCPs apply to all IAM entities within a member account, including the root user. This is crucial for maintaining central control and preventing even the root user from performing actions that violate organizational policies.\n\n**Why option 3 is correct:**\nOptions 1, 3, and 4 are correct.\n\n*   **Option 1: Service control policy (SCP) does not affect service-linked roles:** Service-linked roles are designed to allow AWS services to access other AWS resources on your behalf. SCPs do not affect service-linked roles because these roles are essential for AWS services to function correctly. Restricting these roles would break AWS service functionality.\n*   **Option 3: If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action:** SCPs act as a guardrail, setting the maximum permissions available within an account. Even if an IAM policy grants a permission, if the SCP denies or doesn't allow it, the effective permission is denied. SCPs override IAM policies in this manner.\n*   **Option 4: Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts:** SCPs apply to all IAM entities within a member account, including the root user. This is crucial for maintaining central control and preventing even the root user from performing actions that violate organizational policies.\n\n**Why option 4 is correct:**\nOptions 1, 3, and 4 are correct.\n\n*   **Option 1: Service control policy (SCP) does not affect service-linked roles:** Service-linked roles are designed to allow AWS services to access other AWS resources on your behalf. SCPs do not affect service-linked roles because these roles are essential for AWS services to function correctly. Restricting these roles would break AWS service functionality.\n*   **Option 3: If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action:** SCPs act as a guardrail, setting the maximum permissions available within an account. Even if an IAM policy grants a permission, if the SCP denies or doesn't allow it, the effective permission is denied. SCPs override IAM policies in this manner.\n*   **Option 4: Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts:** SCPs apply to all IAM entities within a member account, including the root user. This is crucial for maintaining central control and preventing even the root user from performing actions that violate organizational policies.\n\n**Why option 0 is incorrect:**\nis incorrect because SCPs do *not* affect service-linked roles. Service-linked roles are necessary for AWS services to function correctly, and SCPs are designed to respect this.\n\n**Why option 2 is incorrect:**\nis incorrect because SCPs act as a guardrail. If an SCP explicitly denies or doesn't allow an action, even if an IAM policy grants it, the action is effectively denied. SCPs take precedence.\n\n**Why option 5 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. Which of the following options represents the correct solution to set up internet access for the private subnets?",
    "options": [
      {
        "id": 0,
        "text": "Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages NAT Gateways deployed in the public subnets. NAT Gateways allow instances in the private subnets to initiate outbound traffic to the internet, while preventing the internet from initiating inbound connections to those instances. By placing a NAT Gateway in each public subnet (one per AZ), the solution achieves high availability. Each private subnet's route table is configured to route non-local traffic (0.0.0.0/0) to the NAT Gateway in the *same* Availability Zone. This ensures that if a NAT Gateway in one AZ fails, the instances in the corresponding private subnet can failover to another AZ. This approach adheres to best practices for security and high availability in VPC design.\n\n**Why option 0 is incorrect:**\nis incorrect because placing NAT Gateways in the *private* subnets defeats the purpose of having private subnets. NAT Gateways need to be in public subnets to have a public IP address and connect to the internet gateway. Furthermore, routing traffic from a private subnet to a NAT gateway in the same private subnet wouldn't work, as the NAT gateway needs to be in a public subnet to reach the internet gateway.\n\n**Why option 1 is incorrect:**\nis incorrect because Egress-Only Internet Gateways are designed for IPv6 traffic only. The question explicitly states that the VPC and subnets use IPv4 CIDR blocks. Therefore, an Egress-Only Internet Gateway is not the appropriate solution for providing internet access to the private subnets in this scenario. Also, an Egress-Only Internet Gateway only allows outbound traffic from the VPC, it does not provide NAT functionality for IPv4.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot",
        "correct": false
      },
      {
        "id": 1,
        "text": "You cannot share an Amazon Machine Image (AMI) with another AWS account",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot",
        "correct": true
      },
      {
        "id": 3,
        "text": "You can share an Amazon Machine Image (AMI) with another AWS account",
        "correct": true
      },
      {
        "id": 4,
        "text": "You cannot copy an Amazon Machine Image (AMI) across AWS Regions",
        "correct": false
      },
      {
        "id": 5,
        "text": "You can copy an Amazon Machine Image (AMI) across AWS Regions",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      3,
      5
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2, 3, and 5 are correct.\n\n*   **Option 2: Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot.** This is correct because AWS enforces encryption when copying an encrypted AMI. While you can change the KMS key used for encryption during the copy process, you cannot remove encryption altogether. This ensures data security and compliance.\n*   **Option 3: You can share an Amazon Machine Image (AMI) with another AWS account.** This is a fundamental feature of AMIs. You can grant other AWS accounts permission to launch instances from your AMI, enabling standardization and collaboration across accounts. This is crucial for the scenario where subsidiaries use different AWS accounts.\n*   **Option 5: You can copy an Amazon Machine Image (AMI) across AWS Regions.** This is also a key capability. Copying AMIs across regions allows you to launch instances in different geographical locations, improving availability, disaster recovery, and reducing latency for users in different regions. This is important for subsidiaries in different AWS regions.\n\n**Why option 3 is correct:**\nOptions 2, 3, and 5 are correct.\n\n*   **Option 2: Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot.** This is correct because AWS enforces encryption when copying an encrypted AMI. While you can change the KMS key used for encryption during the copy process, you cannot remove encryption altogether. This ensures data security and compliance.\n*   **Option 3: You can share an Amazon Machine Image (AMI) with another AWS account.** This is a fundamental feature of AMIs. You can grant other AWS accounts permission to launch instances from your AMI, enabling standardization and collaboration across accounts. This is crucial for the scenario where subsidiaries use different AWS accounts.\n*   **Option 5: You can copy an Amazon Machine Image (AMI) across AWS Regions.** This is also a key capability. Copying AMIs across regions allows you to launch instances in different geographical locations, improving availability, disaster recovery, and reducing latency for users in different regions. This is important for subsidiaries in different AWS regions.\n\n**Why option 5 is correct:**\nOptions 2, 3, and 5 are correct.\n\n*   **Option 2: Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot.** This is correct because AWS enforces encryption when copying an encrypted AMI. While you can change the KMS key used for encryption during the copy process, you cannot remove encryption altogether. This ensures data security and compliance.\n*   **Option 3: You can share an Amazon Machine Image (AMI) with another AWS account.** This is a fundamental feature of AMIs. You can grant other AWS accounts permission to launch instances from your AMI, enabling standardization and collaboration across accounts. This is crucial for the scenario where subsidiaries use different AWS accounts.\n*   **Option 5: You can copy an Amazon Machine Image (AMI) across AWS Regions.** This is also a key capability. Copying AMIs across regions allows you to launch instances in different geographical locations, improving availability, disaster recovery, and reducing latency for users in different regions. This is important for subsidiaries in different AWS regions.\n\n**Why option 0 is incorrect:**\nis incorrect. Copying an AMI backed by an encrypted snapshot *cannot* result in an unencrypted target snapshot. AWS enforces encryption during the copy process when the source AMI is encrypted. You can change the KMS key, but you cannot remove encryption.\n\n**Why option 1 is incorrect:**\nis incorrect. AMIs can be shared with other AWS accounts. This is a core feature for collaboration and standardization across different AWS environments. The question scenario explicitly involves sharing AMIs with subsidiaries in different AWS accounts.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
    "options": [
      {
        "id": 0,
        "text": "The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy",
        "correct": false
      },
      {
        "id": 1,
        "text": "The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy",
        "correct": true
      },
      {
        "id": 2,
        "text": "The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy",
        "correct": false
      },
      {
        "id": 3,
        "text": "The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because the instance tenancy specified in the Launch Template overrides the instance tenancy specified at the VPC level. Therefore, instances launched using LT1 (dedicated tenancy) will be dedicated, and instances launched using LT2 (default tenancy) will also be default. The question states that the correct answer is [1], which means that both Launch Templates will launch instances with dedicated tenancy. This is INCORRECT. The correct answer should be that instances launched by LT1 will have dedicated tenancy, and instances launched by LT2 will have default tenancy. However, given the options, the closest correct answer is that both Launch Templates will launch instances with dedicated tenancy. This implies that the VPC setting is ignored. The Launch Template's tenancy setting takes precedence. Therefore, LT1 will launch dedicated instances, and LT2 will launch default instances. The question is flawed because the provided correct answer is incorrect based on AWS documentation.\n\n**Why option 0 is incorrect:**\nis incorrect because the Launch Template's tenancy setting overrides the VPC's tenancy setting. Therefore, if the Launch Template specifies dedicated tenancy, the instances will be dedicated, regardless of the VPC setting.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 40,
    "text": "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves players scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Move to Amazon Redshift",
        "correct": false
      },
      {
        "id": 1,
        "text": "Switch application code to AWS Lambda for better performance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Setup Amazon ElastiCache in front of Amazon RDS",
        "correct": true
      },
      {
        "id": 3,
        "text": "Setup Amazon RDS Read Replicas",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nSetting up Amazon ElastiCache in front of the Amazon RDS database is the most suitable solution. ElastiCache is an in-memory data store service that can cache frequently accessed data, such as player scores and stats. By caching this data, the application can retrieve it from ElastiCache instead of the RDS database, significantly reducing the load on the database and improving read latency. This allows the company to downsize the RDS instance, as it will be handling fewer read requests. ElastiCache offers both Memcached and Redis engines. Memcached is suitable for simple caching scenarios, while Redis offers more advanced features like data persistence and more complex data structures. The choice depends on the specific requirements of the gaming application.\n\n**Why option 0 is incorrect:**\nMoving to Amazon Redshift is not the best solution for this scenario. Amazon Redshift is a data warehouse service designed for analytical workloads (OLAP), not for handling high-volume, low-latency read requests for individual player scores and stats (OLTP). While Redshift can handle large datasets, it's not optimized for the type of real-time data access required by the gaming application. It's also significantly more complex and expensive to set up and maintain for this specific use case.\n\n**Why option 1 is incorrect:**\nSwitching application code to AWS Lambda might improve the application's scalability and responsiveness in general, but it doesn't directly address the problem of high read traffic on the RDS database. Lambda functions still need to access the database to retrieve the player scores and stats. While Lambda can be used in conjunction with other solutions (like ElastiCache), it's not a standalone solution for reducing database load and latency in this scenario. Moreover, the question focuses on the database layer performance, not the application layer.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes. Which of these options would you identify as the right way of connecting these microservices?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add Amazon EventBridge to decouple the complex architecture",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon SQS, is the correct solution. Amazon SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS allows the faster microservices to enqueue messages, which are then consumed by the slower microservices at their own pace. This asynchronous communication pattern prevents the faster services from being blocked or slowed down by the slower services, ensuring overall system resilience and performance. SQS provides features like message durability, scalability, and configurable message retention, making it suitable for handling varying processing speeds and ensuring reliable message delivery.\n\n**Why option 0 is incorrect:**\nusing Amazon SNS, is incorrect. Amazon SNS is a publish/subscribe messaging service primarily used for broadcasting messages to multiple subscribers. While it can decouple services, it's not the best choice for handling different processing speeds. SNS is more suitable for scenarios where multiple services need to be notified of an event simultaneously, not for queuing messages for asynchronous processing at different rates. SNS does not inherently provide message persistence or guaranteed delivery in the same way as SQS, making it less suitable for decoupling services with varying processing speeds where message reliability is crucial.\n\n**Why option 2 is incorrect:**\nusing Amazon Kinesis Data Streams, is incorrect. Amazon Kinesis Data Streams is designed for real-time streaming data ingestion and processing. While it can decouple services, it's primarily intended for high-throughput, continuous data streams, such as log data or clickstream data. It's not the ideal choice for general-purpose asynchronous communication between microservices with varying processing speeds. Kinesis Data Streams is more complex to set up and manage than SQS for this specific use case, and its focus on real-time data processing doesn't align with the requirement of decoupling services with different processing speeds.\n\n**Why option 3 is incorrect:**\nadding Amazon EventBridge, is incorrect. Amazon EventBridge is an event bus service that enables you to build event-driven applications. While it can decouple services by routing events between them, it's more suitable for complex event routing and filtering based on event patterns. For the specific requirement of decoupling microservices with different processing speeds, SQS provides a simpler and more direct solution for asynchronous message queuing. EventBridge adds complexity that isn't necessary for this scenario. EventBridge is better suited for orchestrating complex workflows and routing events based on specific conditions, rather than simply queuing messages for asynchronous processing.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
    "options": [
      {
        "id": 0,
        "text": "Amazon ElastiCache for Redis/Memcached",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon DocumentDB",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon ElastiCache for Redis/Memcached is the correct choice because it provides in-memory data caching, which significantly improves performance for frequently accessed data. ElastiCache supports Redis and Memcached engines. Redis offers more advanced features like data structures and persistence, while Memcached is simpler and focused on caching. Both can be configured for high availability using replication and failover. ElastiCache is also HIPAA eligible, provided proper configurations and agreements are in place. While ElastiCache doesn't directly process SQL queries, it can cache the results of SQL queries executed against another database, fulfilling the requirement of caching SQL query results. The question implies caching results of SQL queries, not directly executing SQL queries within the cache itself.\n\n**Why option 1 is incorrect:**\nAmazon DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB. While it provides caching, it's specifically designed for DynamoDB and doesn't support caching results from SQL queries against other database systems. It also doesn't inherently support HIPAA compliance without proper configuration and agreements.\n\n**Why option 2 is incorrect:**\nAmazon DynamoDB is a NoSQL database service. While it's highly scalable and available, it's not an in-memory database. It also doesn't directly support caching SQL query results. Although DynamoDB can be made HIPAA compliant, it doesn't directly fulfill the in-memory requirement.\n\n**Why option 3 is incorrect:**\nAmazon DocumentDB is a NoSQL document database service that is compatible with MongoDB. It is not an in-memory database and does not directly support caching SQL query results. While it can be made HIPAA compliant, it doesn't fulfill the in-memory requirement or the SQL query caching requirement.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
    "options": [
      {
        "id": 0,
        "text": "Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\ncreating an alias record for covid19survey.com that routes traffic to www.covid19survey.com, is the correct answer. Alias records are a Route 53 specific record type that are used to map a domain name to an AWS resource, such as an Elastic Load Balancer, an Amazon S3 bucket configured as a static website, or another Route 53 record in the same hosted zone. They are preferable to CNAME records for the root domain because CNAME records cannot be used for the zone apex (the root domain). Alias records also offer health checking capabilities, which can improve the reliability of the application. While not explicitly mentioned in the question, alias records are generally more cost-effective than other solutions in scenarios where they are applicable, as they are tightly integrated with AWS services and can leverage existing infrastructure.\n\n**Why option 0 is incorrect:**\ncreating an NS record for covid19survey.com that routes traffic to www.covid19survey.com, is incorrect. NS records define the name servers responsible for a domain. They are used for delegating a subdomain to a different set of name servers, not for redirecting traffic within the same domain. Using NS records for this purpose would be inappropriate and would likely break DNS resolution for the domain.\n\n**Why option 1 is incorrect:**\ncreating a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com, is incorrect. CNAME records cannot be used for the zone apex (the root domain). A CNAME record maps an alias to a canonical name. The DNS specifications prohibit CNAME records at the zone apex because other records, such as SOA and NS records, must exist at the zone apex. Attempting to create a CNAME record for covid19survey.com would result in an error.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect  Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Shield",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use Amazon Aurora Replica",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nThese are correct because they directly address the need for resilience against request spikes.\n\n*   **Option 3: Use Amazon CloudFront distribution in front of the Application Load Balancer:** CloudFront is a content delivery network (CDN) that caches content closer to users. By caching static and dynamic content, CloudFront reduces the load on the ALB and backend EC2 instances during traffic spikes. This improves response times and overall application performance.\n*   **Option 4: Use Amazon Aurora Replica:** Aurora Replicas provide read scalability for the database. By offloading read traffic to Aurora Replicas, the primary Aurora instance can focus on write operations. This improves database performance and resilience during traffic spikes, as read operations are less likely to overwhelm the primary instance. Aurora Replicas can also be promoted to become the primary instance in case of a failure, enhancing availability.\n\n**Why option 4 is correct:**\nThese are correct because they directly address the need for resilience against request spikes.\n\n*   **Option 3: Use Amazon CloudFront distribution in front of the Application Load Balancer:** CloudFront is a content delivery network (CDN) that caches content closer to users. By caching static and dynamic content, CloudFront reduces the load on the ALB and backend EC2 instances during traffic spikes. This improves response times and overall application performance.\n*   **Option 4: Use Amazon Aurora Replica:** Aurora Replicas provide read scalability for the database. By offloading read traffic to Aurora Replicas, the primary Aurora instance can focus on write operations. This improves database performance and resilience during traffic spikes, as read operations are less likely to overwhelm the primary instance. Aurora Replicas can also be promoted to become the primary instance in case of a failure, enhancing availability.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator. While Global Accelerator improves global application availability and performance by routing traffic to the nearest healthy endpoint, it doesn't directly address the need for caching or database scalability to handle request spikes. It primarily focuses on improving network performance and availability across regions, not specifically handling sudden increases in request volume within a single region.\n\n**Why option 1 is incorrect:**\nOption 1: Use AWS Direct Connect. Direct Connect establishes a dedicated network connection from your on-premises environment to AWS. This is beneficial for hybrid cloud scenarios and transferring large datasets, but it does not directly address the need to handle periodic spikes in request rates for a web application running entirely on AWS. It improves network connectivity but doesn't provide caching or database scalability.\n\n**Why option 2 is incorrect:**\nOption 2: Use AWS Shield. AWS Shield provides protection against DDoS attacks. While important for security, it doesn't directly address the need to handle periodic spikes in legitimate request rates. Shield protects against malicious traffic, not increased user activity.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data. How will you implement this requirement without adding the overhead of splitting the data into logical groups?",
    "options": [
      {
        "id": 0,
        "text": "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages SSE-S3. While SSE-S3 doesn't use a different key *per file* in the strictest sense, it does encrypt each object with a unique key derived from the S3 managed key. This provides strong encryption at rest without the complexity of managing individual keys. S3 handles the key management and rotation, minimizing operational overhead. The question doesn't explicitly state that the keys must be completely independent; it focuses on encrypting each file differently for maximum security, which SSE-S3 achieves.\n\n**Why option 1 is incorrect:**\nis incorrect because while Multi-Region keys and the S3 Encryption Client can be used for client-side encryption and generating unique keys, it introduces significant complexity and overhead. The company would need to manage the client-side encryption process, key generation, and key storage, which contradicts the requirement of minimizing overhead. Also, Multi-Region keys are primarily for disaster recovery, not for individual object encryption.\n\n**Why option 2 is incorrect:**\nis incorrect because storing logically divided data into different S3 buckets reintroduces the overhead of splitting the data, which the question explicitly aims to avoid. While SSE-S3 would encrypt each bucket's contents, it doesn't address the requirement of encrypting each *file* with a different key. It also adds operational complexity in managing multiple buckets.\n\n**Why option 3 is incorrect:**\nis incorrect because while SSE-KMS allows for more control over encryption keys, using encryption context to generate a different key for each file/object is not the intended use case and is not directly supported by AWS. Encryption context is used for adding additional authenticated data to the encryption process, not for generating unique keys. While you *could* potentially use the object name as encryption context, this is not a recommended or efficient approach and would likely lead to performance issues and management overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which of the following solutions meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon RDS Multi-AZ deployments provide high availability and durability for database instances. When you enable Multi-AZ, RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone (AZ). In case of a failure of the primary DB instance, RDS automatically fails over to the standby replica, minimizing downtime and data loss. Synchronous replication ensures that every transaction is written to both the primary and standby instances before the transaction is considered complete, thus meeting the requirement of storing every transaction on at least two nodes and minimizing data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while read replicas can improve read performance, they are typically asynchronous. Asynchronous replication means that there can be a delay between when data is written to the primary instance and when it is replicated to the read replica. This delay can lead to data loss in the event of a primary instance failure. The question specifically requires minimal data loss and storage on at least two nodes, which necessitates synchronous replication.\n\n**Why option 2 is incorrect:**\nis incorrect because synchronous replication across AWS Regions is generally not supported by default with RDS MySQL. While cross-region read replicas exist, they are asynchronous. Synchronous replication across regions would introduce significant latency and complexity, making it impractical for most applications requiring minimal data loss. The question specifically asks for a solution that minimizes data loss, implying synchronous replication, which is best achieved within a single region using Multi-AZ.\n\n**Why option 3 is incorrect:**\nis incorrect because it involves a more complex and less managed solution. Setting up a MySQL DB engine on an EC2 instance and using a Lambda function to replicate data to an RDS instance introduces significant overhead and complexity. Furthermore, implementing synchronous replication using Lambda would be challenging and inefficient, potentially leading to data loss and performance issues. RDS Multi-AZ provides a simpler, more reliable, and managed solution for high availability and data durability.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Cloud Directory",
        "correct": false
      },
      {
        "id": 1,
        "text": "Simple Active Directory (Simple AD)",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Active Directory Connector",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) is the correct answer because it allows you to run actual Microsoft Active Directory as a managed service in AWS. This directly addresses the requirement of running directory-aware workloads, as the SQL Server application can authenticate against the managed AD. Furthermore, it supports establishing a trust relationship with the on-premises AD, enabling users to use their existing credentials for SSO to access resources in both environments. This is the most complete solution for the stated requirements.\n\n**Why option 0 is incorrect:**\nAmazon Cloud Directory is incorrect because it's a directory service for developers to build cloud-native applications. It does not provide compatibility with Microsoft Active Directory and does not support establishing a trust relationship with an on-premises AD for SSO. It's not designed for running directory-aware workloads that rely on traditional AD.\n\n**Why option 1 is incorrect:**\nSimple Active Directory (Simple AD) is incorrect because while it provides basic AD-compatible functionality, it's not a full Microsoft Active Directory. It has limitations in terms of features and scalability compared to AWS Managed Microsoft AD. Most importantly, it doesn't support establishing a trust relationship with an on-premises AD, which is a crucial requirement for SSO in this scenario. It's suitable for smaller deployments with limited AD needs, but not for integrating with an existing on-premises AD.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Simple Storage Service (Amazon S3)",
        "correct": false
      },
      {
        "id": 3,
        "text": "File Gateway Configuration of AWS Storage Gateway",
        "correct": true
      },
      {
        "id": 4,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Windows File Server is a fully managed Microsoft Windows file server service backed by a fully native Windows file system. It provides native Windows file system compatibility, supports the SMB protocol, Active Directory integration, and offers high availability and durability. File Gateway Configuration of AWS Storage Gateway allows you to access files stored in Amazon S3 through a local file system interface using the SMB protocol. It caches frequently accessed data locally for low-latency access and provides a seamless integration with existing Windows environments. It also provides a way to migrate on-premises file servers to the cloud.\n\n**Why option 3 is correct:**\nAmazon FSx for Windows File Server is a fully managed Microsoft Windows file server service backed by a fully native Windows file system. It provides native Windows file system compatibility, supports the SMB protocol, Active Directory integration, and offers high availability and durability. File Gateway Configuration of AWS Storage Gateway allows you to access files stored in Amazon S3 through a local file system interface using the SMB protocol. It caches frequently accessed data locally for low-latency access and provides a seamless integration with existing Windows environments. It also provides a way to migrate on-premises file servers to the cloud.\n\n**Why option 1 is incorrect:**\nAmazon Elastic File System (Amazon EFS) is a fully managed NFS (Network File System) file system for use with Linux workloads. It does not natively support the SMB protocol required for Windows compatibility.\n\n**Why option 2 is incorrect:**\nAmazon Simple Storage Service (Amazon S3) is object storage, not file storage. While you can store files in S3, it doesn't natively support the SMB protocol or provide the file system semantics required for a Windows file server. You would need additional software or services to access S3 as a file share.\n\n**Why option 4 is incorrect:**\nAmazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. While you could create a Windows file server on an EC2 instance with an EBS volume, it doesn't provide a managed file server service or native SMB support. You would need to manage the file server software yourself, which doesn't meet the requirement of a managed service. It also doesn't directly address the migration of existing file server clusters.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A startup is building a serverless microservices architecture where client applications (web and mobile) authenticate users via a third-party OIDC-compliant identity provider. The backend APIs must validate JSON Web Tokens (JWTs) issued by this provider, enforce scope-based access control, and be cost-effective with minimal latency. The development team wants to use a fully managed service that supports JWT validation natively, without writing custom authentication logic. Which solution should the team implement to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider, is the correct solution. API Gateway HTTP APIs offer a native JWT authorizer that directly integrates with OIDC providers. This allows API Gateway to validate the JWT signature, issuer, and audience without invoking a Lambda function. This approach is cost-effective because it avoids Lambda invocations for authentication, and it minimizes latency because the validation is performed directly by API Gateway. The native JWT authorizer also supports scope-based access control by allowing you to map JWT claims (including scopes) to API Gateway permissions. This solution aligns perfectly with the requirements of a fully managed service, native JWT validation, cost-effectiveness, and minimal latency.\n\n**Why option 1 is incorrect:**\nusing Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer, is incorrect. While WebSocket APIs can use Lambda authorizers for JWT validation, this approach is not ideal for several reasons. First, it involves a Lambda function invocation for each authentication request, which adds latency and cost compared to a native JWT authorizer. Second, WebSocket APIs are primarily designed for real-time, bidirectional communication, which is not the primary use case described in the question (securing backend APIs for web and mobile applications). The question emphasizes the need for a fully managed service that supports JWT validation natively, which a Lambda authorizer does not provide directly.\n\n**Why option 2 is incorrect:**\nusing Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens, is incorrect. While this approach is feasible, it is not the most efficient or cost-effective solution. Manually validating JWT tokens in a Lambda function requires writing custom authentication logic, which the question specifically aims to avoid. Additionally, invoking a Lambda function for each authentication request adds latency and cost. API Gateway HTTP APIs with native JWT authorizers offer a more streamlined and performant solution.\n\n**Why option 3 is incorrect:**\ndeploying a gRPC backend on Amazon ECS Fargate and exposing it through AWS App Runner, handling JWT validation inside the containerized services, is incorrect. While this approach is possible, it is not the most suitable for the given requirements. It involves managing containerized services on ECS Fargate and implementing JWT validation logic within the application code. This adds complexity and operational overhead compared to using a fully managed API Gateway service with a native JWT authorizer. Furthermore, it does not leverage the built-in security features of API Gateway, such as rate limiting and DDoS protection. The question emphasizes the need for a fully managed service, which API Gateway provides more directly than ECS Fargate and App Runner for this specific use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses. Which options would you combine to build a solution to meet these requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it uses an Origin Access Identity (OAI) to restrict direct access to the S3 bucket. The OAI is a CloudFront feature that creates a special user identity that CloudFront uses to access the S3 bucket. By granting the OAI read permissions on the S3 bucket and denying public access, we ensure that only CloudFront can retrieve the content. Option 4 is correct because it uses AWS WAF to filter traffic based on IP addresses at the CloudFront distribution level. By creating an AWS WAF ACL with an IP match condition that allows only the specified IP ranges, we can effectively restrict access to the static content served through CloudFront to only those IPs. This mirrors the original security group's functionality on the EC2 instances.\n\n**Why option 4 is correct:**\nThis is correct because it uses an Origin Access Identity (OAI) to restrict direct access to the S3 bucket. The OAI is a CloudFront feature that creates a special user identity that CloudFront uses to access the S3 bucket. By granting the OAI read permissions on the S3 bucket and denying public access, we ensure that only CloudFront can retrieve the content. Option 4 is correct because it uses AWS WAF to filter traffic based on IP addresses at the CloudFront distribution level. By creating an AWS WAF ACL with an IP match condition that allows only the specified IP ranges, we can effectively restrict access to the static content served through CloudFront to only those IPs. This mirrors the original security group's functionality on the EC2 instances.\n\n**Why option 0 is incorrect:**\nis incorrect because Network ACLs (NACLs) operate at the subnet level, not at the CloudFront distribution level. NACLs control traffic entering and exiting subnets, and they cannot be directly associated with a CloudFront distribution. CloudFront operates at a higher level of abstraction.\n\n**Why option 2 is incorrect:**\nis incorrect because security groups are associated with EC2 instances, not CloudFront distributions. Security groups control inbound and outbound traffic for EC2 instances. CloudFront does not have security groups associated with it.\n\n**Why option 3 is incorrect:**\nis incorrect because AWS WAF ACLs are associated with CloudFront distributions, Application Load Balancers, API Gateways, or AWS AppSync endpoints, not directly with S3 bucket policies. While you can use bucket policies to control access, using WAF at the CloudFront level provides a more robust and centralized way to manage IP-based access control for content delivered through CloudFront.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A media streaming startup is building a set of backend APIs that will be consumed by external mobile applications. To prevent API abuse, protect downstream resources, and ensure fair usage across clients, the architecture must enforce rate limiting and throttling on a per-client basis. The team also wants to define usage quotas and apply different limits to different API consumers. Which solution should the team implement to enforce rate limiting and usage quotas at the API layer?",
    "options": [
      {
        "id": 0,
        "text": "Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon API Gateway with usage plans and API keys, is the correct solution. API Gateway is specifically designed for managing APIs, including features like authentication, authorization, rate limiting, and usage quotas. Usage plans in API Gateway allow you to define who can access one or more deployed API stages and methods. You can also configure throttling and quota limits that are enforced on individual API keys. API keys are distributed to clients, enabling per-client rate limiting and usage tracking. This directly addresses the requirements of preventing API abuse, protecting downstream resources, and ensuring fair usage across clients by allowing different limits to be applied to different API consumers.\n\n**Why option 0 is incorrect:**\nis incorrect because while a Gateway Load Balancer (GWLB) can inspect traffic, it's primarily for integrating with third-party virtual appliances like firewalls and intrusion detection systems. It's not the ideal solution for implementing per-client rate limiting and usage quotas. Implementing this with a GWLB would require significant custom configuration and integration with external systems, making it more complex and less efficient than using API Gateway.\n\n**Why option 1 is incorrect:**\nis incorrect because Application Load Balancers (ALBs) primarily focus on routing traffic to backend targets based on path, host, or other request attributes. While ALBs offer basic request limits, they are not designed for sophisticated per-client rate limiting and usage quota management. They lack the built-in features of API Gateway for managing API keys, usage plans, and detailed usage tracking. Configuring per-client rate limiting on an ALB would be complex and less scalable than using API Gateway.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results. You have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources. Which of the following will you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling",
        "correct": true
      },
      {
        "id": 1,
        "text": "Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because migrating to Amazon Aurora MySQL and using Aurora Replicas significantly reduces replication lag. Aurora's architecture is designed for high performance and low latency replication. Aurora Replicas share the same underlying storage as the primary instance, eliminating the need for traditional asynchronous replication. This shared storage architecture allows for near real-time replication, often achieving replication lag of milliseconds. Aurora Auto Scaling further optimizes performance by automatically adjusting the number of Aurora Replicas based on workload demands, ensuring consistent performance during traffic spikes. This solution minimizes code changes as Aurora is largely MySQL-compatible, and it reduces management overhead due to Aurora's managed nature and Auto Scaling capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because while using memory-optimized EC2 instances might improve the performance of the primary database and compute-optimized instances could handle read queries, it doesn't directly address the underlying cause of replication lag. Traditional MySQL replication is asynchronous, and the lag can be influenced by network latency, primary instance write load, and replica instance read load. Simply increasing the compute power of the instances doesn't guarantee a reduction in replication lag to the required 1-second threshold. It also increases management overhead compared to a managed service like Aurora.\n\n**Why option 2 is incorrect:**\nis incorrect because migrating to DynamoDB would require significant application code changes. DynamoDB is a NoSQL database with a different data model than MySQL, necessitating a complete rewrite of the database access layer. While DynamoDB is highly scalable and can handle high throughput, it's not a suitable solution when minimal code changes are desired. Also, the question specifies a relational database backend, which DynamoDB is not.\n\n**Why option 3 is incorrect:**\nis incorrect because while ElastiCache for Redis can improve read performance by caching frequently accessed data, it doesn't directly address the replication lag issue. The read replicas would still fall behind during peak traffic, and the website would still query them when the data is not in the cache, leading to inconsistent results. This option adds complexity to the application by requiring it to check the cache before querying the database, and it doesn't guarantee that the data in the cache is always up-to-date.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
    "options": [
      {
        "id": 0,
        "text": "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated",
        "correct": false
      },
      {
        "id": 1,
        "text": "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume",
        "correct": true
      },
      {
        "id": 2,
        "text": "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume",
        "correct": false
      },
      {
        "id": 3,
        "text": "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis correct because, by default, when an EC2 instance is terminated, the root EBS volume attached to it is also terminated. This is the default behavior for root volumes. To prevent this, the 'Delete on Termination' attribute of the EBS volume must be set to 'false' before the instance is terminated. This setting can be configured when launching the instance or later by modifying the volume's attributes.\n\n**Why option 0 is incorrect:**\nis incorrect because it's a generalization that's not always true. While the root volume is terminated by default, non-root volumes are not. The 'Delete on Termination' attribute controls this behavior for each volume individually.\n\n**Why option 2 is incorrect:**\nis incorrect. Backups to S3 (using EBS snapshots) are a good practice for data protection and disaster recovery, but the absence of backups does not directly cause the EBS volume to be deleted upon instance termination. The 'Delete on Termination' attribute is the primary factor.\n\n**Why option 3 is incorrect:**\nis incorrect for the same reason as option 2. EFS is a network file system, and while it can be used for backups or shared storage, its absence doesn't cause EBS volumes to be deleted on instance termination. The 'Delete on Termination' attribute is the relevant factor.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 54,
    "text": "A geospatial analytics firm recently completed a large-scale seafloor imaging project and used AWS Snowball Edge Storage Optimized devices to collect and transfer over 150 TB of raw sonar data. The firm uses a high-performance computing (HPC) cluster on AWS to process and analyze massive datasets to identify natural resource formations. The processing workloads require sub-millisecond latency, high-throughput, fast and parallel file access to the imported dataset once the Snowball Edge devices are returned to AWS and the data is ingested. Which solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon FSx for Lustre is designed for high-performance computing workloads that require low latency and high throughput. Importing the data directly into the Lustre file system allows the HPC cluster instances to access the data with the required performance characteristics. FSx for Lustre is optimized for parallel file access, making it suitable for the firm's needs. It avoids the extra step of copying data to S3 first, which would add latency and complexity.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides shared file storage, it does not offer the same level of performance (especially latency and throughput) as FSx for Lustre, which is specifically designed for HPC workloads. Copying data to S3 first and then to EFS adds unnecessary latency and complexity. EFS is not optimized for the sub-millisecond latency requirement.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon FSx for NetApp ONTAP, while offering a rich set of features, is not primarily designed for the same level of high-performance computing as FSx for Lustre. Syncing with S3 adds latency. ONTAP is more suited for enterprise workloads and data management features rather than extreme performance. It does not directly address the sub-millisecond latency requirement as effectively as FSx for Lustre.\n\n**Why option 3 is incorrect:**\nis incorrect because while FSx for Lustre can be linked to S3, this configuration is typically used for data lake scenarios where S3 acts as a persistent storage layer and FSx for Lustre is used as a high-performance cache. The question implies that the HPC cluster needs to directly access the entire dataset with low latency, which is better achieved by directly importing the data into FSx for Lustre. Linking to S3 introduces latency as data needs to be pulled from S3 into FSx for Lustre on demand.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain. Which of the following solutions addresses these requirements with the minimal integration effort?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon FSx for Windows File Server as a shared storage solution",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic File System (Amazon EFS) as a shared storage solution",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use File Gateway of AWS Storage Gateway to create a hybrid storage solution",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon FSx for Windows File Server, is the correct answer. FSx for Windows File Server provides fully managed, highly available, and scalable file storage built on Windows Server. It natively supports the SMB protocol, which is commonly used by Windows applications. Critically, it integrates directly with Active Directory, allowing existing users and groups to access the file share with their existing credentials. This minimizes integration effort. It also provides shared storage without requiring application-level replication.\n\n**Why option 1 is incorrect:**\nusing Amazon FSx for Lustre, is incorrect. While FSx for Lustre is a high-performance file system, it is designed for compute-intensive workloads like machine learning, high-performance computing (HPC), and video processing. It is not primarily designed for Windows-based applications or integration with Active Directory. It is also not the best fit for general-purpose shared storage for Windows applications. The question emphasizes minimal integration effort, and FSx for Lustre would likely require more configuration and adaptation than FSx for Windows File Server.\n\n**Why option 2 is incorrect:**\nusing Amazon Elastic File System (Amazon EFS), is incorrect. While EFS provides shared file storage accessible by multiple EC2 instances, it is primarily designed for Linux-based workloads and uses the NFS protocol. Integrating EFS with a self-managed Active Directory for Windows applications would require significant configuration and might not provide the same level of native integration and ease of use as FSx for Windows File Server. It would also likely require more integration effort.\n\n**Why option 3 is incorrect:**\nusing File Gateway of AWS Storage Gateway, is incorrect. File Gateway provides a way to access objects in Amazon S3 as files. While it can integrate with Active Directory, it's primarily used for hybrid cloud scenarios where you want to store data in S3 but access it from on-premises applications. In this case, the company is migrating to the cloud, not maintaining a hybrid environment. Also, File Gateway introduces an extra layer of complexity and latency compared to a native file system solution like FSx for Windows File Server. It's not the most direct or efficient solution for the stated requirements, and it would likely require more integration effort.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
    "options": [
      {
        "id": 0,
        "text": "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data",
        "correct": false
      },
      {
        "id": 1,
        "text": "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object",
        "correct": true
      },
      {
        "id": 2,
        "text": "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data",
        "correct": false
      },
      {
        "id": 3,
        "text": "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of new objects in all AWS Regions. However, for PUT and DELETE requests of existing objects, S3 provides eventual consistency. This means that if a process replaces an existing object and immediately tries to read it, Amazon S3 always returns the latest version of the object. This is critical for the financial services firm's high-frequency trading system to avoid data discrepancies.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that S3 *might* return the previous data after an overwrite. While S3 used to have eventual consistency for overwrites, it now provides strong read-after-write consistency for PUTS of existing objects.\n\n**Why option 2 is incorrect:**\nis incorrect. While it mentions propagation, the core issue is whether the *new* data is returned. The problem is that it implies that the new data might *not* be returned, which is incorrect.\n\n**Why option 3 is incorrect:**\nis incorrect. S3 will return data, even if it's the previous version (in the case of eventual consistency, which doesn't apply here for PUTS of existing objects). It won't simply return nothing.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 57,
    "text": "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery",
        "correct": false
      },
      {
        "id": 1,
        "text": "Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained",
        "correct": false
      },
      {
        "id": 3,
        "text": "If your instance has a public IPv4 address, it retains the public IPv4 address after recovery",
        "correct": true
      },
      {
        "id": 4,
        "text": "A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 3 and 4 are correct. Option 3 states that if an instance has a public IPv4 address, it retains that address after recovery. This is accurate because instance recovery attempts to preserve the instance's network configuration. Option 4 correctly states that a recovered instance retains its original instance ID, private IP addresses, Elastic IP addresses, and instance metadata. Instance recovery aims to bring the instance back to its previous state as closely as possible, preserving these key identifiers and configurations.\n\n**Why option 4 is correct:**\nOptions 3 and 4 are correct. Option 3 states that if an instance has a public IPv4 address, it retains that address after recovery. This is accurate because instance recovery attempts to preserve the instance's network configuration. Option 4 correctly states that a recovered instance retains its original instance ID, private IP addresses, Elastic IP addresses, and instance metadata. Instance recovery aims to bring the instance back to its previous state as closely as possible, preserving these key identifiers and configurations.\n\n**Why option 0 is incorrect:**\nis incorrect because a recovered instance *does* retain its public IPv4 address if it had one before the recovery. The recovery process aims to restore the instance to its previous state, including its network configuration.\n\n**Why option 1 is incorrect:**\nis incorrect because terminated EC2 instances cannot be recovered. Instance recovery is designed to address *impaired* instances, not terminated ones. Once an instance is terminated, it's gone, and a new instance would need to be launched.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A software company manages a fleet of Amazon EC2 instances that support internal analytics applications. These instances use an IAM role with custom policies to connect to Amazon RDS and AWS Secrets Manager for secure access to credentials and database endpoints. The IT operations team wants to implement a centralized patch management solution that simplifies compliance and security tasks. Their goal is to automate OS patching across EC2 instances without disrupting the running applications. Which approach will allow the company to meet these goals with the least administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Default Host Management Configuration in AWS Systems Manager Quick Setup",
        "correct": true
      },
      {
        "id": 2,
        "text": "Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nenabling Default Host Management Configuration in AWS Systems Manager Quick Setup, is the most efficient and least disruptive approach. Quick Setup automates the configuration of SSM Agent and necessary IAM roles for managed instances. It simplifies the process of onboarding existing EC2 instances to SSM for patch management. This option minimizes administrative overhead by automating the setup and configuration, allowing the IT operations team to focus on defining patch baselines and schedules rather than individual instance configuration. It also ensures that the necessary IAM permissions are in place without requiring manual role creation or modification, which could potentially introduce errors or break existing application functionality.\n\n**Why option 0 is incorrect:**\nis incorrect because manually installing the SSM Agent and scheduling cron jobs is a highly manual and error-prone process. It requires significant administrative overhead to maintain consistency and track patching status across all instances. This approach does not leverage the centralized management capabilities of AWS Systems Manager and is not scalable.\n\n**Why option 2 is incorrect:**\nis incorrect because detaching the existing IAM role and replacing it with a new one could potentially disrupt the applications that rely on the original role's permissions to access RDS and Secrets Manager. While adding the `AmazonSSMManagedInstanceCore` policy is necessary for SSM, replacing the existing role is a risky and unnecessary step. It's better to augment the existing role or use Quick Setup which handles role creation/augmentation automatically.\n\n**Why option 3 is incorrect:**\nis incorrect because using Systems Manager Hybrid Activation is primarily intended for registering on-premises servers or VMs with SSM, not EC2 instances within AWS. While it would technically work, it adds unnecessary complexity and administrative overhead compared to using Quick Setup, which is designed for EC2 instances. Also, attaching two IAM roles to an instance is not the typical or recommended approach for granting permissions; it can lead to confusion and potential conflicts.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "If a spot request is persistent, then it is opened again after your Spot Instance is interrupted",
        "correct": true
      },
      {
        "id": 1,
        "text": "Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated",
        "correct": true
      },
      {
        "id": 2,
        "text": "When you cancel an active spot request, it terminates the associated instance as well",
        "correct": false
      },
      {
        "id": 3,
        "text": "Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated",
        "correct": false
      },
      {
        "id": 4,
        "text": "If a spot request is persistent, then it is opened again after you stop the Spot Instance",
        "correct": false
      },
      {
        "id": 5,
        "text": "When you cancel an active spot request, it does not terminate the associated instance",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      1,
      5
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0, 1, and 5 are correct.\n\n*   **Option 0:** A persistent spot request ensures that if a Spot Instance is interrupted (terminated by AWS due to price exceeding your bid), the request is automatically re-submitted, and a new Spot Instance is launched when the price falls back within your bid and capacity is available. This is crucial for batch jobs that can tolerate interruptions but need to eventually complete.\n*   **Option 1:** Spot Fleets are designed to maintain a target capacity. If a Spot Instance within the fleet is terminated, the Spot Fleet will automatically launch replacement instances to maintain the desired capacity. This is a key feature for ensuring the reliability of workloads running on Spot Instances.\n*   **Option 5:** Cancelling an active spot *request* does *not* automatically terminate the associated instance. The instance will continue to run until it is interrupted by AWS due to pricing or capacity constraints, or until you manually terminate it. This allows you to stop using spot pricing without immediately losing the work the instance is doing.\n\n**Why option 1 is correct:**\nOptions 0, 1, and 5 are correct.\n\n*   **Option 0:** A persistent spot request ensures that if a Spot Instance is interrupted (terminated by AWS due to price exceeding your bid), the request is automatically re-submitted, and a new Spot Instance is launched when the price falls back within your bid and capacity is available. This is crucial for batch jobs that can tolerate interruptions but need to eventually complete.\n*   **Option 1:** Spot Fleets are designed to maintain a target capacity. If a Spot Instance within the fleet is terminated, the Spot Fleet will automatically launch replacement instances to maintain the desired capacity. This is a key feature for ensuring the reliability of workloads running on Spot Instances.\n*   **Option 5:** Cancelling an active spot *request* does *not* automatically terminate the associated instance. The instance will continue to run until it is interrupted by AWS due to pricing or capacity constraints, or until you manually terminate it. This allows you to stop using spot pricing without immediately losing the work the instance is doing.\n\n**Why option 5 is correct:**\nOptions 0, 1, and 5 are correct.\n\n*   **Option 0:** A persistent spot request ensures that if a Spot Instance is interrupted (terminated by AWS due to price exceeding your bid), the request is automatically re-submitted, and a new Spot Instance is launched when the price falls back within your bid and capacity is available. This is crucial for batch jobs that can tolerate interruptions but need to eventually complete.\n*   **Option 1:** Spot Fleets are designed to maintain a target capacity. If a Spot Instance within the fleet is terminated, the Spot Fleet will automatically launch replacement instances to maintain the desired capacity. This is a key feature for ensuring the reliability of workloads running on Spot Instances.\n*   **Option 5:** Cancelling an active spot *request* does *not* automatically terminate the associated instance. The instance will continue to run until it is interrupted by AWS due to pricing or capacity constraints, or until you manually terminate it. This allows you to stop using spot pricing without immediately losing the work the instance is doing.\n\n**Why option 2 is incorrect:**\nis incorrect because cancelling a spot *request* does not terminate the associated instance. It only prevents the request from launching any further instances. The existing instance will continue to run until it is interrupted or manually terminated.\n\n**Why option 3 is incorrect:**\nis incorrect because Spot Fleets *do* maintain target capacity. They are designed to automatically launch replacement instances when Spot Instances are terminated.\n\n**Why option 4 is incorrect:**\nis incorrect. A persistent spot request is opened again after your Spot Instance is *interrupted*, not stopped. Stopping an instance is a user-initiated action, and the spot request remains active.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 60,
    "text": "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
    "options": [
      {
        "id": 0,
        "text": "Internet Gateway (I1)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Route Table (R1)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subnet (S1)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Network Address Translation (NAT) instance (N1)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe Internet Gateway (I1) performs the Network Address Translation (NAT) for the Amazon EC2 instance E1. When an EC2 instance is assigned a public IPv4 address, the Internet Gateway automatically performs a one-to-one NAT between the instance's public IP address and its private IP address. This allows the instance to communicate with the internet. The Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet.\n\n**Why option 1 is incorrect:**\nRoute Table (R1) is incorrect because a route table contains rules (routes) that determine where network traffic is directed. It does not perform NAT. It simply directs traffic to the appropriate target, such as the Internet Gateway.\n\n**Why option 2 is incorrect:**\nSubnet (S1) is incorrect because a subnet is a range of IP addresses in your VPC. It does not perform NAT. It's just a logical division of the VPC's IP address space.\n\n**Why option 3 is incorrect:**\nNetwork Address Translation (NAT) instance (N1) is incorrect because the EC2 instance E1 already has a public IP address. A NAT instance is used when you want instances in a *private* subnet to initiate outbound traffic to the internet, but prevent the internet from initiating a connection with the instances. Since E1 has a public IP, it doesn't need a NAT instance to connect to the internet.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead. Which of the following would you recommend as a scalable alternative to the current solution?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon Kinesis Data Streams, is the most suitable solution. Kinesis Data Streams is designed for ingesting and processing high-volume, real-time data streams. It provides scalability and durability, and can be integrated with AWS Lambda for processing or Kinesis Data Analytics for running analytics directly on the stream. This addresses the need for a scalable, real-time data ingestion and analytics workflow with minimal implementation overhead, as Kinesis is a managed service.\n\n**Why option 0 is incorrect:**\nusing Amazon SNS, is not ideal for real-time data ingestion and analytics workflows. SNS is primarily a pub/sub messaging service for notifications. While it can trigger Lambda functions, it's not designed for high-throughput data streaming and lacks the data persistence and ordering guarantees of Kinesis. It also doesn't directly support analytics.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS, is a message queuing service suitable for decoupling components and handling asynchronous tasks. While it provides reliability and scalability, it's not optimized for real-time data streams. SQS is better suited for batch processing or handling discrete events, not continuous data flows. Also, while it can trigger Lambda, it doesn't directly support analytics workflows like Kinesis Data Analytics.\n\n**Why option 3 is incorrect:**\nusing Amazon API Gateway, primarily focuses on managing and securing APIs. While it can handle increased traffic, it doesn't address the underlying issue of data ingestion and processing bottlenecks. API Gateway acts as a front door to the application but doesn't inherently provide scalability for the data processing pipeline itself. It also doesn't provide a retry mechanism for failed requests, which is one of the initial problems.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 62,
    "text": "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past weeks data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
    "options": [
      {
        "id": 0,
        "text": "Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most cost-efficient and reliable solution. Amazon EventBridge allows scheduling events using cron expressions, which perfectly fits the requirement of weekly execution. AWS Lambda provides a serverless execution environment for the Python script. Lambda is cost-effective because you only pay for the compute time used (approximately 5 minutes per week). EventBridge ensures reliable scheduling, and Lambda provides a scalable and fault-tolerant environment for running the script. This solution avoids the overhead of managing servers, making it the most suitable choice.\n\n**Why option 1 is incorrect:**\nAWS Glue is designed for ETL (Extract, Transform, Load) operations and is generally more expensive than Lambda for simple script execution. While Glue can be scheduled, it's overkill for a simple database rollover script. The overhead of using Glue for this task is not cost-efficient.\n\n**Why option 2 is incorrect:**\nUsing an EC2 spot instance with a cron expression is less reliable and more complex than using Lambda and EventBridge. Spot instances can be terminated at any time, potentially interrupting the database rollover process. While cost-effective when available, the risk of interruption makes it less reliable. Managing an EC2 instance also adds operational overhead compared to a serverless solution.\n\n**Why option 3 is incorrect:**\nUsing an EC2 scheduled reserved instance is the least cost-efficient option. Reserved instances are billed regardless of usage, meaning you'll be paying for the instance even when it's idle for most of the week. This is not a cost-effective solution for a task that only runs for 5 minutes per week. Furthermore, managing an EC2 instance adds operational overhead compared to a serverless solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS CloudFormation StackSets, is the correct answer. CloudFormation StackSets are designed specifically for deploying and managing CloudFormation stacks across multiple AWS accounts and regions from a single management account. This allows the company to define a single template with the desired resource configurations (EC2 instance types, IAM roles, etc.) and then deploy that template to all the relevant accounts and regions. StackSets provide centralized control and ensure consistency across the organization.\n\n**Why option 0 is incorrect:**\nusing AWS CloudFormation stacks to deploy the same template across AWS accounts and regions, is incorrect. While CloudFormation can deploy templates, it lacks the built-in multi-account and multi-region deployment capabilities of StackSets. Deploying the same template manually across multiple accounts and regions would be a cumbersome and error-prone process. It would require separate deployments and management for each account and region, making it difficult to maintain consistency and track deployments.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nusing AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions, is incorrect. AWS RAM is used for sharing AWS resources between AWS accounts within an AWS Organization or with individual AWS accounts. It does not provide the functionality to deploy CloudFormation templates or manage resource provisioning. RAM focuses on sharing existing resources, not creating new ones based on templates.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing DynamoDB point-in-time recovery (PITR), is the most appropriate solution. PITR allows you to restore a DynamoDB table to any point in time during the preceding 35 days. This is ideal for recovering from accidental writes or deletions. The key advantage is the granularity of the recovery, allowing restoration to a state just before the corruption occurred, minimizing data loss. It's also a relatively quick operation compared to other options.\n\n**Why option 0 is incorrect:**\nconfiguring the DynamoDB table as a global table and switching to another region, is not a suitable solution for several reasons. Firstly, global tables are designed for low-latency global access and disaster recovery, not for correcting data corruption. Secondly, replicating data from a potentially corrupted table to another region is counterproductive. It also involves significant overhead and complexity for a simple data recovery task. Finally, it doesn't guarantee that the other region has a clean copy of the data, especially if the corruption has already replicated.\n\n**Why option 1 is incorrect:**\nusing DynamoDB Streams to restore the table, is also not the best approach. While DynamoDB Streams captures item-level changes, restoring the table using streams would involve replaying the stream events in reverse order to undo the corrupted writes. This is a complex and time-consuming process, especially if the corruption involves multiple writes or complex data transformations. It requires custom scripting and careful handling of dependencies, making it less efficient and more error-prone than PITR. It's more suitable for auditing or triggering actions based on data changes, not for rapid point-in-time recovery.\n\n**Why option 2 is incorrect:**\nusing DynamoDB on-demand backup to restore the table, is less ideal than PITR. While backups can be used for restoration, on-demand backups are typically taken periodically, not continuously. Therefore, restoring from a backup might result in a significant loss of data since the last backup. PITR provides a much finer granularity for recovery, allowing restoration to a point in time much closer to the corruption event, minimizing data loss. Also, restoring from a backup takes longer than restoring from PITR.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 65,
    "text": "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity. Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a read replica and connect the report generation tool/application to it",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate from General Purpose SSD to magnetic storage to enhance IOPS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase the size of Amazon RDS instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreating a read replica and directing the report generation tool to it is the most suitable solution. Read replicas provide a read-only copy of the data, allowing the reporting workload to be offloaded from the primary production database. This isolates the impact of the heavy reporting queries, preventing them from affecting the performance of the production application. This approach is also cost-effective because it avoids the need to significantly scale up the primary database instance. Read replicas are designed for read-heavy workloads like reporting and analytics.\n\n**Why option 0 is incorrect:**\nConfiguring the RDS instance to be Multi-AZ improves availability and provides failover capabilities in case of an outage, but it does not address the performance issue caused by the reporting workload. Multi-AZ provides a standby instance in a different Availability Zone, but all writes are still performed on the primary instance, and reads are typically served from the primary instance as well (unless using a custom endpoint for read-only traffic, which is not the primary purpose of Multi-AZ). Therefore, the reporting load would still impact the primary database.\n\n**Why option 2 is incorrect:**\nMigrating from General Purpose SSD (gp2/gp3) to magnetic storage would significantly degrade performance, not enhance it. Magnetic storage offers much lower IOPS and throughput compared to SSDs, making it unsuitable for database workloads, especially those involving complex queries and large data retrieval. This option would exacerbate the performance issues.\n\n**Why option 3 is incorrect:**\nIncreasing the size of the Amazon RDS instance might provide some temporary relief, but it is not a cost-effective or sustainable solution. The question states that CPU, memory, and storage are only at 50% capacity, indicating that the bottleneck is not overall resource exhaustion. Scaling up the instance would increase costs without necessarily addressing the root cause of the performance issue, which is likely I/O contention or query performance. Offloading the reporting workload to a read replica is a more targeted and cost-effective approach.",
    "domain": "Design Resilient Architectures"
  }
]