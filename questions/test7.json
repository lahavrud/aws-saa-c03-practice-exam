[
  {
    "id": 1,
    "text": "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines. Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
    "options": [
      {
        "id": 0,
        "text": "On-Demand Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Spot Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Dedicated Instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Dedicated Hosts",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nDedicated Instances are EC2 instances that run on hardware dedicated to a single customer. This addresses the single-tenancy requirement for compliance. Dedicated Instances are generally more cost-effective than Dedicated Hosts, especially when you don't need the additional control and visibility that Dedicated Hosts provide. Dedicated Instances share the underlying hardware with other Dedicated Instances from the same account, but no other AWS customers. This provides the necessary isolation at a lower cost than Dedicated Hosts, making it the most cost-effective option for single-tenant hardware.\n\n**Why option 0 is incorrect:**\nOn-Demand Instances run on shared hardware, meaning multiple AWS customers can have their instances running on the same physical server. This does not meet the single-tenancy requirement for compliance.\n\n**Why option 1 is incorrect:**\nSpot Instances also run on shared hardware and are subject to interruption. While cost-effective, they do not provide the required single-tenancy and are therefore unsuitable for compliance-driven isolation.\n\n**Why option 3 is incorrect:**\nDedicated Hosts provide the greatest level of control and visibility, allowing you to use your existing server-bound software licenses. However, they are the most expensive option. While they meet the single-tenancy requirement, the question specifically asks for the *most cost-effective* solution. Dedicated Instances provide single-tenancy at a lower cost.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 2,
    "text": "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses. What can you do to decrease the replication cost?",
    "options": [
      {
        "id": 0,
        "text": "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Amazon EC2 instances private IP for the replication",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Private Link between the two Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Elastic Fabric Adapter (EFA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing the private IP addresses of the EC2 instances for replication is the most cost-effective solution. Communication within the same AWS region using private IP addresses is free of charge. Since the instances are already in the same region, switching to private IP addresses eliminates the data transfer costs associated with using public IP addresses. This approach also improves security by keeping the replication traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nAssigning Elastic IP addresses (EIPs) does not reduce replication costs. EIPs are static public IP addresses, and using them for replication would still incur the same data transfer costs as using the existing public IP addresses. EIPs are primarily used for maintaining a consistent public IP address for an instance, not for cost optimization in internal communication.\n\n**Why option 2 is incorrect:**\nCreating a Private Link is designed for providing secure access to services without exposing them to the public internet. While it enhances security, it's an overkill for this scenario. Private Link is more suitable for connecting VPCs or on-premises networks to AWS services, not for internal communication between EC2 instances within the same region. It also adds complexity and cost compared to simply using private IP addresses.\n\n**Why option 3 is incorrect:**\nElastic Fabric Adapter (EFA) is a network interface that enables high levels of inter-instance communication. EFAs are designed for high performance computing (HPC) and machine learning (ML) applications that require low latency and high throughput. While EFA could improve the replication performance, it is not necessary for this scenario and is more expensive and complex than simply using private IP addresses. The primary goal is to reduce cost, not necessarily to improve performance.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets. Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "correct": true
      },
      {
        "id": 1,
        "text": "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity",
        "correct": true
      },
      {
        "id": 2,
        "text": "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted",
        "correct": false
      },
      {
        "id": 3,
        "text": "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "correct": false
      },
      {
        "id": 4,
        "text": "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Spot Instances are indeed spare EC2 capacity offered at significantly reduced prices (up to 90% off On-Demand). A crucial characteristic of Spot Instances is their potential for interruption by AWS when the capacity is needed back, and AWS provides a 2-minute notification before the interruption.\n\n**Why option 1 is correct:**\nThis is correct because a Spot Fleet is designed to fulfill a target capacity using a combination of Spot Instances and, optionally, On-Demand Instances. This flexibility allows for a more resilient and reliable deployment compared to relying solely on Spot Instances. The inclusion of On-Demand instances provides a safety net when Spot prices rise or capacity becomes unavailable.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Spot Fleets do not allow you to request Spot instances for a fixed duration like 1 to 6 hours. This sounds more like EC2 Spot Blocks, which are no longer available. Spot Fleets aim to maintain a target capacity, and instances can still be interrupted with a 2-minute warning, although the fleet management attempts to replace interrupted instances.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while Spot Fleets utilize spare EC2 capacity and can offer cost savings, they are not *usually* interrupted. Spot Fleets are designed to be more resilient than individual Spot Instances. The fleet management attempts to maintain the target capacity by requesting new Spot Instances when others are interrupted. The 2-minute interruption notification applies to the underlying Spot Instances within the fleet, but the fleet itself aims to minimize the impact of these interruptions.\n\n**Why option 4 is incorrect:**\nThis is incorrect because Spot Fleets can consist of both Spot Instances and On-Demand instances. This is a key feature of Spot Fleets, allowing for a more reliable and predictable capacity compared to relying solely on Spot Instances.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets. How can you provide these users access in the least possible time, with minimal changes?",
    "options": [
      {
        "id": 0,
        "text": "Create a policy and assign it manually to the 50 users",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a group, attach the policy to the group and place the users in the group",
        "correct": true
      },
      {
        "id": 3,
        "text": "Update the Amazon S3 bucket policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by creating a dedicated IAM group for the 50 users requiring write access. Attaching the necessary policy to the group grants the permissions to all members of the group. Adding the 50 users to the group is a relatively quick and efficient process compared to modifying each user's permissions individually. This approach also simplifies future management, as permission changes can be made at the group level, affecting all members simultaneously. It also aligns with the principle of least privilege by only granting write access to those who need it.\n\n**Why option 0 is incorrect:**\nAssigning a policy manually to each of the 50 users is time-consuming and error-prone. It does not scale well and makes future permission management difficult. This approach violates the principle of least privilege and is not an efficient way to manage permissions for a large number of users.\n\n**Why option 1 is incorrect:**\nCreating AWS MFA users and linking them to existing IAM users is not the correct approach for granting write access. MFA adds a layer of security but does not inherently grant permissions. Linking IAM users with MFA users is not a standard or recommended practice. This option also introduces unnecessary complexity and does not directly address the requirement of granting write access to the specified users.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements. Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation",
        "correct": false
      },
      {
        "id": 1,
        "text": "You can specify instance store volumes for an instance when you launch or restart it",
        "correct": false
      },
      {
        "id": 2,
        "text": "An instance store is a network storage type",
        "correct": false
      },
      {
        "id": 3,
        "text": "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved",
        "correct": true
      },
      {
        "id": 4,
        "text": "You can't detach an instance store volume from one instance and attach it to a different instance",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Instance Store volumes are physically attached to the host machine. When an AMI is created from an instance, the data on the instance store volumes is not included in the AMI. The AMI only captures the data on EBS volumes, not instance store volumes. This characteristic aligns with the requirement of not persisting data.\n\n**Why option 4 is correct:**\nThis is correct because Instance Store volumes are physically attached to the host machine on which the EC2 instance runs. Therefore, they cannot be detached and reattached to another instance. This is a fundamental limitation of instance store volumes and a key differentiator from EBS volumes, which are network-attached and can be detached and reattached.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while Instance Store is reset when you stop or terminate an instance, the data is NOT preserved during hibernation. Hibernation saves the in-memory state to the EBS root volume, but it does not save the data on the Instance Store volumes. Therefore, the data on the Instance Store is lost when the instance is hibernated.\n\n**Why option 1 is incorrect:**\nThis is incorrect because you can only specify instance store volumes for an instance when you *launch* it. You cannot add or modify instance store volumes after the instance has been launched, nor can you specify them when restarting an instance. The instance type determines the available instance store volumes.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application. Which of the following represents the best way of configuring a solution for this requirement with minimal effort?",
    "options": [
      {
        "id": 0,
        "text": "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CLI to run the user data scripts only once while launching the instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run the custom scripts as user data scripts on the Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by utilizing user data scripts, which are executed during the initial boot process of an EC2 instance. By default, user data scripts are executed only once during the first boot. This aligns with the 'only once' requirement and represents a straightforward, built-in mechanism, thus minimizing effort.\n\n**Why option 0 is incorrect:**\nWhile it's possible to modify the EC2 instance configuration to control user data script execution, it adds unnecessary complexity. The default behavior of user data scripts is to run only once, so modifying the configuration is not the most efficient or minimal effort approach. This option also doesn't specify how to ensure the scripts run only once, implying additional configuration steps.\n\n**Why option 1 is incorrect:**\nUsing the AWS CLI to manually run user data scripts contradicts the requirement of minimal effort and automation. It requires manual intervention for each instance launch, which is not scalable or efficient. The purpose of user data is to automate this process during instance creation.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A media company is modernizing its legacy image processing application by migrating it from an on-premises environment to AWS. The application handles a high volume of image transformation jobs, generating large output files. To support rapid growth, the company wants a cloud-native solution that automatically scales, minimizes manual intervention, and avoids managing servers or infrastructure. The team also wants to improve workflow automation to handle task sequencing and job state transitions. Which solution best meets these requirements while ensuring the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution effectively addresses all requirements. AWS Batch allows for running batch computing workloads without managing servers, automatically scaling resources based on job requirements. AWS Step Functions provides a serverless orchestration service to define and manage the workflow, including task sequencing and job state transitions. Amazon S3 offers scalable and durable storage for the large output files, eliminating the need to manage storage infrastructure. This combination provides a fully managed, scalable, and automated solution with minimal operational overhead.\n\n**Why option 1 is incorrect:**\nWhile Lambda can be used for image processing, relying solely on Lambda and EC2 Spot Instances introduces complexities in managing the workflow and handling large output files. Lambda has execution time limits and memory constraints that might be problematic for large image transformations. EC2 Spot Instances, while cost-effective, can be interrupted, requiring additional logic for handling failures and retries. Amazon FSx is a file system service, which is not the most cost-effective or scalable solution for storing large volumes of processed images compared to S3. This option also requires more manual configuration and management than the correct answer.\n\n**Why option 2 is incorrect:**\nDeploying Amazon EKS with self-managed EC2 worker nodes introduces significant operational overhead. Managing the Kubernetes cluster, scaling worker nodes, and handling infrastructure maintenance contradicts the requirement for minimizing manual intervention and avoiding server management. While Amazon SQS can queue jobs, it doesn't provide the workflow orchestration capabilities of Step Functions. Amazon EBS volumes are not ideal for storing large volumes of processed images due to cost and scalability limitations compared to S3.\n\n**Why option 3 is incorrect:**\nUsing EC2 Auto Scaling groups with a static fleet of instances does not fully leverage the benefits of a serverless architecture. While Auto Scaling can adjust the number of instances, it still requires managing the instances and their configuration. Triggering jobs through Step Functions is a good approach, but storing results on attached EBS volumes is less scalable and more expensive than using Amazon S3. This option also requires more manual configuration and management than the correct answer.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 8,
    "text": "A global photography startup hosts a static image-sharing site on an Amazon S3 bucket. The website allows users from different parts of the world to upload, view, and download photos through their mobile devices. As the platform has gained popularity, users have started experiencing latency issues, especially when uploading and downloading images. The team needs a solution to enhance global performance but wants to implement it with minimal development effort and without redesigning the application. Which solution will most effectively address the performance issues with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution effectively addresses the performance issues with minimal operational overhead. CloudFront, with the S3 bucket as the origin, caches the images closer to the users, significantly improving download speeds. S3 Transfer Acceleration utilizes geographically optimized AWS edge locations to accelerate uploads to S3, reducing latency for users uploading from different parts of the world. This combination provides a comprehensive solution for both upload and download performance without requiring significant changes to the application architecture.\n\n**Why option 1 is incorrect:**\nCreating multiple S3 buckets in different regions and replicating data introduces significant complexity and operational overhead. Managing data replication across regions is challenging and costly. While CloudFront can be configured to use different origins based on user location, the complexity of managing multiple buckets and replication outweighs the benefits, especially given the requirement for minimal development effort. This solution also involves significant application redesign.\n\n**Why option 2 is incorrect:**\nMigrating the website from S3 to EC2 instances in multiple regions is a major architectural change that requires significant development effort and operational overhead. It involves managing EC2 instances, load balancers, and data synchronization across regions. While AWS Global Accelerator can improve performance, the complexity and cost of this solution are not justified, especially considering the requirement for minimal development effort. S3 is designed for static content delivery and is a more suitable solution for this use case.\n\n**Why option 3 is incorrect:**\nWhile AWS Global Accelerator can accelerate both uploads and downloads, it is generally more suited for dynamic content and applications. Using CloudFront in conjunction with S3 Transfer Acceleration is a more cost-effective and efficient solution for static content delivery. Global Accelerator also requires reconfiguring the website to route requests through the accelerator, which adds complexity. CloudFront is designed specifically for caching static content and is a better fit for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second. How can you efficiently prevent attackers from overwhelming your application?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Shield Advanced and setup a rate-based rule",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Sticky Sessions on the Application Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule",
        "correct": true
      },
      {
        "id": 3,
        "text": "Define a network access control list (network ACL) on your Application Load Balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by allowing you to define rules that block or rate-limit requests based on their origin or other characteristics. A rate-based rule in AWS WAF counts the requests from each IP address and blocks those IP addresses that exceed a specified threshold within a defined time period. This effectively mitigates the DDoS attack by preventing attackers from overwhelming the application while allowing legitimate users with lower request rates to access the application.\n\n**Why option 0 is incorrect:**\nWhile AWS Shield Advanced provides comprehensive DDoS protection, including rate-based rules, it's a more expensive and complex solution than using AWS WAF for this specific scenario. The question emphasizes efficiency, and AWS WAF provides a more targeted and cost-effective approach for mitigating the described attack. Shield Advanced is better suited for more sophisticated and larger-scale attacks.\n\n**Why option 1 is incorrect:**\nSticky sessions (session affinity) on the Application Load Balancer ensure that requests from the same client are consistently routed to the same EC2 instance. This does not prevent attackers from overwhelming the application. Attackers can still send a high volume of requests, even if those requests are directed to the same instance. Sticky sessions are designed to maintain user sessions, not to mitigate DDoS attacks.\n\n**Why option 3 is incorrect:**\nNetwork ACLs (NACLs) operate at the subnet level and control traffic entering and leaving subnets. While NACLs can block traffic based on IP addresses or ports, they are not well-suited for rate limiting or identifying malicious traffic based on request patterns. They lack the granularity and intelligence to differentiate between legitimate and malicious requests based on request rates. Furthermore, NACLs are stateless, meaning they don't track request rates over time.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances. Which solution should a solutions architect implement to address these requirements in the most secure manner?",
    "options": [
      {
        "id": 0,
        "text": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements effectively. Placing the Network Load Balancer (NLB) in the public subnet allows it to receive incoming traffic from the internet. The NLB can then perform TLS termination, offloading the processing burden from the EC2 instances. The Auto Scaling group, residing in the private subnet, provides the backend compute capacity. This setup enhances security by isolating the EC2 instances from direct internet exposure, as they are only accessible through the NLB. The NLB forwards traffic to the instances in the private subnet.\n\n**Why option 0 is incorrect:**\nPlacing the Network Load Balancer in the private subnet would prevent it from being directly accessible from the internet. A Network Load Balancer needs to be in a public subnet to receive traffic from the internet. Therefore, it cannot serve as the entry point for a publicly accessible web application. Also, the question asks to offload TLS termination, which is not possible if the NLB is not publicly accessible.\n\n**Why option 1 is incorrect:**\nWhile placing the Auto Scaling group in the public subnet would allow the instances to be directly accessible from the internet, it would expose them to security risks and would not be considered a best practice. The Network Load Balancer still needs to be in the public subnet to receive traffic from the internet. Also, this option does not provide a secure architecture as the EC2 instances are directly exposed to the internet.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading. As a solutions architect, which of the following services would you recommend for the given use case?",
    "options": [
      {
        "id": 0,
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon ElastiCache for Redis",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon ElastiCache for Memcached",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because ElastiCache for Memcached is designed for multi-threaded environments. Memcached's architecture allows for efficient handling of concurrent requests, making it well-suited for applications with high read loads and multi-threaded access patterns. It is a distributed, in-memory object caching system that is often used to speed up dynamic web applications by alleviating database load.\n\n**Why option 0 is incorrect:**\nThis is incorrect because DynamoDB Accelerator (DAX) is specifically designed for caching DynamoDB tables. It's not a general-purpose caching solution suitable for relational databases. While DAX improves DynamoDB read performance, it doesn't address the need for caching relational database query results.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Global Accelerator is a networking service that improves the performance of your users' traffic by directing it to the optimal AWS endpoint. It does not provide caching functionality. It focuses on improving network performance and availability, not caching database queries.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 12,
    "text": "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration. Which of the following options would you suggest?",
    "options": [
      {
        "id": 0,
        "text": "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis configuration provides high availability by placing one NAT gateway in a public subnet in each Availability Zone. This ensures that if one AZ fails, the EC2 instances in the other AZ can still access the internet through the NAT gateway in their AZ. The routing tables in the private subnets should be configured to route traffic destined for the internet to the NAT gateway in the same AZ. This approach minimizes cross-AZ traffic and associated costs.\n\n**Why option 0 is incorrect:**\nPlacing both NAT gateways in a single public subnet defeats the purpose of high availability across Availability Zones. If the Availability Zone containing the public subnet and NAT gateways fails, all EC2 instances will lose internet connectivity.\n\n**Why option 1 is incorrect:**\nUsing only one NAT gateway creates a single point of failure. If the Availability Zone containing the NAT gateway fails, all EC2 instances will lose internet connectivity. This does not meet the high availability requirement.\n\n**Why option 3 is incorrect:**\nNAT gateways must be placed in public subnets, not private subnets. Private subnets do not have direct internet access. Placing NAT gateways in private subnets would not allow the EC2 instances to access the internet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon Relational Database Service (Amazon RDS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon OpenSearch Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon ElastiCache",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Redshift",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon ElastiCache is a fully managed, in-memory data store and caching service. It supports both Memcached and Redis engines. For high read volumes and low latency requirements, ElastiCache provides a suitable caching layer in front of DynamoDB. It can significantly reduce the load on DynamoDB by serving frequently accessed data from the cache, resulting in improved performance and reduced costs.\n\n**Why option 4 is correct:**\nThis is correct because Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB. It's designed specifically to improve read performance for DynamoDB tables. DAX delivers up to a 10x performance improvement—from milliseconds to microseconds—even at millions of requests per second. It is a write-through cache, so data is always consistent. Given the specific need for low latency and high request rates for DynamoDB reads, DAX is an ideal choice.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon RDS is a relational database service. While RDS can be used for caching in some scenarios, it's not optimized for the high-volume, low-latency caching requirements described in the question. DynamoDB is a NoSQL database, and using a relational database as a cache for it would introduce unnecessary complexity and overhead. ElastiCache or DAX are better suited for this purpose.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon OpenSearch Service (formerly Elasticsearch Service) is a search and analytics engine. While it can be used to store and search data, it's not primarily designed as a caching layer for DynamoDB. It's more suitable for use cases involving full-text search, log analytics, and application monitoring. It doesn't provide the low-latency, in-memory caching capabilities required in this scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 14,
    "text": "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ). Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Create a Spot Fleet request",
        "correct": false
      },
      {
        "id": 1,
        "text": "Assign an Amazon EC2 Instance Role to perform the necessary API calls",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it",
        "correct": true
      },
      {
        "id": 5,
        "text": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because the EC2 instance needs permissions to perform actions like attaching an Elastic IP address or scaling itself. Assigning an IAM role to the instance is the recommended and secure way to grant these permissions, avoiding the need to store credentials directly on the instance.\n\n**Why option 2 is correct:**\nThis is correct because an Auto Scaling Group (ASG) configured with min=1, max=1, and desired=1 across two Availability Zones ensures that there is always one instance running. If the instance in one AZ fails, the ASG will automatically launch a new instance in the other AZ, providing automatic recovery from AZ failures. This configuration also maintains the single-server requirement of the monolith application.\n\n**Why option 4 is correct:**\nThis is correct because an Elastic IP (EIP) address provides a static public IP address that can be remapped to a different instance in case of failure. Using a user-data script to attach the EIP ensures that the new instance in the other AZ automatically gets the same public IP address as the failed instance, maintaining connectivity and allowing clients to continue accessing the application without any DNS changes. This is crucial for a monolith application that relies on a consistent IP address.\n\n**Why option 0 is incorrect:**\nSpot Fleets are cost-effective for fault-tolerant and flexible workloads that can handle interruptions. However, for a critical monolith application that requires continuous availability, Spot Instances are not suitable due to the possibility of being terminated with short notice. Using Spot Fleets would introduce instability and potential downtime, which contradicts the requirement for automatic recovery from AZ failures.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 5 is incorrect:**\nWhile this option creates an ASG spanning two AZs, setting min=1, max=2, and desired=2 means that two instances will be running simultaneously. This violates the requirement that the application must be deployed on a single web server, as it's a monolith application not designed for distributed mode. It also increases costs unnecessarily.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory. Which AWS Directory Service is the best fit for this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Simple Active Directory (Simple AD)",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Active Directory Connector",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Transit Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct choice because it's a fully managed Microsoft Active Directory service hosted on AWS. It allows you to run directory-aware applications in the AWS Cloud and natively supports establishing trust relationships with your existing on-premises Active Directory domains. This fulfills both requirements outlined in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because while Simple AD provides basic directory services, it does not support establishing trust relationships with on-premises Active Directory domains. It's a simpler, less feature-rich directory service suitable for smaller deployments without complex integration needs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because AD Connector is a proxy service that allows you to connect to your existing on-premises Active Directory from AWS. It doesn't create a new Active Directory in the cloud, nor does it run directory-aware workloads in AWS. It simply forwards directory requests to your on-premises AD.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because AWS Transit Gateway is a networking service used to connect multiple VPCs and on-premises networks. It doesn't provide any directory services or Active Directory functionality. It's completely unrelated to the problem domain.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon Kinesis Data Streams to ingest the data",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up AWS Fargate with Amazon ECS to process the data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Database Migration Service (AWS DMS) to ingest the data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up AWS Lambda with AWS Step Functions to process the data",
        "correct": false
      },
      {
        "id": 4,
        "text": "Provision Amazon EC2 instances in an Auto Scaling group to process the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon Kinesis Data Streams is designed for ingesting real-time streaming data. It can handle high volumes of data and provides the necessary infrastructure for capturing and storing the incoming stream before processing. It allows for real-time data ingestion, which is a key requirement of the problem.\n\n**Why option 1 is correct:**\nThis is correct because AWS Fargate with Amazon ECS provides a serverless compute environment for running containerized applications. Given the 30-minute processing time, Lambda is not suitable due to its execution time limits. Fargate allows for long-running tasks and can scale horizontally to handle the incoming data volume. Using ECS with Fargate allows for containerization of the processing job, enabling easy deployment and scaling.\n\n**Why option 2 is incorrect:**\nThis is incorrect because AWS Database Migration Service (DMS) is designed for migrating databases, not for ingesting real-time streaming data. It's not suitable for the described scenario.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Lambda has execution time limits (typically 15 minutes). The job takes 30 minutes to complete, making Lambda unsuitable. While Step Functions can orchestrate Lambda functions, the underlying limitation of Lambda's execution time still applies. Also, while Lambda can be triggered by Kinesis, the processing time makes it unsuitable.\n\n**Why option 4 is incorrect:**\nThis is incorrect because provisioning EC2 instances in an Auto Scaling group, while scalable, is not a serverless solution. The question specifically asks for a serverless solution. Managing EC2 instances involves more operational overhead compared to Fargate.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A healthcare startup is deploying an AWS-based analytics platform that processes sensitive patient records. The application backend uses Amazon RDS for structured data and Amazon S3 for storing medical files. S3 Event Notifications trigger AWS Lambda for real-time data classification and alerting. The startup uses AWS IAM Identity Center to manage federated access from their enterprise directory. Development, operations, and compliance teams require granular and secure access to RDS and S3 resources, based strictly on their job roles. The company must follow the principle of least privilege while minimizing manual administrative work. Which solution should the company implement to meet these requirements with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each team’s responsibilities. Assign users to accounts and let SCPs enforce the required access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS IAM Identity Center integrated with the organization’s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution directly addresses the requirements by leveraging IAM Identity Center's permission sets. Permission sets allow defining least-privilege policies for RDS and S3, granting specific permissions based on team roles. By assigning users to groups within IAM Identity Center and mapping these groups to the appropriate permission sets, the company can achieve granular, role-based access control with minimal manual administrative overhead. This approach integrates seamlessly with their existing IAM Identity Center setup and promotes the principle of least privilege.\n\n**Why option 0 is incorrect:**\nWhile SCPs can enforce access boundaries, they operate at the organizational unit (OU) level, which might be too broad for the granular access control required for individual teams and roles within the organization. SCPs are best suited for setting guardrails and preventing actions across accounts, not for fine-grained permissions within a single account. Furthermore, assigning users to accounts solely for access control adds unnecessary complexity and operational overhead.\n\n**Why option 1 is incorrect:**\nCreating individual IAM users for each team member is not scalable and increases administrative overhead significantly. Managing individual IAM users and their access keys is cumbersome and prone to errors. While IAM Access Analyzer can help identify unused permissions, it doesn't eliminate the manual effort required to create and maintain individual IAM users and their associated policies. This approach doesn't leverage the existing IAM Identity Center setup and violates the requirement to minimize operational overhead.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A media company is relocating its legacy infrastructure to AWS. The on-premises environment consists of multiple virtualized workloads that are tightly coupled to their host operating systems and cannot be containerized or re-architected due to software constraints. Each workload currently runs on a standalone virtual machine. The engineering team plans to run these workloads on Amazon EC2 instances without modifying their core design. The company needs a solution that ensures high availability and fault tolerance in the AWS Cloud. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure",
        "correct": true
      },
      {
        "id": 1,
        "text": "Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct solution because it addresses the high availability and fault tolerance requirements without requiring any changes to the application architecture. Generating an AMI for each legacy server allows for easy migration to EC2. Launching two instances in different Availability Zones provides redundancy. The Network Load Balancer (NLB) is crucial because it distributes traffic across the instances and performs health checks. If one instance fails, the NLB automatically redirects traffic to the healthy instance, ensuring minimal downtime and high availability. The NLB is suitable here because the question doesn't mention any need for application-level routing, which would necessitate an ALB.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because the question explicitly states that the workloads cannot be containerized. Therefore, deploying them to Amazon ECS using Fargate is not a viable solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because while Auto Scaling groups can provide fault tolerance, setting the minimum and maximum capacity to 1 defeats the purpose of Auto Scaling for high availability. If the single instance fails, Auto Scaling will launch a replacement, but there will be a period of downtime while the new instance is being provisioned. The Application Load Balancer (ALB) is not necessary here, and adds complexity without providing additional benefit over an NLB.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because relying on manual restoration from backups is not a high availability solution. While backups are important for disaster recovery, they do not provide the automatic failover required for high availability. The recovery plan involves manual intervention, which introduces significant downtime and does not meet the requirement for fault tolerance.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A healthcare analytics firm operates a backend application within a private subnet of its VPC. The application is fronted by an Application Load Balancer (ALB) and accesses Amazon S3 to store medical reports. The VPC includes both a NAT gateway and an internet gateway, but the company's strict compliance policy prohibits any data traffic from traversing the internet. The team must redesign the architecture to comply with the security policy and improve cost-efficiency. Which solution best satisfies these requirements in the most cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 interface VPC endpoint and modify the security group to allow access from the application’s private subnet. Route all S3 traffic through the interface endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by creating a gateway VPC endpoint for S3. Gateway endpoints allow direct, private access to S3 from within the VPC without using the internet. Updating the route table to direct S3 traffic through the endpoint ensures that all S3 requests stay within the AWS network. This is also the most cost-effective solution as gateway endpoints are free to use; you only pay for the S3 usage itself. This eliminates the need for NAT gateway bandwidth charges for S3 traffic.\n\n**Why option 0 is incorrect:**\nWhile an interface VPC endpoint provides private connectivity to S3, it is more expensive than a gateway endpoint. Interface endpoints use AWS PrivateLink, which incurs hourly charges and data processing fees. A gateway endpoint fulfills the requirement of keeping traffic within the AWS network at a lower cost. Also, the question explicitly asks for the *most* cost-effective solution.\n\n**Why option 1 is incorrect:**\nModifying the S3 bucket policy to allow requests only from the NAT gateway's Elastic IP address does not prevent traffic from traversing the internet. The NAT gateway still uses the internet to access S3. This solution only restricts access to the S3 bucket to requests originating from the NAT gateway, but it doesn't address the core requirement of keeping traffic within the AWS network. Furthermore, if the NAT Gateway is replaced or reconfigured, the Elastic IP address may change, breaking the application's access to S3.\n\n**Why option 2 is incorrect:**\nCreating a VPC peering connection with another VPC that has direct access to S3 and using proxy EC2 instances is a complex and costly solution. VPC peering itself is free, but the proxy EC2 instances would incur compute costs, and managing these instances adds operational overhead. This solution is significantly more expensive and complex than using a gateway VPC endpoint, which provides a direct and cost-effective way to access S3 privately.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
    "options": [
      {
        "id": 0,
        "text": "Elastic Fabric Adapter (EFA)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Elastic IP Address (EIP)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Elastic Network Adapter (ENA)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Elastic Network Interface (ENI)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct choice because Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communication at scale on AWS. EFA supports OS bypass, which allows HPC and machine learning applications to bypass the operating system kernel and communicate directly with the EFA device. This reduces latency and improves performance for tightly coupled workloads.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Elastic IP Addresses (EIPs) are static IPv4 addresses designed for dynamic cloud computing. They are primarily used for maintaining a consistent public IP address for an instance, especially after failures or restarts. EIPs do not directly contribute to improving network performance or reducing latency for HPC workloads.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Elastic Network Adapters (ENAs) provide the necessary network performance for most network use cases, but they don't offer the low latency and high throughput capabilities required for tightly coupled HPC workloads. ENA is a good choice for general-purpose networking, but EFA is specifically designed for HPC.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Elastic Network Interfaces (ENIs) are virtual network interfaces that you can attach to EC2 instances. While ENIs provide basic network connectivity, they do not offer the specialized features like OS bypass that are necessary for optimizing network performance in HPC environments. ENIs are more general-purpose and do not provide the low-latency, high-throughput capabilities of EFA.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this. Which of the following settings of the VPC need to be enabled? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "enableVpcHostnames",
        "correct": false
      },
      {
        "id": 1,
        "text": "enableVpcSupport",
        "correct": false
      },
      {
        "id": 2,
        "text": "enableDnsHostnames",
        "correct": true
      },
      {
        "id": 3,
        "text": "enableDnsSupport",
        "correct": true
      },
      {
        "id": 4,
        "text": "enableDnsDomain",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because `enableDnsHostnames` allows instances in the VPC to receive a DNS hostname. While not strictly required for private hosted zones to function, it's a common and often necessary configuration for instances to be able to resolve names within the zone. If instances don't have hostnames, resolving them via DNS becomes more difficult. It allows Route 53 to assign DNS hostnames to instances launched in the VPC, facilitating name resolution.\n\n**Why option 3 is correct:**\nThis is correct because `enableDnsSupport` enables DNS resolution within the VPC. Without this setting enabled, instances within the VPC will not be able to resolve DNS queries using the Amazon-provided DNS server. This is a fundamental requirement for using Route 53 private hosted zones, as the instances need to be able to query the DNS server to resolve the internal domain names.\n\n**Why option 0 is incorrect:**\nThis is incorrect because `enableVpcHostnames` is not a valid VPC attribute. The correct attribute is `enableDnsHostnames` which is already covered in another option.\n\n**Why option 1 is incorrect:**\nThis is incorrect because `enableVpcSupport` is not a valid VPC attribute related to DNS configuration. There is no such setting in VPC configuration.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift. What do you recommend? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Migrate the Amazon Redshift underlying storage to Amazon S3 IA",
        "correct": false
      },
      {
        "id": 1,
        "text": "Analyze the cold data with Amazon Athena",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a smaller Amazon Redshift Cluster with the cold data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the data to Amazon S3 Glacier Deep Archive after 30 days",
        "correct": false
      },
      {
        "id": 4,
        "text": "Move the data to Amazon S3 Standard IA after 30 days",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon Athena allows you to query data directly in Amazon S3 using standard SQL. It's a serverless query service, so you only pay for the queries you run. This eliminates the need to maintain a Redshift cluster for infrequently accessed data, significantly reducing costs. Athena provides immediate query start as it directly queries the data in S3.\n\n**Why option 4 is correct:**\nThis is correct because moving the data to Amazon S3 Standard IA (Infrequent Access) after 30 days provides a cost-effective storage solution for data that is not frequently accessed. S3 Standard IA offers lower storage costs compared to S3 Standard, while still providing fast access when needed. This aligns with the requirement of cost optimization and maintaining SQL querying capability when combined with Athena.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Amazon Redshift's underlying storage is already optimized for performance within the Redshift cluster. Changing the underlying storage to S3 IA would not be a supported or recommended configuration. Redshift manages its own storage and data distribution for optimal query performance. This option does not address the need to reduce Redshift costs for infrequently accessed data.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because while creating a smaller Redshift cluster could reduce costs compared to the original cluster, it still requires maintaining a Redshift cluster, which incurs costs even when the data is not actively being queried. Athena offers a more cost-effective solution for querying data in S3 on an as-needed basis. Also, migrating data to a new cluster takes time and effort.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because while Amazon S3 Glacier Deep Archive is the cheapest storage option, it's designed for long-term archiving and retrieval can take hours. This violates the requirement of immediate query start. Also, you cannot directly query data in Glacier Deep Archive with SQL. You would need to restore the data first, which adds significant delay and complexity.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A financial services firm operates a mission-critical transaction processing platform hosted in the AWS us-east-2 Region. The backend is powered by a MySQL-compatible Amazon Aurora cluster, with high transaction volumes throughout the day. As part of its business continuity planning, the firm has selected us-west-2 as its designated disaster recovery (DR) Region. The firm has defined strict DR objectives: Recovery Point Objective (RPO): ≤ 5 minutes Recovery Time Objective (RTO): ≤ 15 minutes Leadership has asked for a DR solution that ensures fast cross-regional failover with minimal operational overhead and configuration effort. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events",
        "correct": false
      },
      {
        "id": 3,
        "text": "Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the most suitable solution because Aurora Global Database is designed for exactly this scenario: fast cross-region disaster recovery with minimal operational overhead. It uses storage-based replication to maintain a secondary Aurora cluster in a different AWS Region. This replication is typically very fast, easily meeting the RPO of ≤ 5 minutes. Aurora Global Database also provides managed failover capabilities, which can achieve the RTO of ≤ 15 minutes with minimal manual intervention. The managed failover process automates many of the steps involved in promoting the secondary cluster, reducing the time and effort required for failover. This solution also minimizes operational overhead because Aurora handles the replication and failover process, reducing the need for custom scripting or manual configuration.\n\n**Why option 0 is incorrect:**\nThis solution is incorrect because using Lambda functions to export and import snapshots every 5 minutes is not efficient or reliable for achieving the required RPO and RTO. Snapshots are point-in-time backups, and restoring from a snapshot takes time, likely exceeding the 15-minute RTO. Furthermore, managing the Lambda functions, storage, and custom scripts adds significant operational overhead. The process is also prone to errors and inconsistencies, making it a less desirable DR solution.\n\n**Why option 1 is incorrect:**\nThis solution is incorrect because while creating an Aurora read replica in us-west-2 provides data replication, it requires a manual promotion process for failover. Manual promotion can be time-consuming and error-prone, potentially exceeding the 15-minute RTO. Monitoring replication health and executing the promotion process also adds operational overhead. Although this option is better than using snapshots, it doesn't provide the automated failover capabilities needed to meet the strict RTO and minimize operational effort. The manual intervention required makes it less ideal than Aurora Global Database.\n\n**Why option 2 is incorrect:**\nThis solution is incorrect because while AWS DMS can replicate data continuously, it typically introduces higher latency than Aurora Global Database's storage-based replication. This increased latency may make it difficult to consistently meet the 5-minute RPO. Furthermore, AWS DMS requires more configuration and management than Aurora Global Database. The manual failover process also increases the RTO and operational overhead compared to the managed failover provided by Aurora Global Database. The complexity and potential latency make it a less suitable option.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance. Which database would you recommend using?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Aurora Serverless",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB with On-Demand Capacity",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon DynamoDB with Provisioned Capacity and Auto Scaling",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon Aurora Serverless is a fully managed, MySQL- and PostgreSQL-compatible, relational database that automatically starts up, shuts down, and scales capacity up or down based on your application's needs. It is well-suited for OLTP workloads that have infrequent, intermittent, or unpredictable traffic. It provides relational query capabilities and automatically scales to handle unpredictable spikes in usage without requiring manual capacity provisioning.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while Amazon DynamoDB with On-Demand Capacity can handle unpredictable traffic spikes, it is a NoSQL database and does not natively support relational queries. The application requires relational query support, making DynamoDB unsuitable.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon ElastiCache is an in-memory data store and cache service. It is not a database and does not support relational queries or persistent storage for OLTP applications. It is primarily used for caching frequently accessed data to improve application performance, not as a primary database.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while Amazon DynamoDB with Provisioned Capacity and Auto Scaling can handle traffic spikes, it still doesn't support relational queries. Also, the question is looking for a solution that requires minimal configuration and management. Aurora Serverless provides a more hands-off approach compared to configuring provisioned capacity and auto-scaling for DynamoDB. DynamoDB is also a NoSQL database.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection. What is the MOST cost-effective way to establish such a connection?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Direct Connect",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an AWS Site-to-Site VPN connection",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an Internet Gateway between the on-premises data center and AWS cloud",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a bastion host on Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nsetting up an AWS Site-to-Site VPN connection, is the most cost-effective solution. Site-to-Site VPN uses the public internet to establish a secure, encrypted connection between the on-premises data center and the AWS cloud. It's relatively quick to set up and doesn't require dedicated hardware or long-term commitments like Direct Connect. Since the bandwidth requirement is low and the traffic volume is small, the performance limitations of using the internet are not a significant concern. The cost is primarily based on VPN gateway hours and data transfer, making it economical for low-bandwidth, low-traffic scenarios.\n\n**Why option 0 is incorrect:**\nsetting up AWS Direct Connect, is incorrect because Direct Connect is designed for high-bandwidth, low-latency connections. It involves establishing a dedicated network connection between the on-premises data center and AWS, which is significantly more expensive and time-consuming to set up than a VPN. Given the low bandwidth and small traffic volume requirements, Direct Connect is an overkill and not cost-effective.\n\n**Why option 2 is incorrect:**\nsetting up an Internet Gateway between the on-premises data center and AWS cloud, is incorrect because an Internet Gateway allows resources in a VPC to connect to the internet. It does not establish a secure connection between the on-premises data center and AWS. While it allows connectivity, it lacks the necessary encryption and security features required for a secure connection. It also doesn't directly facilitate a connection between an on-premise data center and AWS in the way a VPN or Direct Connect does. An Internet Gateway is a component within a VPC, not a connection point to an external network.\n\n**Why option 3 is incorrect:**\nsetting up a bastion host on Amazon EC2, is incorrect because a bastion host provides secure access to other EC2 instances within a private subnet. It doesn't establish a secure connection between the on-premises data center and AWS. While a bastion host can be part of a larger solution, it doesn't address the core requirement of creating a secure connection between the data center and AWS.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 26,
    "text": "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Make the Amazon S3 bucket public",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating an Origin Access Identity (OAI), which acts as a virtual user. CloudFront uses this OAI to authenticate with S3. The S3 bucket policy is then updated to grant access only to the specified OAI. This effectively blocks direct access to the S3 bucket from any other source, including users, while allowing CloudFront to serve the content.\n\n**Why option 0 is incorrect:**\nSecurity groups operate at the instance level (EC2) and do not apply to S3 buckets. S3 buckets use bucket policies for access control. Therefore, updating security groups is not a viable solution for securing S3 access.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nWhile bucket policies are the correct mechanism for controlling access to S3, authorizing an IAM role attached to CloudFront is not the standard or recommended approach. CloudFront distributions do not typically have IAM roles attached to them in the context of origin access control. The Origin Access Identity (OAI) is the preferred method for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A research firm archives experimental datasets generated by automated laboratory equipment. Each dataset is about 10 MB in size and is initially accessed frequently for analysis within the first month. After this period, the access rate drops significantly, but the data must remain immediately retrievable if needed. Due to compliance policies, each dataset must be retained in AWS storage for exactly 4 years before deletion. The firm currently stores the data in Amazon S3 Standard storage and wants to minimize costs without compromising data availability or retrieval speed. Which solution meets these requirements most cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation",
        "correct": false
      },
      {
        "id": 2,
        "text": "Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by transitioning the data to S3 Standard-IA after the initial month of frequent access. S3 Standard-IA offers lower storage costs compared to S3 Standard for data that is infrequently accessed but still requires rapid retrieval when needed. The lifecycle policy also ensures that the data is automatically deleted after exactly 4 years, satisfying the compliance requirement. This combination provides a cost-effective solution without compromising data availability or retrieval speed.\n\n**Why option 0 is incorrect:**\nS3 Glacier Flexible Retrieval (formerly Glacier) is designed for archival data where retrieval times of several hours are acceptable. While it's the cheapest storage option, it doesn't meet the requirement of immediate retrievability. The question explicitly states that data must be immediately retrievable if needed, making Glacier an unsuitable choice.\n\n**Why option 1 is incorrect:**\nS3 Glacier Instant Retrieval is more expensive than S3 Standard-IA. While it offers immediate retrieval, the cost savings compared to S3 Standard are not as significant as with S3 Standard-IA, especially considering the infrequent access pattern after the first month. Therefore, it is not the most cost-effective solution.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EFS Infrequent Access",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon EFS Standard",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 Standard-IA",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-optimal solution because Amazon EFS Infrequent Access (IA) is designed for files that are not accessed every day. It automatically and transparently moves files to a lower-cost storage class when they haven't been accessed for a certain period. Since the data is only accessed for about a week a year, most of the data will reside in the cheaper IA storage class, significantly reducing storage costs while still providing POSIX compliance.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon EFS Standard is designed for frequently accessed files and is more expensive than EFS Infrequent Access. Given the infrequent access pattern described in the question, EFS Standard would be a less cost-effective option.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon S3 is object storage and does not provide POSIX compliance. The question explicitly requires a POSIX compliant file storage system.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon S3 is object storage and does not provide POSIX compliance. The question explicitly requires a POSIX compliant file storage system.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining. Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
    "options": [
      {
        "id": 0,
        "text": "AWS Firewall Manager",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Shield Advanced",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon GuardDuty",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Web Application Firewall (AWS WAF)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and unauthorized behavior. It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized activity. In this scenario, GuardDuty can detect the EC2 instance querying IP addresses used for cryptocurrency mining, as it aligns with known malicious activity. It can then generate security findings to alert the security team.\n\n**Why option 0 is incorrect:**\nAWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. While it can help manage security policies, it does not directly detect or alert on specific malicious activity like querying cryptocurrency mining IP addresses. It's more focused on managing existing firewalls (like AWS WAF or Network Firewall) rather than threat detection itself.\n\n**Why option 1 is incorrect:**\nAWS Shield Advanced provides enhanced DDoS protection for your applications running on AWS. It protects against more sophisticated and larger attacks than AWS Shield Standard. While it's a valuable security service, it's not designed to detect or prevent unauthorized behavior like an EC2 instance querying cryptocurrency mining IP addresses. It focuses on mitigating DDoS attacks, not general threat detection.\n\n**Why option 3 is incorrect:**\nAWS Web Application Firewall (AWS WAF) helps protect your web applications from common web exploits and bots. It operates at the application layer (Layer 7) and filters HTTP/HTTPS traffic. While it can block requests based on specific patterns or IP addresses, it's not designed to detect general malicious activity or unauthorized behavior like an EC2 instance querying cryptocurrency mining IP addresses. It's primarily focused on protecting web applications from web-based attacks.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS. Which of the following options do you recommend as the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Run on AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run on an Application Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run on Amazon EMR",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run on a Spot Instance with a persistent request type",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the most cost-effective solution because Spot Instances offer significant discounts compared to On-Demand instances. The process can withstand interruptions, making it a good candidate for Spot Instances. Using a persistent request type ensures that the instance will be automatically replaced if terminated, minimizing downtime and ensuring the nightly process eventually completes. This leverages the fault tolerance of the application to reduce costs.\n\n**Why option 0 is incorrect:**\nThis is incorrect because AWS Lambda functions have a maximum execution duration limit (currently 15 minutes). Since the process takes 30 minutes to run, it cannot be executed within a single Lambda function without significant refactoring to break it down into smaller, chained functions, which adds complexity and potentially cost.\n\n**Why option 1 is incorrect:**\nThis is incorrect because an Application Load Balancer (ALB) is used for distributing incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses. It is not designed to run batch processing jobs or scheduled tasks. While an ALB could be used to trigger a process on an EC2 instance, the ALB itself doesn't execute the Python script, and it adds unnecessary complexity and cost for this use case. The cost of the ALB itself would be incurred regardless of the runtime of the process.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon EMR (Elastic MapReduce) is a managed cluster platform that lets you run big data frameworks such as Apache Hadoop and Apache Spark to process vast amounts of data. It is an overkill for a simple 30-minute Python script. EMR is designed for large-scale data processing and analytics, and using it for this purpose would be significantly more expensive than other options.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "An edtech startup runs its course-management platform inside a private subnet in a VPC on AWS. The application uses Amazon Cognito user pools for authentication. Now, the team wants to extend the application so that authenticated users can upload and access personal course-related documents in Amazon S3. The solution must ensure scalable, fine-grained and secure access control to the S3 bucket and maintain private network architecture for the application. Which combination of steps will enable secure S3 integration for this workload? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because creating an S3 VPC endpoint establishes a private connection between the application running in the VPC and S3. This ensures that traffic to S3 does not traverse the public internet, fulfilling the requirement of maintaining a private network architecture. It allows the application in the private subnet to access S3 without needing an internet gateway, NAT gateway, or public IP address.\n\n**Why option 3 is correct:**\nThis is correct because Cognito Identity Pools (Federated Identities) provide a mechanism to grant users temporary AWS credentials to access AWS resources like S3 after they have authenticated with a Cognito User Pool (or other identity provider). This allows for fine-grained access control by defining IAM roles that specify what actions users can perform on the S3 bucket. The temporary credentials ensure that users don't have long-lived access keys, enhancing security. This approach also allows for scalability as Cognito manages the credential vending process.\n\n**Why option 1 is incorrect:**\nThis is incorrect because relying solely on a custom HTTP header for authentication is not a secure practice. Headers can be easily spoofed or manipulated, making it vulnerable to unauthorized access. While it might add a layer of obscurity, it doesn't provide robust security or fine-grained control.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while a Lambda function can act as a proxy for S3 uploads, invoking it after each user login is not the correct approach. Lambda functions should be invoked when a user attempts to upload a file, not just after login. Also, this approach adds unnecessary complexity and latency to the upload process. The Identity Pool approach is more efficient and secure for granting temporary credentials.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A retail startup runs a high-traffic order processing system on AWS. The architecture includes a frontend web tier using EC2 instances behind an Application Load Balancer, a processing tier powered by EC2 instances, and a data layer using Amazon DynamoDB. The frontend and processing tiers are decoupled using Amazon SQS. Recently, the engineering team observed that during unpredictable traffic surges, order processing slows down significantly, SQS queue depth increases rapidly, and the processing-tier EC2 instances hit 100% CPU usage. Which solution will help improve the application’s responsiveness and scalability during peak load periods?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the problem directly by using an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier based on the `ApproximateNumberOfMessages` in the SQS queue. This metric accurately reflects the workload on the processing tier. By scaling the processing tier in response to the queue depth, the system can dynamically adjust its capacity to handle the incoming load, preventing CPU saturation and maintaining responsiveness during peak periods. Target tracking policies simplify scaling configuration by automatically adjusting the number of instances to maintain a specified target value for a chosen metric.\n\n**Why option 0 is incorrect:**\nScheduling batch processing jobs every 10 minutes using EventBridge is not an efficient solution for handling unpredictable traffic surges. It introduces a fixed delay and doesn't dynamically adjust to the real-time queue depth. The 10-minute interval might be too long during peak periods, leading to continued slowdowns, or too short during off-peak periods, resulting in underutilization of resources. Batch processing is more suitable for periodic tasks rather than real-time scaling.\n\n**Why option 1 is incorrect:**\nWhile Kinesis Data Streams can handle high-throughput data ingestion, it doesn't directly address the CPU utilization issue in the processing tier. Adding Kinesis would introduce additional complexity and might not be necessary if SQS is already effectively decoupling the frontend and processing tiers. The core problem is the processing tier's inability to keep up with the incoming messages in the SQS queue, not the ingestion of order events from the web tier. The processing tier needs to scale based on the SQS queue depth, not the incoming order events.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time. As a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Store the images using the Amazon S3 Standard-IA storage class",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the images using the Amazon S3 Intelligent-Tiering storage class",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the most cost-effective solution because Amazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on changing access patterns. It eliminates the need for manual monitoring and data movement, reducing operational overhead and ensuring optimal storage costs without performance impact. It is designed to optimize costs by automatically moving data to the most cost-effective access tier based on usage patterns, making it ideal for scenarios where access patterns are unknown or change over time.\n\n**Why option 0 is incorrect:**\nWhile Amazon S3 Standard-IA is suitable for infrequently accessed data, it requires knowing beforehand which objects are infrequently accessed. The question states that some images are frequently accessed, and others are not. Using Standard-IA for all images would be inefficient and potentially more expensive for frequently accessed images. It doesn't dynamically adapt to changing access patterns.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nWhile this approach would work, it involves developing and maintaining a custom data monitoring application on EC2. This adds operational overhead and complexity. Furthermore, using S3 One Zone-IA introduces a risk of data loss if the Availability Zone becomes unavailable, which is not ideal for a social media company's image storage. Intelligent Tiering offers a managed solution that avoids the need for custom application development and maintenance and provides better availability.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 34,
    "text": "A tech enterprise operates several workloads using Amazon EC2, AWS Fargate, and AWS Lambda across various teams. To optimize compute costs, the company has purchased Compute Savings Plans. The cloud operations team needs to implement a solution that not only monitors utilization but also sends automated alerts when coverage levels of the Compute Savings Plans fall below a defined threshold. What is the MOST operationally efficient way to achieve this?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most operationally efficient solution because AWS Budgets is specifically designed for cost management and monitoring. It allows you to create budgets for Savings Plans coverage, define thresholds, and configure notifications. This approach requires minimal setup and maintenance compared to custom solutions or manual monitoring. It directly addresses the requirement of monitoring Savings Plans coverage and alerting when it falls below a defined threshold using a managed AWS service.\n\n**Why option 1 is incorrect:**\nThis option involves creating a custom script, storing data in S3, and using QuickSight for visualization. While it can achieve the desired outcome, it introduces significant operational overhead in terms of script maintenance, data storage management, and QuickSight dashboard maintenance. It's less efficient than using AWS Budgets, which is a purpose-built service for this type of monitoring.\n\n**Why option 2 is incorrect:**\nCompute Optimizer primarily focuses on right-sizing EC2 and Fargate instances and providing recommendations for cost optimization. While it can provide some insights into Savings Plans coverage, it's not its primary function, and relying on it solely for Savings Plans coverage monitoring and alerting is less direct and efficient than using AWS Budgets. The automatic notifications are not specifically tailored for Savings Plans coverage thresholds.\n\n**Why option 3 is incorrect:**\nCreating a custom dashboard in CloudWatch and using metric math to estimate coverage is a complex and error-prone approach. It requires a deep understanding of the underlying metrics and the ability to accurately estimate coverage. It also involves manually configuring alarms and managing the dashboard. This is significantly less efficient than using AWS Budgets, which provides built-in support for Savings Plans coverage monitoring and alerting.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern. Which of the following Amazon SQS queue types is the best fit to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Queue Service (Amazon SQS) temporary queues",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Simple Queue Service (Amazon SQS) dead-letter queues",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Simple Queue Service (Amazon SQS) delay queues",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Simple Queue Service (Amazon SQS) FIFO queues",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because temporary queues in Amazon SQS are specifically designed to simplify request-response patterns. They automatically manage the creation and deletion of reply queues, reducing development overhead and deployment complexity. This is particularly beneficial for high-throughput scenarios where managing a large number of reply queues manually would be cumbersome and costly. The automatic management of these queues reduces the operational burden and associated costs.\n\n**Why option 1 is incorrect:**\nDead-letter queues are used for handling messages that cannot be processed successfully after a certain number of attempts. They are not directly related to simplifying request-response patterns or reducing development time. Their primary purpose is to isolate problematic messages for further investigation and prevent them from indefinitely retrying and potentially causing issues.\n\n**Why option 2 is incorrect:**\nDelay queues postpone the delivery of messages for a specified duration. While they can be useful in certain scenarios, they do not directly address the requirements of a high-throughput request-response pattern or reduce development time and deployment costs. They introduce a delay, which is not desirable in a high-throughput scenario.\n\n**Why option 3 is incorrect:**\nFIFO (First-In-First-Out) queues guarantee that messages are processed in the order they are sent. While ordering can be important in some applications, it is not the primary focus of this question, which emphasizes high throughput and cost-effectiveness for a request-response pattern. FIFO queues also have lower throughput limits compared to standard queues, making them less suitable for high-throughput scenarios unless specifically required for ordering.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales. Which of the following is the MOST cost-optimal solution to fix this issue?",
    "options": [
      {
        "id": 0,
        "text": "Create a Read Replica in another Region as the Master database and point the analytics workload there",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Read Replica in the same Region as the Master database and point the analytics workload there",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the analytics application to AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the most cost-effective solution because a Read Replica in the same region provides a separate database instance for the analytics workload without the added latency and cost associated with cross-region replication. It allows the analytics workload to run without impacting the performance of the primary database used by the e-commerce application. Since the question emphasizes cost-optimization, avoiding cross-region data transfer costs is crucial.\n\n**Why option 0 is incorrect:**\nCreating a Read Replica in another region would introduce cross-region data transfer costs and potentially higher latency, making it less cost-optimal than a Read Replica within the same region. While it would isolate the analytics workload, the added expense isn't justified when a same-region replica achieves the same goal more efficiently.\n\n**Why option 2 is incorrect:**\nEnabling Multi-AZ provides high availability and failover capabilities, but it doesn't address the performance issue caused by the analytics workload. The standby database in a Multi-AZ setup is not intended for read operations; it's primarily for failover purposes. Therefore, it wouldn't isolate the analytics workload and wouldn't solve the performance problem.\n\n**Why option 3 is incorrect:**\nMigrating the analytics application to AWS Lambda is not a suitable solution in this scenario. Analytics workloads typically involve complex queries and large datasets, which are not well-suited for the stateless and short-lived nature of Lambda functions. Furthermore, it would require significant code changes and potentially introduce new complexities without necessarily improving cost-effectiveness.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A technology startup has stabilized its cloud infrastructure after a successful product launch. The backend services are now running at a predictable rate with minimal scaling events. The application architecture includes workloads running on Amazon EC2, AWS Lambda functions for asynchronous processing, container workloads on AWS Fargate, and machine learning inference models deployed with Amazon SageMaker. The company is now focusing on reducing long-term operational expenses without redesigning its architecture. The company wants to apply long-term pricing discounts with the least administrative overhead and the broadest service coverage possible using the fewest number of savings plans. Which combination of savings plans will satisfy these requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances",
        "correct": true
      },
      {
        "id": 2,
        "text": "Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type",
        "correct": false
      },
      {
        "id": 4,
        "text": "Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because SageMaker Savings Plans are specifically designed to provide cost savings on SageMaker usage, including training, inference, and notebook instances. Given that the company uses SageMaker for machine learning inference, this plan directly addresses a significant portion of their compute costs. It offers a dedicated discount for SageMaker workloads, aligning with the requirement of reducing long-term operational expenses without architectural changes.\n\n**Why option 4 is correct:**\nThis is correct because Compute Savings Plans offer the broadest coverage among the Savings Plan options. They provide discounts for usage across EC2, Fargate, and Lambda. This aligns perfectly with the startup's architecture, which includes all three services. By purchasing a Compute Savings Plan, the company can achieve significant cost savings across a large portion of their infrastructure with a single plan, minimizing administrative overhead and maximizing coverage.\n\n**Why option 0 is incorrect:**\nThis is incorrect because creating Reserved Instances for each EC2 instance and monitoring their utilization involves significant administrative overhead. While Reserved Instances offer cost savings, managing them individually and tracking their utilization requires ongoing effort. The question specifically asks for a solution with the least administrative overhead. Furthermore, Reserved Instances only cover EC2 and do not address the costs associated with Lambda, Fargate, or SageMaker.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a hybrid deployment discount plan is not a standard AWS Savings Plan offering. Savings Plans are specific to AWS services. The question focuses on optimizing costs within the AWS cloud environment, not hybrid deployments. This option introduces a non-existent plan and is therefore not a valid solution.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution. As a solutions architect, which of the following options would you recommend as the solution?",
    "options": [
      {
        "id": 0,
        "text": "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by providing a highly available and scalable bastion host setup. A Network Load Balancer (NLB) is suitable for TCP traffic, which is commonly used for SSH or RDP connections to bastion hosts. The NLB distributes traffic across multiple EC2 instances acting as bastion hosts. The Auto Scaling Group ensures that the desired number of bastion hosts are always running, automatically replacing any instances that fail. The NLB provides a single point of entry and automatically routes traffic to healthy instances, ensuring high availability.\n\n**Why option 1 is incorrect:**\nWhile an Application Load Balancer (ALB) can also distribute traffic across multiple EC2 instances managed by an Auto Scaling Group, it's designed for HTTP/HTTPS traffic. Bastion hosts typically use SSH or RDP, which are TCP-based protocols. An NLB is more suitable for these protocols because it operates at Layer 4 and provides better performance and lower latency for TCP traffic.\n\n**Why option 2 is incorrect:**\nAssigning an Elastic IP (EIP) to each EC2 instance does not provide high availability. If one instance fails, the administrator needs to manually reassign the EIP to a new instance, which introduces downtime. This solution also doesn't provide load balancing, so only one instance is actively handling traffic at a time. An Auto Scaling group can replace failed instances, but the manual EIP reassignment makes this option not highly available.\n\n**Why option 3 is incorrect:**\nA VPC Endpoint allows private connections to AWS services without traversing the public internet. While it enhances security for accessing AWS services from within the VPC, it doesn't provide a solution for accessing resources *within* the VPC from the public internet via a bastion host. Bastion hosts need to be publicly accessible, and VPC Endpoints are for private connectivity.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class. Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
    "options": [
      {
        "id": 0,
        "text": "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Storage class analysis only provides recommendations for Standard to Standard IA classes",
        "correct": true
      },
      {
        "id": 2,
        "text": "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because S3 Analytics storage class analysis is designed to observe access patterns and provide recommendations for transitioning data from the Standard storage class to the Standard IA (Infrequent Access) storage class. This helps optimize costs by moving less frequently accessed data to a cheaper storage tier.\n\n**Why option 0 is incorrect:**\nThis is incorrect because S3 Analytics does not directly recommend transitions from Standard to Standard One-Zone IA. While Standard One-Zone IA is a valid storage class, the primary focus of S3 Analytics is to recommend transitions to Standard IA based on access patterns.\n\n**Why option 2 is incorrect:**\nThis is incorrect because S3 Analytics primarily focuses on transitions to Standard IA. While transitioning to Glacier Flexible Retrieval is possible, S3 Analytics doesn't directly provide recommendations for this transition based on access patterns in the same way it does for Standard IA. Other mechanisms like lifecycle policies are more commonly used for Glacier transitions.\n\n**Why option 3 is incorrect:**\nThis is incorrect because, similar to Glacier Flexible Retrieval, S3 Analytics doesn't directly provide recommendations for transitions to Glacier Deep Archive based on access patterns. Lifecycle policies are the more common approach for transitioning to Glacier Deep Archive.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities. Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby",
        "correct": true
      },
      {
        "id": 1,
        "text": "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",
        "correct": true
      },
      {
        "id": 4,
        "text": "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon RDS Multi-AZ deployments are designed for high availability and durability. During maintenance windows, RDS performs operating system updates by first applying them to the standby instance. Once the standby is updated, it is promoted to become the primary instance. The original primary instance is then updated and becomes the new standby. This process minimizes downtime during maintenance operations.\n\n**Why option 3 is correct:**\nThis is correct because a key feature of Amazon RDS Multi-AZ is automatic failover. If the primary database instance fails due to issues like hardware failure, network outage, or instance unavailability, Amazon RDS automatically promotes the standby instance to become the new primary instance. This failover process helps to maintain database availability and minimize application downtime.\n\n**Why option 1 is incorrect:**\nThis is incorrect because automated backups in RDS Multi-AZ are taken from the standby instance, not the primary. This avoids suspending I/O activity on the primary database during the backup process, ensuring minimal impact on application performance.\n\n**Why option 2 is incorrect:**\nThis is incorrect because updates to the database instance in a Multi-AZ deployment are synchronously replicated to the standby instance. Synchronous replication ensures that the standby instance has an up-to-date copy of the data, which is crucial for a seamless failover. Asynchronous replication would introduce the risk of data loss during a failover.\n\n**Why option 4 is incorrect:**\nThis is incorrect because the standby instance in a Multi-AZ deployment is not designed to serve read requests. Its primary purpose is to provide a hot standby for failover. To enhance read scalability, you should consider using Amazon RDS Read Replicas, which are specifically designed for read-heavy workloads.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A digital media firm is scaling its cloud footprint and wants to isolate development, testing, and production workloads using separate AWS accounts. It also wants a centralized approach to managing networking infrastructure such as subnets and gateways, without repeating configurations in every account. Additionally, the solution must enforce security best practices—like mandatory logging and guardrails—when new accounts are created. The firm prefers a low-maintenance, governance-driven setup. Which solution best meets these goals while minimizing operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements effectively by using AWS Control Tower to create and govern AWS accounts, ensuring isolation between development, testing, and production. Control Tower automates the setup of a multi-account environment based on AWS best practices, including mandatory logging and guardrails. Deploying a centralized VPC in a shared networking account allows for centralized management of network resources like subnets and gateways, reducing configuration duplication. Sharing the subnets across workload accounts using AWS Resource Access Manager (AWS RAM) enables these accounts to utilize the centralized network infrastructure without needing to create their own. This approach minimizes operational overhead by leveraging Control Tower's automation and governance features.\n\n**Why option 0 is incorrect:**\nWhile AWS Organizations can create accounts and AWS RAM can share subnets, relying on manual SCPs for guardrails increases operational overhead and is less automated than using AWS Control Tower. This option lacks the comprehensive governance and automation features provided by Control Tower, making it a less desirable solution for the firm's low-maintenance requirement. Furthermore, it doesn't inherently enforce best practices during account creation like Control Tower does.\n\n**Why option 1 is incorrect:**\nDeploying separate VPCs in each workload account increases operational complexity and contradicts the requirement for centralized network management. While Gateway Load Balancers can centralize security inspection, this approach adds overhead and cost compared to a centralized VPC. AWS Control Tower is appropriate for account creation, but the network architecture is not optimal. This solution does not minimize operational overhead as effectively as a centralized VPC approach.\n\n**Why option 2 is incorrect:**\nAWS Service Catalog is useful for provisioning resources, but it doesn't provide the comprehensive account governance and management capabilities of AWS Control Tower. While AWS Config conformance packs can enforce networking guardrails, they require more manual configuration and maintenance compared to Control Tower's automated guardrails. This option doesn't address the requirement for centralized networking as effectively as a shared VPC and doesn't provide the same level of automated governance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
    "options": [
      {
        "id": 0,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis configuration is not allowed because Elastic Load Balancing is a regional service. It cannot distribute traffic to EC2 instances located in different regions. In this case, the EC2 instances are deployed in us-east-1 and us-west-1, which are different regions. Therefore, a single ELB cannot manage traffic across these two regions.\n\n**Why option 0 is incorrect:**\nThis configuration is allowed because Elastic Load Balancing can distribute traffic across multiple Availability Zones within the same region. All four instances are in us-east-1, and they are distributed across two AZs, which is a valid setup for ELB.\n\n**Why option 2 is incorrect:**\nThis configuration is allowed because Elastic Load Balancing can distribute traffic to EC2 instances within the same Availability Zone and region. Although it's generally recommended to distribute instances across multiple AZs for high availability, it's still a valid configuration to have all instances in a single AZ within the same region.\n\n**Why option 3 is incorrect:**\nThis configuration is allowed because Elastic Load Balancing can distribute traffic to EC2 instances within the same Availability Zone and region. Although it's generally recommended to distribute instances across multiple AZs for high availability, it's still a valid configuration to have all instances in a single AZ within the same region.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "A global enterprise maintains a hybrid cloud environment and wants to transfer large volumes of data between its on-premises data center and Amazon S3 for backup and analytics workflows. The company has already established a Direct Connect (DX) connection to AWS and wants to ensure high-bandwidth, low-latency, and secure private connectivity without traversing the public internet. The architecture must be designed to access Amazon S3 directly from on-premises systems using this DX connection. Which configuration should the network engineering team implement to allow direct access to Amazon S3 from the on-premises data center using Direct Connect?",
    "options": [
      {
        "id": 0,
        "text": "Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a Public Virtual Interface (Public VIF) allows access to public AWS service endpoints, including Amazon S3 public IP addresses, over the Direct Connect connection. By provisioning a Public VIF, the on-premises systems can directly access S3 buckets using their public endpoints without routing traffic over the public internet. This leverages the dedicated bandwidth and low latency provided by Direct Connect, fulfilling the requirements of high-bandwidth, low-latency, and secure private connectivity.\n\n**Why option 1 is incorrect:**\nThis is incorrect because using a VPN connection over the public internet defeats the purpose of having a Direct Connect connection. The question explicitly states the need to avoid traversing the public internet. A VPN would introduce additional latency and is not the optimal solution for high-bandwidth data transfer.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while a Private VIF and VPC endpoint can provide private connectivity to S3, it requires routing the traffic through a VPC. The question implies a direct connection requirement without the overhead of routing through a VPC. While this setup is valid, it's not the most direct or efficient way to access S3 from on-premises using Direct Connect when the primary goal is to avoid public internet and leverage the DX connection directly. A Public VIF is a more direct path to S3 in this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Require HTTPS for communication between Amazon CloudFront and your S3 origin",
        "correct": false
      },
      {
        "id": 1,
        "text": "Require HTTPS for communication between Amazon CloudFront and your custom origin",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront signed URLs",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront signed cookies",
        "correct": true
      },
      {
        "id": 4,
        "text": "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because signed URLs allow you to control access to individual files for a limited time. You generate a URL with an expiration date and time, and only users with that URL can access the content. The application can verify the user's subscription status and then generate a signed URL for them to access the content. This effectively restricts access to only bona fide subscribers.\n\n**Why option 3 is correct:**\nThis is correct because signed cookies allow you to control access to multiple restricted files. After a user authenticates (e.g., by logging in and confirming their subscription), your application can set a signed cookie. CloudFront then uses this cookie to verify that the user is authorized to access the content. This is useful when you want to grant access to multiple files without generating individual signed URLs for each file. This is also suitable for streaming scenarios where the user needs to access multiple segments of the video.\n\n**Why option 0 is incorrect:**\nThis is incorrect because requiring HTTPS between CloudFront and S3 only encrypts the data in transit. It doesn't restrict access based on user subscription status. Anyone with the CloudFront distribution URL could still access the content, regardless of whether they are a subscriber or not.\n\n**Why option 1 is incorrect:**\nThis is incorrect because requiring HTTPS between CloudFront and a custom origin also only encrypts the data in transit. It doesn't restrict access based on user subscription status. The custom origin would still need to implement an authentication mechanism to verify the user's subscription status, and this option doesn't provide that. Forwarding HTTPS requests with specific ciphers doesn't address the requirement of restricting content to subscribers.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in read request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Shield",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Direct Connect",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Aurora Replica",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a CloudFront distribution caches content closer to users, reducing the load on the Application Load Balancer and the backend EC2 instances. By caching static and frequently accessed content, CloudFront significantly reduces the number of requests that reach the origin server, thereby improving the application's ability to handle spikes in read requests. This also improves the user experience by reducing latency.\n\n**Why option 4 is correct:**\nThis is correct because Amazon Aurora Replicas provide read-only copies of the data in the Aurora database. By directing read requests to these replicas, the load on the primary Aurora instance is reduced, improving the database's ability to handle spikes in read request rates. Aurora Replicas are designed for read scaling and can significantly improve the performance and availability of read-heavy applications.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Global Accelerator improves the performance of TCP and UDP traffic by routing traffic through AWS's global network infrastructure. While it can improve performance, it doesn't directly address the need to scale read requests for the Aurora database or cache content to reduce load on the origin servers. Global Accelerator is more suitable for improving global application availability and performance, not specifically for handling read request spikes within a region.\n\n**Why option 2 is incorrect:**\nThis is incorrect because AWS Shield provides protection against DDoS attacks. While important for overall security, it does not address the specific requirement of scaling read requests or caching content to handle periodic spikes in read request rates. Shield protects against malicious traffic, not legitimate increases in user activity.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Direct Connect establishes a dedicated network connection from on-premises to AWS. This is beneficial for hybrid cloud scenarios and transferring large amounts of data, but it does not help in scaling read requests or caching content to handle spikes in read request rates for a social media application. Direct Connect focuses on network connectivity, not application scaling.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations. Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "VPC Route Table",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Inspector",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon GuardDuty",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Shield Advanced",
        "correct": true
      },
      {
        "id": 4,
        "text": "AWS Web Application Firewall (AWS WAF)",
        "correct": true
      },
      {
        "id": 5,
        "text": "VPC Security Group",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4,
      5
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because AWS Firewall Manager can be used to centrally manage AWS Shield Advanced protections. It allows you to apply Shield Advanced protections consistently across your accounts and resources, helping to mitigate DDoS attacks.\n\n**Why option 4 is correct:**\nThis is correct because AWS Firewall Manager can centrally manage AWS WAF rules. This allows you to define and enforce web application firewall rules across multiple AWS accounts and applications, providing consistent protection against common web exploits.\n\n**Why option 5 is correct:**\nThis is correct because AWS Firewall Manager can be used to manage VPC Security Groups. It enables you to define and enforce security group rules across your VPCs within your AWS Organization, ensuring consistent network security policies.\n\n**Why option 0 is incorrect:**\nThis is incorrect because AWS Firewall Manager does not directly configure VPC Route Tables. Route tables are managed separately and define how network traffic is routed within a VPC.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Firewall Manager does not directly configure Amazon Inspector. Amazon Inspector is a vulnerability management service that assesses AWS resources for security vulnerabilities and deviations from best practices. While important for security, it's not directly managed by Firewall Manager.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS. As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon MQ",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Simple Queue Service (Amazon SQS) Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon SQS FIFO (First-In-First-Out)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best option because Amazon MQ is a managed message broker service that supports popular message brokers, including RabbitMQ. It allows you to use industry-standard APIs, protocols, and clients to migrate without rewriting code. Amazon MQ handles the provisioning, setup, and maintenance of the message broker, simplifying the migration process and reducing operational overhead. It provides a drop-in replacement for RabbitMQ, making it the quickest and easiest migration path.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon SNS is a publish/subscribe messaging service primarily used for broadcasting messages to multiple subscribers. It doesn't offer the same message queuing and broker functionalities as RabbitMQ, making it unsuitable for a direct migration. SNS would require significant architectural changes and code modifications.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon SQS is a fully managed message queuing service, but it uses a different protocol than RabbitMQ. Migrating to SQS would require significant code changes to adapt to the SQS API and message format. While SQS is a viable messaging service, it doesn't provide the quick and easy migration path needed in this scenario.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon SQS FIFO queues provide strict message ordering, which might be a requirement in some cases, but like standard SQS queues, they use a different protocol than RabbitMQ. Migrating to SQS FIFO would require significant code changes to adapt to the SQS API and message format. It doesn't offer the quick and easy migration path needed in this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 48,
    "text": "A digital media company wants to track user engagement across its streaming platform by capturing events such as video starts, pauses, and search queries. These events must be ingested and analyzed in real time to improve user experience and optimize recommendations. The platform experiences unpredictable spikes in traffic during popular content releases. The company needs a highly scalable and serverless solution that can seamlessly adjust to changing workloads without manual provisioning. Which solution will meet these requirements in the MOST efficient and scalable way?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by using Kinesis Data Streams in on-demand capacity mode. On-demand capacity mode automatically scales the stream's capacity in response to varying workloads, eliminating the need for manual provisioning and ensuring the system can handle unpredictable traffic spikes. The Lambda function acts as a consumer, processing the events in real time, which allows for immediate analysis and optimization of the user experience. This combination provides a serverless and highly scalable solution for real-time event processing.\n\n**Why option 0 is incorrect:**\nWhile Kinesis Data Firehose can ingest user events and store them in S3, using Athena with scheduled queries introduces a delay in analysis. This approach does not meet the real-time analysis requirement. Furthermore, scheduled queries are not ideal for handling unpredictable traffic spikes, as they are not dynamically adjusted based on workload.\n\n**Why option 2 is incorrect:**\nSNS and SQS can be used for event-driven architectures, but they are not designed for high-throughput data ingestion and real-time analytics of streaming data. Using Glue jobs scheduled at fixed intervals introduces latency and does not provide real-time analysis. Additionally, this approach is not as efficient or scalable as Kinesis Data Streams for this specific use case.\n\n**Why option 3 is incorrect:**\nDeploying a fleet of EC2 instances running Apache Kafka introduces significant operational overhead, including manual scaling and infrastructure management. This approach contradicts the serverless requirement. While Kafka is a powerful streaming platform, it is not the most efficient or cost-effective solution for this scenario compared to Kinesis Data Streams in on-demand mode. Using Athena for periodic queries also fails to meet the real-time analysis requirement.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
    "options": [
      {
        "id": 0,
        "text": "VPC Endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "VPC Peering connection",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS VPN CloudHub",
        "correct": true
      },
      {
        "id": 3,
        "text": "Software VPN",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by providing a simple and cost-effective way to enable communication between multiple VPN connections and a Direct Connect connection. AWS VPN CloudHub allows you to create a hub-and-spoke VPN network, where the VPC acts as the central hub. The branch offices (Site-to-Site VPNs) and the corporate headquarters (Direct Connect) can connect to the CloudHub, enabling them to communicate with each other. It simplifies routing and management compared to other solutions, especially when dealing with multiple VPN connections.\n\n**Why option 0 is incorrect:**\nThis is incorrect because VPC Endpoints are used to privately connect to AWS services from within your VPC, without using public IPs. They do not facilitate connectivity between different networks like on-premises locations or branch offices. VPC Endpoints are not relevant to the requirement of enabling communication between the headquarters and branch offices.\n\n**Why option 1 is incorrect:**\nThis is incorrect because VPC Peering connections are used to connect two VPCs together. While you could theoretically create VPC peering connections between the VPC connected to the headquarters and separate VPCs for each branch office, this would not directly solve the problem. The branch offices are not VPCs; they are on-premises networks connected via VPN. Furthermore, VPC peering does not transitively route traffic. You would need to create separate peering connections between each VPC, leading to a complex and less scalable solution compared to VPN CloudHub. It also doesn't address the Direct Connect connection.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
    "options": [
      {
        "id": 0,
        "text": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic",
        "correct": true
      },
      {
        "id": 1,
        "text": "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior",
        "correct": false
      },
      {
        "id": 3,
        "text": "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Security Groups operate at the instance level and are stateful. When you allow inbound traffic on a specific port, the response traffic is automatically allowed back out, regardless of outbound rules. Network ACLs, however, operate at the subnet level and are stateless. This means that you need to explicitly allow both inbound and outbound traffic. If only inbound traffic is allowed in the Network ACL, the response traffic from the EC2 instance will be blocked, preventing the connection from being established.\n\n**Why option 1 is incorrect:**\nThis is incorrect because it reverses the stateful and stateless nature of Security Groups and Network ACLs. Security Groups are stateful, and Network ACLs are stateless.\n\n**Why option 2 is incorrect:**\nThis is incorrect because modifying Network ACL rules from the command line is a valid operation and does not inherently block or cause erratic behavior. The issue is with the configuration of the rules themselves, not the method of modification.\n\n**Why option 3 is incorrect:**\nThis is incorrect because IAM Roles are assigned to EC2 instances and are used to grant permissions to AWS services. They are not directly associated with Security Groups or Network ACLs in the way described. Security Groups control network traffic based on IP addresses, protocols, and ports, while Network ACLs provide an additional layer of security at the subnet level.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement. As a solutions architect, which of the following will you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of high availability and immediate document retrieval. Amazon S3 is designed for 99.999999999% durability and 99.99% availability. By storing the documents in S3, the application benefits from S3's inherent redundancy and availability across multiple AZs. Furthermore, S3 provides low-latency access to objects, ensuring immediate document retrieval when requested. Using EBS as the root volume for the EC2 instances is a standard practice and doesn't conflict with using S3 for document storage.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because while Amazon S3 Glacier provides cost-effective archival storage, it is not suitable for immediate document retrieval. Retrieving data from Glacier can take several hours, which violates the requirement of immediate document access. Also, EBS as a root volume is independent of the document store.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because creating snapshots and building new volumes in other AZs is a manual and time-consuming process. It does not provide the immediate availability required by the application. While snapshots are useful for disaster recovery, they do not offer a real-time, highly available solution. The recovery time objective (RTO) would be too high.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because Instance Store volumes are ephemeral, meaning the data stored on them is lost when the instance is stopped, terminated, or fails. This makes them unsuitable for storing critical documents that require high availability and durability. Provisioned IOPS does not change the ephemeral nature of Instance Store. Also, mounting these volumes to multiple EC2 instances is not a standard or reliable way to achieve data consistency and availability.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 52,
    "text": "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "AWS Transit Gateway",
        "correct": true
      },
      {
        "id": 1,
        "text": "VPC Peering Connection",
        "correct": false
      },
      {
        "id": 2,
        "text": "Virtual private gateway (VGW)",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS PrivateLink",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because AWS Transit Gateway is designed to simplify network connectivity between multiple VPCs and on-premises networks. It acts as a central hub, allowing you to connect VPCs in a star topology, regardless of the AWS account they reside in. It also supports Direct Connect integration for connecting to on-premises infrastructure. Transit Gateway reduces the operational complexity of managing numerous point-to-point connections, such as VPC peering, and provides centralized routing and security policies.\n\n**Why option 1 is incorrect:**\nThis is incorrect because VPC Peering is a one-to-one connection between two VPCs. While you can connect multiple VPCs using peering, it quickly becomes complex and difficult to manage, especially with a large number of VPCs. It doesn't inherently support a star topology or direct integration with Direct Connect for on-premises connectivity. The number of peering connections grows quadratically with the number of VPCs, leading to significant management overhead.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a Virtual Private Gateway (VGW) is used to establish VPN connections or Direct Connect connections from a VPC to on-premises networks. While a VGW is necessary for Direct Connect, it doesn't solve the problem of connecting multiple VPCs together in a scalable and manageable way. Each VPC would need its own VGW, and you'd still need a separate mechanism (like VPC peering) to connect the VPCs, which adds complexity.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS PrivateLink provides private connectivity between VPCs and supported AWS services, services hosted by other AWS accounts (referred to as endpoint services), and supported AWS Marketplace partner services. It's not designed for connecting multiple VPCs together in a general-purpose network or for connecting to on-premises networks. It focuses on providing secure access to specific services without exposing traffic to the public internet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A global logistics provider operates several legacy applications on virtual machines (VMs) within a private data center. Due to accelerated business growth and limited capacity in its existing infrastructure, the provider decides to migrate select applications to AWS. The company opts for a lift-and-shift strategy for its non-mission-critical systems to meet tight migration deadlines. The solution must support rapid migration without requiring extensive application refactoring. Which combination of actions will best support this migration approach? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Launch a cutover instance after completing testing and confirming that replication is up-to-date",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use",
        "correct": false
      },
      {
        "id": 2,
        "text": "Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations",
        "correct": false
      },
      {
        "id": 5,
        "text": "Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because launching a cutover instance after thorough testing and verifying replication ensures a smooth transition to the AWS environment. It minimizes downtime and confirms that the migrated application is functioning as expected before going live.\n\n**Why option 2 is correct:**\nThis is correct because performing initial replication and launching test instances in AWS is crucial for validating the migrated VMs. This allows the logistics provider to identify and address any compatibility issues or configuration errors before the final cutover, ensuring a successful migration.\n\n**Why option 3 is correct:**\nThis is correct because AWS Application Migration Service (MGN) is specifically designed for lift-and-shift migrations. Installing the AWS Replication Agent on the source VMs enables continuous replication of data to AWS, facilitating a rapid and automated migration process without requiring significant application changes.\n\n**Why option 1 is incorrect:**\nWhile AWS CloudEndure Disaster Recovery can be used for migration, AWS Application Migration Service (MGN) is the recommended service for lift-and-shift migrations. CloudEndure is more focused on disaster recovery scenarios, and MGN offers features specifically tailored for migration projects.\n\n**Why option 4 is incorrect:**\nAmazon EC2 Auto Scaling is designed for automatically scaling the number of EC2 instances based on demand. It doesn't directly address the initial migration of VMs from a private data center. While Auto Scaling can be used after the migration, it's not a suitable solution for the migration process itself.\n\n**Why option 5 is incorrect:**\nShutting down the source VMs and manually creating AMIs is a less efficient and more error-prone approach compared to using a dedicated migration service like AWS Application Migration Service (MGN). It also introduces significant downtime during the migration process. This method does not provide continuous replication and testing capabilities before cutover.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes. Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
    "options": [
      {
        "id": 0,
        "text": "Throughput Optimized HDD Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provisioned IOPS SSD Amazon EBS volumes",
        "correct": true
      },
      {
        "id": 2,
        "text": "General-purpose SSD-based Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Cold HDD Amazon EBS volumes",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct choice because Provisioned IOPS SSD (io1 and io2) volumes are the only EBS volume types that support Multi-Attach. Multi-Attach enables you to attach one io1 or io2 volume to multiple EC2 instances simultaneously. This allows for shared access to the data on the volume, which is the core requirement of the scenario. The other volume types do not support this functionality.\n\n**Why option 0 is incorrect:**\nThroughput Optimized HDD (st1) volumes do not support Multi-Attach. They are designed for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as big data, data warehouses, and log processing. While they offer good performance for these workloads, they cannot be attached to multiple instances simultaneously.\n\n**Why option 2 is incorrect:**\nGeneral Purpose SSD (gp2 and gp3) volumes are versatile and provide a balance of price and performance for a wide variety of workloads. However, they do not support Multi-Attach. Therefore, they cannot be used to provide shared data access to multiple EC2 instances simultaneously.\n\n**Why option 3 is incorrect:**\nCold HDD (sc1) volumes are the lowest cost HDD volume type and are designed for infrequently accessed data. They do not support Multi-Attach. They are not suitable for scenarios requiring shared data access among multiple EC2 instances.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 dedicated instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 dedicated hosts",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 on-demand instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 reserved instances (RI)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of using existing server-bound licenses. Dedicated Hosts allow you to bring your own licenses (BYOL) for operating systems and other software that are licensed on a per-server basis. This is because you have dedicated physical hardware, allowing you to comply with licensing terms that require dedicated hardware. While Dedicated Instances also provide dedicated hardware, Dedicated Hosts offer more control and flexibility in terms of instance placement and license management, making them the more suitable choice when BYOL is a primary concern.\n\n**Why option 0 is incorrect:**\nWhile Dedicated Instances provide dedicated hardware, they don't offer the same level of control over instance placement as Dedicated Hosts. Dedicated Instances are still placed on hardware shared with other AWS customers, just not at the instance level. This can sometimes cause licensing issues, as some software vendors require complete dedication of the physical server. Dedicated Hosts are a better choice when BYOL is a primary concern.\n\n**Why option 2 is incorrect:**\nOn-demand instances do not provide dedicated hardware and therefore do not allow for the use of server-bound licenses. They are a pay-as-you-go model where you don't have control over the underlying hardware, making them unsuitable for BYOL scenarios.\n\n**Why option 3 is incorrect:**\nReserved Instances (RI) are a billing discount applied to EC2 instances. They do not dictate the underlying hardware. You can apply RIs to On-Demand, Dedicated Instances, or Dedicated Hosts. Therefore, RIs do not address the core requirement of using existing server-bound licenses. They are a cost-saving mechanism, but not a solution for license compliance.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data. How can you build this index efficiently?",
    "options": [
      {
        "id": 0,
        "text": "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using an application to iterate through the S3 bucket and then using the byte range fetch feature to only retrieve the first 250 bytes of each file. This minimizes the amount of data transferred from S3, which is crucial given the large number of files and the total data volume. Storing the extracted metadata in RDS PostgreSQL allows for efficient indexing and querying.\n\n**Why option 0 is incorrect:**\nWhile this option also uses S3 Select Byte Range Fetch, it is less efficient than option 2. S3 Select is designed for querying data within S3 objects using SQL-like expressions. While it can fetch byte ranges, it introduces unnecessary overhead for this specific task, as the primary goal is simply to retrieve a fixed byte range, not to perform complex filtering or transformations. A direct byte range fetch is more streamlined and efficient.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because the Amazon RDS Import feature is designed for loading large datasets into RDS, typically from a file or a stream. It's not designed for selectively extracting data from individual S3 objects based on byte ranges. Furthermore, importing the entire 50 TB of data into RDS just to extract the first 250 bytes from each file would be extremely inefficient and costly. RDS is also not intended to store the entire file content, only the metadata.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort. What will you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by providing high availability for both the application and database tiers. The Auto Scaling group distributes EC2 instances across multiple Availability Zones, ensuring that if one AZ fails, the application remains available. The Application Load Balancer distributes traffic across the healthy EC2 instances. Configuring RDS MySQL in a multi-AZ configuration provides automatic failover to a standby replica in another Availability Zone in case of a primary instance failure, minimizing downtime and maintenance overhead. This approach leverages managed services for high availability and reduces the operational burden on the startup.\n\n**Why option 0 is incorrect:**\nWhile this option provides high availability for the application tier with the Auto Scaling group and Application Load Balancer, using a read replica for the RDS MySQL database does not provide automatic failover in case of a primary database failure. A read replica is primarily used for read scaling and offloading read traffic from the primary database. It requires manual intervention to promote the read replica to a standalone instance in case of a failure, which increases downtime and maintenance effort.\n\n**Why option 1 is incorrect:**\nPlacing the Auto Scaling group in a single Availability Zone negates the high availability requirement for the application tier. If that single AZ fails, the entire application becomes unavailable. While the multi-AZ RDS configuration provides database high availability, the application tier's single point of failure makes this option unsuitable.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Client Side Encryption",
        "correct": true
      },
      {
        "id": 1,
        "text": "Server-side encryption with AWS KMS keys (SSE-KMS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct choice because the company needs to use its proprietary encryption algorithm. Client-side encryption allows the company to encrypt the data before it is sent to S3, giving them full control over the encryption process and the ability to use their own algorithm. This meets the stringent security requirements and the need for a specific encryption scheme.\n\n**Why option 1 is incorrect:**\nThis is incorrect because SSE-KMS uses AWS Key Management Service (KMS) to manage the encryption keys. While secure, it doesn't allow the company to use its proprietary encryption algorithm. The encryption is performed by AWS using KMS-managed keys, not the company's own algorithm.\n\n**Why option 2 is incorrect:**\nThis is incorrect because SSE-C allows the company to provide the encryption keys to AWS, but the encryption algorithm is still managed by AWS. The company cannot use its proprietary encryption algorithm with SSE-C. AWS handles the encryption using the provided key, not the company's custom algorithm.\n\n**Why option 3 is incorrect:**\nThis is incorrect because SSE-S3 uses Amazon S3 managed keys for encryption. The encryption algorithm and key management are entirely handled by AWS. The company has no control over the encryption algorithm and cannot use its proprietary algorithm.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A global enterprise has onboarded multiple departments into isolated AWS accounts that are part of a unified AWS Organizations structure. Recently, a critical operational alert was missed because it was delivered to the root user’s email address of an account, which is only monitored intermittently. The enterprise wants to redesign its notification handling process to ensure that future communications - categorized by billing, security, and operational relevance - are received promptly by the appropriate teams. The solution should align with AWS security best practices and offer centralized oversight without depending on individual users. Which solution meets these requirements in the most secure and scalable way?",
    "options": [
      {
        "id": 0,
        "text": "Configure each AWS account’s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as “billing,” “security,” or “operations.” Keep the current root email addresses as they are, and rely on this service to triage alerts",
        "correct": false
      },
      {
        "id": 2,
        "text": "Assign each AWS account’s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change each AWS account’s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the problem by first configuring each AWS account's root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. This ensures that all root user emails are captured and reviewed centrally, preventing missed alerts. Furthermore, it leverages AWS alternate contacts, which are designed for this purpose, using company-managed distribution lists for billing, security, and operations. This ensures that service-specific notifications are routed directly to the appropriate teams, aligning with the requirement for prompt delivery and avoiding reliance on individual users. Using distribution lists also promotes scalability and maintainability, as team membership changes can be managed within the lists without requiring changes to the AWS account configurations. This approach aligns with AWS security best practices by minimizing the use of the root user account and delegating responsibilities to dedicated teams.\n\n**Why option 1 is incorrect:**\nWhile setting up a centralized email forwarding service might seem like a viable option, inspecting email content based on keywords is complex, prone to errors, and can introduce security vulnerabilities. It also doesn't directly utilize AWS's built-in alternate contacts feature, which is designed for this purpose. Relying on keyword filtering for critical alerts is not a robust or secure solution, and maintaining the current root email addresses without addressing the monitoring issue defeats the purpose of the redesign.\n\n**Why option 2 is incorrect:**\nAssigning root user email addresses to individual team members is a poor security practice. It creates a dependency on specific individuals and makes it difficult to manage access and ensure continuity in case of personnel changes. Encouraging regular monitoring is not a reliable solution, as it depends on human diligence, which is prone to errors. This approach also doesn't provide centralized oversight and introduces a single point of failure.\n\n**Why option 3 is incorrect:**\nChanging the root email to a departmental email list is better than assigning it to an individual, but it still doesn't address the need for centralized oversight and monitoring of root user communications. While configuring IAM notification settings can route some alerts, it doesn't cover all types of notifications that might be sent to the root user. Discarding the use of AWS alternate contacts is a mistake, as they are the recommended way to manage service-specific notifications. This option also fails to provide a comprehensive solution for all notification types.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution correctly addresses the requirement for private access to both S3 and DynamoDB. Gateway endpoints are designed specifically for these services, allowing EC2 instances in a private subnet to access them without using public IPs or NAT gateways. Creating separate gateway endpoints for each service ensures that traffic to S3 and DynamoDB remains within the AWS network. Adding the gateway endpoints as targets in the VPC's route table directs traffic destined for S3 and DynamoDB through the gateway endpoints, effectively establishing the private connection.\n\n**Why option 0 is incorrect:**\nWhile creating a gateway endpoint for S3 is correct, creating an *interface* endpoint for DynamoDB is not the most efficient or cost-effective solution in this scenario. Gateway endpoints are specifically designed for S3 and DynamoDB and are generally preferred for these services due to their simplicity and cost. Interface endpoints, while providing private connectivity, are more suitable for services where gateway endpoints are not available. Using an interface endpoint for DynamoDB adds unnecessary complexity and cost.\n\n**Why option 2 is incorrect:**\nCreating interface endpoints for both S3 and DynamoDB provides private connectivity, but it's not the most optimal solution. Gateway endpoints are designed specifically for S3 and DynamoDB and are generally preferred due to their simplicity and cost-effectiveness. Interface endpoints are better suited for other AWS services or partner services that don't have gateway endpoints. Using interface endpoints for S3 and DynamoDB adds unnecessary complexity and potentially higher costs.\n\n**Why option 3 is incorrect:**\nCreating a gateway endpoint for DynamoDB is correct for private access to DynamoDB. However, Origin Access Identity (OAI) is used to restrict access to S3 buckets to CloudFront distributions, not for providing private access from EC2 instances. Connecting to S3 using the private IP address is not a standard or supported method for accessing S3 from within a VPC. The correct approach for private S3 access is to use a gateway endpoint.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
    "options": [
      {
        "id": 0,
        "text": "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use delay queues to postpone the delivery of new messages to the queue for a few seconds",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution directly addresses the requirement of postponing message delivery. Delay queues allow you to configure a delay (up to 15 minutes) when a message is added to the queue. This ensures that the message is not visible to consumers until the specified delay has elapsed.\n\n**Why option 0 is incorrect:**\nVisibility timeout is the amount of time a message is invisible to other consumers *after* it has been received by a consumer. It doesn't delay the initial delivery of the message to a consumer. It's used to prevent message loss if a consumer fails to process a message within the visibility timeout period. It's not designed for postponing the initial delivery.\n\n**Why option 1 is incorrect:**\nDead-letter queues (DLQs) are used for handling messages that cannot be processed successfully after a certain number of attempts. They are used to store messages that have exceeded their maximum receive count. DLQs are not designed for postponing the delivery of new messages. They are used for handling failed message processing.\n\n**Why option 2 is incorrect:**\nSQS FIFO queues guarantee that messages are processed exactly once, in the order that they are sent. While FIFO queues provide ordering, they do not inherently provide a mechanism to delay the initial delivery of messages. The primary purpose of FIFO queues is to maintain message order, not to postpone delivery.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 62,
    "text": "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS. As a solutions architect, which of the following solutions would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by creating two separate standard SQS queues, one for pro users and one for lite users. By configuring EC2 instances to prioritize polling the pro queue, the application ensures that messages related to pro users are processed before those from lite users. Standard queues offer high throughput and are suitable for this scenario. The EC2 instances act as consumers, and their configuration determines the processing order. This approach allows for prioritization without relying on FIFO queues, which might introduce unnecessary complexity if strict ordering within each user type isn't required.\n\n**Why option 0 is incorrect:**\nUsing FIFO queues is not necessary if the order of processing within the pro or lite user groups is not important. FIFO queues have lower throughput than standard queues. Additionally, while long polling can improve efficiency, it doesn't directly address the prioritization requirement. The combination of FIFO and polling strategies doesn't guarantee that pro messages will be processed before lite messages consistently. The primary goal is prioritization between user types, not strict ordering within each type.\n\n**Why option 1 is incorrect:**\nWhile creating separate standard queues is a good starting point, simply using short and long polling doesn't guarantee that pro messages will be processed before lite messages. Long polling improves efficiency by reducing empty responses, but it doesn't inherently prioritize one queue over another. The EC2 instances need to be configured to actively prioritize polling the pro queue to ensure the desired processing order.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "An organization has rolled out a multi-account architecture using AWS Control Tower to isolate development environments. Each developer has their own dedicated AWS account to provision and test workloads. However, the company is concerned about unexpected spikes in resource usage and AWS spending from individual developer accounts. The leadership team wants to implement a cost control mechanism that can proactively enforce budget limits, ensure automatic responses to overspending, and require minimal ongoing administrative effort. What is the most efficient solution to meet this goal with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Lambda function to run daily in each developer’s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Budgets to define spending thresholds for each developer’s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developer’s primary IAM role when the budget threshold is crossed",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirements by leveraging AWS Budgets. AWS Budgets allows defining spending thresholds for each developer account. It provides budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Critically, it supports attaching Budgets actions, enabling automatic application of a restrictive DenyAll IAM policy to the developer's primary IAM role when the budget threshold is crossed. This effectively prevents further resource provisioning and cost accumulation, fulfilling the proactive enforcement and automatic response requirements with minimal operational overhead. The DenyAll policy ensures that the developer cannot create or modify resources, effectively stopping further spending.\n\n**Why option 0 is incorrect:**\nWhile this option attempts to monitor costs, it relies on a Lambda function running in each developer account, which increases operational overhead for deployment, maintenance, and potential errors. Furthermore, using AWS Config remediation rules triggered by cost analysis is a more complex and less direct approach compared to AWS Budgets actions. The daily execution frequency might not be frequent enough to prevent significant overspending before the function runs. Cost Explorer API calls and custom logic add complexity and potential points of failure.\n\n**Why option 2 is incorrect:**\nThis option relies on developers to actively monitor their resource consumption and take action, which is not a proactive or automated solution. It increases the operational burden on the developers and does not guarantee timely responses to overspending. Email notifications and dashboards are helpful for visibility, but they do not enforce budget limits or automatically prevent further cost accumulation. This approach is reactive rather than proactive.\n\n**Why option 3 is incorrect:**\nWhile AWS Service Catalog can help control resource types and pricing, it does not directly address the requirement of proactively enforcing budget limits and automatically responding to overspending. The scheduled Lambda function to stop and restart resources is a crude method of cost control that can disrupt development workflows and may not prevent overspending within the allowed timeframe. It also adds operational overhead for managing the Lambda functions and schedules in each developer account. This approach is more about resource management than cost control and doesn't directly address the budget enforcement requirement.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress. As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
    "options": [
      {
        "id": 0,
        "text": "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-optimal solution because read replicas are specifically designed to offload read traffic from the primary database. By creating a read replica with the same compute and storage capacity as the primary, the analytics department can run their reports without impacting the performance of the primary database. Using the same capacity ensures that the read replica can handle the reporting workload effectively. This avoids unnecessary scaling and associated costs while still meeting the performance requirements.\n\n**Why option 1 is incorrect:**\nUsing a standby instance in a Multi-AZ configuration is primarily for high availability and disaster recovery. While it's true that you *could* read from a standby instance (depending on the database engine), it's not its primary purpose. Furthermore, reducing the compute and storage capacity of the standby instance would likely lead to performance issues when running the analytics reports, defeating the purpose of offloading the workload. Also, reading from a standby instance in a Multi-AZ setup is generally discouraged and can introduce complexities. The cost savings from reducing capacity are likely to be offset by the performance degradation and operational overhead.\n\n**Why option 2 is incorrect:**\nCreating a read replica with half the compute and storage capacity might seem cost-effective initially, but it's unlikely to provide sufficient performance for the analytics reports. If the reports are causing performance issues on the primary database, reducing the resources on the read replica will likely result in the reports taking longer to run or even failing. This option doesn't effectively address the performance problem and might lead to increased costs in the long run due to troubleshooting and potential scaling later.\n\n**Why option 3 is incorrect:**\nUsing a standby instance in a Multi-AZ configuration with the same compute and storage capacity as the primary is an expensive solution for offloading read traffic. Multi-AZ is primarily for high availability, and while you *could* read from it, it's not its intended use case. Read replicas are a more cost-effective and appropriate solution for read-heavy workloads like reporting. This option incurs unnecessary costs associated with the Multi-AZ setup without providing a significant benefit over using a read replica.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 65,
    "text": "A global financial services provider operates data analytics workloads across multiple AWS Regions. The company stores regulated datasets in Amazon S3 buckets and requires visibility into security and compliance configurations. As part of a new audit initiative, the compliance team must identify all S3 buckets across the environment that do not have versioning enabled. The solution must scale across all Regions and accounts with minimal manual intervention. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirement by providing a centralized view of S3 bucket metrics, including versioning status, across all Regions. Amazon S3 Storage Lens with advanced metrics and recommendations offers a per-bucket dashboard that allows filtering and viewing of versioning status. This eliminates the need for manual checks or custom scripting, minimizing operational overhead and scaling efficiently across the environment. The advanced metrics provide the necessary data for the compliance team to identify buckets without versioning enabled.\n\n**Why option 0 is incorrect:**\nIAM Access Analyzer focuses on identifying unintended resource access and generating IAM policies. While it can help secure S3 buckets, it doesn't directly provide a report on bucket versioning status. Reviewing analyzer reports for this specific purpose would be indirect and require more manual effort than using S3 Storage Lens. It also doesn't directly identify buckets *without* versioning enabled, requiring inference from access patterns.\n\n**Why option 2 is incorrect:**\nCreating a centralized S3 Multi-Region Access Point (MRAP) is primarily for improving application availability and performance by routing requests to the closest S3 bucket. While you could potentially use it to programmatically check versioning, it adds unnecessary complexity and overhead for this specific task. It also doesn't inherently provide a report or centralized view of versioning status. The primary purpose of MRAP is not compliance or auditing.\n\n**Why option 3 is incorrect:**\nThis solution involves configuring CloudTrail, EventBridge, and Lambda, which introduces significant operational overhead. While it can detect changes to bucket versioning configurations, it requires setting up and maintaining multiple services, including writing and deploying Lambda code. It's also reactive, only detecting changes after they occur, rather than providing a current state view. This approach is more complex and less efficient than using S3 Storage Lens.",
    "domain": "Design Secure Architectures"
  }
]