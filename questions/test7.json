[
  {
    "id": 1,
    "text": "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines. Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
    "options": [
      {
        "id": 0,
        "text": "On-Demand Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Spot Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Dedicated Instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Dedicated Hosts",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses. What can you do to decrease the replication cost?",
    "options": [
      {
        "id": 0,
        "text": "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Amazon EC2 instances private IP for the replication",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Private Link between the two Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Elastic Fabric Adapter (EFA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets. Can you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "correct": true
      },
      {
        "id": 1,
        "text": "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity",
        "correct": true
      },
      {
        "id": 2,
        "text": "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted",
        "correct": false
      },
      {
        "id": 3,
        "text": "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "correct": false
      },
      {
        "id": 4,
        "text": "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 1 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets. How can you provide these users access in the least possible time, with minimal changes?",
    "options": [
      {
        "id": 0,
        "text": "Create a policy and assign it manually to the 50 users",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a group, attach the policy to the group and place the users in the group",
        "correct": true
      },
      {
        "id": 3,
        "text": "Update the Amazon S3 bucket policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements. Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation",
        "correct": false
      },
      {
        "id": 1,
        "text": "You can specify instance store volumes for an instance when you launch or restart it",
        "correct": false
      },
      {
        "id": 2,
        "text": "An instance store is a network storage type",
        "correct": false
      },
      {
        "id": 3,
        "text": "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved",
        "correct": true
      },
      {
        "id": 4,
        "text": "You can't detach an instance store volume from one instance and attach it to a different instance",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 4 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application. Which of the following represents the best way of configuring a solution for this requirement with minimal effort?",
    "options": [
      {
        "id": 0,
        "text": "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CLI to run the user data scripts only once while launching the instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run the custom scripts as user data scripts on the Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A media company is modernizing its legacy image processing application by migrating it from an on-premises environment to AWS. The application handles a high volume of image transformation jobs, generating large output files. To support rapid growth, the company wants a cloud-native solution that automatically scales, minimizes manual intervention, and avoids managing servers or infrastructure. The team also wants to improve workflow automation to handle task sequencing and job state transitions. Which solution best meets these requirements while ensuring the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 8,
    "text": "A global photography startup hosts a static image-sharing site on an Amazon S3 bucket. The website allows users from different parts of the world to upload, view, and download photos through their mobile devices. As the platform has gained popularity, users have started experiencing latency issues, especially when uploading and downloading images. The team needs a solution to enhance global performance but wants to implement it with minimal development effort and without redesigning the application. Which solution will most effectively address the performance issues with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second. How can you efficiently prevent attackers from overwhelming your application?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Shield Advanced and setup a rate-based rule",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Sticky Sessions on the Application Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule",
        "correct": true
      },
      {
        "id": 3,
        "text": "Define a network access control list (network ACL) on your Application Load Balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances. Which solution should a solutions architect implement to address these requirements in the most secure manner?",
    "options": [
      {
        "id": 0,
        "text": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading. As a solutions architect, which of the following services would you recommend for the given use case?",
    "options": [
      {
        "id": 0,
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon ElastiCache for Redis",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon ElastiCache for Memcached",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 12,
    "text": "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration. Which of the following options would you suggest?",
    "options": [
      {
        "id": 0,
        "text": "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes. As a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon Relational Database Service (Amazon RDS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon OpenSearch Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon ElastiCache",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Redshift",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 4 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 14,
    "text": "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ). Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Create a Spot Fleet request",
        "correct": false
      },
      {
        "id": 1,
        "text": "Assign an Amazon EC2 Instance Role to perform the necessary API calls",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it",
        "correct": true
      },
      {
        "id": 5,
        "text": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 2 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 4 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.\n\n**Why option 5 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory. Which AWS Directory Service is the best fit for this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Simple Active Directory (Simple AD)",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Active Directory Connector",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Transit Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon Kinesis Data Streams to ingest the data",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up AWS Fargate with Amazon ECS to process the data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Database Migration Service (AWS DMS) to ingest the data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up AWS Lambda with AWS Step Functions to process the data",
        "correct": false
      },
      {
        "id": 4,
        "text": "Provision Amazon EC2 instances in an Auto Scaling group to process the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 1 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A healthcare startup is deploying an AWS-based analytics platform that processes sensitive patient records. The application backend uses Amazon RDS for structured data and Amazon S3 for storing medical files. S3 Event Notifications trigger AWS Lambda for real-time data classification and alerting. The startup uses AWS IAM Identity Center to manage federated access from their enterprise directory. Development, operations, and compliance teams require granular and secure access to RDS and S3 resources, based strictly on their job roles. The company must follow the principle of least privilege while minimizing manual administrative work. Which solution should the company implement to meet these requirements with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each teams responsibilities. Assign users to accounts and let SCPs enforce the required access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS IAM Identity Center integrated with the organizations directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A media company is relocating its legacy infrastructure to AWS. The on-premises environment consists of multiple virtualized workloads that are tightly coupled to their host operating systems and cannot be containerized or re-architected due to software constraints. Each workload currently runs on a standalone virtual machine. The engineering team plans to run these workloads on Amazon EC2 instances without modifying their core design. The company needs a solution that ensures high availability and fault tolerance in the AWS Cloud. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure",
        "correct": true
      },
      {
        "id": 1,
        "text": "Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A healthcare analytics firm operates a backend application within a private subnet of its VPC. The application is fronted by an Application Load Balancer (ALB) and accesses Amazon S3 to store medical reports. The VPC includes both a NAT gateway and an internet gateway, but the company's strict compliance policy prohibits any data traffic from traversing the internet. The team must redesign the architecture to comply with the security policy and improve cost-efficiency. Which solution best satisfies these requirements in the most cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 interface VPC endpoint and modify the security group to allow access from the applications private subnet. Route all S3 traffic through the interface endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
    "options": [
      {
        "id": 0,
        "text": "Elastic Fabric Adapter (EFA)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Elastic IP Address (EIP)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Elastic Network Adapter (ENA)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Elastic Network Interface (ENI)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this. Which of the following settings of the VPC need to be enabled? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "enableVpcHostnames",
        "correct": false
      },
      {
        "id": 1,
        "text": "enableVpcSupport",
        "correct": false
      },
      {
        "id": 2,
        "text": "enableDnsHostnames",
        "correct": true
      },
      {
        "id": 3,
        "text": "enableDnsSupport",
        "correct": true
      },
      {
        "id": 4,
        "text": "enableDnsDomain",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 3 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift. What do you recommend? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Migrate the Amazon Redshift underlying storage to Amazon S3 IA",
        "correct": false
      },
      {
        "id": 1,
        "text": "Analyze the cold data with Amazon Athena",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a smaller Amazon Redshift Cluster with the cold data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the data to Amazon S3 Glacier Deep Archive after 30 days",
        "correct": false
      },
      {
        "id": 4,
        "text": "Move the data to Amazon S3 Standard IA after 30 days",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 4 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A financial services firm operates a mission-critical transaction processing platform hosted in the AWS us-east-2 Region. The backend is powered by a MySQL-compatible Amazon Aurora cluster, with high transaction volumes throughout the day. As part of its business continuity planning, the firm has selected us-west-2 as its designated disaster recovery (DR) Region. The firm has defined strict DR objectives: Recovery Point Objective (RPO):  5 minutes Recovery Time Objective (RTO):  15 minutes Leadership has asked for a DR solution that ensures fast cross-regional failover with minimal operational overhead and configuration effort. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events",
        "correct": false
      },
      {
        "id": 3,
        "text": "Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance. Which database would you recommend using?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Aurora Serverless",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB with On-Demand Capacity",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon DynamoDB with Provisioned Capacity and Auto Scaling",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection. What is the MOST cost-effective way to establish such a connection?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Direct Connect",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an AWS Site-to-Site VPN connection",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an Internet Gateway between the on-premises data center and AWS cloud",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a bastion host on Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nsetting up an AWS Site-to-Site VPN connection, is the most cost-effective solution. Site-to-Site VPN uses the public internet to establish a secure, encrypted connection between the on-premises data center and the AWS cloud. It's relatively quick to set up and doesn't require dedicated hardware or long-term commitments like Direct Connect. Since the bandwidth requirement is low and the traffic volume is small, the performance limitations of using the internet are not a significant concern. The cost is primarily based on VPN gateway hours and data transfer, making it economical for low-bandwidth, low-traffic scenarios.\n\n**Why option 0 is incorrect:**\nsetting up AWS Direct Connect, is incorrect because Direct Connect is designed for high-bandwidth, low-latency connections. It involves establishing a dedicated network connection between the on-premises data center and AWS, which is significantly more expensive and time-consuming to set up than a VPN. Given the low bandwidth and small traffic volume requirements, Direct Connect is an overkill and not cost-effective.\n\n**Why option 2 is incorrect:**\nsetting up an Internet Gateway between the on-premises data center and AWS cloud, is incorrect because an Internet Gateway allows resources in a VPC to connect to the internet. It does not establish a secure connection between the on-premises data center and AWS. While it allows connectivity, it lacks the necessary encryption and security features required for a secure connection. It also doesn't directly facilitate a connection between an on-premise data center and AWS in the way a VPN or Direct Connect does. An Internet Gateway is a component within a VPC, not a connection point to an external network.\n\n**Why option 3 is incorrect:**\nsetting up a bastion host on Amazon EC2, is incorrect because a bastion host provides secure access to other EC2 instances within a private subnet. It doesn't establish a secure connection between the on-premises data center and AWS. While a bastion host can be part of a larger solution, it doesn't address the core requirement of creating a secure connection between the data center and AWS.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 26,
    "text": "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Make the Amazon S3 bucket public",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A research firm archives experimental datasets generated by automated laboratory equipment. Each dataset is about 10 MB in size and is initially accessed frequently for analysis within the first month. After this period, the access rate drops significantly, but the data must remain immediately retrievable if needed. Due to compliance policies, each dataset must be retained in AWS storage for exactly 4 years before deletion. The firm currently stores the data in Amazon S3 Standard storage and wants to minimize costs without compromising data availability or retrieval speed. Which solution meets these requirements most cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation",
        "correct": false
      },
      {
        "id": 2,
        "text": "Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EFS Infrequent Access",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon EFS Standard",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 Standard-IA",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining. Which AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
    "options": [
      {
        "id": 0,
        "text": "AWS Firewall Manager",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Shield Advanced",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon GuardDuty",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Web Application Firewall (AWS WAF)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS. Which of the following options do you recommend as the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Run on AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run on an Application Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run on Amazon EMR",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run on a Spot Instance with a persistent request type",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "An edtech startup runs its course-management platform inside a private subnet in a VPC on AWS. The application uses Amazon Cognito user pools for authentication. Now, the team wants to extend the application so that authenticated users can upload and access personal course-related documents in Amazon S3. The solution must ensure scalable, fine-grained and secure access control to the S3 bucket and maintain private network architecture for the application. Which combination of steps will enable secure S3 integration for this workload? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 3 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A retail startup runs a high-traffic order processing system on AWS. The architecture includes a frontend web tier using EC2 instances behind an Application Load Balancer, a processing tier powered by EC2 instances, and a data layer using Amazon DynamoDB. The frontend and processing tiers are decoupled using Amazon SQS. Recently, the engineering team observed that during unpredictable traffic surges, order processing slows down significantly, SQS queue depth increases rapidly, and the processing-tier EC2 instances hit 100% CPU usage. Which solution will help improve the applications responsiveness and scalability during peak load periods?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time. As a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Store the images using the Amazon S3 Standard-IA storage class",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the images using the Amazon S3 Intelligent-Tiering storage class",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 34,
    "text": "A tech enterprise operates several workloads using Amazon EC2, AWS Fargate, and AWS Lambda across various teams. To optimize compute costs, the company has purchased Compute Savings Plans. The cloud operations team needs to implement a solution that not only monitors utilization but also sends automated alerts when coverage levels of the Compute Savings Plans fall below a defined threshold. What is the MOST operationally efficient way to achieve this?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern. Which of the following Amazon SQS queue types is the best fit to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Queue Service (Amazon SQS) temporary queues",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Simple Queue Service (Amazon SQS) dead-letter queues",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Simple Queue Service (Amazon SQS) delay queues",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Simple Queue Service (Amazon SQS) FIFO queues",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales. Which of the following is the MOST cost-optimal solution to fix this issue?",
    "options": [
      {
        "id": 0,
        "text": "Create a Read Replica in another Region as the Master database and point the analytics workload there",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Read Replica in the same Region as the Master database and point the analytics workload there",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the analytics application to AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A technology startup has stabilized its cloud infrastructure after a successful product launch. The backend services are now running at a predictable rate with minimal scaling events. The application architecture includes workloads running on Amazon EC2, AWS Lambda functions for asynchronous processing, container workloads on AWS Fargate, and machine learning inference models deployed with Amazon SageMaker. The company is now focusing on reducing long-term operational expenses without redesigning its architecture. The company wants to apply long-term pricing discounts with the least administrative overhead and the broadest service coverage possible using the fewest number of savings plans. Which combination of savings plans will satisfy these requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances",
        "correct": true
      },
      {
        "id": 2,
        "text": "Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type",
        "correct": false
      },
      {
        "id": 4,
        "text": "Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 4 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution. As a solutions architect, which of the following options would you recommend as the solution?",
    "options": [
      {
        "id": 0,
        "text": "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class. Which of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
    "options": [
      {
        "id": 0,
        "text": "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Storage class analysis only provides recommendations for Standard to Standard IA classes",
        "correct": true
      },
      {
        "id": 2,
        "text": "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities. Which of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby",
        "correct": true
      },
      {
        "id": 1,
        "text": "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",
        "correct": true
      },
      {
        "id": 4,
        "text": "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 3 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A digital media firm is scaling its cloud footprint and wants to isolate development, testing, and production workloads using separate AWS accounts. It also wants a centralized approach to managing networking infrastructure such as subnets and gateways, without repeating configurations in every account. Additionally, the solution must enforce security best practiceslike mandatory logging and guardrailswhen new accounts are created. The firm prefers a low-maintenance, governance-driven setup. Which solution best meets these goals while minimizing operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
    "options": [
      {
        "id": 0,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "A global enterprise maintains a hybrid cloud environment and wants to transfer large volumes of data between its on-premises data center and Amazon S3 for backup and analytics workflows. The company has already established a Direct Connect (DX) connection to AWS and wants to ensure high-bandwidth, low-latency, and secure private connectivity without traversing the public internet. The architecture must be designed to access Amazon S3 directly from on-premises systems using this DX connection. Which configuration should the network engineering team implement to allow direct access to Amazon S3 from the on-premises data center using Direct Connect?",
    "options": [
      {
        "id": 0,
        "text": "Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Require HTTPS for communication between Amazon CloudFront and your S3 origin",
        "correct": false
      },
      {
        "id": 1,
        "text": "Require HTTPS for communication between Amazon CloudFront and your custom origin",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront signed URLs",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront signed cookies",
        "correct": true
      },
      {
        "id": 4,
        "text": "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 3 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because there is no built-in configuration in the S3 console to require additional confirmation for deletion. While custom solutions could be built, this is not a standard or readily available feature and does not provide the same level of protection as MFA Delete or Versioning.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect  Associate, you have been tasked to make the application more resilient to periodic spikes in read request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Shield",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Direct Connect",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Aurora Replica",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 4 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations. Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "VPC Route Table",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Inspector",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon GuardDuty",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Shield Advanced",
        "correct": true
      },
      {
        "id": 4,
        "text": "AWS Web Application Firewall (AWS WAF)",
        "correct": true
      },
      {
        "id": 5,
        "text": "VPC Security Group",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4,
      5
    ],
    "explanation": "**Why option 3 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 4 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 5 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS. As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon MQ",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Simple Queue Service (Amazon SQS) Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon SQS FIFO (First-In-First-Out)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 48,
    "text": "A digital media company wants to track user engagement across its streaming platform by capturing events such as video starts, pauses, and search queries. These events must be ingested and analyzed in real time to improve user experience and optimize recommendations. The platform experiences unpredictable spikes in traffic during popular content releases. The company needs a highly scalable and serverless solution that can seamlessly adjust to changing workloads without manual provisioning. Which solution will meet these requirements in the MOST efficient and scalable way?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
    "options": [
      {
        "id": 0,
        "text": "VPC Endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "VPC Peering connection",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS VPN CloudHub",
        "correct": true
      },
      {
        "id": 3,
        "text": "Software VPN",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance. As a solutions architect, how will you fix this issue?",
    "options": [
      {
        "id": 0,
        "text": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic",
        "correct": true
      },
      {
        "id": 1,
        "text": "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior",
        "correct": false
      },
      {
        "id": 3,
        "text": "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 1 is incorrect:**\nis incorrect because while it creates IAM policies for DynamoDB and S3, attaching them to the EC2 instance profile grants those permissions to *all* Pods running on those nodes. Kubernetes RBAC controls access *within* the cluster, not to AWS resources. This approach doesn't provide the fine-grained control needed to restrict UI Pods to DynamoDB and data-processing Pods to S3. All pods running on the node would inherit the permissions of the instance profile.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement. As a solutions architect, which of the following will you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 1 is incorrect:**\nstoring the data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect for similar reasons as S3 Standard-IA. While it offers even lower storage costs, it also has higher retrieval costs and a 30-day minimum storage duration charge. More importantly, S3 One Zone-IA stores data in a single Availability Zone, which makes it less durable than S3 Standard. While durability isn't explicitly mentioned as a primary concern, it's generally a good practice to use S3 Standard unless there's a very specific reason to use a lower-durability option. The frequent access pattern and the 30-day minimum storage duration make this option less cost-effective. Also, the loss of an AZ could impact the analytics pipeline.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 52,
    "text": "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "AWS Transit Gateway",
        "correct": true
      },
      {
        "id": 1,
        "text": "VPC Peering Connection",
        "correct": false
      },
      {
        "id": 2,
        "text": "Virtual private gateway (VGW)",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS PrivateLink",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A global logistics provider operates several legacy applications on virtual machines (VMs) within a private data center. Due to accelerated business growth and limited capacity in its existing infrastructure, the provider decides to migrate select applications to AWS. The company opts for a lift-and-shift strategy for its non-mission-critical systems to meet tight migration deadlines. The solution must support rapid migration without requiring extensive application refactoring. Which combination of actions will best support this migration approach? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Launch a cutover instance after completing testing and confirming that replication is up-to-date",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use",
        "correct": false
      },
      {
        "id": 2,
        "text": "Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations",
        "correct": false
      },
      {
        "id": 5,
        "text": "Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 2 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 3 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 5 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes. Which Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
    "options": [
      {
        "id": 0,
        "text": "Throughput Optimized HDD Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provisioned IOPS SSD Amazon EBS volumes",
        "correct": true
      },
      {
        "id": 2,
        "text": "General-purpose SSD-based Amazon EBS volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Cold HDD Amazon EBS volumes",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 dedicated instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 dedicated hosts",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 on-demand instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 reserved instances (RI)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data. How can you build this index efficiently?",
    "options": [
      {
        "id": 0,
        "text": "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort. What will you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SNS is a publish/subscribe messaging service that does not guarantee message order. While Lambda can process the updates, SNS is not the right choice for ordered processing. Also, running a SQL database on an EC2 instance increases management overhead compared to a managed database service like DynamoDB or RDS.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Client Side Encryption",
        "correct": true
      },
      {
        "id": 1,
        "text": "Server-side encryption with AWS KMS keys (SSE-KMS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A global enterprise has onboarded multiple departments into isolated AWS accounts that are part of a unified AWS Organizations structure. Recently, a critical operational alert was missed because it was delivered to the root users email address of an account, which is only monitored intermittently. The enterprise wants to redesign its notification handling process to ensure that future communications - categorized by billing, security, and operational relevance - are received promptly by the appropriate teams. The solution should align with AWS security best practices and offer centralized oversight without depending on individual users. Which solution meets these requirements in the most secure and scalable way?",
    "options": [
      {
        "id": 0,
        "text": "Configure each AWS accounts root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as billing, security, or operations. Keep the current root email addresses as they are, and rely on this service to triage alerts",
        "correct": false
      },
      {
        "id": 2,
        "text": "Assign each AWS accounts root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change each AWS accounts root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table. How would you go about providing private access to these AWS resources which are not part of this custom VPC?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
    "options": [
      {
        "id": 0,
        "text": "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use delay queues to postpone the delivery of new messages to the queue for a few seconds",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 62,
    "text": "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS. As a solutions architect, which of the following solutions would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "An organization has rolled out a multi-account architecture using AWS Control Tower to isolate development environments. Each developer has their own dedicated AWS account to provision and test workloads. However, the company is concerned about unexpected spikes in resource usage and AWS spending from individual developer accounts. The leadership team wants to implement a cost control mechanism that can proactively enforce budget limits, ensure automatic responses to overspending, and require minimal ongoing administrative effort. What is the most efficient solution to meet this goal with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Lambda function to run daily in each developers account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Budgets to define spending thresholds for each developers account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developers primary IAM role when the budget threshold is crossed",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress. As an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
    "options": [
      {
        "id": 0,
        "text": "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 65,
    "text": "A global financial services provider operates data analytics workloads across multiple AWS Regions. The company stores regulated datasets in Amazon S3 buckets and requires visibility into security and compliance configurations. As part of a new audit initiative, the compliance team must identify all S3 buckets across the environment that do not have versioning enabled. The solution must scale across all Regions and accounts with minimal manual intervention. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the correct answer. S3 automatically partitions data to scale and handle high request rates. However, if object keys are sequentially named (e.g., using timestamps or sequential IDs), all requests might be directed to a single partition, leading to throttling. Using customer-specific prefixes distributes the load across multiple partitions within the same bucket, effectively increasing the aggregate request rate the service can handle. This approach is resource-efficient because it avoids creating a large number of buckets, which can add management overhead and potentially increase costs. It's also cost-optimal because it leverages S3's built-in scalability features without requiring significant changes to the underlying storage infrastructure.\n\n**Why option 0 is incorrect:**\nis incorrect because creating a new S3 bucket for each customer is not resource-efficient or cost-optimal. While it would solve the request rate issue, managing a large number of buckets can become complex and expensive. S3 has limits on the number of buckets per account, and managing permissions, lifecycle policies, and other configurations across thousands of buckets would be a significant operational burden. It also introduces unnecessary overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon EFS is not designed for this use case. EFS is a network file system suitable for applications that require shared file storage across multiple EC2 instances. It's generally more expensive than S3 for storing large amounts of data and is not optimized for high-volume object storage and retrieval. EFS also has different performance characteristics and is not a direct replacement for S3 in this scenario. It doesn't address the core problem of high request rates to a single storage location.\n\n**Why option 3 is incorrect:**\nis incorrect because creating a new S3 bucket for each day's data might improve scalability to some extent, but it's not as efficient or cost-optimal as using prefixes. It still requires managing multiple buckets, albeit fewer than creating a bucket per customer. Also, it might not fully address the issue if a single day's data still generates a high request rate. The prefix approach is more granular and adaptable to varying customer activity levels.",
    "domain": "Design Secure Architectures"
  }
]