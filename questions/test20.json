[
  {
    "id": 0,
    "text": "A company wants to migrate an on-premises data center to AWS. The data center hosts a \nstorage server that stores data in an NFS-based file system. The storage server holds 200 GB of \ndata. The company needs to migrate the data without interruption to existing services. Multiple \nresources in AWS must be able to access the data by using the NFS protocol. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon FSx for Lustre file system.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket to receive the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Manually use an operating system copy command to push the data into the AWS destination.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon EFS provides a scalable, high performance NFS file system that can be accessed from multiple resources in AWS. AWS DataSync can perform the migration from the on-prem NFS server to EFS without interruption to existing services. This avoids having to manually move the data which could cause downtime. DataSync incrementally syncs changed data. EFS and DataSync together provide a cost-optimized approach compared to using S3 or FSx, while still meeting the requirements. Manually copying 200 GB of data to AWS would be slow and risky compared to using DataSync.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that \nhave an SMB file share mounted as a volume in the us-east-1 Region. The company has a \nrecovery point objective (RPO) of 5 minutes for planned system maintenance or unplanned \nservice disruptions. The company needs to replicate the file system to the us-west-2 Region. The \nreplicated data must not be deleted by any user for 5 years. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n345",
    "options": [
      {
        "id": 0,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A solutions architect is designing a security solution for a company that wants to provide \ndevelopers with individual AWS accounts through AWS Organizations, while also maintaining \nstandard security controls. Because the individual developers will have AWS account root user-\nlevel access to their own accounts, the solutions architect wants to ensure that the mandatory \nAWS CloudTrail configuration that is applied to new developer accounts is not modified. \n \nWhich action meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM policy that prohibits changes to CloudTrail. and attach it to the root user.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new trail in CloudTrail from within the developer accounts with the organization trails",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a service-linked role for CloudTrail with a policy condition that allows changes only from",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company is planning to deploy a business-critical application in the AWS Cloud. The application \nrequires durable storage with consistent, low-latency performance. \n \nWhich type of storage should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Instance store volume",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon ElastiCache for Memcached cluster",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume",
        "correct": true
      },
      {
        "id": 3,
        "text": "Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-\nwest-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. \n \nWhich solution will meet this requirement with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A company is creating a new web application for its subscribers. The application will consist of a \nstatic single page and a persistent database layer. The application will have millions of users for 4 \nhours in the morning, but the application will have only a few thousand users during the rest of \nthe day. The company's data architects have requested the ability to rapidly evolve their schema. \n \nWhich solutions will meet these requirements and provide the MOST scalability? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 4,
        "text": "Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A company uses Amazon API Gateway to manage its REST APIs that third-party service \nproviders access. The company must protect the REST APIs from SQL injection and cross-site \nscripting attacks. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Shield.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS WAF.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS Shield in",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS WAF in CloudFront.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company wants to provide users with access to AWS resources. The company has 1,500 users \nand manages their access to on-premises resources through Active Directory user groups on the \ncorporate network. However, the company does not want users to have to maintain another \nidentity to access the resources. A solutions architect must manage user access to the AWS \nresources while preserving access to the on-premises resources. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM user for each user in the company. Attach the appropriate policies to each user.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies",
        "correct": false
      },
      {
        "id": 2,
        "text": "Define cross-account roles with the appropriate policies attached. Map the roles to the Active",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company is hosting a website behind multiple Application Load Balancers. The company has \ndifferent distribution rights for its content around the world. A solutions architect needs to ensure \nthat users are served the correct content without violating distribution rights. \n \nWhich configuration should the solutions architect choose to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudFront with AWS WAF.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Application Load Balancers with AWS WAF",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Route 53 with a geolocation policy",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon Route 53 with a geoproximity routing policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company stores its data on premises. The amount of data is growing beyond the company's \navailable capacity. \n \nThe company wants to migrate its data from the on-premises location to an Amazon S3 bucket. \nThe company needs a solution that will automatically validate the integrity of the data after the \ntransfer. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company wants to migrate two DNS servers to AWS. The servers host a total of approximately \n200 zones and receive 1 million requests each day on average. The company wants to maximize \navailability while minimizing the operational overhead that is related to the management of the \ntwo servers. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create 200 new hosted zones in the Amazon Route 53 console Import zone files.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A global company runs its applications in multiple AWS accounts in AWS Organizations. The \ncompany's applications use multipart uploads to upload data to multiple Amazon S3 buckets \nacross AWS Regions. The company wants to report on incomplete multipart uploads for cost \ncompliance purposes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Config with a rule to report the incomplete multipart upload object count.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a service control policy (SCP) to report the incomplete multipart upload object count.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 Storage Lens to report the incomplete multipart upload object count.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company runs a production database on Amazon RDS for MySQL. The company wants to \nupgrade the database version for security compliance reasons. Because the database contains \ncritical data, the company wants a quick solution to upgrade and test functionality without losing \nany data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS Blue/Green Deployments to deploy and test production changes.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A solutions architect is creating a data processing job that runs once daily and can take up to 2 \nhours to complete. If the job is interrupted, it has to restart from the beginning. \n \nHow should the solutions architect address this issue in the MOST cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 14,
    "text": "A social media company wants to store its database of user profiles, relationships, and \ninteractions in the AWS Cloud. The company needs an application to monitor any changes in the \ndatabase. The application needs to analyze the relationships between the data entities and to \nprovide recommendations to users. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company is creating a new application that will store a large amount of data. The data will be \nanalyzed hourly and will be modified by several Amazon EC2 Linux instances that are deployed \nacross multiple Availability Zones. The needed amount of storage space will continue to grow for \nthe next 6 months. \n \nWhich storage solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-\nAZ DB instance. Increases in traffic are causing performance problems. The company determines \nthat database queries are the primary reason for the slow performance. \n \nWhat should a solutions architect do to improve the application's performance?",
    "options": [
      {
        "id": 0,
        "text": "Serve read traffic from the Multi-AZ standby replica.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the DB instance to use Transfer Acceleration.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read replica from the source DB instance. Serve read traffic from the read replica.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Kinesis Data Firehose between the application and Amazon RDS to increase the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A company collects 10 GB of telemetry data daily from various machines. The company stores \nthe data in an Amazon S3 bucket in a source data account. \n \nThe company has hired several consulting agencies to use this data for analysis. Each agency \nneeds read access to the data for its analysts. The company must share the data from the source \ndata account by choosing a solution that maximizes security and operational efficiency. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 global tables to replicate data for each agency.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Make the S3 bucket public for a limited time. Inform only the agencies.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure cross-account access for the S3 bucket to the accounts that the agencies own.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up an IAM user for each analyst in the source data account. Grant each user access to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS \nfile shares. Applications that run on Amazon EC2 instances access the file shares. The company \nneeds a storage disaster recovery (DR) solution in a secondary Region. The data that is \nreplicated in the secondary Region needs to be accessed by using the same protocols as the \nprimary Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A development team is creating an event-based application that uses AWS Lambda functions. \nEvents will be generated when files are added to an Amazon S3 bucket. The development team \ncurrently has Amazon Simple Notification Service (Amazon SNS) configured as the event target \nfrom Amazon S3. \n \nWhat should a solutions architect do to process the events from Amazon S3 in a scalable way?",
    "options": [
      {
        "id": 0,
        "text": "Create an SNS subscription that processes the event in Amazon Elastic Container Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect is designing a new service behind Amazon API Gateway. The request \npatterns for the service will be unpredictable and can change suddenly from 0 requests to over \n500 per second. The total size of the data that needs to be persisted in a backend database is \ncurrently less than 1 GB with unpredictable future growth. Data can be queried using simple key-\nvalue requests. \n \nWhich combination ofAWS services would meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "AWS Fargate",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 Auto Scaling",
        "correct": false
      },
      {
        "id": 4,
        "text": "MySQL-compatible Amazon Aurora",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A company collects and shares research data with the company's employees all over the world. \nThe company wants to collect and store the data in an Amazon S3 bucket and process the data \nin the AWS Cloud. The company will share the data with the company's employees. The \ncompany needs a secure solution in the AWS Cloud that minimizes operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM user for each employee. Create an IAM policy for each employee to allow S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company is building a new furniture inventory application. The company has deployed the \napplication on a fleet ofAmazon EC2 instances across multiple Availability Zones. The EC2 \ninstances run behind an Application Load Balancer (ALB) in their VPC. \n \nA solutions architect has observed that incoming traffic seems to favor one EC2 instance, \nresulting in latency for some requests. \n \nWhat should the solutions architect do to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Disable session affinity (sticky sessions) on the ALB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Replace the ALB with a Network Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Increase the number of EC2 instances in each Availability Zone",
        "correct": false
      },
      {
        "id": 3,
        "text": "Adjust the frequency of the health checks on the ALB's target group",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company has an application workflow that uses an AWS Lambda function to download and \ndecrypt files from Amazon S3. These files are encrypted using AWS Key Management Service \n(AWS KMS) keys. A solutions architect needs to design a solution that will ensure the required \npermissions are set correctly. \n \nWhich combination of actions accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Attach the kms:decrypt permission to the Lambda function's resource policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Grant the decrypt permission for the Lambda IAM role in the KMS key's policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Grant the decrypt permission for the Lambda resource policy in the KMS key's policy.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a new IAM role with the kms:decrypt permission and attach the execution role to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company runs several websites on AWS for its different brands. Each website generates tens \nof gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution \nto give the company's developers the ability to analyze traffic patterns across all the company's \nwebsites. This analysis by the developers will occur on demand once a week over the course of \nseveral months. The solution must support queries with standard SQL. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n355 \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the logs in Amazon S3. Use Amazon Athena tor analysis.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the logs in Amazon RDS. Use a database client for analysis.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 27,
    "text": "An international company has a subdomain for each country that the company operates in. The \nsubdomains are formatted as example.com, country1.example.com, and country2.example.com. \nThe company's workloads are behind an Application Load Balancer. The company wants to \nencrypt the website data that is in transit. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the AWS Certificate Manager (ACM) console to request a private certificate for the apex top",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Certificate Manager (ACM) console to request a public and private certificate for the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Validate domain ownership by email address. Switch to DNS validation by adding the required",
        "correct": false
      },
      {
        "id": 4,
        "text": "Validate domain ownership for the domain by adding the required DNS records to the DNS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company is required to use cryptographic keys in its on-premises key manager. The key \nmanager is outside of the AWS Cloud because of regulatory and compliance requirements. The \ncompany wants to manage encryption and decryption by using cryptographic keys that are \nretained outside of the AWS Cloud and that support a variety of external key managers from \ndifferent vendors. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudHSM key store backed by a CloudHSM cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Key Management Service (AWS KMS) external key store backed by an external key",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the default AWS Key Management Service (AWS KMS) managed key store.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a custom key store backed by an AWS CloudHSM cluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A solutions architect needs to host a high performance computing (HPC) workload in the AWS \nCloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel \naccess to a shared file system to enable distributed processing of large datasets. Datasets will be \naccessed across multiple instances simultaneously. The workload requires access latency within \n1 ms. After processing has completed, engineers will need access to the dataset for manual \npostprocessing. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from",
        "correct": false
      },
      {
        "id": 1,
        "text": "Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure AWS Resource Access Manager to share an Amazon S3 bucket so that it can be",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "A gaming company is building an application with Voice over IP capabilities. The application will \nserve traffic to users across the world. The application needs to be highly available with an \nautomated failover across AWS Regions. The company wants to minimize the latency of users \nwithout relying on IP address caching on user devices. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator with health checks.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 with a geolocation routing policy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution that includes multiple origins.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer that uses path-based routing.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 31,
    "text": "A weather forecasting company needs to process hundreds of gigabytes of data with sub-\nmillisecond latency. The company has a high performance computing (HPC) environment in its \ndata center and wants to expand its forecasting capabilities. \n \nA solutions architect must identify a highly available cloud storage solution that can handle large \namounts of sustained throughput. Files that are stored in the solution should be accessible to \nthousands of compute instances that will simultaneously access and process the entire dataset. \n \nWhat should the solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n357",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon FSx for Lustre scratch file systems.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon FSx for Lustre persistent file systems.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "An ecommerce company runs a PostgreSQL database on premises. The database stores data by \nusing high IOPS Amazon Elastic Block Store (Amazon EBS) block storage. The daily peak I/O \ntransactions per second do not exceed 15,000 IOPS. The company wants to migrate the \ndatabase to Amazon RDS for PostgreSQL and provision disk IOPS performance independent of \ndisk storage capacity. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the EBS magnetic volume type to achieve maximum IOPS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 33,
    "text": "A company wants to migrate its on-premises Microsoft SQL Server Enterprise edition database to \nAWS. The company's online application uses the database to process transactions. The data \nanalysis team uses the same production database to run reports for analytical processing. The \ncompany wants to reduce operational overhead by moving to managed services wherever \npossible. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate to Amazon RDS for Microsoft SOL Server. Use read replicas for reporting purposes",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate to Microsoft SQL Server on Amazon EC2. Use Always On read replicas for reporting",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate to Amazon DynamoDB. Use DynamoDB on-demand replicas for reporting purposes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate to Amazon Aurora MySQL. Use Aurora read replicas for reporting purposes",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company stores a large volume of image files in an Amazon S3 bucket. The images need to be \nreadily available for the first 180 days. The images are infrequently accessed for the next 180 \ndays. After 360 days, the images need to be archived but must be available instantly upon \nrequest. After 5 years, only auditors can access the images. The auditors must be able to retrieve \nthe images within 12 hours. The images cannot be lost during this process. \n \nA developer will use S3 Standard storage for the first 180 days. The developer needs to configure \nan S3 Lifecycle rule. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n358 \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A company has a large data workload that runs for 6 hours each day. The company cannot lose \nany data while the process is running. A solutions architect is designing an Amazon EMR cluster \nconfiguration to support this critical data workload. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure a long-running cluster that runs the primary node and core nodes on On-Demand",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a transient cluster that runs the primary node and core nodes on On-Demand",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a transient cluster that runs the primary node on an On-Demand Instance and the core",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a long-running cluster that runs the primary node on an On-Demand Instance, the core",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "A company maintains an Amazon RDS database that maps users to cost centers. The company \nhas accounts in an organization in AWS Organizations. The company needs a solution that will \ntag all resources that are created in a specific AWS account in the organization. The solution \nmust tag each resource with the cost center ID of the user who created the resource. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the specific AWS account to a new organizational unit (OU) in Organizations from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function to tag the resources after the Lambda function looks up the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS CloudFormation stack to deploy an AWS Lambda function. Configure the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function to tag the resources with a default value. Configure an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 37,
    "text": "A company recently migrated its web application to the AWS Cloud. The company uses an \nAmazon EC2 instance to run multiple processes to host the application. The processes include \nan Apache web server that serves static content. The Apache web server makes requests to a \nPHP application that uses a local Redis server for user sessions. \n \nThe company wants to redesign the architecture to be highly available and to use AWS managed \nsolutions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Elastic Beanstalk to host the static content and the PHP application. Configure Elastic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Lambda to host the static content and the PHP application. Use an Amazon API",
        "correct": false
      },
      {
        "id": 2,
        "text": "Keep the backend code on the EC2 instance. Create an Amazon ElastiCache for Redis cluster",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon CloudFront distribution with an Amazon S3 endpoint to an S3 bucket that is",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group that has a \ntarget group. The company designed the application to work with session affinity (sticky sessions) \nfor a better user experience. \n \nThe application must be available publicly over the internet as an endpoint. A WAF must be \napplied to the endpoint for additional security. Session affinity (sticky sessions) must be \nconfigured on the endpoint. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a public Network Load Balancer. Specify the application target group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Gateway Load Balancer. Specify the application target group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a public Application Load Balancer. Specify the application target group.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a second target group. Add Elastic IP addresses to the EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a web ACL in AWS WAF. Associate the web ACL with the endpoint",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company runs a website that stores images of historical events. Website users need the ability \nto search and view images based on the year that the event in the image occurred. On average, \nusers request each image only once or twice a year. The company wants a highly available \nsolution to store and deliver the images to users. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store images in Amazon Elastic Block Store (Amazon EBS). Use a web server that runs on",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store images in Amazon Elastic File System (Amazon EFS). Use a web server that runs on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store images in Amazon S3 Standard. Use S3 Standard to directly deliver images by using a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3 Standard-IA",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 40,
    "text": "A company has multiple AWS accounts in an organization in AWS Organizations that different \nbusiness units use. The company has multiple offices around the world. The company needs to \nupdate security group rules to allow new office CIDR ranges or to remove old CIDR ranges \nacross the organization. The company wants to centralize the management of security group \nrules to minimize the administrative overhead that updating CIDR ranges requires. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create VPC security groups in the organization's management account. Update the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPC customer managed prefix list that contains the list of CIDRs. Use AWS Resource",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS managed prefix list. Use an AWS Security Hub policy to enforce the security",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create security groups in a central administrative AWS account. Create an AWS Firewall",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 41,
    "text": "A company uses an on-premises network-attached storage (NAS) system to provide file shares to \nits high performance computing (HPC) workloads. The company wants to migrate its latency-\nsensitive HPC workloads and its storage to the AWS Cloud. The company must be able to \nprovide NFS and SMB multi-protocol access from the file system. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n361 \nWhich solution will meet these requirements with the LEAST latency? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy compute optimized EC2 instances into a cluster placement group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy compute optimized EC2 instances into a partition placement group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Attach the EC2 instances to an Amazon FSx for Lustre file system.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach the EC2 instances to an Amazon FSx for OpenZFS file system.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Attach the EC2 instances to an Amazon FSx for NetApp ONTAP file system.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A company is relocating its data center and wants to securely transfer 50 TB of data to AWS \nwithin 2 weeks. The existing data center has a Site-to-Site VPN connection to AWS that is 90% \nutilized. \n \nWhich AWS service should a solutions architect use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "AWS DataSync with a VPC endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball Edge Storage Optimized",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A company hosts an application on Amazon EC2 On-Demand Instances in an Auto Scaling \ngroup. Application peak hours occur at the same time each day. Application users report slow \napplication performance at the start of peak hours. The application performs normally 2-3 hours \nafter peak hours begin. The company wants to ensure that the application works properly at the \nstart of peak hours. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Application Load Balancer to distribute traffic properly to the instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 44,
    "text": "A company runs applications on AWS that connect to the company's Amazon RDS database. \nThe applications scale on weekends and at peak times of the year. The company wants to scale \nthe database more effectively for its applications that connect to the database. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB with connection pooling with a target group configuration for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS Proxy with a target group for the database. Change the applications to use the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a custom proxy that runs on Amazon EC2 as an intermediary to the database. Change the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Lambda function to provide connection pooling with a target group configuration for",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company uses AWS Cost Explorer to monitor its AWS costs. The company notices that \nAmazon Elastic Block Store (Amazon EBS) storage and snapshot costs increase every month. \nHowever, the company does not purchase additional EBS storage every month. The company \nwants to optimize monthly costs for its current storage usage. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use logs in Amazon CloudWatch Logs to monitor the storage utilization of Amazon EBS. Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a custom script to monitor space usage. Use Amazon EBS Elastic Volumes to reduce the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Delete all expired and unused snapshots to reduce snapshot costs.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Delete all nonessential snapshots. Use Amazon Data Lifecycle Manager to create and manage",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 0 is incorrect:**\nis incorrect because partition placement groups are primarily used for large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. While they provide fault tolerance by distributing instances across partitions, they do not offer the low latency and high throughput benefits of cluster placement groups, which are more critical for HPC applications.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "A company is developing a new application on AWS. The application consists of an Amazon \nElastic Container Service (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for \nthe application, and an Amazon RDS for MySQL database that contains the dataset for the \napplication. The dataset contains sensitive information. The company wants to ensure that only \nthe ECS cluster can access the data in the RDS for MySQL database and the data in the S3 \nbucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Key Management Service (AWS KMS) AWS managed key to encrypt both the S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket policy that restricts bucket access to the ECS task execution role. Create a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a VPC endpoint for Amazon RDS for MySQL. Update the RDS for MySQL security group",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A company has a web application that runs on premises. The application experiences latency \nissues during peak hours. The latency issues occur twice each month. At the start of a latency \nissue, the application's CPU utilization immediately increases to 10 times its normal amount. \n \nThe company wants to migrate the application to AWS to improve latency. The company also \nwants to scale the application automatically when application demand increases. The company \nwill use AWS Elastic Beanstalk for application deployment. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 48,
    "text": "A company has customers located across the world. The company wants to use automation to \nsecure its systems and network infrastructure. The company's security team must be able to track \nand audit all incremental changes to the infrastructure. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Organizations to set up the infrastructure. Use AWS Config to track changes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CloudFormation to set up the infrastructure. Use AWS Config to track changes.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Organizations to set up the infrastructure. Use AWS Service Catalog to track changes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS CloudFormation to set up the infrastructure. Use AWS Service Catalog to track",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A startup company is hosting a website for its customers on an Amazon EC2 instance. The \nwebsite consists of a stateless Python application and a MySQL database. The website serves \nonly a small amount of traffic. The company is concerned about the reliability of the instance and \nneeds to migrate to a highly available architecture. The company cannot modify the application \ncode. \n \nWhich combination of actions should a solutions architect take to achieve high availability for the \nwebsite? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n364",
    "options": [
      {
        "id": 0,
        "text": "Provision an internet gateway in each Availability Zone in use.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to synchronize the database data across multiple EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company is moving its data and applications to AWS during a multiyear migration project. The \ncompany wants to securely access data on Amazon S3 from the company's AWS Region and \nfrom the company's on-premises location. The data must not traverse the internet. The company \nhas established an AWS Direct Connect connection between its Region and its on-premises \nlocation. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create gateway endpoints for Amazon S3. Use the gateway endpoints to securely access the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a gateway in AWS Transit Gateway to access Amazon S3 securely from the Region and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create interface endpoints for Amazon S3. Use the interface endpoints to securely access the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an AWS Key Management Service (AWS KMS) key to access the data securely from the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 1 is incorrect:**\nis incorrect because while it creates IAM policies for DynamoDB and S3, attaching them to the EC2 instance profile grants those permissions to *all* Pods running on those nodes. Kubernetes RBAC controls access *within* the cluster, not to AWS resources. This approach doesn't provide the fine-grained control needed to restrict UI Pods to DynamoDB and data-processing Pods to S3. All pods running on the node would inherit the permissions of the instance profile.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company created a new organization in AWS Organizations. The organization has multiple \naccounts for the company's development teams. The development team members use AWS IAM \nIdentity Center (AWS Single Sign-On) to access the accounts. For each of the company's \napplications, the development teams must use a predefined application name to tag resources \nthat are created. \n \nA solutions architect needs to design a solution that gives the development team the ability to \ncreate resources only if the application name tag has an approved value. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM group that has a conditional Allow policy that requires the application name tag to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a cross-account role that has a Deny policy for any resource that has the application name",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a resource group in AWS Resource Groups to validate that the tags are applied to all",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a tag policy in Organizations that has a list of allowed application names.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 1 is incorrect:**\nstoring the data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect for similar reasons as S3 Standard-IA. While it offers even lower storage costs, it also has higher retrieval costs and a 30-day minimum storage duration charge. More importantly, S3 One Zone-IA stores data in a single Availability Zone, which makes it less durable than S3 Standard. While durability isn't explicitly mentioned as a primary concern, it's generally a good practice to use S3 Standard unless there's a very specific reason to use a lower-durability option. The frequent access pattern and the 30-day minimum storage duration make this option less cost-effective. Also, the loss of an AZ could impact the analytics pipeline.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company runs its databases on Amazon RDS for PostgreSQL. The company wants a secure \nsolution to manage the master user password by rotating the password every 30 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EventBridge to schedule a custom AWS Lambda function to rotate the password",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the modify-db-instance command in the AWS CLI to change the password.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate password",
        "correct": true
      },
      {
        "id": 3,
        "text": "Integrate AWS Systems Manager Parameter Store with Amazon RDS for PostgreSQL to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company performs tests on an application that uses an Amazon DynamoDB table. The tests \nrun for 4 hours once a week. The company knows how many read and write operations the \napplication performs to the table each second during the tests. The company does not currently \nuse DynamoDB for any other use case. A solutions architect needs to optimize the costs for the \ntable. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Choose on-demand mode. Update the read and write capacity units appropriately.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Choose provisioned mode. Update the read and write capacity units appropriately.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase DynamoDB reserved capacity for a 1-year term.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase DynamoDB reserved capacity for a 3-year term.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 54,
    "text": "A company runs its applications on Amazon EC2 instances. The company performs periodic \nfinancial assessments of its AWS costs. The company recently identified unusual spending. \n \nThe company needs a solution to prevent unusual spending. The solution must monitor costs and \nnotify responsible stakeholders in the event of unusual spending. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n366",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS Budgets template to create a zero spend budget.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create AWS Pricing Calculator estimates for the current running workload pricing details.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudWatch to monitor costs and to identify unusual spending.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A marketing company receives a large amount of new clickstream data in Amazon S3 from a \nmarketing campaign. The company needs to analyze the clickstream data in Amazon S3 quickly. \nThen the company needs to determine whether to process the data further in the data pipeline. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create external tables in a Spark catalog. Configure jobs in AWS Glue to query the data.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an AWS Glue crawler to crawl the data. Configure Amazon Athena to query the data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create external tables in a Hive metastore. Configure Spark jobs in Amazon EMR to query the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Glue crawler to crawl the data. Configure Amazon Kinesis Data Analytics to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A company runs an SMB file server in its data center. The file server stores large files that the \ncompany frequently accesses for up to 7 days after the file creation date. After 7 days, the \ncompany needs to be able to access the files with a maximum retrieval time of 24 hours. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 File Gateway to increase the company's storage space. Create an S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon FSx File Gateway to increase the company's storage space. Create an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure access to Amazon S3 for each user. Create an S3 Lifecycle policy to transition the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group. The \napplication uses a database that runs on an Amazon RDS for PostgreSQL DB instance. The \napplication performs slowly when traffic increases. The database experiences a heavy read load \nduring periods of high traffic. \n \nWhich actions should a solutions architect take to resolve these performance issues? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Turn on auto scaling for the DB instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a read replica for the DB instance. Configure the application to send read traffic to the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Convert the DB instance to a Multi-AZ DB instance deployment. Configure the application to send",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon ElastiCache cluster. Configure the application to cache query results in the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the Auto Scaling group subnets to ensure that the EC2 instances are provisioned in the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SNS is a publish/subscribe messaging service that does not guarantee message order. While Lambda can process the updates, SNS is not the right choice for ordered processing. Also, running a SQL database on an EC2 instance increases management overhead compared to a managed database service like DynamoDB or RDS.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) \nvolumes to run an application. The company creates one snapshot of each EBS volume every \nday to meet compliance requirements. The company wants to implement an architecture that \nprevents the accidental deletion of EBS volume snapshots. The solution must not change the \nadministrative rights of the storage administrator user. \n \nWhich solution will meet these requirements with the LEAST administrative effort?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that has permission to delete snapshots. Attach the role to a new EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM policy that denies snapshot deletion. Attach the policy to the storage administrator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add tags to the snapshots. Create retention rules in Recycle Bin for EBS snapshots that have the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Lock the EBS snapshots to prevent deletion.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 \ninstances, and databases that are deployed in an Amazon VPC. The company wants to capture \ninformation about traffic to and from the network interfaces in near real time in its Amazon VPC. \nThe company wants to send the information to Amazon OpenSearch Service for analysis. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company is developing an application that will run on a production Amazon Elastic Kubernetes \nService (Amazon EKS) cluster. The EKS cluster has managed node groups that are provisioned \nwith On-Demand Instances. \n \nThe company needs a dedicated EKS cluster for development work. The company will use the \ndevelopment cluster infrequently to test the resiliency of the application. The EKS cluster must \nmanage all the nodes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a managed node group that contains only Spot Instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create two managed node groups. Provision one node group with On-Demand Instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group that has a launch configuration that uses Spot Instances. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a managed node group that contains only On-Demand Instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 61,
    "text": "A company stores sensitive data in Amazon S3. A solutions architect needs to create an \nencryption solution. The company needs to fully control the ability of users to create, rotate, and \ndisable encryption keys with minimal effort for any data that must be encrypted. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a customer managed key by using AWS Key Management Service (AWS KMS). Use the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS managed key by using AWS Key Management Service (AWS KMS). Use the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company wants to back up its on-premises virtual machines (VMs) to AWS. The company's \nbackup solution exports on-premises backups to an Amazon S3 bucket as objects. The S3 \nbackups must be retained for 30 days and must be automatically deleted after 30 days. \n \nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket that has S3 Object Lock enabled.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an S3 bucket that has object versioning enabled.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a default retention period of 30 days for the objects.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an S3 Lifecycle policy to protect the objects for 30 days.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure an S3 Lifecycle policy to expire the objects after 30 days.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 63,
    "text": "A solutions architect needs to copy files from an Amazon S3 bucket to an Amazon Elastic File \nSystem (Amazon EFS) file system and another S3 bucket. The files must be copied continuously. \nNew files are added to the original S3 bucket consistently. The copied files should be overwritten \nonly if the source file changes. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n370 \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function. Mount the file system to the function. Set up an S3 event",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance in the same VPC as the file system. Mount the file system.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store \n(Amazon EBS) volumes. The company must ensure that all data is encrypted at rest by using \nAWS Key Management Service (AWS KMS). The company must be able to control rotation of the \nencryption keys. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a customer managed key. Use the key to encrypt the EBS volumes.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an AWS managed key to encrypt the EBS volumes. Use the key to configure automatic key",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an external KMS key with imported key material. Use the key to encrypt the EBS volumes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS owned key to encrypt the EBS volumes.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.",
    "domain": "Design Secure Architectures"
  }
]