[
  {
    "id": 0,
    "text": "A company wants to migrate an on-premises data center to AWS. The data center hosts a \nstorage server that stores data in an NFS-based file system. The storage server holds 200 GB of \ndata. The company needs to migrate the data without interruption to existing services. Multiple \nresources in AWS must be able to access the data by using the NFS protocol. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon FSx for Lustre file system.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket to receive the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Manually use an operating system copy command to push the data into the AWS destination.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon EFS provides a scalable, high performance NFS file system that can be accessed from multiple resources in AWS. AWS DataSync can perform the migration from the on-prem NFS server to EFS without interruption to existing services. This avoids having to manually move the data which could cause downtime. DataSync incrementally syncs changed data. EFS and DataSync together provide a cost-optimized approach compared to using S3 or FSx, while still meeting the requirements. Manually copying 200 GB of data to AWS would be slow and risky compared to using DataSync.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that \nhave an SMB file share mounted as a volume in the us-east-1 Region. The company has a \nrecovery point objective (RPO) of 5 minutes for planned system maintenance or unplanned \nservice disruptions. The company needs to replicate the file system to the us-west-2 Region. The \nreplicated data must not be deleted by any user for 5 years. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n345",
    "options": [
      {
        "id": 0,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A solutions architect is designing a security solution for a company that wants to provide \ndevelopers with individual AWS accounts through AWS Organizations, while also maintaining \nstandard security controls. Because the individual developers will have AWS account root user-\nlevel access to their own accounts, the solutions architect wants to ensure that the mandatory \nAWS CloudTrail configuration that is applied to new developer accounts is not modified. \n \nWhich action meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM policy that prohibits changes to CloudTrail. and attach it to the root user.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new trail in CloudTrail from within the developer accounts with the organization trails",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a service-linked role for CloudTrail with a policy condition that allows changes only from",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company is planning to deploy a business-critical application in the AWS Cloud. The application \nrequires durable storage with consistent, low-latency performance. \n \nWhich type of storage should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Instance store volume",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon ElastiCache for Memcached cluster",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume",
        "correct": true
      },
      {
        "id": 3,
        "text": "Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/ebs/volume-types/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-\nwest-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. \n \nWhich solution will meet this requirement with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/about-aws/whats-new/2015/03/amazon-s3-introduces-cross-region- replication/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A company is creating a new web application for its subscribers. The application will consist of a \nstatic single page and a persistent database layer. The application will have millions of users for 4 \nhours in the morning, but the application will have only a few thousand users during the rest of \nthe day. The company's data architects have requested the ability to rapidly evolve their schema. \n \nWhich solutions will meet these requirements and provide the MOST scalability? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 4,
        "text": "Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nFor tables using on-demand mode, DynamoDB instantly accommodates customers’ workloads as they ramp up or down to any previously observed traffic level. If the level of traffic hits a new peak, DynamoDB adapts rapidly to accommodate the workload. https://aws.amazon.com/blogs/aws/amazon-dynamodb-on-demand-no-capacity-planning-and- pay-per-request-pricing/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A company uses Amazon API Gateway to manage its REST APIs that third-party service \nproviders access. The company must protect the REST APIs from SQL injection and cross-site \nscripting attacks. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Shield.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS WAF.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS Shield in",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS WAF in CloudFront.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-xss-conditions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company wants to provide users with access to AWS resources. The company has 1,500 users \nand manages their access to on-premises resources through Active Directory user groups on the \ncorporate network. However, the company does not want users to have to maintain another \nidentity to access the resources. A solutions architect must manage user access to the AWS \nresources while preserving access to the on-premises resources. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM user for each user in the company. Attach the appropriate policies to each user.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies",
        "correct": false
      },
      {
        "id": 2,
        "text": "Define cross-account roles with the appropriate policies attached. Map the roles to the Active",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://aws.amazon.com/identity/saml/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company is hosting a website behind multiple Application Load Balancers. The company has \ndifferent distribution rights for its content around the world. A solutions architect needs to ensure \nthat users are served the correct content without violating distribution rights. \n \nWhich configuration should the solutions architect choose to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudFront with AWS WAF.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Application Load Balancers with AWS WAF",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Route 53 with a geolocation policy",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon Route 53 with a geoproximity routing policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company stores its data on premises. The amount of data is growing beyond the company's \navailable capacity. \n \nThe company wants to migrate its data from the on-premises location to an Amazon S3 bucket. \nThe company needs a solution that will automatically validate the integrity of the data after the \ntransfer. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nDuring a transfer, AWS DataSync always checks the integrity of your data, but you can specify how and when this verification happens with the following options: Verify only the data transferred (recommended) – DataSync calculates the checksum of transferred files and metadata at the source location. https://docs.aws.amazon.com/datasync/latest/userguide/configure-data-verification-options.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company wants to migrate two DNS servers to AWS. The servers host a total of approximately \n200 zones and receive 1 million requests each day on average. The company wants to maximize \navailability while minimizing the operational overhead that is related to the management of the \ntwo servers. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create 200 new hosted zones in the Amazon Route 53 console Import zone files.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/migrate-dns-domain-in-use.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A global company runs its applications in multiple AWS accounts in AWS Organizations. The \ncompany's applications use multipart uploads to upload data to multiple Amazon S3 buckets \nacross AWS Regions. The company wants to report on incomplete multipart uploads for cost \ncompliance purposes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Config with a rule to report the incomplete multipart upload object count.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a service control policy (SCP) to report the incomplete multipart upload object count.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 Storage Lens to report the incomplete multipart upload object count.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nS3 storage lenses can be used to find incomplete multipart uploads: https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting- incomplete-multipart-uploads-to-lower-amazon-s3-costs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company runs a production database on Amazon RDS for MySQL. The company wants to \nupgrade the database version for security compliance reasons. Because the database contains \ncritical data, the company wants a quick solution to upgrade and test functionality without losing \nany data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS Blue/Green Deployments to deploy and test production changes.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nYou can make changes to the RDS DB instances in the green environment without affecting production workloads. For example, you can upgrade the major or minor DB engine version, upgrade the underlying file system configuration, or change database parameters in the staging environment. You can thoroughly test changes in the green environment. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/blue-green-deployments- overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A solutions architect is creating a data processing job that runs once daily and can take up to 2 \nhours to complete. If the job is interrupted, it has to restart from the beginning. \n \nHow should the solutions architect address this issue in the MOST cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 14,
    "text": "A social media company wants to store its database of user profiles, relationships, and \ninteractions in the AWS Cloud. The company needs an application to monitor any changes in the \ndatabase. The application needs to analyze the relationships between the data entities and to \nprovide recommendations to users. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nWith Amazon Neptune, you can create sophisticated, interactive graph applications that can query billions of relationships in milliseconds. https://aws.amazon.com/neptune/features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company is creating a new application that will store a large amount of data. The data will be \nanalyzed hourly and will be modified by several Amazon EC2 Linux instances that are deployed \nacross multiple Availability Zones. The needed amount of storage space will continue to grow for \nthe next 6 months. \n \nWhich storage solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nShared File System: Amazon EFS allows multiple Amazon EC2 instances to mount the same file system simultaneously, making it easy for multiple instances to access and modify the data concurrently. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-\nAZ DB instance. Increases in traffic are causing performance problems. The company determines \nthat database queries are the primary reason for the slow performance. \n \nWhat should a solutions architect do to improve the application's performance?",
    "options": [
      {
        "id": 0,
        "text": "Serve read traffic from the Multi-AZ standby replica.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the DB instance to use Transfer Acceleration.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read replica from the source DB instance. Serve read traffic from the read replica.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Kinesis Data Firehose between the application and Amazon RDS to increase the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAfter you create a read replica from a source DB instance, the source becomes the primary DB instance. When you make updates to the primary DB instance, Amazon RDS copies them asynchronously to the read replica. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A company collects 10 GB of telemetry data daily from various machines. The company stores \nthe data in an Amazon S3 bucket in a source data account. \n \nThe company has hired several consulting agencies to use this data for analysis. Each agency \nneeds read access to the data for its analysts. The company must share the data from the source \ndata account by choosing a solution that maximizes security and operational efficiency. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 global tables to replicate data for each agency.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Make the S3 bucket public for a limited time. Inform only the agencies.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure cross-account access for the S3 bucket to the accounts that the agencies own.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up an IAM user for each analyst in the source data account. Grant each user access to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS \nfile shares. Applications that run on Amazon EC2 instances access the file shares. The company \nneeds a storage disaster recovery (DR) solution in a secondary Region. The data that is \nreplicated in the secondary Region needs to be accessed by using the same protocols as the \nprimary Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nYou can use NetApp SnapMirror to schedule periodic replication of your FSx for ONTAP file system to or from a second file system. This capability is available for both in-Region and cross- Region deployments. https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/scheduled-replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A development team is creating an event-based application that uses AWS Lambda functions. \nEvents will be generated when files are added to an Amazon S3 bucket. The development team \ncurrently has Amazon Simple Notification Service (Amazon SNS) configured as the event target \nfrom Amazon S3. \n \nWhat should a solutions architect do to process the events from Amazon S3 in a scalable way?",
    "options": [
      {
        "id": 0,
        "text": "Create an SNS subscription that processes the event in Amazon Elastic Container Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon SQS is designed for event-driven and scalable message processing. It can handle large volumes of messages and automatically scales based on the incoming workload. This allows for better load distribution and scaling as compared to direct Lambda invocation.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect is designing a new service behind Amazon API Gateway. The request \npatterns for the service will be unpredictable and can change suddenly from 0 requests to over \n500 per second. The total size of the data that needs to be persisted in a backend database is \ncurrently less than 1 GB with unpredictable future growth. Data can be queried using simple key-\nvalue requests. \n \nWhich combination ofAWS services would meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "AWS Fargate",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 Auto Scaling",
        "correct": false
      },
      {
        "id": 4,
        "text": "MySQL-compatible Amazon Aurora",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A company collects and shares research data with the company's employees all over the world. \nThe company wants to collect and store the data in an Amazon S3 bucket and process the data \nin the AWS Cloud. The company will share the data with the company's employees. The \ncompany needs a secure solution in the AWS Cloud that minimizes operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM user for each employee. Create an IAM policy for each employee to allow S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company is building a new furniture inventory application. The company has deployed the \napplication on a fleet ofAmazon EC2 instances across multiple Availability Zones. The EC2 \ninstances run behind an Application Load Balancer (ALB) in their VPC. \n \nA solutions architect has observed that incoming traffic seems to favor one EC2 instance, \nresulting in latency for some requests. \n \nWhat should the solutions architect do to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Disable session affinity (sticky sessions) on the ALB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Replace the ALB with a Network Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Increase the number of EC2 instances in each Availability Zone",
        "correct": false
      },
      {
        "id": 3,
        "text": "Adjust the frequency of the health checks on the ALB's target group",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company has an application workflow that uses an AWS Lambda function to download and \ndecrypt files from Amazon S3. These files are encrypted using AWS Key Management Service \n(AWS KMS) keys. A solutions architect needs to design a solution that will ensure the required \npermissions are set correctly. \n \nWhich combination of actions accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Attach the kms:decrypt permission to the Lambda function's resource policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Grant the decrypt permission for the Lambda IAM role in the KMS key's policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Grant the decrypt permission for the Lambda resource policy in the KMS key's policy.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a new IAM role with the kms:decrypt permission and attach the execution role to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "A company wants to monitor its AWS costs for financial review. The cloud operations team is \ndesigning an architecture in the AWS Organizations management account to query AWS Cost \nand Usage Reports for all member accounts. The team must run this query once a month and \nprovide a detailed analysis of the bill. \n \nWhich solution is the MOST scalable and cost-effective way to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3 Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/big-data/analyze-amazon-s3-storage-costs-using-aws-cost-and- usage-reports-amazon-s3-inventory-and-amazon-athena/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 25,
    "text": "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto \nScaling group in the AWS Cloud. The application will transmit data by using UDP packets. The \ncompany wants to ensure that the application can scale out and in as traffic increases and \ndecreases. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Attach a Network Load Balancer to the Auto Scaling group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an Application Load Balancer to the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company runs several websites on AWS for its different brands. Each website generates tens \nof gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution \nto give the company's developers the ability to analyze traffic patterns across all the company's \nwebsites. This analysis by the developers will occur on demand once a week over the course of \nseveral months. The solution must support queries with standard SQL. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n355 \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the logs in Amazon S3. Use Amazon Athena tor analysis.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the logs in Amazon RDS. Use a database client for analysis.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 27,
    "text": "An international company has a subdomain for each country that the company operates in. The \nsubdomains are formatted as example.com, country1.example.com, and country2.example.com. \nThe company's workloads are behind an Application Load Balancer. The company wants to \nencrypt the website data that is in transit. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the AWS Certificate Manager (ACM) console to request a private certificate for the apex top",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Certificate Manager (ACM) console to request a public and private certificate for the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Validate domain ownership by email address. Switch to DNS validation by adding the required",
        "correct": false
      },
      {
        "id": 4,
        "text": "Validate domain ownership for the domain by adding the required DNS records to the DNS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company is required to use cryptographic keys in its on-premises key manager. The key \nmanager is outside of the AWS Cloud because of regulatory and compliance requirements. The \ncompany wants to manage encryption and decryption by using cryptographic keys that are \nretained outside of the AWS Cloud and that support a variety of external key managers from \ndifferent vendors. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudHSM key store backed by a CloudHSM cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Key Management Service (AWS KMS) external key store backed by an external key",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the default AWS Key Management Service (AWS KMS) managed key store.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a custom key store backed by an AWS CloudHSM cluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/kms/latest/developerguide/keystore-external.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A solutions architect needs to host a high performance computing (HPC) workload in the AWS \nCloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel \naccess to a shared file system to enable distributed processing of large datasets. Datasets will be \naccessed across multiple instances simultaneously. The workload requires access latency within \n1 ms. After processing has completed, engineers will need access to the dataset for manual \npostprocessing. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from",
        "correct": false
      },
      {
        "id": 1,
        "text": "Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure AWS Resource Access Manager to share an Amazon S3 bucket so that it can be",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Lustre is a fully managed, high-performance file system optimized for HPC workloads. It is designed to deliver sub-millisecond latencies and high throughput, making it ideal for applications that require parallel access to shared storage, such as simulations and data analytics.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "A gaming company is building an application with Voice over IP capabilities. The application will \nserve traffic to users across the world. The application needs to be highly available with an \nautomated failover across AWS Regions. The company wants to minimize the latency of users \nwithout relying on IP address caching on user devices. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator with health checks.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 with a geolocation routing policy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution that includes multiple origins.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer that uses path-based routing.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-benefits-of-migrating.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 31,
    "text": "A weather forecasting company needs to process hundreds of gigabytes of data with sub-\nmillisecond latency. The company has a high performance computing (HPC) environment in its \ndata center and wants to expand its forecasting capabilities. \n \nA solutions architect must identify a highly available cloud storage solution that can handle large \namounts of sustained throughput. Files that are stored in the solution should be accessible to \nthousands of compute instances that will simultaneously access and process the entire dataset. \n \nWhat should the solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n357",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon FSx for Lustre scratch file systems.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon FSx for Lustre persistent file systems.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "An ecommerce company runs a PostgreSQL database on premises. The database stores data by \nusing high IOPS Amazon Elastic Block Store (Amazon EBS) block storage. The daily peak I/O \ntransactions per second do not exceed 15,000 IOPS. The company wants to migrate the \ndatabase to Amazon RDS for PostgreSQL and provision disk IOPS performance independent of \ndisk storage capacity. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the EBS magnetic volume type to achieve maximum IOPS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 33,
    "text": "A company wants to migrate its on-premises Microsoft SQL Server Enterprise edition database to \nAWS. The company's online application uses the database to process transactions. The data \nanalysis team uses the same production database to run reports for analytical processing. The \ncompany wants to reduce operational overhead by moving to managed services wherever \npossible. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate to Amazon RDS for Microsoft SOL Server. Use read replicas for reporting purposes",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate to Microsoft SQL Server on Amazon EC2. Use Always On read replicas for reporting",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate to Amazon DynamoDB. Use DynamoDB on-demand replicas for reporting purposes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate to Amazon Aurora MySQL. Use Aurora read replicas for reporting purposes",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company stores a large volume of image files in an Amazon S3 bucket. The images need to be \nreadily available for the first 180 days. The images are infrequently accessed for the next 180 \ndays. After 360 days, the images need to be archived but must be available instantly upon \nrequest. After 5 years, only auditors can access the images. The auditors must be able to retrieve \nthe images within 12 hours. The images cannot be lost during this process. \n \nA developer will use S3 Standard storage for the first 180 days. The developer needs to configure \nan S3 Lifecycle rule. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n358 \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A company has a large data workload that runs for 6 hours each day. The company cannot lose \nany data while the process is running. A solutions architect is designing an Amazon EMR cluster \nconfiguration to support this critical data workload. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure a long-running cluster that runs the primary node and core nodes on On-Demand",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a transient cluster that runs the primary node and core nodes on On-Demand",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a transient cluster that runs the primary node on an On-Demand Instance and the core",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a long-running cluster that runs the primary node on an On-Demand Instance, the core",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nA transient cluster provides cost savings because it runs only during the computation time, and it provides scalability and flexibility in a cloud environment.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "A company maintains an Amazon RDS database that maps users to cost centers. The company \nhas accounts in an organization in AWS Organizations. The company needs a solution that will \ntag all resources that are created in a specific AWS account in the organization. The solution \nmust tag each resource with the cost center ID of the user who created the resource. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the specific AWS account to a new organizational unit (OU) in Organizations from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function to tag the resources after the Lambda function looks up the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS CloudFormation stack to deploy an AWS Lambda function. Configure the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function to tag the resources with a default value. Configure an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 37,
    "text": "A company recently migrated its web application to the AWS Cloud. The company uses an \nAmazon EC2 instance to run multiple processes to host the application. The processes include \nan Apache web server that serves static content. The Apache web server makes requests to a \nPHP application that uses a local Redis server for user sessions. \n \nThe company wants to redesign the architecture to be highly available and to use AWS managed \nsolutions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Elastic Beanstalk to host the static content and the PHP application. Configure Elastic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Lambda to host the static content and the PHP application. Use an Amazon API",
        "correct": false
      },
      {
        "id": 2,
        "text": "Keep the backend code on the EC2 instance. Create an Amazon ElastiCache for Redis cluster",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon CloudFront distribution with an Amazon S3 endpoint to an S3 bucket that is",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group that has a \ntarget group. The company designed the application to work with session affinity (sticky sessions) \nfor a better user experience. \n \nThe application must be available publicly over the internet as an endpoint. A WAF must be \napplied to the endpoint for additional security. Session affinity (sticky sessions) must be \nconfigured on the endpoint. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a public Network Load Balancer. Specify the application target group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Gateway Load Balancer. Specify the application target group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a public Application Load Balancer. Specify the application target group.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a second target group. Add Elastic IP addresses to the EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a web ACL in AWS WAF. Associate the web ACL with the endpoint",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company runs a website that stores images of historical events. Website users need the ability \nto search and view images based on the year that the event in the image occurred. On average, \nusers request each image only once or twice a year. The company wants a highly available \nsolution to store and deliver the images to users. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store images in Amazon Elastic Block Store (Amazon EBS). Use a web server that runs on",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store images in Amazon Elastic File System (Amazon EFS). Use a web server that runs on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store images in Amazon S3 Standard. Use S3 Standard to directly deliver images by using a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3 Standard-IA",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 40,
    "text": "A company has multiple AWS accounts in an organization in AWS Organizations that different \nbusiness units use. The company has multiple offices around the world. The company needs to \nupdate security group rules to allow new office CIDR ranges or to remove old CIDR ranges \nacross the organization. The company wants to centralize the management of security group \nrules to minimize the administrative overhead that updating CIDR ranges requires. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create VPC security groups in the organization's management account. Update the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPC customer managed prefix list that contains the list of CIDRs. Use AWS Resource",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS managed prefix list. Use an AWS Security Hub policy to enforce the security",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create security groups in a central administrative AWS account. Create an AWS Firewall",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 41,
    "text": "A company uses an on-premises network-attached storage (NAS) system to provide file shares to \nits high performance computing (HPC) workloads. The company wants to migrate its latency-\nsensitive HPC workloads and its storage to the AWS Cloud. The company must be able to \nprovide NFS and SMB multi-protocol access from the file system. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n361 \nWhich solution will meet these requirements with the LEAST latency? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy compute optimized EC2 instances into a cluster placement group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy compute optimized EC2 instances into a partition placement group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Attach the EC2 instances to an Amazon FSx for Lustre file system.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach the EC2 instances to an Amazon FSx for OpenZFS file system.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Attach the EC2 instances to an Amazon FSx for NetApp ONTAP file system.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A company is relocating its data center and wants to securely transfer 50 TB of data to AWS \nwithin 2 weeks. The existing data center has a Site-to-Site VPN connection to AWS that is 90% \nutilized. \n \nWhich AWS service should a solutions architect use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "AWS DataSync with a VPC endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball Edge Storage Optimized",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A company hosts an application on Amazon EC2 On-Demand Instances in an Auto Scaling \ngroup. Application peak hours occur at the same time each day. Application users report slow \napplication performance at the start of peak hours. The application performs normally 2-3 hours \nafter peak hours begin. The company wants to ensure that the application works properly at the \nstart of peak hours. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Application Load Balancer to distribute traffic properly to the instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 44,
    "text": "A company runs applications on AWS that connect to the company's Amazon RDS database. \nThe applications scale on weekends and at peak times of the year. The company wants to scale \nthe database more effectively for its applications that connect to the database. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB with connection pooling with a target group configuration for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS Proxy with a target group for the database. Change the applications to use the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a custom proxy that runs on Amazon EC2 as an intermediary to the database. Change the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Lambda function to provide connection pooling with a target group configuration for",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company uses AWS Cost Explorer to monitor its AWS costs. The company notices that \nAmazon Elastic Block Store (Amazon EBS) storage and snapshot costs increase every month. \nHowever, the company does not purchase additional EBS storage every month. The company \nwants to optimize monthly costs for its current storage usage. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use logs in Amazon CloudWatch Logs to monitor the storage utilization of Amazon EBS. Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a custom script to monitor space usage. Use Amazon EBS Elastic Volumes to reduce the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Delete all expired and unused snapshots to reduce snapshot costs.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Delete all nonessential snapshots. Use Amazon Data Lifecycle Manager to create and manage",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "A company is developing a new application on AWS. The application consists of an Amazon \nElastic Container Service (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for \nthe application, and an Amazon RDS for MySQL database that contains the dataset for the \napplication. The dataset contains sensitive information. The company wants to ensure that only \nthe ECS cluster can access the data in the RDS for MySQL database and the data in the S3 \nbucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Key Management Service (AWS KMS) AWS managed key to encrypt both the S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket policy that restricts bucket access to the ECS task execution role. Create a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a VPC endpoint for Amazon RDS for MySQL. Update the RDS for MySQL security group",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe most comprehensive solution as it leverages VPC endpoints for both Amazon RDS and Amazon S3, along with proper network-level controls to restrict access to only the necessary resources from the ECS cluster.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A company has a web application that runs on premises. The application experiences latency \nissues during peak hours. The latency issues occur twice each month. At the start of a latency \nissue, the application's CPU utilization immediately increases to 10 times its normal amount. \n \nThe company wants to migrate the application to AWS to improve latency. The company also \nwants to scale the application automatically when application demand increases. The company \nwill use AWS Elastic Beanstalk for application deployment. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 48,
    "text": "A company has customers located across the world. The company wants to use automation to \nsecure its systems and network infrastructure. The company's security team must be able to track \nand audit all incremental changes to the infrastructure. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Organizations to set up the infrastructure. Use AWS Config to track changes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CloudFormation to set up the infrastructure. Use AWS Config to track changes.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Organizations to set up the infrastructure. Use AWS Service Catalog to track changes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS CloudFormation to set up the infrastructure. Use AWS Service Catalog to track",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A startup company is hosting a website for its customers on an Amazon EC2 instance. The \nwebsite consists of a stateless Python application and a MySQL database. The website serves \nonly a small amount of traffic. The company is concerned about the reliability of the instance and \nneeds to migrate to a highly available architecture. The company cannot modify the application \ncode. \n \nWhich combination of actions should a solutions architect take to achieve high availability for the \nwebsite? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n364",
    "options": [
      {
        "id": 0,
        "text": "Provision an internet gateway in each Availability Zone in use.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to synchronize the database data across multiple EC2 instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company is moving its data and applications to AWS during a multiyear migration project. The \ncompany wants to securely access data on Amazon S3 from the company's AWS Region and \nfrom the company's on-premises location. The data must not traverse the internet. The company \nhas established an AWS Direct Connect connection between its Region and its on-premises \nlocation. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create gateway endpoints for Amazon S3. Use the gateway endpoints to securely access the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a gateway in AWS Transit Gateway to access Amazon S3 securely from the Region and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create interface endpoints for Amazon S3. Use the interface endpoints to securely access the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an AWS Key Management Service (AWS KMS) key to access the data securely from the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company created a new organization in AWS Organizations. The organization has multiple \naccounts for the company's development teams. The development team members use AWS IAM \nIdentity Center (AWS Single Sign-On) to access the accounts. For each of the company's \napplications, the development teams must use a predefined application name to tag resources \nthat are created. \n \nA solutions architect needs to design a solution that gives the development team the ability to \ncreate resources only if the application name tag has an approved value. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM group that has a conditional Allow policy that requires the application name tag to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a cross-account role that has a Deny policy for any resource that has the application name",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a resource group in AWS Resource Groups to validate that the tags are applied to all",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a tag policy in Organizations that has a list of allowed application names.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company runs its databases on Amazon RDS for PostgreSQL. The company wants a secure \nsolution to manage the master user password by rotating the password every 30 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EventBridge to schedule a custom AWS Lambda function to rotate the password",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the modify-db-instance command in the AWS CLI to change the password.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate password",
        "correct": true
      },
      {
        "id": 3,
        "text": "Integrate AWS Systems Manager Parameter Store with Amazon RDS for PostgreSQL to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company performs tests on an application that uses an Amazon DynamoDB table. The tests \nrun for 4 hours once a week. The company knows how many read and write operations the \napplication performs to the table each second during the tests. The company does not currently \nuse DynamoDB for any other use case. A solutions architect needs to optimize the costs for the \ntable. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Choose on-demand mode. Update the read and write capacity units appropriately.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Choose provisioned mode. Update the read and write capacity units appropriately.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase DynamoDB reserved capacity for a 1-year term.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase DynamoDB reserved capacity for a 3-year term.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nWith provisioned capacity mode, you specify the number of reads and writes per second that you expect your application to require, and you are billed based on that. Furthermore if you can forecast your capacity requirements you can also reserve a portion of DynamoDB provisioned capacity and optimize your costs even further. https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 54,
    "text": "A company runs its applications on Amazon EC2 instances. The company performs periodic \nfinancial assessments of its AWS costs. The company recently identified unusual spending. \n \nThe company needs a solution to prevent unusual spending. The solution must monitor costs and \nnotify responsible stakeholders in the event of unusual spending. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n366",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS Budgets template to create a zero spend budget.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create AWS Pricing Calculator estimates for the current running workload pricing details.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudWatch to monitor costs and to identify unusual spending.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Cost Anomaly Detection is designed to automatically detect unusual spending patterns based on machine learning algorithms. It can identify anomalies and send notifications when it detects unexpected changes in spending. This aligns well with the requirement to prevent unusual spending and notify stakeholders. https://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A marketing company receives a large amount of new clickstream data in Amazon S3 from a \nmarketing campaign. The company needs to analyze the clickstream data in Amazon S3 quickly. \nThen the company needs to determine whether to process the data further in the data pipeline. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create external tables in a Spark catalog. Configure jobs in AWS Glue to query the data.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an AWS Glue crawler to crawl the data. Configure Amazon Athena to query the data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create external tables in a Hive metastore. Configure Spark jobs in Amazon EMR to query the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Glue crawler to crawl the data. Configure Amazon Kinesis Data Analytics to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service, and Athena is a serverless query service that allows you to analyze data directly in Amazon S3 using SQL queries. By configuring an AWS Glue crawler to crawl the data, you can create a schema for the data, and then use Athena to query the data directly without the need to load it into a separate database. This minimizes operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A company runs an SMB file server in its data center. The file server stores large files that the \ncompany frequently accesses for up to 7 days after the file creation date. After 7 days, the \ncompany needs to be able to access the files with a maximum retrieval time of 24 hours. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 File Gateway to increase the company's storage space. Create an S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon FSx File Gateway to increase the company's storage space. Create an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure access to Amazon S3 for each user. Create an S3 Lifecycle policy to transition the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nS3 file gateway supports SMB and S3 Glacier Deep Archive can retrieve data within 12 hours. https://aws.amazon.com/storagegateway/file/s3/ https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/amazon-s3- glacier.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group. The \napplication uses a database that runs on an Amazon RDS for PostgreSQL DB instance. The \napplication performs slowly when traffic increases. The database experiences a heavy read load \nduring periods of high traffic. \n \nWhich actions should a solutions architect take to resolve these performance issues? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Turn on auto scaling for the DB instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a read replica for the DB instance. Configure the application to send read traffic to the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Convert the DB instance to a Multi-AZ DB instance deployment. Configure the application to send",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon ElastiCache cluster. Configure the application to cache query results in the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the Auto Scaling group subnets to ensure that the EC2 instances are provisioned in the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy creating a read replica, you offload read traffic from the primary DB instance to the replica, distributing the load and improving overall performance during periods of heavy read traffic. Amazon ElastiCache can be used to cache frequently accessed data, reducing the load on the database. This is particularly effective for read-heavy workloads, as it allows the application to retrieve data from the cache rather than making repeated database queries.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) \nvolumes to run an application. The company creates one snapshot of each EBS volume every \nday to meet compliance requirements. The company wants to implement an architecture that \nprevents the accidental deletion of EBS volume snapshots. The solution must not change the \nadministrative rights of the storage administrator user. \n \nWhich solution will meet these requirements with the LEAST administrative effort?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that has permission to delete snapshots. Attach the role to a new EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM policy that denies snapshot deletion. Attach the policy to the storage administrator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add tags to the snapshots. Create retention rules in Recycle Bin for EBS snapshots that have the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Lock the EBS snapshots to prevent deletion.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 \ninstances, and databases that are deployed in an Amazon VPC. The company wants to capture \ninformation about traffic to and from the network interfaces in near real time in its Amazon VPC. \nThe company wants to send the information to Amazon OpenSearch Service for analysis. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nVPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. By configuring VPC Flow Logs to send the log data to a log group in Amazon CloudWatch Logs, you can then use Amazon Kinesis Data Firehose to stream the logs from the log group to Amazon OpenSearch Service for analysis. This approach provides near real-time streaming of logs to the analytics service.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company is developing an application that will run on a production Amazon Elastic Kubernetes \nService (Amazon EKS) cluster. The EKS cluster has managed node groups that are provisioned \nwith On-Demand Instances. \n \nThe company needs a dedicated EKS cluster for development work. The company will use the \ndevelopment cluster infrequently to test the resiliency of the application. The EKS cluster must \nmanage all the nodes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a managed node group that contains only Spot Instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create two managed node groups. Provision one node group with On-Demand Instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group that has a launch configuration that uses Spot Instances. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a managed node group that contains only On-Demand Instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 61,
    "text": "A company stores sensitive data in Amazon S3. A solutions architect needs to create an \nencryption solution. The company needs to fully control the ability of users to create, rotate, and \ndisable encryption keys with minimal effort for any data that must be encrypted. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a customer managed key by using AWS Key Management Service (AWS KMS). Use the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS managed key by using AWS Key Management Service (AWS KMS). Use the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis option allows you to create a customer managed key using AWS KMS. With a customer managed key, you have full control over key lifecycle management, including the ability to create, rotate, and disable keys with minimal effort. SSE-KMS also integrates with AWS Identity and Access Management (IAM) for fine-grained access control.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company wants to back up its on-premises virtual machines (VMs) to AWS. The company's \nbackup solution exports on-premises backups to an Amazon S3 bucket as objects. The S3 \nbackups must be retained for 30 days and must be automatically deleted after 30 days. \n \nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket that has S3 Object Lock enabled.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an S3 bucket that has object versioning enabled.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a default retention period of 30 days for the objects.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an S3 Lifecycle policy to protect the objects for 30 days.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure an S3 Lifecycle policy to expire the objects after 30 days.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-retention-date.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 63,
    "text": "A solutions architect needs to copy files from an Amazon S3 bucket to an Amazon Elastic File \nSystem (Amazon EFS) file system and another S3 bucket. The files must be copied continuously. \nNew files are added to the original S3 bucket consistently. The copied files should be overwritten \nonly if the source file changes. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n370 \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function. Mount the file system to the function. Set up an S3 event",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance in the same VPC as the file system. Mount the file system.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS DataSync is designed for efficient and reliable copying of data between different storage solutions. By setting up an AWS DataSync task with the transfer mode set to transfer only data that has changed, you ensure that only the new or modified files are copied. This minimizes data transfer and operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store \n(Amazon EBS) volumes. The company must ensure that all data is encrypted at rest by using \nAWS Key Management Service (AWS KMS). The company must be able to control rotation of the \nencryption keys. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a customer managed key. Use the key to encrypt the EBS volumes.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an AWS managed key to encrypt the EBS volumes. Use the key to configure automatic key",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an external KMS key with imported key material. Use the key to encrypt the EBS volumes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS owned key to encrypt the EBS volumes.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  }
]