[
  {
    "id": 0,
    "text": "A company's website provides users with downloadable historical performance reports. The \nwebsite needs a solution that will scale to meet the company's website demands globally. The \nsolution should be cost-effective, limit the provisioning of infrastructure resources, and provide the \nfastest possible response time. \n \nWhich combination should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon CloudFront and Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Lambda and Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Application Load Balancer with Amazon EC2 Auto Scaling",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Route 53 with internal Application Load Balancers",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis combination directly addresses all the requirements. Amazon S3 provides cost-effective and scalable storage for the reports. Amazon CloudFront, a CDN, caches these reports at edge locations globally, ensuring fast response times for users regardless of their location. It also handles scaling automatically and reduces the load on the origin (S3), minimizing infrastructure provisioning.\n\n**Why option 1 is incorrect:**\nAWS Lambda and Amazon DynamoDB are not suitable for serving static downloadable content like reports. Lambda is a compute service, and DynamoDB is a NoSQL database. While they can be part of a larger solution, they don't directly address the need for content distribution and caching for fast global access to downloadable files. This combination would also require more infrastructure provisioning and management.\n\n**Why option 2 is incorrect:**\nAn Application Load Balancer (ALB) with Amazon EC2 Auto Scaling is designed for distributing traffic to web applications running on EC2 instances. While it provides scalability, it's not the most cost-effective or efficient solution for serving static downloadable files. It requires provisioning and managing EC2 instances, which increases operational overhead and cost compared to using S3 and CloudFront. It also doesn't inherently provide global content distribution.\n\n**Why option 3 is incorrect:**\nAmazon Route 53 can be used for routing traffic to different endpoints, including Application Load Balancers. However, using internal Application Load Balancers doesn't address the need for global content distribution and caching. It also requires provisioning and managing EC2 instances behind the ALBs, increasing cost and complexity. This solution doesn't inherently provide the fastest possible response time globally.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company runs an Oracle database on premises. As part of the company's migration to AWS, \nthe company wants to upgrade the database to the most recent available version. The company \nalso wants to set up disaster recovery (DR) for the database. The company needs to minimize \nthe operational overhead for normal operations and DR setup. The company also needs to \nmaintain access to the database's underlying operating system. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the Oracle database to an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the Oracle database to Amazon RDS for Oracle.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the Oracle database to Amazon RDS Custom for Oracle.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the Oracle database to Amazon RDS for Oracle.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by providing a managed database service with OS access. RDS Custom for Oracle allows for database upgrades and provides the ability to configure DR using native Oracle features like Data Guard. It minimizes operational overhead compared to managing an Oracle database on EC2, while still granting access to the underlying operating system, which standard RDS does not. This allows the company to meet all the specified requirements: database upgrade, DR setup, minimized operational overhead, and OS access.\n\n**Why option 0 is incorrect:**\nThis option, while providing OS access, significantly increases operational overhead. Managing an Oracle database on EC2 requires manual patching, backups, and DR configuration, which contradicts the requirement to minimize operational overhead. While it allows for database upgrades and DR setup, the administrative burden is much higher than using a managed service.\n\n**Why option 1 is incorrect:**\nThis option simplifies database management and provides built-in DR capabilities. However, standard RDS for Oracle does not provide access to the underlying operating system. This violates the requirement to maintain access to the database's underlying operating system. While it minimizes operational overhead and facilitates upgrades, the lack of OS access makes it unsuitable.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company wants to move its application to a serverless solution. The serverless solution needs \nto analyze existing and new data by using SQL. The company stores the data in an Amazon S3 \nbucket. The data requires encryption and must be replicated to a different AWS Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a new S3 bucket. Load the data into the new S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new S3 bucket. Load the data into the new S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Load the data into the existing S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Load the data into the existing S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution leverages Amazon Athena for serverless SQL querying of data in S3. Athena directly integrates with S3 and allows you to run SQL queries without managing any infrastructure. To meet the encryption requirement, S3 offers server-side encryption (SSE) or client-side encryption. For cross-region replication, S3 Cross-Region Replication (CRR) can be enabled on the bucket. Creating a new S3 bucket allows for a clean separation of the data intended for serverless analysis, potentially simplifying access control and lifecycle management. The combination of Athena, S3 SSE/CSE, and S3 CRR provides a fully serverless solution with minimal operational overhead, as AWS manages the underlying infrastructure for these services.\n\n**Why option 1 is incorrect:**\nWhile creating a new S3 bucket is a valid approach for isolating data, the prompt is incomplete. It lacks the crucial components of a serverless SQL query engine (like Athena), encryption, and cross-region replication. Simply creating a new bucket and loading data doesn't address the core requirements of the question.\n\n**Why option 2 is incorrect:**\nLoading data into an existing S3 bucket might work if the existing bucket already has the necessary configurations for encryption and cross-region replication. However, it doesn't inherently provide a serverless SQL query engine. Furthermore, using an existing bucket might introduce complexities related to data governance, access control, and potential conflicts with existing applications using the same bucket. The prompt is incomplete, lacking the crucial components of a serverless SQL query engine (like Athena), encryption, and cross-region replication.\n\n**Why option 3 is incorrect:**\nSimilar to option 2, loading data into an existing S3 bucket without specifying the use of a serverless SQL query engine, encryption, and cross-region replication does not meet all the requirements of the question. It lacks the core components needed for a complete solution. The prompt is incomplete, lacking the crucial components of a serverless SQL query engine (like Athena), encryption, and cross-region replication.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A company runs workloads on AWS. The company needs to connect to a service from an \nexternal provider. The service is hosted in the provider's VPC. According to the company's \nsecurity team, the connectivity must be private and must be restricted to the target service. The \nconnection must be initiated only from the company's VPC. \n \nWhich solution will mast these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC peering connection between the company's VPC and the provider's VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Ask the provider to create a virtual private gateway in its VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a NAT gateway in a public subnet of the company's VPC.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Ask the provider to create a VPC endpoint for the target service.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of private connectivity and restriction to the target service by utilizing a VPC endpoint. A VPC endpoint allows the company's VPC to connect to the provider's service privately, without traversing the public internet. By having the provider create the VPC endpoint for the specific target service, the connection is restricted to only that service, fulfilling the security team's requirement. This also ensures that the connection is initiated from the company's VPC, as the company's resources will access the service through the endpoint.\n\n**Why option 0 is incorrect:**\nWhile VPC peering provides private connectivity between two VPCs, it establishes a connection between the entire VPCs. This does not fulfill the requirement of restricting connectivity to only the target service. VPC peering would expose the entire provider's VPC to the company's VPC, which violates the security team's requirements.\n\n**Why option 1 is incorrect:**\nA Virtual Private Gateway (VGW) is used to establish a VPN connection between a VPC and a remote network, such as a corporate data center. It's not suitable for connecting to a specific service within another VPC. Furthermore, asking the provider to create a VGW in their VPC doesn't directly address the requirement of restricting connectivity to the target service. This option also implies a VPN connection, which is more complex than necessary for this scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. \nThe on-premises database must remain online and accessible during the migration. The Aurora \ndatabase must remain synchronized with the on-premises database. \n \nWhich combination of actions must a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an ongoing replication task.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a database backup of the on-premises database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Database Migration Service (AWS DMS) replication server",
        "correct": false
      },
      {
        "id": 3,
        "text": "Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe question describes a database migration scenario from an on-premises PostgreSQL database to Amazon Aurora PostgreSQL with the requirements of minimal downtime and continuous synchronization during the migration process. The solution needs to ensure that the on-premises database remains online and accessible, and the Aurora database stays synchronized with the on-premises database until the cutover.\n\n**Why option 1 is incorrect:**\nCreating a database backup is a good practice for disaster recovery, but it does not address the requirement of keeping the on-premises database online and synchronized with the Aurora database during the migration. A backup is a point-in-time snapshot and does not provide continuous replication.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nAWS Schema Conversion Tool (SCT) is useful for converting the database schema if there are compatibility issues between the source and target databases. However, in this scenario, both databases are PostgreSQL, so schema conversion is less likely to be a primary concern. More importantly, SCT does not handle the actual data migration or synchronization, so it doesn't address the core requirements.\n\n**Why option 4 is incorrect:**\nAmazon EventBridge (formerly CloudWatch Events) can be used for monitoring database events, but it does not directly contribute to the data migration or synchronization process. It's more relevant for operational monitoring after the migration is complete.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company uses AWS Organizations to create dedicated AWS accounts for each business unit to \nmanage each business unit's account independently upon request. The root email recipient \nmissed a notification that was sent to the root user email address of one account. The company \nwants to ensure that all future notifications are not missed. Future notifications must be limited to \naccount administrators. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the company's email server to forward notification email messages that are sent to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure all AWS account root user email addresses as distribution lists that go to a few",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure all AWS account root user email messages to be sent to one administrator who is",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure all existing AWS accounts and all newly created accounts to use the same root user",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of ensuring notifications are not missed by distributing them to a group of administrators. Configuring the root user email addresses as distribution lists ensures that multiple administrators receive the notifications, increasing the likelihood that someone will see and act upon them. This also aligns with the requirement of limiting notifications to account administrators.\n\n**Why option 0 is incorrect:**\nConfiguring the company's email server to forward notification emails is not a viable solution because it requires access and modification to the company's email infrastructure, which is outside the scope of AWS account management. Furthermore, it doesn't directly address the problem of the root user missing the email in the first place, as the email still needs to reach the root user's inbox before being forwarded. This option also doesn't guarantee that only account administrators will receive the notifications.\n\n**Why option 2 is incorrect:**\nSending all root user email messages to a single administrator creates a single point of failure. If that administrator is unavailable or misses the email, the notification will still be missed, defeating the purpose of the requirement. This option also doesn't scale well as the number of accounts grows.\n\n**Why option 3 is incorrect:**\nUsing the same root user for all accounts is a security risk and violates AWS best practices. It creates a single point of failure for security and access control across all accounts. If the root user credentials are compromised, all accounts are at risk. This also hinders the independent management of each business unit's account, which is a stated goal of using AWS Organizations.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS \naccount. The company needs to create a strategy to access and administer the instances \nremotely and securely. The company needs to implement a repeatable process that works with \nnative AWS services and follows the AWS Well-Architected Framework.  \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use the EC2 serial console to directly access the terminal interface of each instance for",
        "correct": false
      },
      {
        "id": 1,
        "text": "Attach the appropriate IAM role to each existing instance and new instance.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an administrative SSH key pair.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Establish an AWS Site-to-Site VPN connection.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution leverages IAM roles for authentication and authorization, which is a core AWS security best practice. Attaching an IAM role to each EC2 instance allows the instances to assume the role and gain the permissions defined within that role. This eliminates the need to manage SSH keys on individual instances, reducing operational overhead and improving security. The IAM role can be configured to allow access to specific AWS services and resources, ensuring least privilege. This approach is repeatable, as the same IAM role can be attached to new instances as they are launched. It also aligns with the AWS Well-Architected Framework's security pillar by implementing strong identity and access management.\n\n**Why option 0 is incorrect:**\nWhile the EC2 serial console provides direct access to the terminal, it's primarily intended for troubleshooting boot issues or recovering from misconfigurations. It doesn't scale well for routine administration of multiple instances, and it requires direct access to the AWS Management Console, which might not be desirable for all administrators. It also doesn't provide the same level of auditing and control as IAM roles. Using the serial console for regular administration increases operational overhead and is not a best practice for secure remote access.\n\n**Why option 2 is incorrect:**\nCreating an administrative SSH key pair and distributing it across all instances introduces a significant security risk. If the key pair is compromised, all instances become vulnerable. Managing SSH keys across a large number of instances is also operationally complex and prone to errors. This approach does not align with the AWS Well-Architected Framework's security pillar, as it violates the principle of least privilege and increases the attack surface. Furthermore, it doesn't integrate well with native AWS services for identity and access management.\n\n**Why option 3 is incorrect:**\nEstablishing an AWS Site-to-Site VPN connection provides secure network connectivity between an on-premises network and the AWS environment. While it enhances security, it's not directly related to the requirement of accessing and administering EC2 instances remotely. It adds significant operational overhead, including managing the VPN connection, configuring routing, and maintaining the VPN gateway. This solution is more suitable for hybrid cloud scenarios where on-premises resources need to communicate with AWS resources, but it's not the most efficient or cost-effective way to manage EC2 instances remotely. It also doesn't address the need for granular access control and auditing.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. \nThe website is experiencing increased demand from around the world. The company must \ndecrease latency for users who access the website. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Replicate the S3 bucket that contains the website to all AWS Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision accelerators in AWS Global Accelerator.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution in front of the S3 bucket.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable S3 Transfer Acceleration on the bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement of decreasing latency for users around the world by caching the website content in edge locations closer to the users. CloudFront integrates seamlessly with S3 and Route 53, and its pay-as-you-go pricing model is generally more cost-effective than replicating the S3 bucket across all regions or using Global Accelerator for a static website. CloudFront is specifically designed for content delivery and is optimized for this use case, making it the most efficient and cost-effective solution.\n\n**Why option 0 is incorrect:**\nReplicating the S3 bucket to all AWS Regions would be extremely expensive and unnecessary for a static website. While it would reduce latency, the cost would be significantly higher than other solutions, violating the cost-effectiveness requirement. Furthermore, managing and synchronizing data across all regions would add operational complexity.\n\n**Why option 1 is incorrect:**\nProvisioning accelerators in AWS Global Accelerator is more suitable for dynamic content or applications that require consistent performance and availability. While it can improve latency, it is generally more expensive than using CloudFront for a static website. Global Accelerator is better suited for scenarios where you need to route traffic to the nearest healthy endpoint, which is not a primary concern for a static website served from S3.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 8,
    "text": "A company maintains a searchable repository of items on its website. The data is stored in an \nAmazon RDS for MySQL database table that contains more than 10 million rows. The database \nhas 2 TB of General Purpose SSD storage. There are millions of updates against this data every \nday through the company's website. \nThe company has noticed that some insert operations are taking 10 seconds or longer. \nThe company has determined that the database storage performance is the problem. \nWhich solution addresses this performance issue?",
    "options": [
      {
        "id": 0,
        "text": "Change the storage type to Provisioned IOPS SSD",
        "correct": true
      },
      {
        "id": 1,
        "text": "Change the DB instance to a memory optimized instance class",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the DB instance to a burstable performance instance class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Provisioned IOPS SSD storage provides consistent and predictable I/O performance. General Purpose SSD (gp2 or gp3) storage can experience I/O throttling when I/O credits are exhausted, especially with a high volume of write operations. Switching to Provisioned IOPS (io1 or io2) allows you to specify the number of IOPS required for the database, ensuring consistent performance even under heavy load. This directly addresses the identified storage performance bottleneck.\n\n**Why option 1 is incorrect:**\nThis is incorrect because changing the DB instance to a memory-optimized instance class primarily addresses CPU and memory bottlenecks, not storage performance. While more memory can improve caching and reduce disk I/O to some extent, it doesn't directly address the underlying storage performance issue identified in the question.\n\n**Why option 2 is incorrect:**\nThis is incorrect because burstable performance instance classes are designed for workloads with occasional bursts of activity. They rely on CPU credits and can experience performance degradation when these credits are exhausted. The scenario describes a consistently high volume of write operations, making a burstable performance instance class unsuitable. It would likely exacerbate the performance issues.\n\n**Why option 3 is incorrect:**\nThis is incorrect because enabling Multi-AZ RDS read replicas with MySQL native asynchronous replication primarily improves read performance and provides high availability. While read replicas can offload read traffic from the primary database, they do not directly address the slow insert operations on the primary database, which is the identified performance bottleneck. Asynchronous replication can also introduce replication lag, which might not be acceptable for all applications.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A company has thousands of edge devices that collectively generate 1 TB of status alerts each \nday. Each alert is approximately 2 KB in size.  \nA solutions architect needs to implement a solution to ingest and store the alerts for future \nanalysis. \nThe company wants a highly available solution. However, the company needs to minimize costs \nand does not want to manage additional infrastructure. Additionally, the company wants to keep \n14 days of data available for immediate analysis and archive any data older than 14 days. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution directly addresses the requirements by providing a fully managed service for ingesting streaming data. Kinesis Data Firehose can automatically scale to handle the 1 TB of daily alerts. It can be configured to deliver the data to destinations like Amazon S3 for storage and analysis. S3 supports lifecycle policies to automatically archive data older than 14 days, fulfilling the data retention requirement. This approach minimizes operational overhead as it's a managed service, and it's cost-effective compared to managing EC2 instances or using SQS for this volume of data.\n\n**Why option 1 is incorrect:**\nLaunching EC2 instances requires managing the infrastructure, including scaling, patching, and ensuring high availability. This increases operational overhead and costs compared to a managed service like Kinesis Data Firehose. While EC2 instances can process the data, it doesn't align with the requirement to minimize management overhead and costs.\n\n**Why option 2 is incorrect:**\nWhile SQS can ingest data, it's not designed for high-throughput streaming data like Kinesis Data Firehose. SQS requires polling, which can be inefficient and costly for this volume of data. It also doesn't provide built-in features for data transformation, delivery to data stores like S3, or data lifecycle management, making it less suitable than Kinesis Data Firehose for this scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "A company's application integrates with multiple software-as-a-service (SaaS) sources for data \ncollection. The company runs Amazon EC2 instances to receive the data and to upload the data \nto an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data \nalso sends a notification to the user when an upload is complete. The company has noticed slow \napplication performance and wants to improve the performance as much as possible. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group so that EC2 instances can scale out.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Docker container to use instead of an EC2 instance. Host the containerized application",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the performance bottleneck and reduces operational overhead. Amazon AppFlow is a fully managed integration service that allows you to securely transfer data between SaaS applications and AWS services like S3, without writing custom code. By using AppFlow, the EC2 instance is relieved of the data transfer responsibility, improving its performance. AppFlow also handles the complexities of connecting to various SaaS sources, managing authentication, and handling data transformations, significantly reducing operational overhead compared to managing custom data transfer scripts on EC2 instances.\n\n**Why option 0 is incorrect:**\nWhile scaling out EC2 instances might improve performance to some extent, it doesn't address the fundamental issue of the EC2 instance performing multiple tasks. It also increases operational overhead as you need to manage more instances. The EC2 instances would still be responsible for receiving data, uploading to S3, and sending notifications, which could still lead to performance bottlenecks. Furthermore, Auto Scaling adds complexity to the architecture.\n\n**Why option 2 is incorrect:**\nCreating EventBridge rules for each SaaS source is not the correct approach for transferring data. EventBridge is designed for event-driven architectures, where it routes events between different services. It doesn't handle data transfer directly. While EventBridge could be used to trigger a data transfer process, it would still require another service (like Lambda or EC2) to handle the actual data transfer, adding complexity and operational overhead. It doesn't directly address the data transfer bottleneck.\n\n**Why option 3 is incorrect:**\nUsing Docker containers instead of EC2 instances doesn't fundamentally solve the performance problem. It simply changes the deployment method. The containerized application would still be responsible for receiving data, uploading to S3, and sending notifications, so the same performance bottlenecks would likely persist. While containerization can offer benefits like portability and consistency, it doesn't inherently improve performance in this scenario. It also doesn't reduce operational overhead compared to using AppFlow.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "A company runs a highly available image-processing application on Amazon EC2 instances in a \nsingle VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The \nEC2 instances do not communicate with each other. However, the EC2 instances download \nimages from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The \ncompany is concerned about data transfer charges. \nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n110",
    "options": [
      {
        "id": 0,
        "text": "Launch the NAT gateway in each Availability Zone",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the NAT gateway with a NAT instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a gateway VPC endpoint for Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision an EC2 Dedicated Host to run the EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by creating a direct, private connection between the VPC and S3. A gateway VPC endpoint for S3 allows EC2 instances to access S3 without using the internet or a NAT gateway. This eliminates Regional data transfer charges associated with using a NAT gateway for S3 access within the same region.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while launching a NAT gateway in each Availability Zone improves availability and potentially reduces latency for instances within that AZ, it doesn't eliminate data transfer charges. Data transfer between the EC2 instances and the NAT gateway, and from the NAT gateway to S3, still incurs Regional data transfer costs. It also increases the cost of running multiple NAT Gateways.\n\n**Why option 1 is incorrect:**\nThis is incorrect because replacing the NAT gateway with a NAT instance doesn't solve the data transfer cost issue and introduces additional management overhead. NAT instances also require more manual configuration and maintenance compared to NAT gateways. Data transfer costs will still be incurred between the EC2 instances and the NAT instance, and from the NAT instance to S3. NAT instances are also less scalable and highly available than NAT Gateways.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 12,
    "text": "A company has an on-premises application that generates a large amount of time-sensitive data \nthat is backed up to Amazon S3. The application has grown and there are user complaints about \ninternet bandwidth limitations. A solutions architect needs to design a long-term solution that \nallows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for \ninternal users. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "Establish a new AWS Direct Connect connection and direct backup traffic through this new",
        "correct": true
      },
      {
        "id": 2,
        "text": "Order daily AWS Snowball devices Load the data onto the Snowball devices and return the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Submit a support ticket through the AWS Management Console. Request the removal of S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nEstablishing a new AWS Direct Connect connection provides a dedicated network connection from the on-premises environment to AWS. This dedicated connection bypasses the public internet for backup traffic, thus minimizing the impact on internet bandwidth for internal users. It also allows for faster and more reliable data transfer to S3 compared to relying solely on the internet.\n\n**Why option 0 is incorrect:**\nEstablishing AWS VPN connections and proxying all traffic through a VPC gateway endpoint would still route the backup traffic over the internet, albeit through an encrypted tunnel. While the VPC gateway endpoint provides secure access to S3, it doesn't address the core issue of internet bandwidth limitations. It might even add overhead due to encryption, potentially worsening the situation.\n\n**Why option 2 is incorrect:**\nOrdering daily AWS Snowball devices is not a long-term solution for timely backups. While Snowball can be used for initial data migration or infrequent large data transfers, it is not practical for daily backups due to the logistical overhead of shipping devices. It also doesn't provide the continuous, automated backup capability required for a long-term solution.\n\n**Why option 3 is incorrect:**\nSubmitting a support ticket to request the removal of S3 is not a valid solution. S3 is being used for backups, and removing it would eliminate the company's backup strategy. This option does not address the requirements of timely backups or minimizing impact on internet bandwidth; it simply removes the backup target.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company has an Amazon S3 bucket that contains critical data. The company must protect the \ndata from accidental deletion. \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable versioning on the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable MFA Delete on the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a bucket policy on the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable default encryption on the S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a lifecycle policy for the objects in the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling versioning on an S3 bucket allows you to recover from accidental deletion or overwrites. When versioning is enabled, every object modification or deletion creates a new version of the object, preserving the previous version. If an object is accidentally deleted, you can simply restore the previous version.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while MFA Delete provides an additional layer of security against deletion, it primarily protects against *malicious* deletions by requiring multi-factor authentication. It is not strictly necessary for protecting against *accidental* deletions, which are better addressed by versioning.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a bucket policy defines access control and permissions for the bucket. While a bucket policy can be used to restrict who can delete objects, it doesn't prevent accidental deletion by authorized users. It's more about access control than data recovery.\n\n**Why option 3 is incorrect:**\nThis is incorrect because default encryption protects data at rest by encrypting objects stored in the S3 bucket. While important for data security, it does not prevent accidental deletion. Encryption addresses data confidentiality, not data availability or recoverability.\n\n**Why option 4 is incorrect:**\nThis is incorrect because a lifecycle policy automates the transition of objects between storage classes or their deletion after a specified period. While useful for cost optimization and data management, it could potentially *cause* accidental deletion if configured incorrectly. It doesn't protect against accidental deletion; it manages the lifecycle of objects, including eventual deletion.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company has a data ingestion workflow that consists the following: \n \n- An Amazon Simple Notification Service (Amazon SNS) topic for \nnotifications about new data deliveries. \n- An AWS Lambda function to process the data and record metadata \n \nThe company observes that the ingestion workflow fails occasionally because of network \nconnectivity issues. When such a failure occurs, the Lambda function does not ingest the \ncorresponding data unless the company manually reruns the job.  \nWhich combination of actions should a solutions architect take to ensure that the Lambda \nfunction ingests all data in the future? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the Lambda function In multiple Availability Zones.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe It to me SNS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the CPU and memory that are allocated to the Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase provisioned throughput for the Lambda function.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by introducing a queue (SQS) between the SNS topic and the Lambda function. SQS acts as a buffer, storing messages published to the SNS topic. If the Lambda function fails to process a message due to network issues, the message remains in the SQS queue until the Lambda function successfully retrieves and processes it. This ensures that no data is lost, providing a reliable and fault-tolerant ingestion workflow.\n\n**Why option 0 is incorrect:**\nConfiguring the Lambda function in multiple Availability Zones (AZs) improves the availability of the Lambda function itself, protecting against AZ-level failures. However, it does not address the issue of message loss due to network connectivity problems between SNS and Lambda. If a network issue occurs before the Lambda function is invoked, the message is still lost, regardless of how many AZs the Lambda function is deployed in.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nIncreasing the CPU and memory allocated to the Lambda function might improve its performance and reduce the likelihood of timeouts, but it does not address the fundamental problem of message loss due to network connectivity issues. Even with increased resources, a network outage can still prevent the Lambda function from receiving the message in the first place.\n\n**Why option 4 is incorrect:**\nIncreasing provisioned throughput for the Lambda function is not a valid concept. Lambda functions scale automatically based on demand. Provisioned concurrency is a different feature that pre-initializes Lambda function instances to reduce cold starts, but it doesn't address the core problem of message loss due to network issues.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company's web application is running on Amazon EC2 instances behind an Application Load \nBalancer. The company recently changed its policy, which now requires the application to be \naccessed from one specific country only. \nWhich configuration will meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the security group on the Application Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS WAF on the Application Load Balancer in a VPC.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the network ACL for the subnet that contains the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using AWS WAF (Web Application Firewall) to inspect incoming HTTP/HTTPS requests. AWS WAF can be configured with rules that filter traffic based on the geographic location (country) of the request's origin IP address. By associating the WAF with the Application Load Balancer, you can effectively block requests originating from countries other than the allowed one. The VPC association is implicit as the ALB resides within a VPC.\n\n**Why option 0 is incorrect:**\nSecurity groups act as firewalls at the instance level and operate on IP addresses and ports. While you could technically try to create a security group rule that allows only IP ranges associated with a specific country, this approach is highly impractical and unreliable. IP address ranges for countries change frequently, making it difficult to maintain an accurate and up-to-date security group rule. Furthermore, security groups don't have built-in country-based filtering capabilities.\n\n**Why option 1 is incorrect:**\nSimilar to EC2 security groups, security groups on the Application Load Balancer operate on IP addresses and ports. They lack the built-in functionality to filter traffic based on the originating country. Maintaining an accurate list of IP ranges for a country within a security group would be complex and prone to errors due to frequent changes in IP address allocations.\n\n**Why option 3 is incorrect:**\nNetwork ACLs (NACLs) operate at the subnet level and control traffic entering and exiting subnets. Like security groups, NACLs work with IP addresses and ports. While you could theoretically attempt to restrict traffic based on IP ranges associated with a specific country, this is not a practical solution. IP address ranges for countries change frequently, making it difficult to maintain an accurate and up-to-date NACL rule. Furthermore, NACLs are stateless, meaning you need to configure both inbound and outbound rules, adding to the complexity.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n112 \nA company has a multi-tier application that runs six front-end web servers in an Amazon EC2 \nAuto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A \nsolutions architect needs to modify the infrastructure to be highly available without modifying the \napplication. \n \nWhich architecture should the solutions architect choose that provides high availability?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group that uses three instances across each of two Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the Auto Scaling group to use three instances across each of two Availability Zones.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling template that can be used to quickly create more instances in another",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of high availability by distributing the EC2 instances across multiple Availability Zones. If one Availability Zone fails, the ALB will automatically route traffic to the healthy instances in the other Availability Zone, ensuring the application remains available. This approach doesn't require any application modifications.\n\n**Why option 0 is incorrect:**\nCreating Auto Scaling groups across multiple *Regions* introduces complexity and latency due to cross-region communication. While it provides disaster recovery capabilities, it's overkill for achieving high availability within a region. The question specifically asks for high availability without modifying the application, and cross-region deployments often require application-level changes for data synchronization and failover.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nChanging the ALB to a round-robin configuration doesn't address the underlying issue of single Availability Zone failure. If the single AZ where all instances reside fails, the ALB will still be unable to route traffic to any healthy instances. Round-robin is a load balancing algorithm, not a high availability solution in this context.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "Organizers for a global event want to put daily reports online as static HTML pages. The pages \nare expected to generate millions of views from users around the world. The files are stored In an \nAmazon S3 bucket. A solutions architect has been asked to design an efficient and effective \nsolution. Which action should the solutions architect take to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Generate presigned URLs for the files.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use cross-Region replication to all Regions.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the geoproximity feature of Amazon Route 53.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront with the S3 bucket as its origin.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by utilizing Amazon CloudFront, a CDN, to cache the static HTML pages in edge locations around the world. This reduces latency for users accessing the content from different geographical locations. CloudFront also provides scalability and protection against high traffic volumes, ensuring the availability of the reports. Using S3 as the origin is a standard and efficient way to serve static content through CloudFront.\n\n**Why option 0 is incorrect:**\nThis is incorrect because presigned URLs are designed for temporary access to S3 objects, typically for authenticated users. They are not suitable for serving static content to a large, unauthenticated global audience. Generating and managing presigned URLs for millions of users would be complex and inefficient.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while cross-Region replication improves availability and disaster recovery, it doesn't directly address the latency issue for global users. Users would still be accessing the S3 bucket in a specific region, potentially experiencing high latency if they are far from that region. It also increases storage costs without providing the benefits of a CDN.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while Route 53 geoproximity routing can direct users to the closest endpoint, it doesn't cache the content. Users would still be directly accessing the S3 bucket, which can lead to higher latency and increased load on the S3 bucket. CloudFront provides caching at edge locations, which is a more efficient solution for serving static content globally.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A company runs an application using Amazon ECS. The application creates resized versions of \nan original image and then makes Amazon S3 API calls to store the resized images in Amazon \nS3. \nHow can a solutions architect ensure that the application has permission to access Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a security group that allows access from Amazon ECS to Amazon S3, and update the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by leveraging the `taskRoleArn` parameter in the ECS task definition. This allows you to associate an IAM role with the ECS task. The IAM role contains the necessary permissions to access S3. When the application within the task makes S3 API calls, it automatically assumes the permissions defined in the associated IAM role. This is the recommended and most secure way to grant permissions to ECS tasks, as it avoids the need to manage credentials within the application or rely on instance roles, which would grant broader permissions than necessary.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because there is no 'S3 role' in IAM. IAM roles are assigned to ECS tasks, not directly to S3. While you could modify the ECS instance role, this is not the best practice as it grants permissions to all tasks running on that instance, violating the principle of least privilege. Also, simply updating the role won't automatically apply the changes to running tasks; they need to be relaunched to pick up the new role.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because creating an IAM user and then relaunching the EC2 instances does not directly grant the ECS task the necessary permissions. IAM users are typically used for human users or applications running outside of AWS. Hardcoding IAM user credentials within the application or storing them on the EC2 instances is a security risk and is not recommended. Furthermore, relaunching the EC2 instances does not automatically associate the IAM user with the ECS task. The task still needs a mechanism to assume the IAM user's credentials, which is not addressed by this solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A solutions architect needs to securely store a database user name and password that an \napplication uses to access an Amazon RDS DB instance. The application that accesses the \ndatabase runs on an Amazon EC2 instance. The solutions architect wants to create a secure \nparameter in AWS Systems Manager Parameter Store. \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that has read access to the Parameter Store parameter.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM policy that allows read access to the Parameter Store parameter.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM trust relationship between the DB instance and the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an IAM role attached to the EC2 instance provides the necessary permissions for the instance to access the secure parameter in Parameter Store. The role grants the EC2 instance the authority to perform actions on AWS resources, including reading the specified parameter. This approach avoids hardcoding credentials or using less secure methods, adhering to the principle of least privilege.\n\n**Why option 1 is incorrect:**\nWhile an IAM policy is necessary, it's not sufficient on its own. An IAM policy defines the permissions, but it needs to be attached to an IAM identity (user, group, or role) to be effective. Simply creating a policy without associating it with the EC2 instance doesn't grant the instance access to the Parameter Store.\n\n**Why option 2 is incorrect:**\nIAM trust relationships are used to allow one AWS account or service to assume a role in another account or service. In this scenario, the EC2 instance needs to access Parameter Store within the same account. A trust relationship is not the correct mechanism for granting access within the same account. The EC2 instance needs permission to *use* the parameter, not assume a role in Parameter Store.\n\n**Why option 3 is incorrect:**\nIAM trust relationships between the DB instance and the EC2 instance are not relevant to accessing credentials stored in Parameter Store. The DB instance doesn't need to grant the EC2 instance access to the database credentials. The EC2 instance needs access to Parameter Store to retrieve the credentials, and then it uses those credentials to connect to the DB instance. This option confuses the authentication process for accessing the database with the process of retrieving the credentials.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A company is running a batch application on Amazon EC2 instances.  \nThe application consists of a backend with multiple Amazon RDS databases.  \nThe application is causing a high number of leads on the databases.  \nA solutions architect must reduce the number of database reads while ensuring high availability. \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add Amazon RDS read replicas.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon ElasbCache for Redls.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Route 53 DNS caching.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Memcached.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by introducing an in-memory data store (Redis) to cache frequently accessed data. By caching data closer to the application, subsequent reads can be served from the cache instead of the database, significantly reducing the load on the RDS instances. Redis also supports replication and failover, contributing to high availability.\n\n**Why option 0 is incorrect:**\nWhile read replicas can help distribute the read load across multiple database instances, they don't actually reduce the total number of reads performed. The application still needs to query a database instance (either the primary or a replica) for each read request. Caching is a more effective way to reduce the number of reads reaching the database.\n\n**Why option 2 is incorrect:**\nRoute 53 DNS caching only caches DNS records, which map domain names to IP addresses. It does not cache database query results. Therefore, it will not reduce the number of reads on the RDS databases.\n\n**Why option 3 is incorrect:**\nWhile Memcached is also an in-memory caching service, Redis is generally preferred for more complex caching scenarios and offers features like persistence and more advanced data structures that can be beneficial for reducing database load and improving application performance. Also, the question mentions high availability, and Redis offers more robust high availability features compared to Memcached.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n114 \nA security team wants to limit access to specific services or actions in all of the team's AWS \naccounts. All accounts belong to a large organization in AWS Organizations. The solution must \nbe scalable and there must be a single point where permissions can be maintained.  \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Create an ACL to provide access to the services or actions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a security group to allow accounts and attach it to user groups.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create cross-account roles in each account to deny access to the services or actions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a service control policy in the root organizational unit to deny access to the services or",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using Service Control Policies (SCPs) within AWS Organizations. SCPs allow you to centrally manage permissions for all accounts within an organization or organizational unit (OU). By creating an SCP in the root OU to deny access to specific services or actions, the security team can enforce these restrictions across all accounts in the organization. SCPs are scalable and provide a single point of control for managing permissions, fulfilling the stated requirements.\n\n**Why option 0 is incorrect:**\nAccess Control Lists (ACLs) are used to control network traffic at the subnet level, not to manage access to AWS services or actions. They are not suitable for managing permissions across multiple AWS accounts within an organization.\n\n**Why option 1 is incorrect:**\nSecurity groups control inbound and outbound traffic for EC2 instances. They are not designed to manage access to AWS services or actions across multiple AWS accounts. They operate at the instance level and are not a centralized solution for managing permissions within an AWS Organization.\n\n**Why option 2 is incorrect:**\nCreating cross-account roles in each account to deny access is not scalable and would be difficult to maintain. It would require creating and managing roles in each individual account, which contradicts the requirement for a single point of permission management. SCPs are a much more efficient and centralized approach.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is concerned about the security of its public web application due to recent web \nattacks. The application uses an Application Load Balancer (ALB). A solutions architect must \nreduce the risk of DDoS attacks against the application. \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon Inspector agent to the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon Macie to prevent attacks.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable AWS Shield Advanced to prevent attacks.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon GuardDuty to monitor the ALB.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution directly addresses the requirement of preventing DDoS attacks. AWS Shield Advanced provides enhanced DDoS protection capabilities, including 24/7 access to the AWS DDoS Response Team (DRT) and custom mitigations tailored to the application. It integrates with services like ALB to provide comprehensive protection against a wide range of DDoS attack vectors. Shield Advanced offers more sophisticated detection and mitigation techniques compared to the standard AWS Shield protection that is automatically enabled.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Amazon Inspector is a vulnerability management service that assesses EC2 instances and container images for software vulnerabilities and unintended network exposure. It does not directly protect against DDoS attacks targeting an ALB. While identifying vulnerabilities is important for overall security, it doesn't address the specific requirement of DDoS mitigation.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because Amazon Macie is a data security and data privacy service that uses machine learning and pattern matching to discover and protect sensitive data in AWS. It is primarily focused on data discovery and protection, not on preventing DDoS attacks against an ALB. Macie helps identify sensitive data like personally identifiable information (PII) and intellectual property, but it doesn't have DDoS mitigation capabilities.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and unauthorized behavior. While GuardDuty can detect some anomalies that might indicate a DDoS attack, it doesn't actively prevent or mitigate them. It provides alerts and findings that can be used to investigate and respond to security threats, but it's not a DDoS protection service like AWS Shield Advanced.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A company runs a production application on a fleet of Amazon EC2 instances. The application \nreads the data from an Amazon SQS queue and processes the messages in parallel. The \nmessage volume is unpredictable and often has intermittent traffic. This application should \ncontinually process messages without any downtime. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances exclusively to handle the maximum capacity required.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances exclusively to handle the maximum capacity required.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement for continuous processing and cost optimization. Reserved Instances provide a cost-effective solution for the baseline capacity needed to handle the consistent message volume. On-Demand Instances can then be used to handle the unpredictable and intermittent traffic spikes. This approach ensures that the application can scale up quickly to meet demand without the risk of interruption associated with Spot Instances, while also minimizing costs by using Reserved Instances for the predictable baseline load.\n\n**Why option 0 is incorrect:**\nUsing Spot Instances exclusively is not a reliable solution for an application that requires continuous processing without downtime. Spot Instances can be terminated with short notice if the Spot price exceeds the bid price, leading to interruptions in message processing and potential data loss. While Spot Instances are cost-effective when available, they are not suitable for applications with strict availability requirements.\n\n**Why option 1 is incorrect:**\nUsing Reserved Instances exclusively to handle the maximum capacity required would be very expensive. Reserved Instances are a good choice for consistent, predictable workloads, but purchasing enough Reserved Instances to handle the peak load, which only occurs intermittently, would result in significant wasted resources and unnecessary costs during periods of low traffic. This option does not optimize for cost-effectiveness.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 \norigin to store a static website. The companys security policy requires that all website traffic be \ninspected by AWS WAF. \nHow should the solutions architect comply with these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 bucket policy lo accept requests coming from the AWS WAF Amazon Resource",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using an Origin Access Identity (OAI). An OAI is a CloudFront user that can be granted permission to read objects in your S3 bucket. By configuring CloudFront to use an OAI and then granting the OAI read permissions on the S3 bucket, you ensure that only CloudFront can access the S3 bucket. To integrate WAF, you associate the WAF web ACL with the CloudFront distribution. This ensures that all traffic passing through CloudFront is inspected by WAF before being served to the user or forwarded to the S3 origin. This approach satisfies both the security policy (WAF inspection) and controlled access to the S3 bucket.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while an S3 bucket policy can restrict access, it cannot directly enforce WAF inspection. WAF operates at the CloudFront distribution level, not directly on the S3 bucket. An S3 bucket policy alone cannot guarantee that all traffic is inspected by WAF before reaching the bucket. Furthermore, configuring an S3 bucket policy to accept requests from AWS WAF ARNs is not the standard way to integrate WAF with S3 via CloudFront. WAF is associated with the CloudFront distribution, and the OAI controls access to the bucket.\n\n**Why option 1 is incorrect:**\nThis is incorrect because CloudFront doesn't directly forward requests to AWS WAF. AWS WAF is associated with the CloudFront distribution. When a request comes to CloudFront, WAF inspects the request based on the rules defined in the Web ACL. If the request passes the WAF rules, CloudFront proceeds to fetch the content from the origin (S3 in this case). There's no explicit forwarding mechanism. This option misrepresents how WAF integrates with CloudFront.\n\n**Why option 2 is incorrect:**\nThis is incorrect because relying solely on security groups to restrict access to the S3 bucket is not the recommended approach when using CloudFront. Security groups are typically used to control access to EC2 instances. While you could theoretically find the CloudFront IP ranges and allow them in a security group associated with an EC2 instance acting as a proxy, this is complex, difficult to maintain (CloudFront IP ranges can change), and not applicable to an S3 bucket. The correct approach is to use an Origin Access Identity (OAI) for S3 bucket access control.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company wants to manage Amazon Machine Images (AMIs). The company currently copies \nAMIs to the same AWS Region where the AMIs were created. The company needs to design an \napplication that captures AWS API calls and sends alerts whenever the Amazon EC2 \nCreatelmage API operation is called within the company's account. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution directly addresses the requirement by leveraging Amazon EventBridge (formerly CloudWatch Events). EventBridge can be configured to listen for specific API calls, such as `CreateImage`, and trigger actions when those calls are made. This approach avoids the need for custom polling or log analysis, minimizing operational overhead. EventBridge is designed for event-driven architectures and integrates seamlessly with other AWS services for alerting (e.g., SNS, Lambda). It provides a managed and scalable solution for capturing and reacting to API events.\n\n**Why option 0 is incorrect:**\nThis solution is incorrect because it involves querying CloudTrail logs using a Lambda function. While this approach can achieve the desired outcome, it introduces significant operational overhead. The Lambda function needs to be developed, deployed, and maintained. It also requires configuring appropriate IAM permissions and handling potential scaling issues. Furthermore, polling CloudTrail logs adds latency and complexity compared to a native event-driven solution. The need to query logs periodically makes it less efficient and adds unnecessary overhead.\n\n**Why option 1 is incorrect:**\nThis solution is incorrect because while CloudTrail can be configured to send logs to an SNS topic, it doesn't directly trigger an alert based on a specific API call like `CreateImage`. The SNS topic would receive all CloudTrail events, requiring a separate service (like Lambda) to filter and process the events to identify the `CreateImage` calls and then send the alert. This adds complexity and operational overhead compared to using EventBridge, which can directly target specific API calls. The SNS topic alone is insufficient to meet the requirements without additional processing.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "An online retail company has more than 50 million active customers and receives more than \n25,000 orders each day. The company collects purchase data for customers and stores this data \nin Amazon S3. Additional customer data is stored in Amazon RDS. The company wants to make \nall the data available to various teams so that the teams can perform analytics. The solution must \nprovide the ability to manage fine-grained permissions for the data and must minimize operational \noverhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the purchase data to write directly to Amazon RDS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a data lake by using AWS Lake Formation.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Redshift cluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by creating a data lake using AWS Lake Formation. Lake Formation simplifies the process of building, securing, and managing data lakes. It allows you to define data catalogs, transform data, and enforce fine-grained access control policies across various data sources, including S3 and RDS. Lake Formation integrates with AWS Glue for ETL operations and provides a centralized location for managing permissions, thus minimizing operational overhead and enabling secure data access for analytics teams.\n\n**Why option 0 is incorrect:**\nThis is incorrect because migrating purchase data directly to Amazon RDS would likely overwhelm the database, especially given the high volume of daily orders. RDS is not designed for storing large volumes of unstructured or semi-structured data like purchase history. It also doesn't address the need for a centralized analytics platform or fine-grained permissions across both purchase and customer data.\n\n**Why option 1 is incorrect:**\nThis is incorrect because periodically copying data from Amazon RDS to Amazon S3 using a Lambda function is a rudimentary approach that doesn't provide the necessary data governance, security, and scalability for a data lake. It would require significant manual effort to manage the data transfer, schema evolution, and access control. This approach also lacks the centralized metadata management and fine-grained permissions that Lake Formation provides.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company owns an asynchronous API that is used to ingest user requests and, based on the \nrequest type, dispatch requests to the appropriate microservice for processing. The company is \nusing Amazon API Gateway to deploy the API front end, and an AWS Lambda function that \ninvokes Amazon DynamoDB to store user requests before dispatching them to the processing \nmicroservices. The company provisioned as much DynamoDB throughput as its budget allows, \nbut the company is still experiencing availability issues and is losing user requests. \nWhat should a solutions architect do to address this issue without impacting existing users?",
    "options": [
      {
        "id": 0,
        "text": "Add throttling on the API Gateway with server-side throttling limits.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a secondary index in DynamoDB for the table with the user requests.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the problem by decoupling the Lambda function from DynamoDB using an SQS queue. The Lambda function can quickly enqueue the user requests into the SQS queue, and then a separate process (e.g., another Lambda function or a consumer application) can consume messages from the queue and write them to DynamoDB at a rate that DynamoDB can handle. This buffering mechanism prevents the Lambda function from being throttled by DynamoDB and ensures that user requests are not lost. SQS provides durability and guarantees message delivery, making it a reliable solution for buffering writes.\n\n**Why option 0 is incorrect:**\nAdding throttling on API Gateway, while helpful for preventing abuse or overwhelming the backend, does not address the root cause of the problem, which is DynamoDB throughput limitations. Throttling at the API Gateway level would simply reject requests, leading to the same issue of losing user requests. The problem is within the DynamoDB write capacity.\n\n**Why option 1 is incorrect:**\nWhile DAX can improve read performance for DynamoDB, it does not directly address the write throughput limitations that are causing the data loss. DAX is a cache, and writes still need to be processed by DynamoDB. Lambda buffering writes to DynamoDB using DAX is not a valid use case. DAX is designed for read-heavy workloads, not write-heavy workloads where DynamoDB is being overwhelmed.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The \ncompany must ensure that no API calls and no data are routed through public internet routes. \nOnly the EC2 instance can have access to upload data to the S3 bucket. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the AWS provided, publicly available ip-ranges.json tile to obtain the private IP address of the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an interface VPC endpoint for S3 creates a private connection between the VPC and S3 using AWS PrivateLink. This allows the EC2 instance to communicate with S3 without traversing the public internet. The EC2 instance can access S3 using its private IP address within the VPC, ensuring secure and private data transfer. Interface endpoints provide granular control over access using security groups and endpoint policies, fulfilling the requirement that only the EC2 instance can access S3.\n\n**Why option 1 is incorrect:**\nThis is incorrect because a gateway VPC endpoint for S3 only supports requests originating from within the VPC. While it does provide a private connection to S3, it only supports GET and PUT requests. It does not support all S3 API calls. Also, gateway endpoints use route tables to direct traffic to S3, which might not be as granular as security groups and endpoint policies for controlling access from a specific EC2 instance.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while obtaining the private IP address of S3 might seem like a way to bypass public internet, it's not a supported or secure method. S3 is a managed service, and its IP addresses are dynamic and subject to change. Directly using an S3 IP address is not a reliable or recommended approach for accessing S3 securely or privately. Furthermore, simply knowing the IP address doesn't establish a private connection; traffic would still route over the public internet unless a VPC endpoint is configured.\n\n**Why option 3 is incorrect:**\nThis is incorrect because the `ip-ranges.json` file contains public IP address ranges for AWS services, not private IP addresses. Using these public IP addresses would defeat the purpose of avoiding public internet routes. Even if it contained private IP addresses, directly using them is not a supported or secure method for accessing S3, as explained in the previous option.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A gaming company hosts a browser-based application on AWS. The users of the application \nconsume a large number of videos and images that are stored in Amazon S3. This content is the \nsame for all users. \nThe application has increased in popularity, and millions of users worldwide accessing these \nmedia files. The company wants to provide the files to the users while reducing the load on the \norigin. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Global Accelerator accelerator in front of the web servers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon CloudFront web distribution in front of the S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy an Amazon ElastiCache for Redis instance in front of the web servers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon ElastiCache for Memcached instance in front of the web servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by caching the content closer to the users, reducing latency and improving performance. CloudFront is a content delivery network (CDN) specifically designed for this purpose. It caches content at edge locations worldwide, so when a user requests a video or image, CloudFront serves it from the nearest edge location, minimizing the distance the data has to travel. This significantly reduces the load on the S3 bucket, as CloudFront handles most of the requests. CloudFront is also designed to be cost-effective for serving static content at scale.\n\n**Why option 0 is incorrect:**\nWhile AWS Global Accelerator can improve the performance of applications by routing traffic to the optimal endpoint, it's primarily designed for dynamic content and improving the performance of TCP and UDP traffic. It doesn't cache content like a CDN, so it wouldn't significantly reduce the load on the S3 bucket for static content delivery. It's also generally more expensive than CloudFront for this specific use case.\n\n**Why option 2 is incorrect:**\nAmazon ElastiCache is an in-memory data store service that is primarily used for caching frequently accessed data from databases or other data sources. It's not designed for serving static content directly to users. While it could be used to cache metadata about the videos and images, it wouldn't reduce the load on the S3 bucket for the actual content delivery. Also, placing ElastiCache in front of web servers doesn't directly address the problem of serving static content from S3 efficiently to a global user base.\n\n**Why option 3 is incorrect:**\nAmazon ElastiCache for Memcached is similar to Redis but is simpler and generally used for caching smaller data objects. Like Redis, it's not designed for serving static content directly to users and wouldn't significantly reduce the load on the S3 bucket for content delivery. It's more suited for caching database query results or session data.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 30,
    "text": "A company has two applications: a sender application that sends messages with payloads to be \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n118 \nprocessed and a processing application intended to receive the messages with payloads. The \ncompany wants to implement an AWS service to handle messages between the two applications. \nThe sender application can send about 1.000 messages each hour. The messages may take up \nto 2 days to be processed. If the messages fail to process, they must be retained so that they do \nnot impact the processing of any remaining messages. \nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon EC2 instance running a Redis database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Kinesis data stream to receive the messages from the sender application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by providing a fully managed queueing service. Amazon SQS allows for asynchronous communication, message durability, and the ability to handle processing failures through features like dead-letter queues (DLQs). If a message fails to process after a certain number of retries, it can be moved to a DLQ for further investigation without blocking the processing of other messages. SQS is also highly scalable and requires minimal operational overhead, making it operationally efficient.\n\n**Why option 0 is incorrect:**\nSetting up an EC2 instance running Redis would require significant operational overhead for managing the instance, ensuring its availability, and scaling it as needed. Redis is an in-memory data store, which means message durability would be a concern. While Redis can be configured for persistence, it adds complexity and doesn't inherently provide the dead-letter queue functionality needed to isolate failed messages. This option is not operationally efficient compared to managed queueing services.\n\n**Why option 1 is incorrect:**\nAmazon Kinesis Data Streams is designed for real-time streaming data, typically at much higher volumes than 1000 messages per hour. While it can be used for message queuing, it's not the most appropriate service for this scenario. Kinesis Streams requires more configuration and management than SQS and doesn't natively provide features like dead-letter queues for handling processing failures. It's also more expensive for this relatively low message volume. Kinesis is better suited for high-throughput data ingestion and processing, not simple asynchronous message queuing.\n\n**Why option 3 is incorrect:**\nAmazon SNS is a publish/subscribe messaging service primarily used for broadcasting messages to multiple subscribers. It doesn't inherently provide message queuing or durability. While SNS can be combined with SQS to achieve a similar outcome, using SNS directly for this scenario would not meet the requirement of retaining messages that fail to process without impacting other messages. SNS delivers messages to all subscribers, and if a subscriber fails to process a message, it's typically lost. It also lacks built-in dead-letter queue functionality.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company has an AWS account used for software engineering. The AWS account has access to \nthe company's on-premises data center through a pair of AWS Direct Connect connections. All \nnon-VPC traffic routes to the virtual private gateway. \nA development team recently created an AWS Lambda function through the console. \nThe development team needs to allow the function to access a database that runs in a private \nsubnet in the company's data center. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Lambda function to run in the VPC with the appropriate security group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the route tables in the VPC to allow the Lambda function to access the on-premises data",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by configuring the VPC route tables to direct traffic from the Lambda function's subnet to the virtual private gateway (VGW) associated with the Direct Connect connection. Since all non-VPC traffic already routes to the VGW, adding a route in the VPC route table pointing to the VGW for the on-premises network (or a specific subnet containing the database) will allow the Lambda function to reach the database. The Lambda function needs to be configured to run within the VPC to leverage this routing.\n\n**Why option 0 is incorrect:**\nWhile configuring the Lambda function to run within the VPC is necessary for it to access resources within the VPC or on-premises via Direct Connect, simply running it in the VPC with a security group is insufficient. The security group controls inbound and outbound traffic, but it doesn't define the routing path. The Lambda function needs a route to reach the on-premises network.\n\n**Why option 1 is incorrect:**\nSetting up a VPN connection is redundant and unnecessary. The company already has a Direct Connect connection, which provides a dedicated, private network connection to the on-premises data center. Establishing a VPN connection would add unnecessary complexity and cost, and would not be the most efficient solution. The existing Direct Connect connection should be leveraged.\n\n**Why option 3 is incorrect:**\nCreating an Elastic IP address and attempting to route Lambda traffic through it is not a viable solution. Lambda functions do not have static IP addresses, and you cannot directly associate an Elastic IP address with a Lambda function. Lambda functions are designed to be ephemeral and scale automatically, making static IP assignment impractical. Furthermore, this approach wouldn't utilize the existing Direct Connect connection.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is \nprocessed sequentially, but the order of results does not matter. The application uses a \nmonolithic architecture. The only way that the company can scale the application to meet \nincreased demand is to increase the size of the instances. \nThe company's developers have decided to rewrite the application to use a microservices \narchitecture on Amazon Elastic Container Service (Amazon ECS). \nWhat should a solutions architect recommend for communication between the microservices?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function to pass messages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon DynamoDB table.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Given the requirement that the order of results does not matter, SQS provides a reliable and scalable way for microservices to communicate asynchronously. Each microservice can enqueue messages to SQS, and other microservices can dequeue and process them independently. This decoupling improves resilience and allows for independent scaling of the microservices.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon SNS is a publish/subscribe messaging service. While it can be used for communication between microservices, it's best suited for scenarios where multiple subscribers need to receive the same message. In this case, the requirement is for sequential processing of data, implying a one-to-one or one-to-few relationship between microservices processing specific data units. SNS is not designed for queuing and ensuring that each message is processed by a specific service. It broadcasts messages to all subscribers.\n\n**Why option 2 is incorrect:**\nThis is incorrect because AWS Lambda is a compute service that lets you run code without provisioning or managing servers. While Lambda functions can be triggered by various events, including messages from SQS or SNS, using Lambda as a direct message passing mechanism between microservices is not a common or efficient pattern. It would require creating a Lambda function for each communication path, adding unnecessary complexity and overhead. SQS is a more direct and suitable solution for asynchronous messaging.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon DynamoDB is a NoSQL database service. While microservices might use DynamoDB for data storage and retrieval, it's not designed for direct communication between services. Using DynamoDB as a message queue would be inefficient and complex, requiring constant polling and custom logic to manage message delivery and processing. SQS is a purpose-built service for message queuing and is a much better fit for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A hospital wants to create digital copies for its large collection of historical written records. The \nhospital will continue to add hundreds of new documents each day. The hospital's data team will \nscan the documents and will upload the documents to the AWS Cloud. A solutions architect must \nimplement a solution to analyze the documents, extract the medical information, and store the \ndocuments so that an application can run SQL queries on the data. The solution must maximize \nscalability and operational efficiency. \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Write the document information to an Amazon EC2 instance that runs a MySQL database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Write the document information to an Amazon S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group of Amazon EC2 instances to run a custom application that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that runs when new documents are uploaded.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an AWS Lambda function that runs when new documents are uploaded.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nStoring the documents in an Amazon S3 bucket is a highly scalable and cost-effective solution for storing large amounts of unstructured data. S3 provides virtually unlimited storage and integrates well with other AWS services for processing and analysis. This addresses the need for a scalable storage solution for the scanned documents.\n\n**Why option 0 is incorrect:**\nUsing an EC2 instance with a MySQL database for storing the document information is not ideal for scalability and operational efficiency. Managing a MySQL database on EC2 requires manual configuration, patching, and scaling, which adds operational overhead. It's also not as scalable as other AWS services designed for large-scale data storage.\n\n**Why option 2 is incorrect:**\nCreating an Auto Scaling group of EC2 instances to run a custom application is a viable option for processing the documents, but it doesn't directly address the initial storage of the documents. While it can be part of the overall solution, it's not a necessary first step and doesn't inherently maximize operational efficiency compared to serverless alternatives. The question asks for a *combination* of steps, and this option alone doesn't fulfill all the requirements.\n\n**Why option 3 is incorrect:**\nCreating an AWS Lambda function that runs when new documents are uploaded is a good approach for triggering the document analysis process. However, it does not address where the documents are stored. It needs to be paired with a storage solution like S3.\n\n**Why option 4 is incorrect:**\nCreating an AWS Lambda function that runs when new documents are uploaded is a good approach for triggering the document analysis process. However, it does not address where the documents are stored. It needs to be paired with a storage solution like S3.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A solutions architect is optimizing a website for an upcoming musical event. Videos of the \nperformances will be streamed in real time and then will be available on demand. The event is \nexpected to attract a global online audience. \nWhich service will improve the performance of both the real-lime and on-demand streaming?",
    "options": [
      {
        "id": 0,
        "text": "Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Route 53",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 Transfer Acceleration",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon CloudFront is a content delivery network (CDN) that caches content at edge locations around the world. By caching the video streams (both live and on-demand) closer to the users, CloudFront reduces latency and improves the viewing experience. It also provides protection against DDoS attacks and can handle high traffic volumes, making it suitable for a global audience. CloudFront integrates well with other AWS services like S3 (for VOD) and Media Services (for live streaming).\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Global Accelerator improves the performance of TCP and UDP traffic by routing it through AWS's global network infrastructure. While it can improve performance, it's primarily designed for applications that are sensitive to packet loss, jitter, and latency over the public internet. It's less suited for caching video content like CloudFront and doesn't directly address the need for caching content closer to the end users for both live and on-demand streaming.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon Route 53 is a highly available and scalable DNS web service. While Route 53 is essential for directing users to the website, it doesn't directly improve the performance of video streaming. It resolves domain names to IP addresses but doesn't cache content or reduce latency in the same way as a CDN. Using Route 53 with latency-based routing could help direct users to the closest CloudFront edge location, but CloudFront itself is the primary service for improving streaming performance.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon S3 Transfer Acceleration uses CloudFront's edge locations to accelerate uploads to S3. While it can improve the speed of uploading video files to S3, it doesn't improve the performance of streaming those videos to end users. The question specifically asks about improving the performance of both real-time and on-demand streaming, which requires a CDN like CloudFront.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 35,
    "text": "A company wants to migrate its MySQL database from on premises to AWS. The company \nrecently experienced a database outage that significantly impacted the business. To ensure this \ndoes not happen again, the company wants a reliable database solution on AWS that minimizes \ndata loss and stores every transaction on at least two nodes. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon RDS DB instance with synchronous replication to three nodes in three",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirements by providing a managed database service (RDS) with Multi-AZ functionality. Multi-AZ deployments in RDS for MySQL provide synchronous replication to a standby instance in a different Availability Zone. This ensures that every transaction is written to at least two nodes (the primary and standby), minimizing data loss in case of a primary instance failure. The standby instance automatically takes over if the primary fails, providing high availability.\n\n**Why option 0 is incorrect:**\nWhile creating an RDS instance with replication to three nodes might seem like a good idea for redundancy, RDS MySQL does not natively support synchronous replication to three nodes. Multi-AZ provides synchronous replication to a single standby. Attempting to configure synchronous replication to three nodes would likely involve a more complex and potentially less reliable custom solution.\n\n**Why option 2 is incorrect:**\nCreating a read replica in a separate AWS Region provides asynchronous replication. While read replicas are useful for offloading read traffic and disaster recovery, they do not guarantee minimal data loss in the event of a primary instance failure. Data loss can occur because the replication is asynchronous, meaning that transactions committed to the primary instance might not yet be replicated to the read replica at the time of failure. The question specifically requires minimizing data loss and storing every transaction on at least two nodes, which asynchronous replication does not guarantee.\n\n**Why option 3 is incorrect:**\nCreating a MySQL instance on EC2 and implementing a custom replication solution using Lambda functions adds significant operational overhead and complexity. While it's possible to achieve high availability with this approach, it requires manual configuration, monitoring, and management of the replication process. This solution is less reliable and more prone to errors compared to using a managed service like RDS with Multi-AZ. Furthermore, the question emphasizes reliability and minimizing data loss, which are better addressed by a managed service with built-in synchronous replication.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 36,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n121 \nAn ecommerce company hosts its analytics application in the AWS Cloud. The application \ngenerates about 300 MB of data each month. The data is stored in JSON format. The company is \nevaluating a disaster recovery solution to back up the data. The data must be accessible in \nmilliseconds if it is needed, and the data must be kept for 30 days. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon S3 Glacier",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon RDS for PostgreSQL",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon S3 Standard provides low latency access (milliseconds) and is designed for frequently accessed data. Storing 300MB of data for 30 days in S3 Standard is relatively inexpensive. It directly addresses the requirements of millisecond access and 30-day retention in a cost-effective manner. S3 also offers built-in versioning and replication options for disaster recovery, further enhancing its suitability.\n\n**Why option 0 is incorrect:**\nAmazon OpenSearch Service is designed for searching and analyzing large volumes of data. While it offers millisecond access, it's significantly more expensive than S3 for simply storing and retrieving 300MB of data. It's overkill for this scenario and not cost-effective.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier is designed for long-term archival storage and has retrieval times ranging from minutes to hours, which does not meet the millisecond access requirement. While it's very cost-effective for archival, it's unsuitable for this use case due to the slow retrieval times.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has a Windows-based application that must be migrated to AWS. The application \nrequires the use of a shared Windows file system attached to multiple Amazon EC2 Windows \ninstances that are deployed across multiple Availability Zones. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Storage Gateway in volume gateway mode.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon FSx for Windows File Server.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a file system by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing a fully managed Windows file server in AWS. Amazon FSx for Windows File Server is built on Windows Server and provides native support for the SMB protocol, Active Directory integration, and Windows NTFS file system features. It can be accessed by multiple EC2 Windows instances across multiple Availability Zones, making it suitable for shared file system requirements in a Windows environment.\n\n**Why option 0 is incorrect:**\nAWS Storage Gateway in volume gateway mode provides block-based storage, not a file system. It is used for hybrid cloud scenarios, caching data locally, and replicating it to AWS. It doesn't provide a shared file system accessible by multiple EC2 instances across multiple Availability Zones.\n\n**Why option 2 is incorrect:**\nAmazon EFS is a network file system designed for Linux-based instances. It does not natively support the SMB protocol or Windows NTFS features required for Windows-based applications. While it can be used with Windows instances using third-party solutions, it's not the ideal solution for a native Windows file system requirement.\n\n**Why option 3 is incorrect:**\nAmazon EBS is a block storage service that provides persistent storage volumes for EC2 instances. An EBS volume can only be attached to a single EC2 instance at a time. It cannot be shared between multiple instances concurrently, making it unsuitable for the shared file system requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of \nthe information submitted by users is sensitive. The application uses HTTPS but needs another \nlayer of security. The sensitive information should be protected throughout the entire application \nstack, and access to the information should be restricted to certain applications. \nWhich action should the solutions architect take?",
    "options": [
      {
        "id": 0,
        "text": "Configure a CloudFront signed URL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a CloudFront signed cookie.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a CloudFront field-level encryption profile.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because CloudFront field-level encryption allows you to encrypt specific data fields in POST requests at the edge (CloudFront) before they are sent to your origin. This ensures that sensitive data is protected throughout the entire application stack, as only the intended application with the correct decryption key can access the data in its original form. This addresses the requirements of adding another layer of security and restricting access to specific applications.\n\n**Why option 0 is incorrect:**\nSigned URLs are used to control access to content stored in CloudFront or S3 for a limited time or to specific users. They do not encrypt the data itself, and therefore do not meet the requirement of protecting sensitive information throughout the entire application stack. They primarily address content access control, not data encryption.\n\n**Why option 1 is incorrect:**\nSigned cookies are similar to signed URLs in that they control access to content. They allow you to control access to multiple restricted files, such as all of the files in a subscribers' area of a website. Like signed URLs, they do not encrypt the data itself and do not protect sensitive information throughout the entire application stack. They are for controlling access to content, not encrypting data.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted \nwhen it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated \nevery year. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Move the data to the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the data with customer key material before moving the data to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements because AWS KMS customer managed keys can be configured to automatically rotate keys annually. S3 can then use this KMS key for server-side encryption (SSE-KMS). This provides encryption at rest and automatic key rotation without requiring manual intervention, thus minimizing operational overhead.\n\n**Why option 0 is incorrect:**\nThis option fails to address the requirement of encrypting the data at rest in the S3 bucket. Simply moving the data without encryption leaves it vulnerable to unauthorized access.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "An application runs on Amazon EC2 instances in private subnets. The application needs to \naccess an Amazon DynamoDB table. What is the MOST secure way to access the table while \nensuring that the traffic does not leave the AWS network?",
    "options": [
      {
        "id": 0,
        "text": "Use a VPC endpoint for DynamoDB.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use a NAT gateway in a public subnet.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a NAT instance in a private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the internet gateway attached to the VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a VPC endpoint for DynamoDB creates a direct, private connection between the VPC and DynamoDB. This eliminates the need for internet access, ensuring that all traffic remains within the AWS network. It also avoids the need for NAT gateways or instances, which would require routing traffic through public subnets and potentially exposing it to the internet.\n\n**Why option 1 is incorrect:**\nThis is incorrect because a NAT gateway, while allowing instances in private subnets to initiate outbound connections to the internet, requires the instances to send traffic to a public subnet. This exposes the traffic to the internet and introduces a potential security risk, contradicting the requirement for the most secure solution.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a NAT instance, similar to a NAT gateway, requires traffic to be routed through a public subnet to access the internet. This introduces a security risk and does not keep the traffic within the AWS network. Furthermore, NAT instances require more management overhead than NAT gateways or VPC endpoints.\n\n**Why option 3 is incorrect:**\nThis is incorrect because using an internet gateway would require the EC2 instances to have public IP addresses or use a NAT gateway/instance to access the internet. This directly contradicts the requirement to keep the traffic within the AWS network and introduces significant security risks.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company provides an API to its users that automates inquiries for tax computations based on \nitem prices. The company experiences a larger number of inquiries during the holiday season \nonly that cause slower response times. A solutions architect needs to design a solution that is \nscalable and elastic. \nWhat should the solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Provide an API hosted on an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Design a REST API using Amazon API Gateway that accepts the item names.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Application Load Balancer that has two Amazon EC2 instances behind it.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement for scalability and elasticity by leveraging Amazon API Gateway. API Gateway can automatically scale to handle a large number of API requests without requiring manual intervention. It also provides features like caching, throttling, and security, which are beneficial for managing API traffic and protecting the backend services. By using API Gateway, the company can ensure that the API remains responsive even during peak seasons, while also optimizing costs during off-peak seasons by scaling down resources automatically.\n\n**Why option 0 is incorrect:**\nHosting the API on a single Amazon EC2 instance does not provide the required scalability and elasticity. A single EC2 instance has limited capacity and cannot automatically scale up or down based on demand. This would likely lead to performance issues during peak seasons and underutilization of resources during off-peak seasons.\n\n**Why option 2 is incorrect:**\nWhile an Application Load Balancer (ALB) with two EC2 instances provides some level of scalability and high availability, it doesn't offer the same level of automatic scaling and management as API Gateway. The ALB distributes traffic across the EC2 instances, but it requires manual configuration and scaling of the EC2 instances. It also lacks features like caching, throttling, and security that API Gateway provides.\n\n**Why option 3 is incorrect:**\nThis option is incomplete. While using API Gateway is a good start, it doesn't specify what the API Gateway connects to. Simply stating it connects to an API hosted on Amazon is too vague and doesn't guarantee scalability or elasticity. The backend API needs to be scalable as well, and this option doesn't provide enough detail on how that is achieved. It's also less cost-effective than directly integrating API Gateway with a serverless backend like Lambda.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company wants to use high performance computing (HPC) infrastructure on AWS for financial \nrisk modeling. The company's HPC workloads run on Linux. Each HPC workflow runs on \nhundreds of AmazonEC2 Spot Instances, is short-lived, and generates thousands of output files \nthat are ultimately stored in persistent storage for analytics and long-term future use. \nThe company seeks a cloud storage solution that permits the copying of on premises data to \nlong-term persistent storage to make data available for processing by all EC2 instances. The \nsolution should also be a high performance file system that is integrated with persistent storage to \nread and write datasets and output files. \nWhich combination of AWS services meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Lustre integrated with Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon FSx for Windows File Server integrated with Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because Amazon FSx for Lustre is a high-performance, scalable file system optimized for compute-intensive workloads like HPC. It integrates directly with Amazon S3, allowing data to be easily imported from and exported to S3 for persistent storage and long-term analytics. This meets the requirement of a high-performance file system integrated with persistent storage. FSx for Lustre supports Linux-based workloads, aligning with the company's environment. The integration with S3 also allows for easy copying of on-premises data to S3, which can then be accessed by FSx for Lustre.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon FSx for Windows File Server is designed for Windows-based workloads and is not suitable for the company's Linux-based HPC environment. While it can integrate with S3, it's not the optimal choice for Linux HPC workloads.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon S3 Glacier is an archive storage service optimized for infrequent access and low storage costs. It is not a high-performance file system and is not suitable for the read/write intensive operations required by HPC workloads. Amazon EBS is block storage, not a file system, and doesn't directly integrate with Glacier in a way that would support HPC workloads.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon S3 is object storage, not a file system, and while it's suitable for persistent storage, it doesn't provide the high-performance file system capabilities needed for HPC workloads. Amazon EBS is block storage and is not directly integrated with S3 in a manner that provides a high-performance file system interface for HPC. A VPC endpoint simply provides private connectivity to S3 and does not address the file system requirements.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 43,
    "text": "A company is running a publicly accessible serverless application that uses Amazon API \nGateway and AWS Lambda.  \nThe application's traffic recently spiked due to fraudulent requests from botnets. \nWhich steps should a solutions architect take to block requests from unauthorized users? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n124",
    "options": [
      {
        "id": 0,
        "text": "Create a usage plan with an API key that is shared with genuine users only.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Convert the existing public API to a private API.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an IAM role for each user attempting to access the API.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because usage plans with API keys allow you to control access to your API Gateway APIs. By distributing API keys only to genuine users, you can effectively block requests that do not include a valid API key. This provides a layer of authentication and authorization, preventing unauthorized access from botnets.\n\n**Why option 1 is incorrect:**\nIntegrating logic within the Lambda function to filter requests based on IP addresses is inefficient and not scalable. Lambda functions should focus on business logic, not security filtering. Maintaining a list of fraudulent IP addresses within the Lambda function would be complex and resource-intensive. Furthermore, botnets often use rotating IP addresses, making this approach ineffective in the long run.\n\n**Why option 2 is incorrect:**\nWhile AWS WAF is a valid solution for filtering malicious requests, the question asks for *two* solutions. Option 0 is a more immediate and simpler solution to implement for blocking unauthorized access. AWS WAF would require more configuration and analysis of the traffic patterns to identify and block the botnet traffic effectively. It is a good complementary solution, but not the primary one.\n\n**Why option 3 is incorrect:**\nConverting the public API to a private API would restrict access to only those within a VPC or through specific VPC endpoints, which is not the requirement. The application needs to be publicly accessible to genuine users, so making it private would defeat the purpose.\n\n**Why option 4 is incorrect:**\nCreating an IAM role for each user is not practical for a publicly accessible application. IAM roles are designed for AWS services and users within your AWS account, not for external users accessing a public API. Managing individual IAM roles for each user would be overly complex and not suitable for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A solutions architect is designing the architecture of a new application being deployed to the AWS \nCloud. The application will run on Amazon EC2 On-Demand Instances and will automatically \nscale across multiple Availability Zones. The EC2 instances will scale up and down frequently \nthroughout the day. An Application Load Balancer (ALB) will handle the load distribution. The \narchitecture needs to support distributed session data management. The company is willing to \nmake changes to code if needed. \nWhat should the solutions architect do to ensure that the architecture supports distributed session \ndata management?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache to manage and store session data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use session affinity (sticky sessions) of the ALB to manage session data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Session Manager from AWS Systems Manager to manage the session.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct approach because ElastiCache provides a centralized, highly available, and scalable in-memory data store suitable for managing session data. As EC2 instances scale up and down, they can all access and update session data in ElastiCache, ensuring session persistence and consistency. The application code would need to be modified to read and write session data to ElastiCache instead of relying on local storage.\n\n**Why option 1 is incorrect:**\nWhile sticky sessions (session affinity) can maintain user sessions with a specific EC2 instance, they are not a reliable solution for a highly scalable environment where instances are frequently added or removed. If an instance fails or is terminated, the user's session data is lost, and they may be redirected to a new instance without their session. This violates the requirement for distributed session data management and resilience.\n\n**Why option 2 is incorrect:**\nSession Manager is a capability of AWS Systems Manager that allows you to manage your EC2 instances through a browser-based shell or the AWS CLI. It does not provide a mechanism for managing application session data. It's primarily used for administrative access and troubleshooting, not for storing or distributing session information for an application.\n\n**Why option 3 is incorrect:**\nThe GetSessionToken API operation in AWS STS is used to obtain temporary security credentials for federated users or roles. It is not related to managing application session data. It's used for authentication and authorization, not for storing or distributing session information.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company hosts a marketing website in an on-premises data center. The website consists of \nstatic documents and runs on a single server. An administrator updates the website content \ninfrequently and uses an SFTP client to upload new documents. \nThe company decides to host its website on AWS and to use Amazon CloudFront. The \ncompany's solutions architect creates a CloudFront distribution. The solutions architect must \ndesign the most cost-effective and resilient architecture for website hosting to serve as the \nCloudFront origin. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a virtual server by using Amazon Lightsail.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Auto Scaling group for Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a private Amazon S3 bucket.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the most cost-effective and resilient solution for hosting static website content. A private S3 bucket, when configured as a CloudFront origin, allows CloudFront to serve the content to users. The private bucket ensures that users cannot directly access the S3 bucket, enhancing security. CloudFront can be configured to access the S3 bucket using an Origin Access Identity (OAI) or Origin Access Control (OAC). S3 provides high availability and durability for static content, and the pay-as-you-go pricing model is cost-effective for infrequently updated content. The infrequent updates can be handled by uploading new versions of the files to the S3 bucket.\n\n**Why option 0 is incorrect:**\nLightsail provides a virtual server, which is more expensive and requires more management overhead than using S3 for static content. It is not the most cost-effective solution for serving static files. Also, it doesn't inherently provide the same level of resilience as S3.\n\n**Why option 1 is incorrect:**\nAn Auto Scaling group of EC2 instances is overkill for hosting static website content. It is significantly more complex and expensive than using S3. It also requires more operational overhead for managing the instances and ensuring their availability. While it provides resilience, it is not the most cost-effective way to achieve it for static content.\n\n**Why option 3 is incorrect:**\nWhile a public S3 bucket could serve as a CloudFront origin, it is not the most secure option. Making the bucket public exposes the content directly, bypassing CloudFront's caching and security features. Also, configuring AWS Transfer for SFTP adds unnecessary complexity and cost, as the content can be directly uploaded to the S3 bucket. The question emphasizes cost-effectiveness and resilience, and this option fails on both counts.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "A company is designing a cloud communications platform that is driven by APIs. The application \nis hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses \nAmazon API Gateway to provide external users with access to the application through APIs. The \ncompany wants to protect the platform against web exploits like SQL injection and also wants to \ndetect and mitigate large, sophisticated DDoS attacks. \nWhich combination of solutions provides the MOST protection? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS WAF to protect the NLB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Shield Advanced with the NLB.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS WAF to protect Amazon API Gateway.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon GuardDuty with AWS Shield Standard.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Shield Standard with Amazon API Gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because AWS Shield Advanced provides enhanced DDoS protection beyond the standard level. It offers 24/7 access to the AWS DDoS Response Team (DRT) and provides more sophisticated detection and mitigation techniques, which are essential for defending against large, sophisticated DDoS attacks. Protecting the NLB with Shield Advanced directly protects the underlying EC2 instances and the application itself.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while AWS WAF can protect against web exploits like SQL injection, it's typically deployed in front of API Gateway or an Application Load Balancer (ALB), not an NLB. NLBs are designed for TCP, UDP, and TLS traffic, and WAF is designed for HTTP/HTTPS traffic. While WAF *can* technically protect an NLB in some limited scenarios, it's not the primary or recommended use case, especially when API Gateway is already in place. Using WAF to protect API Gateway is a better approach for web exploit protection.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while AWS WAF is a good choice for protecting against web exploits like SQL injection, it doesn't provide the comprehensive DDoS protection needed to mitigate large, sophisticated attacks. While WAF can help with some basic DDoS mitigation, it's not its primary function. AWS Shield Advanced is specifically designed for that purpose.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While it's a valuable security tool, it doesn't directly protect against web exploits or mitigate DDoS attacks. AWS Shield Standard provides basic DDoS protection, but it's not sufficient for mitigating large, sophisticated attacks. The combination of GuardDuty and Shield Standard doesn't address the core requirements of the question.\n\n**Why option 4 is incorrect:**\nThis is incorrect because AWS Shield Standard provides basic DDoS protection, but it's not sufficient for mitigating large, sophisticated attacks. While protecting API Gateway with Shield Standard is better than nothing, it doesn't provide the level of protection required by the question. AWS Shield Advanced is needed for robust DDoS mitigation, and it should be applied to the NLB to protect the underlying infrastructure.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A company has a web application that is based on Java and PHP. The company plans to move \nthe application from on premises to AWS. The company needs the ability to test new site features \nfrequently. The company also needs a highly available and managed solution that requires \nminimum operational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the web application to an AWS Elastic Beanstalk environment.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the web application lo Amazon EC2 instances that are configured with Java and PHP.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Containerize the web application.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because AWS Elastic Beanstalk is a Platform as a Service (PaaS) that simplifies the deployment and management of web applications. It supports Java and PHP, providing a managed environment that handles infrastructure provisioning, operating system maintenance, and application server configuration. Elastic Beanstalk also allows for easy creation of multiple environments for testing new features without impacting the production environment. Its built-in support for load balancing and auto-scaling ensures high availability. This minimizes operational overhead, as AWS manages the underlying infrastructure.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon S3 is an object storage service and is not suitable for hosting web applications directly. While S3 can host static websites, it cannot execute server-side code like Java and PHP without additional services. It does not provide the managed environment or application server needed for the application.\n\n**Why option 2 is incorrect:**\nThis is incorrect because deploying to Amazon EC2 instances requires manual configuration and management of the operating system, web server, and application dependencies. This increases operational overhead and does not provide a managed environment. While EC2 offers flexibility, it does not inherently provide high availability or easy testing environments without significant manual effort.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while containerization offers benefits like portability and consistency, it doesn't directly address the need for a managed solution with minimal operational overhead. Deploying containers to services like ECS or EKS would still require configuration and management of the underlying infrastructure. Elastic Beanstalk can also deploy containerized applications and would be a better fit for the requirements.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company has a Microsoft .NET application that runs on an on-premises Windows Server. The \napplication stores data by using an Oracle Database Standard Edition server. The company is \nplanning a migration to AWS and wants to minimize development changes while moving the \napplication. The AWS application environment should be highly available. \nWhich combination of actions should the company take to meet these requirements? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Refactor the application as serverless with AWS Lambda functions running .NET Core.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nRehosting the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment allows for a lift-and-shift approach, minimizing code changes. Elastic Beanstalk simplifies deployment and management of web applications and services. Deploying in a Multi-AZ environment provides high availability by distributing instances across multiple Availability Zones, protecting against single-AZ failures.\n\n**Why option 0 is incorrect:**\nRefactoring the application as serverless with AWS Lambda functions running .NET Core would require significant development effort to rewrite the application, which violates the requirement to minimize development changes.\n\n**Why option 2 is incorrect:**\nReplatforming the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image (AMI) would likely require code changes to ensure compatibility with the new operating system. While EC2 provides flexibility, it doesn't inherently offer the managed deployment and scaling capabilities of Elastic Beanstalk, making it a less suitable choice for minimizing operational overhead.\n\n**Why option 3 is incorrect:**\nUsing AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon Aurora PostgreSQL is a valid migration strategy, but it would require significant application changes to connect to the new database type. This violates the requirement to minimize development changes.\n\n**Why option 4 is incorrect:**\nUsing AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle RDS is a good choice for database migration, but the question asks for a combination of actions. This option alone does not address the application migration.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 49,
    "text": "A rapidly growing ecommerce company is running its workloads in a single AWS Region. A \nsolutions architect must create a disaster recovery (DR) strategy that includes a different AWS \nRegion. The company wants its database to be up to date in the DR Region with the least \npossible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity \nand must be able to scale up if necessary. \nWhich solution will meet these requirements with the LOWEST recovery time objective (RTO)?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon Aurora global database with a pilot light deployment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Aurora global database with a warm standby deployment",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirements by utilizing Aurora Global Database, which provides low-latency replication across AWS Regions. The warm standby deployment ensures that the DR Region has a scaled-down but functional environment ready to take over with minimal delay. Aurora Global Database replicates data asynchronously with typical latency of less than 1 second, providing a very low RTO for the database portion. The warm standby approach for the remaining infrastructure allows for quick scaling when a failover occurs, meeting the company's need to scale up if necessary. This combination of low-latency database replication and a pre-configured, scaled-down environment in the DR region results in the lowest possible RTO compared to other options.\n\n**Why option 0 is incorrect:**\nWhile Aurora Global Database provides low-latency replication, a pilot light deployment means that the infrastructure in the DR region is mostly shut down and needs to be provisioned and configured during a failover. This significantly increases the RTO compared to a warm standby approach, where the infrastructure is already running at a reduced capacity and can be scaled up more quickly.\n\n**Why option 2 is incorrect:**\nRDS Multi-AZ provides high availability within a single region, not disaster recovery across regions. It does not replicate data to a different region for DR purposes, so it cannot meet the requirement of having an up-to-date database in a different AWS Region. Therefore, it's not suitable for this DR scenario.\n\n**Why option 3 is incorrect:**\nRDS Multi-AZ provides high availability within a single region, not disaster recovery across regions. It does not replicate data to a different region for DR purposes, so it cannot meet the requirement of having an up-to-date database in a different AWS Region. Therefore, it's not suitable for this DR scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company's order system sends requests from clients to Amazon EC2 instances. The EC2 \ninstances process the orders and then store the orders in a database on Amazon RDS. Users \nreport that they must reprocess orders when the system fails. The company wants a resilient \nsolution that can process orders automatically if a system outage occurs. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the EC2 instances into an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the EC2 instances into an Auto Scaling group.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by ensuring that if an EC2 instance fails, the Auto Scaling group will automatically launch a new instance to replace it. This maintains the processing capacity and prevents the need to reprocess orders. Auto Scaling groups provide fault tolerance and high availability for EC2 instances, which directly addresses the problem of system outages causing order reprocessing.\n\n**Why option 0 is incorrect:**\nWhile moving EC2 instances into an Auto Scaling group provides some level of resilience, it doesn't address the distribution of traffic or ensure that requests are routed to healthy instances. Without a load balancer, a single point of failure still exists if the initial instance in the group fails before Auto Scaling can react. The question specifies that users must reprocess orders when the system fails, indicating a lack of proper traffic distribution and health checks.\n\n**Why option 1 is incorrect:**\nAdding an Application Load Balancer (ALB) provides traffic distribution and health checks, which are beneficial for a highly available system. However, the question explicitly states that the orders are stored in RDS *after* processing by the EC2 instances. The problem is not with the distribution of traffic to the EC2 instances, but with the EC2 instances themselves failing and causing the need to reprocess orders. While the ALB would improve the overall architecture, it is not the most direct and cost-effective solution to the specific problem described in the question. The Auto Scaling group alone is sufficient to address the immediate requirement of preventing order reprocessing due to EC2 instance failures.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "A company runs an application on a large fleet of Amazon EC2 instances. The application reads \nand write entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously \ngrows, but the application needs only data from the last 30 days. The company needs a solution \nthat minimizes cost and development effort. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS CloudFormation template to deploy the complete solution.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an EC2 instance that runs a monitoring application from AWS Marketplace.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is",
        "correct": false
      },
      {
        "id": 3,
        "text": "Extend the application to add an attribute that has a value of the current timestamp plus 30 days",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by adding an attribute to each item that represents the expiration timestamp (current timestamp + 30 days). Then, using DynamoDB TTL (Time To Live), the items will be automatically deleted after the expiration timestamp. This minimizes cost because DynamoDB handles the deletion automatically, and it minimizes development effort because it only requires a small modification to the application to add the TTL attribute and enabling TTL on the table.\n\n**Why option 0 is incorrect:**\nUsing CloudFormation to deploy the complete solution doesn't address the core problem of automatically deleting old data. CloudFormation is an infrastructure-as-code tool and would be used to provision the DynamoDB table and other resources, but it doesn't provide a mechanism for data lifecycle management. It would require significant additional development to implement the data deletion logic.\n\n**Why option 1 is incorrect:**\nUsing an EC2 instance running a monitoring application from AWS Marketplace is an overly complex and expensive solution. It would require constant monitoring of the DynamoDB table, developing custom logic to identify and delete old data, and managing the EC2 instance itself. This increases both cost and development effort compared to using DynamoDB TTL.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 52,
    "text": "A company runs a containerized application on a Kubernetes cluster in an on-premises data \ncenter. The company is using a MongoDB database for data storage. \nThe company wants to migrate some of these environments to AWS, but no code changes or \ndeployment method changes are possible at this time. The company needs a solution that \nminimizes operational overhead. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by using Amazon EKS, which is a managed Kubernetes service, allowing the company to run their existing Kubernetes deployments with minimal changes. Using Fargate for compute further reduces operational overhead by eliminating the need to manage EC2 instances for the Kubernetes worker nodes. Fargate handles the underlying infrastructure, patching, and scaling, allowing the company to focus on their application. The question does not specify how MongoDB is being migrated, so the focus is on the container platform.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because while ECS can run containers, it requires changes to the deployment process since it's not Kubernetes. The company specifically wants to avoid deployment method changes. Also, using EC2 worker nodes increases operational overhead because the company has to manage the underlying EC2 instances.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because while ECS with Fargate reduces operational overhead, it requires changes to the deployment process since it's not Kubernetes. The company specifically wants to avoid deployment method changes.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company selves a dynamic website from a fleet of Amazon EC2 instances behind an \nApplication Load Balancer (ALB). The website needs to support multiple languages to serve \ncustomers around the world. The website's architecture is running in the us-west-1 Region and is \nexhibiting high request latency tor users that are located in other parts of the world. The website \nneeds to serve requests quickly and efficiently regardless of a user's location. However the \ncompany does not want to recreate the existing architecture across multiple Regions.  \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Replace the existing architecture with a website that is served from an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon CloudFront distribution with the ALB as the origin.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon API Gateway API that is integrated with the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of reducing latency for global users by caching content closer to them. CloudFront is a content delivery network (CDN) that caches static and dynamic content at edge locations around the world. By configuring CloudFront with the ALB as the origin, requests from users will be served from the nearest edge location, significantly reducing latency. This approach avoids the need to replicate the entire architecture in multiple regions, fulfilling the company's requirement of not wanting to recreate the existing architecture across multiple regions. The ALB handles the dynamic content, and CloudFront caches what it can, improving performance for all users.\n\n**Why option 0 is incorrect:**\nReplacing the existing architecture with a static website hosted on S3 would not meet the requirement of serving a dynamic website. The question explicitly states that the website is dynamic, implying that it requires server-side processing or database interaction, which S3 alone cannot provide. While S3 is excellent for static content, it's not suitable for the described scenario.\n\n**Why option 2 is incorrect:**\nWhile API Gateway can be used to expose APIs, it doesn't inherently solve the latency issue for global users. API Gateway itself would still be located in a single region (or require multi-region deployment, which the question wants to avoid). It adds an extra layer of complexity without directly addressing the core problem of geographical latency. It also doesn't provide caching capabilities like CloudFront.\n\n**Why option 3 is incorrect:**\nLaunching EC2 instances in each region and configuring Nginx as a cache server would essentially involve recreating the architecture in multiple regions, which the company explicitly wants to avoid. This approach is also more complex to manage and maintain compared to using a CDN like CloudFront. Furthermore, it requires managing the replication and invalidation of cached content across multiple Nginx instances, adding significant operational overhead.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 54,
    "text": "A telemarketing company is designing its customer call center functionality on AWS. The \ncompany needs a solution to provides multiples ipsafcar recognition and generates transcript \nfiles. The company wants to query the transcript files to analyze the business patterns. The \ntranscript files must be stored for 7 years for auditing policies. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Rekognition for multiple speaker recognition.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Transcribe for multiple speaker recognition.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Translate for multiple speaker recognition.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Rekognition for multiple speaker recognition.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement for multiple speaker recognition directly through its speaker diarization feature. It also generates transcript files, which can then be queried for business pattern analysis. Furthermore, the generated files can be stored in Amazon S3, which offers durable and cost-effective storage options suitable for long-term retention, satisfying the 7-year auditing policy requirement.\n\n**Why option 0 is incorrect:**\nWhile Amazon Rekognition can perform facial recognition and analyze images and videos, it is not designed for multiple speaker recognition or generating transcript files from audio. Its primary focus is on visual content analysis.\n\n**Why option 2 is incorrect:**\nAmazon Translate is a machine translation service. It translates text from one language to another. It does not perform speaker recognition or generate transcript files from audio. Therefore, it doesn't meet the core requirements of the scenario.\n\n**Why option 3 is incorrect:**\nThis option is a duplicate of option 0 and is incorrect for the same reasons. Amazon Rekognition is not designed for audio transcription or speaker diarization.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store \nall the data for 5 years and must delete all the data after 5 years. The company also must \nindefinitely keep audit logs of actions that are performed within the database. Currently, the \ncompany has automated backups configured for Aurora. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n130",
    "options": [
      {
        "id": 0,
        "text": "Take a manual snapshot of the DB cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a lifecycle policy for the automated backups.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure automated backup retention for 5 years.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon CloudWatch Logs export for the DB cluster.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use AWS Backup to take the backups and to keep the backups for 5 years.",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because configuring an Amazon CloudWatch Logs export for the DB cluster allows you to stream the database audit logs to CloudWatch Logs. CloudWatch Logs then can be configured to retain the logs indefinitely, meeting the requirement to keep audit logs of actions performed within the database indefinitely.\n\n**Why option 0 is incorrect:**\nTaking a manual snapshot of the DB cluster only provides a point-in-time backup. While useful, it doesn't address the automated deletion of data after 5 years or the indefinite retention of audit logs. It also requires manual intervention, which is less desirable than an automated solution.\n\n**Why option 1 is incorrect:**\nLifecycle policies are typically used for object storage like S3, not for database backups. Automated backups for Aurora are managed through the backup retention settings within the Aurora configuration itself, not through lifecycle policies.\n\n**Why option 2 is incorrect:**\nConfiguring automated backup retention for 5 years addresses the data retention requirement. However, the question asks for *two* steps to meet *all* requirements. This option only handles the data retention, not the audit log retention. Furthermore, the question states that the data must be *deleted* after 5 years. Automated backups retained for 5 years do not automatically delete the data after that period; they simply expire the backups.\n\n**Why option 4 is incorrect:**\nUsing AWS Backup to take backups and keep them for 5 years addresses the data retention requirement but not the audit log retention. AWS Backup, like automated backups, does not automatically delete the data after 5 years; it only manages the backup retention. Also, it doesn't address the indefinite storage of audit logs.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A company has a small Python application that processes JSON documents and outputs the \nresults to an on-premises SQL database. The application runs thousands of times each day. The \ncompany wants to move the application to the AWS Cloud. The company needs a highly \navailable solution that maximizes scalability and minimizes operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Place the JSON documents in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Place the JSON documents in an Amazon S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution leverages several AWS services to meet the requirements. Placing the JSON documents in an S3 bucket provides a highly durable and scalable storage solution. An AWS Lambda function can be triggered by S3 events (e.g., object creation) to process the JSON documents. The Lambda function can then connect to the on-premises SQL database (using a VPN or Direct Connect) to write the results. This approach minimizes operational overhead because S3 and Lambda are serverless services that automatically scale and are highly available. Using SQS to decouple the processing from the database writes further enhances scalability and resilience. The Lambda function can write the results to an SQS queue, and another process (potentially another Lambda function or an EC2 instance) can read from the queue and write to the database. This asynchronous approach prevents the Lambda function from being blocked by database latency or availability issues.\n\n**Why option 0 is incorrect:**\nWhile placing JSON documents in an Amazon S3 bucket is a good starting point, this option alone doesn't provide a complete solution. It lacks the processing and database integration components necessary to meet the requirements. It doesn't specify how the documents will be processed or how the results will be written to the on-premises database. Therefore, it's an incomplete solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nPlacing the JSON documents directly into an Amazon SQS queue as messages is not ideal for larger JSON documents. SQS has message size limits, and it's generally better suited for smaller messages that represent tasks or events rather than entire documents. While SQS can be used to decouple the processing from the database writes (as part of a larger solution), it's not the best choice for storing the JSON documents themselves. Additionally, directly placing JSON documents in SQS doesn't address the processing aspect of the application; a separate process would still be needed to read from the queue and process the documents.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A company's infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance \nin a single AWS Region. The company wants to back up its data in a separate Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon Machine Images (AMIs) of the EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create Amazon Elastic Block Store (Amazon EBS) snapshots.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Backup provides a centralized and automated way to manage backups across multiple AWS services, including EC2 and RDS. It allows defining backup policies that can automatically copy backups to a different Region. This minimizes operational overhead as it eliminates the need for manual backup and copy processes. AWS Backup is designed for cross-region backup and recovery, making it the most suitable solution for this scenario.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon Data Lifecycle Manager (DLM) primarily manages the lifecycle of EBS snapshots and AMIs within a single region. While DLM can automate snapshot creation, it doesn't directly handle cross-region copying of RDS backups. Using DLM alone would require additional scripting or tooling to handle RDS backups and cross-region replication, increasing operational overhead.\n\n**Why option 2 is incorrect:**\nThis is incorrect because creating AMIs only addresses the EC2 instance backup. It doesn't handle the RDS database backup. Furthermore, manually copying AMIs to another region adds operational overhead. While AMIs are a valid backup strategy for EC2, they don't provide a comprehensive solution for both EC2 and RDS with minimal overhead.\n\n**Why option 3 is incorrect:**\nThis is incorrect because creating EBS snapshots only addresses the EC2 instance's storage backup. It doesn't handle the RDS database backup. Furthermore, manually copying EBS snapshots to another region adds operational overhead. While EBS snapshots are a valid backup strategy for EC2, they don't provide a comprehensive solution for both EC2 and RDS with minimal overhead.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A company is building a new dynamic ordering website. The company wants to minimize server \nmaintenance and patching. The website must be highly available and must scale read and write \ncapacity as quickly as possible to meet changes in user demand. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host static content in Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Host static content in Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host all the website content on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host all the website content on Amazon EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon S3 provides a highly available, scalable, and cost-effective solution for hosting static website content. S3 eliminates the need for server management and patching, fulfilling the requirement to minimize server maintenance. S3 also automatically scales to handle changes in user demand, ensuring the website remains responsive and highly available. By serving static content from S3, the dynamic ordering website can offload the delivery of images, CSS, JavaScript, and other static assets, improving overall performance and reducing the load on the application servers.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis is incorrect because hosting all website content on Amazon EC2 instances requires managing the operating system, web server software, and security patches. This increases the operational overhead and contradicts the requirement to minimize server maintenance. While EC2 instances can be scaled, the scaling process is not as rapid or automatic as using a managed service like S3 for static content. Additionally, managing the infrastructure for serving static content on EC2 is less cost-effective than using S3.\n\n**Why option 3 is incorrect:**\nThis is incorrect because hosting all website content on Amazon EC2 instances requires managing the operating system, web server software, and security patches. This increases the operational overhead and contradicts the requirement to minimize server maintenance. While EC2 instances can be scaled, the scaling process is not as rapid or automatic as using a managed service like S3 for static content. Additionally, managing the infrastructure for serving static content on EC2 is less cost-effective than using S3.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company uses Amazon S3 as its data lake. The company has a new partner that must use \nSFTP to upload data files. A solutions architect needs to implement a highly available SFTP \nsolution that minimizes operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon S3 File Gateway as an SFTP server.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch Amazon EC2 instances in a private subnet in a VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because AWS Transfer Family provides a fully managed SFTP service that directly integrates with S3. It handles the underlying infrastructure, ensuring high availability and minimizing operational overhead. Configuring the server with public accessibility allows the partner to upload data directly. The service is designed for secure file transfers and simplifies the process of integrating SFTP with S3.\n\n**Why option 1 is incorrect:**\nAmazon S3 File Gateway is designed to provide on-premises applications with access to S3 as a file share. While it can be used for file storage, it doesn't directly provide SFTP access. It's primarily for caching frequently accessed data locally and doesn't fulfill the requirement of providing an SFTP endpoint for the partner to upload files directly to S3. It also adds operational overhead compared to a fully managed SFTP service.\n\n**Why option 2 is incorrect:**\nLaunching an EC2 instance and configuring it as an SFTP server requires significant manual configuration and management, including patching, scaling, and ensuring high availability. This increases operational overhead and doesn't align with the requirement of minimizing it. While technically feasible, it's not the most efficient or recommended approach. Also, placing it in a private subnet would require additional networking configurations (like a NAT gateway) to allow external access, further increasing complexity.\n\n**Why option 3 is incorrect:**\nSimilar to option 2, launching EC2 instances and configuring them as an SFTP server requires significant manual configuration and management, including patching, scaling, and ensuring high availability. This increases operational overhead and doesn't align with the requirement of minimizing it. While technically feasible, it's not the most efficient or recommended approach. Also, placing them in a private subnet would require additional networking configurations (like a NAT gateway) to allow external access, further increasing complexity.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company needs to store contract documents. A contract lasts for 5 years. During the 5-year \nperiod, the company must ensure that the documents cannot be overwritten or deleted. The \ncompany needs to encrypt the documents at rest and rotate the encryption keys automatically \nevery year. \n \nWhich combination of steps should a solutions architect take to meet these requirements with the \nLEAST operational overhead? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Store the documents in Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the documents in Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nStoring the documents in Amazon S3 is the foundation for meeting the requirements. S3 provides the necessary storage infrastructure and integrates well with other AWS services to achieve immutability and encryption.\n\n**Why option 0 is incorrect:**\nWhile storing documents in S3 is a necessary component of the solution, it doesn't address the immutability or encryption requirements on its own. Additional configurations are needed to meet the full set of requirements.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nUsing server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys (SSE-KMS) is a valid encryption option and allows for automatic key rotation. However, this option alone doesn't address the immutability requirement. Object Lock needs to be enabled on the S3 bucket to prevent deletion or overwriting of the objects.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "You have been given a scope to deploy some AWS infrastructure for a large organisation. The \nrequirements are that you will have a lot of EC2 instances but may need to add more when the \naverage utilization of your Amazon EC2 fleet is high and conversely remove them when CPU \nutilization is low. Which AWS services would be best to use to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Auto Scaling, Amazon CloudWatch and AWS Elastic Beanstalk",
        "correct": false
      },
      {
        "id": 1,
        "text": "Auto Scaling, Amazon CloudWatch and Elastic Load Balancing.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon CloudFront, Amazon CloudWatch and Elastic Load Balancing.",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Elastic Beanstalk , Amazon CloudWatch and Elastic Load Balancing.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by using Auto Scaling to automatically adjust the number of EC2 instances based on demand. Amazon CloudWatch monitors the CPU utilization of the EC2 instances and triggers scaling events in Auto Scaling when the utilization crosses predefined thresholds. Elastic Load Balancing distributes incoming traffic across the healthy EC2 instances, ensuring high availability and optimal performance. The combination of these three services provides a complete solution for dynamic scaling based on CPU utilization.\n\n**Why option 0 is incorrect:**\nWhile Auto Scaling and CloudWatch are essential for scaling based on metrics, AWS Elastic Beanstalk is a platform-as-a-service (PaaS) that simplifies deploying and managing web applications. While Beanstalk can utilize Auto Scaling, it's not the most direct or efficient way to address the specific requirement of scaling based on CPU utilization of an existing EC2 fleet. Beanstalk is more suited for deploying and managing applications, not just scaling existing instances.\n\n**Why option 2 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) service used to distribute static and dynamic web content globally. While CloudFront can improve performance for end users, it does not directly address the requirement of scaling EC2 instances based on CPU utilization. The question specifically asks about scaling the EC2 instances themselves, not improving content delivery.\n\n**Why option 3 is incorrect:**\nAWS Elastic Beanstalk is a platform-as-a-service (PaaS) that simplifies deploying and managing web applications. While Beanstalk can utilize Auto Scaling, it's not the most direct or efficient way to address the specific requirement of scaling based on CPU utilization of an existing EC2 fleet. Beanstalk is more suited for deploying and managing applications, not just scaling existing instances. While Elastic Load Balancing is necessary to distribute traffic, Beanstalk is not the core component for scaling based on CPU utilization.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 62,
    "text": "of the below mentioned options is not available when an instance is launched by Auto \nScaling with EC2 Classic?",
    "options": [
      {
        "id": 0,
        "text": "Public IP",
        "correct": false
      },
      {
        "id": 1,
        "text": "Elastic IP",
        "correct": true
      },
      {
        "id": 2,
        "text": "Private DNS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Private IP",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nElastic IPs are not directly supported when launching instances via Auto Scaling in EC2 Classic. While you could associate an Elastic IP after the instance is launched, Auto Scaling itself cannot directly provision and associate an Elastic IP during the instance launch process in EC2 Classic. This is because Elastic IPs are designed to be associated with instances within a VPC and provide a static public IP address. EC2 Classic instances typically receive a public IP address automatically, but this IP address is not persistent like an Elastic IP.\n\n**Why option 0 is incorrect:**\nPublic IPs are available in EC2 Classic. When an instance is launched in EC2 Classic, it typically receives a public IP address automatically, unless explicitly disabled during instance creation. This public IP allows the instance to communicate with the internet.\n\n**Why option 2 is incorrect:**\nPrivate DNS is available in EC2 Classic. Instances launched in EC2 Classic are assigned a private DNS hostname that resolves to the instance's private IP address. This allows for internal communication within the EC2 Classic network.\n\n**Why option 3 is incorrect:**\nPrivate IPs are available in EC2 Classic. Every instance launched in EC2 Classic is assigned a private IP address. This private IP address is used for internal communication within the EC2 Classic network.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 63,
    "text": "A company's application is running on Amazon EC2 instances in a single Region in the event of a \ndisaster a solutions architect needs to ensure that the resources can also be deployed to a \nsecond Region. \nWhich combination of actions should the solutions architect take to accomplish this? (Select \nTWO) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n134",
    "options": [
      {
        "id": 0,
        "text": "Detach a volume on an EC2 instance and copy it to Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch a new EC2 instance from an Amazon Machine image (AMI) in a new Region",
        "correct": true
      },
      {
        "id": 2,
        "text": "Launch a new EC2 instance in a new Region and copy a volume from Amazon S3 to the new",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy an Amazon Machine Image (AMI) of an EC2 instance and specify a different Region for the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon S3 and launch an EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis action directly addresses the requirement of deploying resources in a second region. Launching a new EC2 instance from an AMI in the new region allows the application to be instantiated in the disaster recovery region. This is a fundamental step in replicating the application environment.\n\n**Why option 0 is incorrect:**\nDetaching a volume and copying it to S3 is a valid backup strategy, but it doesn't directly enable launching the application in a new region. It only copies the data. You would still need to create an EC2 instance and attach the volume, making it an incomplete solution.\n\n**Why option 2 is incorrect:**\nWhile launching a new EC2 instance in a new region is part of the solution, copying a volume from S3 to the new instance requires the volume to have been backed up to S3 beforehand. This option is incomplete as it doesn't address how the volume data gets to S3 in the first place, nor does it address the AMI.\n\n**Why option 3 is incorrect:**\nCopying an AMI to a different region is the correct second step. This makes the AMI available in the disaster recovery region, allowing new EC2 instances to be launched from it.\n\n**Why option 4 is incorrect:**\nCopying an EBS volume from S3 and launching an EC2 instance is an incomplete solution. It only addresses the data volume, not the AMI used to launch the instance. Furthermore, it assumes the EBS volume is already in S3, which isn't guaranteed.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A recently acquired company is required to buikl its own infrastructure on AWS and migrate \nmultiple applications to the cloud within a month. \nEach application has approximately 50 TB of data to be transferred. \nAfter the migration is complete this company and its parent company will both require secure \nnetwork connectivity with consistent throughput from their data centers to the applications. \nA solutions architect must ensure one-time data migration and ongoing network connectivity. \nWhich solution will meet these requirements''",
    "options": [
      {
        "id": 0,
        "text": "AWS Direct Connect for both the initial transfer and ongoing connectivity",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Site-to-Site VPN for both the initial transfer and ongoing connectivity",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball for the initial transfer and AWS Direct Connect for ongoing connectivity",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Snowball for the initial transfer and AWS Site-to-Site VPN for ongoing connectivity",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by using AWS Snowball for the initial data transfer. Snowball is designed for transferring large amounts of data quickly and securely, which is ideal for the 50 TB per application migration. After the data is migrated, AWS Direct Connect provides a dedicated network connection between the company's data center and AWS, ensuring consistent throughput and secure connectivity for ongoing operations. This also allows the parent company to have a dedicated connection to the acquired company's resources in AWS.\n\n**Why option 0 is incorrect:**\nWhile AWS Direct Connect provides excellent ongoing network connectivity, it is not the most efficient solution for the initial transfer of 50 TB of data per application. Transferring this much data over a network connection, even a dedicated one, can take a significant amount of time, potentially exceeding the one-month migration window. The initial transfer would be significantly slower than using Snowball.\n\n**Why option 1 is incorrect:**\nAWS Site-to-Site VPN provides secure network connectivity, but it does not offer the same level of consistent throughput as AWS Direct Connect. VPN connections are subject to variations in internet traffic and may not be suitable for applications that require guaranteed bandwidth. Furthermore, transferring 50 TB of data per application over a VPN connection within a month would likely be challenging due to bandwidth limitations and potential network congestion. It is also not as secure as Direct Connect.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  }
]