[
  {
    "id": 0,
    "text": "A company's website provides users with downloadable historical performance reports. The \nwebsite needs a solution that will scale to meet the company's website demands globally. The \nsolution should be cost-effective, limit the provisioning of infrastructure resources, and provide the \nfastest possible response time. \n \nWhich combination should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon CloudFront and Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Lambda and Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Application Load Balancer with Amazon EC2 Auto Scaling",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Route 53 with internal Application Load Balancers",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company runs an Oracle database on premises. As part of the company's migration to AWS, \nthe company wants to upgrade the database to the most recent available version. The company \nalso wants to set up disaster recovery (DR) for the database. The company needs to minimize \nthe operational overhead for normal operations and DR setup. The company also needs to \nmaintain access to the database's underlying operating system. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the Oracle database to an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the Oracle database to Amazon RDS for Oracle.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the Oracle database to Amazon RDS Custom for Oracle.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the Oracle database to Amazon RDS for Oracle.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-custom.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company wants to move its application to a serverless solution. The serverless solution needs \nto analyze existing and new data by using SQL. The company stores the data in an Amazon S3 \nbucket. The data requires encryption and must be replicated to a different AWS Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a new S3 bucket. Load the data into the new S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new S3 bucket. Load the data into the new S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Load the data into the existing S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Load the data into the existing S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon S3 Bucket Keys reduce the cost of Amazon S3 server-side encryption using AWS Key Management Service (SSE-KMS). This new bucket-level key for SSE can reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS. With a few clicks in the AWS Management Console, and without any changes to your client applications, you can configure your bucket to use an S3 Bucket Key for AWS KMS-based encryption on new objects. The Existing S3 bucket might have uncrypted data - encryption will apply new data received after the applying of encryption on the new bucket.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company runs workloads on AWS. The company needs to connect to a service from an \nexternal provider. The service is hosted in the provider's VPC. According to the company's \nsecurity team, the connectivity must be private and must be restricted to the target service. The \nconnection must be initiated only from the company's VPC. \n \nWhich solution will mast these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC peering connection between the company's VPC and the provider's VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Ask the provider to create a virtual private gateway in its VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a NAT gateway in a public subnet of the company's VPC.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Ask the provider to create a VPC endpoint for the target service.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS PrivateLink provides private connectivity between VPCs, AWS services, and your on- premises networks, without exposing your traffic to the public internet. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify your network architecture. Interface **VPC endpoints**, powered by AWS PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace. https://aws.amazon.com/privatelink/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. \nThe on-premises database must remain online and accessible during the migration. The Aurora \ndatabase must remain synchronized with the on-premises database. \n \nWhich combination of actions must a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an ongoing replication task.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a database backup of the on-premises database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Database Migration Service (AWS DMS) replication server",
        "correct": false
      },
      {
        "id": 3,
        "text": "Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Database Migration Service (AWS DMS) helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can also continuously replicate data with low latency from any supported source to any supported target. https://aws.amazon.com/dms/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company uses AWS Organizations to create dedicated AWS accounts for each business unit to \nmanage each business unit's account independently upon request. The root email recipient \nmissed a notification that was sent to the root user email address of one account. The company \nwants to ensure that all future notifications are not missed. Future notifications must be limited to \naccount administrators. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the company's email server to forward notification email messages that are sent to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure all AWS account root user email addresses as distribution lists that go to a few",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure all AWS account root user email messages to be sent to one administrator who is",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure all existing AWS accounts and all newly created accounts to use the same root user",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUse a group email address for the management account's root user https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt- acct.html#best-practices_mgmt-acct_email-address\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS \naccount. The company needs to create a strategy to access and administer the instances \nremotely and securely. The company needs to implement a repeatable process that works with \nnative AWS services and follows the AWS Well-Architected Framework.  \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use the EC2 serial console to directly access the terminal interface of each instance for",
        "correct": false
      },
      {
        "id": 1,
        "text": "Attach the appropriate IAM role to each existing instance and new instance.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an administrative SSH key pair.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Establish an AWS Site-to-Site VPN connection.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/setup-launch-managed- instance.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. \nThe website is experiencing increased demand from around the world. The company must \ndecrease latency for users who access the website. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Replicate the S3 bucket that contains the website to all AWS Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision accelerators in AWS Global Accelerator.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution in front of the S3 bucket.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable S3 Transfer Acceleration on the bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 8,
    "text": "A company maintains a searchable repository of items on its website. The data is stored in an \nAmazon RDS for MySQL database table that contains more than 10 million rows. The database \nhas 2 TB of General Purpose SSD storage. There are millions of updates against this data every \nday through the company's website. \nThe company has noticed that some insert operations are taking 10 seconds or longer. \nThe company has determined that the database storage performance is the problem. \nWhich solution addresses this performance issue?",
    "options": [
      {
        "id": 0,
        "text": "Change the storage type to Provisioned IOPS SSD",
        "correct": true
      },
      {
        "id": 1,
        "text": "Change the DB instance to a memory optimized instance class",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the DB instance to a burstable performance instance class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nProvisioned IOPS volumes are backed by solid-state drives (SSDs) and are the highest performance EBS volumes designed for your critical, I/O intensive database applications. These volumes are ideal for both IOPS-intensive and throughput-intensive workloads that require extremely low latency. https://aws.amazon.com/ebs/features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A company has thousands of edge devices that collectively generate 1 TB of status alerts each \nday. Each alert is approximately 2 KB in size.  \nA solutions architect needs to implement a solution to ingest and store the alerts for future \nanalysis. \nThe company wants a highly available solution. However, the company needs to minimize costs \nand does not want to manage additional infrastructure. Additionally, the company wants to keep \n14 days of data available for immediate analysis and archive any data older than 14 days. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nDefinitely A, it's the most operationally efficient compared to D, which requires a lot of code and infrastructure to maintain. A is mostly managed (firehose is fully managed and S3 lifecycles are also managed).\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "A company's application integrates with multiple software-as-a-service (SaaS) sources for data \ncollection. The company runs Amazon EC2 instances to receive the data and to upload the data \nto an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data \nalso sends a notification to the user when an upload is complete. The company has noticed slow \napplication performance and wants to improve the performance as much as possible. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group so that EC2 instances can scale out.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Docker container to use instead of an EC2 instance. Host the containerized application",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. https://aws.amazon.com/appflow/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "A company runs a highly available image-processing application on Amazon EC2 instances in a \nsingle VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The \nEC2 instances do not communicate with each other. However, the EC2 instances download \nimages from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The \ncompany is concerned about data transfer charges. \nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n110",
    "options": [
      {
        "id": 0,
        "text": "Launch the NAT gateway in each Availability Zone",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the NAT gateway with a NAT instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a gateway VPC endpoint for Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision an EC2 Dedicated Host to run the EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nVPC gateway endpoints allow communication to Amazon S3 and Amazon DynamoDB without incurring data transfer charges within the same Region. On the other hand NAT gateway incurs additional data processing charges. https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common- architectures/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 12,
    "text": "A company has an on-premises application that generates a large amount of time-sensitive data \nthat is backed up to Amazon S3. The application has grown and there are user complaints about \ninternet bandwidth limitations. A solutions architect needs to design a long-term solution that \nallows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for \ninternal users. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "Establish a new AWS Direct Connect connection and direct backup traffic through this new",
        "correct": true
      },
      {
        "id": 2,
        "text": "Order daily AWS Snowball devices Load the data onto the Snowball devices and return the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Submit a support ticket through the AWS Management Console. Request the removal of S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nDirect connect is a dedicated connection between on-prem and AWS, this is the way to ensure stable network connectivity that will not wax and wane like internet connectivity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company has an Amazon S3 bucket that contains critical data. The company must protect the \ndata from accidental deletion. \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable versioning on the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable MFA Delete on the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a bucket policy on the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable default encryption on the S3 bucket.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a lifecycle policy for the objects in the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTo prevent or mitigate future accidental deletions, consider the following features: - Enable versioning to keep historical versions of an object. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company has a data ingestion workflow that consists the following: \n \n- An Amazon Simple Notification Service (Amazon SNS) topic for \nnotifications about new data deliveries. \n- An AWS Lambda function to process the data and record metadata \n \nThe company observes that the ingestion workflow fails occasionally because of network \nconnectivity issues. When such a failure occurs, the Lambda function does not ingest the \ncorresponding data unless the company manually reruns the job.  \nWhich combination of actions should a solutions architect take to ensure that the Lambda \nfunction ingests all data in the future? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the Lambda function In multiple Availability Zones.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe It to me SNS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the CPU and memory that are allocated to the Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase provisioned throughput for the Lambda function.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nA, C, D options are wrong, since Lambda is fully managed service which provides high availability and scalability by its own.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company's web application is running on Amazon EC2 instances behind an Application Load \nBalancer. The company recently changed its policy, which now requires the application to be \naccessed from one specific country only. \nWhich configuration will meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the security group on the Application Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS WAF on the Application Load Balancer in a VPC.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the network ACL for the subnet that contains the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nGeographic (Geo) Match Conditions in AWS WAF. This new condition type allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access. https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic- match/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n112 \nA company has a multi-tier application that runs six front-end web servers in an Amazon EC2 \nAuto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A \nsolutions architect needs to modify the infrastructure to be highly available without modifying the \napplication. \n \nWhich architecture should the solutions architect choose that provides high availability?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group that uses three instances across each of two Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the Auto Scaling group to use three instances across each of two Availability Zones.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling template that can be used to quickly create more instances in another",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nHigh availability can be enabled for this architecture quite simply by modifying the existing Auto Scaling group to use multiple availability zones. The ASG will automatically balance the load so you don't actually need to specify the instances per AZ.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "Organizers for a global event want to put daily reports online as static HTML pages. The pages \nare expected to generate millions of views from users around the world. The files are stored In an \nAmazon S3 bucket. A solutions architect has been asked to design an efficient and effective \nsolution. Which action should the solutions architect take to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Generate presigned URLs for the files.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use cross-Region replication to all Regions.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the geoproximity feature of Amazon Route 53.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront with the S3 bucket as its origin.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nCloudFront is a content delivery network (CDN) offered by Amazon Web Services (AWS). It functions as a reverse proxy service that caches web content across AWS's global data centers, improving loading speeds and reducing the strain on origin servers. CloudFront can be used to efficiently deliver large amounts of static or dynamic content anywhere in the world.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A company runs an application using Amazon ECS. The application creates resized versions of \nan original image and then makes Amazon S3 API calls to store the resized images in Amazon \nS3. \nHow can a solutions architect ensure that the application has permission to access Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a security group that allows access from Amazon ECS to Amazon S3, and update the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe short name or full Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that grants containers in the task permission to call AWS APIs on your behalf.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A solutions architect needs to securely store a database user name and password that an \napplication uses to access an Amazon RDS DB instance. The application that accesses the \ndatabase runs on an Amazon EC2 instance. The solutions architect wants to create a secure \nparameter in AWS Systems Manager Parameter Store. \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that has read access to the Parameter Store parameter.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM policy that allows read access to the Parameter Store parameter.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM trust relationship between the DB instance and the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThere should be the Decrypt access to KMS. \"If you choose the SecureString parameter type when you create your parameter, Systems Manager uses AWS KMS to encrypt the parameter value.\" https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter- store.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A company is running a batch application on Amazon EC2 instances.  \nThe application consists of a backend with multiple Amazon RDS databases.  \nThe application is causing a high number of leads on the databases.  \nA solutions architect must reduce the number of database reads while ensuring high availability. \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add Amazon RDS read replicas.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon ElasbCache for Redls.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Route 53 DNS caching.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Memcached.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUse ElastiCache to reduce reading and choose redis to ensure high availability.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n114 \nA security team wants to limit access to specific services or actions in all of the team's AWS \naccounts. All accounts belong to a large organization in AWS Organizations. The solution must \nbe scalable and there must be a single point where permissions can be maintained.  \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Create an ACL to provide access to the services or actions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a security group to allow accounts and attach it to user groups.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create cross-account roles in each account to deny access to the services or actions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a service control policy in the root organizational unit to deny access to the services or",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nService control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization's access control guidelines. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is concerned about the security of its public web application due to recent web \nattacks. The application uses an Application Load Balancer (ALB). A solutions architect must \nreduce the risk of DDoS attacks against the application. \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon Inspector agent to the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon Macie to prevent attacks.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable AWS Shield Advanced to prevent attacks.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon GuardDuty to monitor the ALB.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Shield is a managed Distributed Denial of Service (DDoS) protection service that helps protect web applications running on AWS from DDoS attacks. AWS Shield Advanced is an additional layer of protection that provides enhanced DDoS protection capabilities, including proactive monitoring and automatic inline mitigations, to help protect against even the largest and most sophisticated DDoS attacks. By enabling AWS Shield Advanced, the solutions architect can help protect the application from DDoS attacks and reduce the risk of disruption to the application.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A company runs a production application on a fleet of Amazon EC2 instances. The application \nreads the data from an Amazon SQS queue and processes the messages in parallel. The \nmessage volume is unpredictable and often has intermittent traffic. This application should \ncontinually process messages without any downtime. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances exclusively to handle the maximum capacity required.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances exclusively to handle the maximum capacity required.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nWe recommend that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 \norigin to store a static website. The companyâ€™s security policy requires that all website traffic be \ninspected by AWS WAF. \nHow should the solutions architect comply with these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 bucket policy lo accept requests coming from the AWS WAF Amazon Resource",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUse an OAI to lockdown CloudFront to S3 origin & enable WAF on CF distribution. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content- restricting-access-to-s3.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web- awswaf.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company wants to manage Amazon Machine Images (AMIs). The company currently copies \nAMIs to the same AWS Region where the AMIs were created. The company needs to design an \napplication that captures AWS API calls and sends alerts whenever the Amazon EC2 \nCreatelmage API operation is called within the company's account. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nTo create an EventBridge rule to send a notification when an AMI is created and in the available state. https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/monitor-ami-events.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "An online retail company has more than 50 million active customers and receives more than \n25,000 orders each day. The company collects purchase data for customers and stores this data \nin Amazon S3. Additional customer data is stored in Amazon RDS. The company wants to make \nall the data available to various teams so that the teams can perform analytics. The solution must \nprovide the ability to manage fine-grained permissions for the data and must minimize operational \noverhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the purchase data to write directly to Amazon RDS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a data lake by using AWS Lake Formation.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Redshift cluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nManage fine-grained access control using AWS Lake Formation. https://aws.amazon.com/blogs/big-data/manage-fine-grained-access-control-using-aws-lake- formation/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company owns an asynchronous API that is used to ingest user requests and, based on the \nrequest type, dispatch requests to the appropriate microservice for processing. The company is \nusing Amazon API Gateway to deploy the API front end, and an AWS Lambda function that \ninvokes Amazon DynamoDB to store user requests before dispatching them to the processing \nmicroservices. The company provisioned as much DynamoDB throughput as its budget allows, \nbut the company is still experiencing availability issues and is losing user requests. \nWhat should a solutions architect do to address this issue without impacting existing users?",
    "options": [
      {
        "id": 0,
        "text": "Add throttling on the API Gateway with server-side throttling limits.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a secondary index in DynamoDB for the table with the user requests.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nbecause all other options put some more charges to DynamoDB. But the company supplied as much as they can for DynamoDB. And it is async request and we need to have retry mechanism not to lose the customer data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The \ncompany must ensure that no API calls and no data are routed through public internet routes. \nOnly the EC2 instance can have access to upload data to the S3 bucket. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the AWS provided, publicly available ip-ranges.json tile to obtain the private IP address of the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A gaming company hosts a browser-based application on AWS. The users of the application \nconsume a large number of videos and images that are stored in Amazon S3. This content is the \nsame for all users. \nThe application has increased in popularity, and millions of users worldwide accessing these \nmedia files. The company wants to provide the files to the users while reducing the load on the \norigin. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Global Accelerator accelerator in front of the web servers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon CloudFront web distribution in front of the S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy an Amazon ElastiCache for Redis instance in front of the web servers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon ElastiCache for Memcached instance in front of the web servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCloud front is best for content delivery. Global Accelerator is best for non-HTTP (TCP/UDP) cases and supports HTTP cases as well but with static IP (elastic IP) or anycast IP address only.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 30,
    "text": "A company has two applications: a sender application that sends messages with payloads to be \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n118 \nprocessed and a processing application intended to receive the messages with payloads. The \ncompany wants to implement an AWS service to handle messages between the two applications. \nThe sender application can send about 1.000 messages each hour. The messages may take up \nto 2 days to be processed. If the messages fail to process, they must be retained so that they do \nnot impact the processing of any remaining messages. \nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon EC2 instance running a Redis database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Kinesis data stream to receive the messages from the sender application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon SQS supports dead-letter queues (DLQ), which other queues (source queues) can target for messages that can't be processed (consumed) successfully. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead- letter-queues.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company has an AWS account used for software engineering. The AWS account has access to \nthe company's on-premises data center through a pair of AWS Direct Connect connections. All \nnon-VPC traffic routes to the virtual private gateway. \nA development team recently created an AWS Lambda function through the console. \nThe development team needs to allow the function to access a database that runs in a private \nsubnet in the company's data center. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Lambda function to run in the VPC with the appropriate security group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the route tables in the VPC to allow the Lambda function to access the on-premises data",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nTo connect to another AWS service, you can use VPC endpoints for private communications between your VPC and supported AWS services. An alternative approach is to use a NAT gateway to route outbound traffic to another AWS service. To give your function access to the internet, route outbound traffic to a NAT gateway in a public subnet. The NAT gateway has a public IP address and can connect to the internet through the VPC's internet gateway. https://docs.aws.amazon.com/lambda/latest/dg/foundation-networking.html#foundation-nw- connecting Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is \nprocessed sequentially, but the order of results does not matter. The application uses a \nmonolithic architecture. The only way that the company can scale the application to meet \nincreased demand is to increase the size of the instances. \nThe company's developers have decided to rewrite the application to use a microservices \narchitecture on Amazon Elastic Container Service (Amazon ECS). \nWhat should a solutions architect recommend for communication between the microservices?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function to pass messages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon DynamoDB table.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nQueue has Limited throughput (300 msg/s without batching, 3000 msg/s with batching whereby up-to 10 msg per batch operation; Msg duplicates not allowed in the queue (exactly-once delivery); Msg order is preserved (FIFO); Queue name must end with .fifo\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A hospital wants to create digital copies for its large collection of historical written records. The \nhospital will continue to add hundreds of new documents each day. The hospital's data team will \nscan the documents and will upload the documents to the AWS Cloud. A solutions architect must \nimplement a solution to analyze the documents, extract the medical information, and store the \ndocuments so that an application can run SQL queries on the data. The solution must maximize \nscalability and operational efficiency. \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Write the document information to an Amazon EC2 instance that runs a MySQL database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Write the document information to an Amazon S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group of Amazon EC2 instances to run a custom application that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that runs when new documents are uploaded.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an AWS Lambda function that runs when new documents are uploaded.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A solutions architect is optimizing a website for an upcoming musical event. Videos of the \nperformances will be streamed in real time and then will be available on demand. The event is \nexpected to attract a global online audience. \nWhich service will improve the performance of both the real-lime and on-demand streaming?",
    "options": [
      {
        "id": 0,
        "text": "Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Route 53",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 Transfer Acceleration",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nYou can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin. One way you can set up video workflows in the cloud is by using CloudFront together with AWS Media Services. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming- video.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 35,
    "text": "A company wants to migrate its MySQL database from on premises to AWS. The company \nrecently experienced a database outage that significantly impacted the business. To ensure this \ndoes not happen again, the company wants a reliable database solution on AWS that minimizes \ndata loss and stores every transaction on at least two nodes. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon RDS DB instance with synchronous replication to three nodes in three",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nQ: What does Amazon RDS manage on my behalf? Amazon RDS manages the work involved in setting up a relational database: from provisioning the infrastructure capacity you request to installing the database software. Once your database is up and running, Amazon RDS automates common administrative tasks such as performing backups and patching the software that powers your database. With optional Multi-AZ deployments, Amazon RDS also manages synchronous data replication across Availability Zones with automatic failover. https://aws.amazon.com/rds/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 36,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n121 \nAn ecommerce company hosts its analytics application in the AWS Cloud. The application \ngenerates about 300 MB of data each month. The data is stored in JSON format. The company is \nevaluating a disaster recovery solution to back up the data. The data must be accessible in \nmilliseconds if it is needed, and the data must be kept for 30 days. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon S3 Glacier",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon RDS for PostgreSQL",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has a Windows-based application that must be migrated to AWS. The application \nrequires the use of a shared Windows file system attached to multiple Amazon EC2 Windows \ninstances that are deployed across multiple Availability Zones. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Storage Gateway in volume gateway mode.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon FSx for Windows File Server.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a file system by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nMicrosoft Windows-based application = shared Windows file system = Amazon FSX for Windows https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of \nthe information submitted by users is sensitive. The application uses HTTPS but needs another \nlayer of security. The sensitive information should be protected throughout the entire application \nstack, and access to the information should be restricted to certain applications. \nWhich action should the solutions architect take?",
    "options": [
      {
        "id": 0,
        "text": "Configure a CloudFront signed URL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a CloudFront signed cookie.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a CloudFront field-level encryption profile.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level- encryption.html \"With Amazon CloudFront, you can enforce secure end-to-end connections to origin servers by Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted \nwhen it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated \nevery year. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Move the data to the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the data with customer key material before moving the data to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html Customer managed keys Automatic key rotation is disabled by default on customer managed keys but authorized users can enable and disable it. When you enable (or re-enable) automatic key rotation, AWS KMS automatically rotates the KMS key one year (approximately 365 days) after the enable date and every year thereafter.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "An application runs on Amazon EC2 instances in private subnets. The application needs to \naccess an Amazon DynamoDB table. What is the MOST secure way to access the table while \nensuring that the traffic does not leave the AWS network?",
    "options": [
      {
        "id": 0,
        "text": "Use a VPC endpoint for DynamoDB.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use a NAT gateway in a public subnet.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a NAT instance in a private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the internet gateway attached to the VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints- dynamodb.html A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet. Your EC2 instances do not require public IP addresses, and you don't need an internet gateway, a NAT device, or a virtual private gateway in your VPC. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company provides an API to its users that automates inquiries for tax computations based on \nitem prices. The company experiences a larger number of inquiries during the holiday season \nonly that cause slower response times. A solutions architect needs to design a solution that is \nscalable and elastic. \nWhat should the solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Provide an API hosted on an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Design a REST API using Amazon API Gateway that accepts the item names.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Application Load Balancer that has two Amazon EC2 instances behind it.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nLambda server-less is scalable and elastic than EC2 api gateway solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company wants to use high performance computing (HPC) infrastructure on AWS for financial \nrisk modeling. The company's HPC workloads run on Linux. Each HPC workflow runs on \nhundreds of AmazonEC2 Spot Instances, is short-lived, and generates thousands of output files \nthat are ultimately stored in persistent storage for analytics and long-term future use. \nThe company seeks a cloud storage solution that permits the copying of on premises data to \nlong-term persistent storage to make data available for processing by all EC2 instances. The \nsolution should also be a high performance file system that is integrated with persistent storage to \nread and write datasets and output files. \nWhich combination of AWS services meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Lustre integrated with Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon FSx for Windows File Server integrated with Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/fsx/lustre/ Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 43,
    "text": "A company is running a publicly accessible serverless application that uses Amazon API \nGateway and AWS Lambda.  \nThe application's traffic recently spiked due to fraudulent requests from botnets. \nWhich steps should a solutions architect take to block requests from unauthorized users? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n124",
    "options": [
      {
        "id": 0,
        "text": "Create a usage plan with an API key that is shared with genuine users only.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Convert the existing public API to a private API.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an IAM role for each user attempting to access the API.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage- plans.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A solutions architect is designing the architecture of a new application being deployed to the AWS \nCloud. The application will run on Amazon EC2 On-Demand Instances and will automatically \nscale across multiple Availability Zones. The EC2 instances will scale up and down frequently \nthroughout the day. An Application Load Balancer (ALB) will handle the load distribution. The \narchitecture needs to support distributed session data management. The company is willing to \nmake changes to code if needed. \nWhat should the solutions architect do to ensure that the architecture supports distributed session \ndata management?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache to manage and store session data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use session affinity (sticky sessions) of the ALB to manage session data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Session Manager from AWS Systems Manager to manage the session.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/vi/caching/session-management/ In order to address scalability and to provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached. ElastiCache offerings for In-Memory key/value stores include ElastiCache for Redis, which can support replication, and ElastiCache for Memcached which does not support replication.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company hosts a marketing website in an on-premises data center. The website consists of \nstatic documents and runs on a single server. An administrator updates the website content \ninfrequently and uses an SFTP client to upload new documents. \nThe company decides to host its website on AWS and to use Amazon CloudFront. The \ncompany's solutions architect creates a CloudFront distribution. The solutions architect must \ndesign the most cost-effective and resilient architecture for website hosting to serve as the \nCloudFront origin. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a virtual server by using Amazon Lightsail.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Auto Scaling group for Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a private Amazon S3 bucket.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS transfer is a cost and doesn't mention using CloudFront. https://aws.amazon.com/aws-transfer-family/pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "A company is designing a cloud communications platform that is driven by APIs. The application \nis hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses \nAmazon API Gateway to provide external users with access to the application through APIs. The \ncompany wants to protect the platform against web exploits like SQL injection and also wants to \ndetect and mitigate large, sophisticated DDoS attacks. \nWhich combination of solutions provides the MOST protection? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS WAF to protect the NLB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Shield Advanced with the NLB.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS WAF to protect Amazon API Gateway.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon GuardDuty with AWS Shield Standard.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Shield Standard with Amazon API Gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Shield Advanced - DDos attacks AWS WAF to protect Amazon API Gateway, because WAF sits before the API Gateway and then comes NLB.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A company has a web application that is based on Java and PHP. The company plans to move \nthe application from on premises to AWS. The company needs the ability to test new site features \nfrequently. The company also needs a highly available and managed solution that requires \nminimum operational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the web application to an AWS Elastic Beanstalk environment.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the web application lo Amazon EC2 instances that are configured with Java and PHP.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Containerize the web application.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nElastic Beanstalk is a fully managed service that makes it easy to deploy and run applications in the AWS; To enable frequent testing of new site features, you can use URL swapping to switch between multiple Elastic Beanstalk environments. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company has a Microsoft .NET application that runs on an on-premises Windows Server. The \napplication stores data by using an Oracle Database Standard Edition server. The company is \nplanning a migration to AWS and wants to minimize development changes while moving the \napplication. The AWS application environment should be highly available. \nWhich combination of actions should the company take to meet these requirements? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Refactor the application as serverless with AWS Lambda functions running .NET Core.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCompany wants to minimize development modifications throughout the process. Option A & C i.e. refactoring or re-platforming options get eliminated. As for option D, oracle to dynamo DB is not possible.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 49,
    "text": "A rapidly growing ecommerce company is running its workloads in a single AWS Region. A \nsolutions architect must create a disaster recovery (DR) strategy that includes a different AWS \nRegion. The company wants its database to be up to date in the DR Region with the least \npossible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity \nand must be able to scale up if necessary. \nWhich solution will meet these requirements with the LOWEST recovery time objective (RTO)?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon Aurora global database with a pilot light deployment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Aurora global database with a warm standby deployment",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIn case of disaster, both pilot light and warm standby offer the capability to limit data loss (RPO). Both offer sufficient RTO performance that enables you to limit downtime. Between these two strategies, you have a choice of optimizing for RTO or for cost. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company's order system sends requests from clients to Amazon EC2 instances. The EC2 \ninstances process the orders and then store the orders in a database on Amazon RDS. Users \nreport that they must reprocess orders when the system fails. The company wants a resilient \nsolution that can process orders automatically if a system outage occurs. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the EC2 instances into an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the EC2 instances into an Auto Scaling group.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "A company runs an application on a large fleet of Amazon EC2 instances. The application reads \nand write entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously \ngrows, but the application needs only data from the last 30 days. The company needs a solution \nthat minimizes cost and development effort. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS CloudFormation template to deploy the complete solution.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an EC2 instance that runs a monitoring application from AWS Marketplace.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is",
        "correct": false
      },
      {
        "id": 3,
        "text": "Extend the application to add an attribute that has a value of the current timestamp plus 30 days",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 52,
    "text": "A company runs a containerized application on a Kubernetes cluster in an on-premises data \ncenter. The company is using a MongoDB database for data storage. \nThe company wants to migrate some of these environments to AWS, but no code changes or \ndeployment method changes are possible at this time. The company needs a solution that \nminimizes operational overhead. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB- compatible databases in the cloud. With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB. https://docs.aws.amazon.com/documentdb/latest/developerguide/what-is.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company selves a dynamic website from a fleet of Amazon EC2 instances behind an \nApplication Load Balancer (ALB). The website needs to support multiple languages to serve \ncustomers around the world. The website's architecture is running in the us-west-1 Region and is \nexhibiting high request latency tor users that are located in other parts of the world. The website \nneeds to serve requests quickly and efficiently regardless of a user's location. However the \ncompany does not want to recreate the existing architecture across multiple Regions.  \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Replace the existing architecture with a website that is served from an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon CloudFront distribution with the ALB as the origin.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon API Gateway API that is integrated with the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nConfiguring caching based on the language of the viewer. If you want CloudFront to cache different versions of your objects based on the language specified in the request, configure CloudFront to forward the Accept-Language header to your origin. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 54,
    "text": "A telemarketing company is designing its customer call center functionality on AWS. The \ncompany needs a solution to provides multiples ipsafcar recognition and generates transcript \nfiles. The company wants to query the transcript files to analyze the business patterns. The \ntranscript files must be stored for 7 years for auditing policies. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Rekognition for multiple speaker recognition.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Transcribe for multiple speaker recognition.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Translate for multiple speaker recognition.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Rekognition for multiple speaker recognition.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon Transcribe now supports speaker labeling for streaming transcription. Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy for you to convert speech-to-text. In live audio transcription, each stream of audio may contain multiple speakers. Now you can conveniently turn on the ability to label speakers, thus helping to identify who is saying what in the output transcript. https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-transcribe-supports-speaker- labeling-streaming-transcription/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store \nall the data for 5 years and must delete all the data after 5 years. The company also must \nindefinitely keep audit logs of actions that are performed within the database. Currently, the \ncompany has automated backups configured for Aurora. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n130",
    "options": [
      {
        "id": 0,
        "text": "Take a manual snapshot of the DB cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a lifecycle policy for the automated backups.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure automated backup retention for 5 years.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon CloudWatch Logs export for the DB cluster.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use AWS Backup to take the backups and to keep the backups for 5 years.",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Backup adds Amazon Aurora database cluster snapshots as its latest protected resource. Starting today, you can use AWS Backup to manage Amazon Aurora database cluster snapshots. AWS Backup can centrally configure backup policies, monitor backup activity, copy a snapshot within and across AWS regions, except for China regions, where snapshots can only be copied from one China region to another.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A company has a small Python application that processes JSON documents and outputs the \nresults to an on-premises SQL database. The application runs thousands of times each day. The \ncompany wants to move the application to the AWS Cloud. The company needs a highly \navailable solution that maximizes scalability and minimizes operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Place the JSON documents in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Place the JSON documents in an Amazon S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy placing the JSON documents in an S3 bucket, the documents will be stored in a highly durable and scalable object storage service. The use of AWS Lambda allows the company to run their Python code to process the documents as they arrive in the S3 bucket without having to worry about the underlying infrastructure. This also allows for horizontal scalability, as AWS Lambda will automatically scale the number of instances of the function based on the incoming rate of requests. The results can be stored in an Amazon Aurora DB cluster, which is a fully-managed, high-performance database service that is compatible with MySQL and PostgreSQL. This will provide the necessary durability and scalability for the results of the processing. https://aws.amazon.com/rds/aurora/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A company's infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance \nin a single AWS Region. The company wants to back up its data in a separate Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon Machine Images (AMIs) of the EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create Amazon Elastic Block Store (Amazon EBS) snapshots.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCross-Region backup Using AWS Backup, you can copy backups to multiple different AWS Regions on demand or automatically as part of a scheduled backup plan. Cross-Region backup is particularly valuable if you have business continuity or compliance requirements to store backups a minimum distance away from your production data. https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A company is building a new dynamic ordering website. The company wants to minimize server \nmaintenance and patching. The website must be highly available and must scale read and write \ncapacity as quickly as possible to meet changes in user demand. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host static content in Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Host static content in Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host all the website content on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host all the website content on Amazon EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOn-demand mode is a good option if any of the following are true: Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company uses Amazon S3 as its data lake. The company has a new partner that must use \nSFTP to upload data files. A solutions architect needs to implement a highly available SFTP \nsolution that minimizes operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon S3 File Gateway as an SFTP server.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch Amazon EC2 instances in a private subnet in a VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Transfer for SFTP, a fully-managed, highly-available SFTP service. You simply create a server, set up user accounts, and associate the server with one or more Amazon Simple Storage Service (Amazon S3) buckets.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company needs to store contract documents. A contract lasts for 5 years. During the 5-year \nperiod, the company must ensure that the documents cannot be overwritten or deleted. The \ncompany needs to encrypt the documents at rest and rotate the encryption keys automatically \nevery year. \n \nWhich combination of steps should a solutions architect take to meet these requirements with the \nLEAST operational overhead? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Store the documents in Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the documents in Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "You have been given a scope to deploy some AWS infrastructure for a large organisation. The \nrequirements are that you will have a lot of EC2 instances but may need to add more when the \naverage utilization of your Amazon EC2 fleet is high and conversely remove them when CPU \nutilization is low. Which AWS services would be best to use to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Auto Scaling, Amazon CloudWatch and AWS Elastic Beanstalk",
        "correct": false
      },
      {
        "id": 1,
        "text": "Auto Scaling, Amazon CloudWatch and Elastic Load Balancing.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon CloudFront, Amazon CloudWatch and Elastic Load Balancing.",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Elastic Beanstalk , Amazon CloudWatch and Elastic Load Balancing.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAuto Scaling enables you to follow the demand curve for your applications closely, reducing the need to manually provision Amazon EC2 capacity in advance. For example, you can set a condition to add new Amazon EC2 instances in increments to the Auto Scaling group when the average utilization of your Amazon EC2 fleet is high; and similarly, you can set a condition to remove instances in the same increments when CPU utilization is low. If you have predictable load changes, you can set a schedule through Auto Scaling to plan your scaling activities. You can use Amazon CloudWatch to send alarms to trigger scaling activities and Elastic Load Balancing to help distribute traffic to your instances within Auto Scaling groups. Auto Scaling enables you to run your Amazon EC2 fleet at optimal utilization. Reference: http://aws.amazon.com/autoscaling/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 62,
    "text": "of the below mentioned options is not available when an instance is launched by Auto \nScaling with EC2 Classic?",
    "options": [
      {
        "id": 0,
        "text": "Public IP",
        "correct": false
      },
      {
        "id": 1,
        "text": "Elastic IP",
        "correct": true
      },
      {
        "id": 2,
        "text": "Private DNS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Private IP",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAuto Scaling supports both EC2 classic and EC2-VPC. When an instance is launched as a part of EC2 classic, it will have the public IP and DNS as well as the private IP and DNS. Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/GettingStartedTutorial.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 63,
    "text": "A company's application is running on Amazon EC2 instances in a single Region in the event of a \ndisaster a solutions architect needs to ensure that the resources can also be deployed to a \nsecond Region. \nWhich combination of actions should the solutions architect take to accomplish this? (Select \nTWO) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n134",
    "options": [
      {
        "id": 0,
        "text": "Detach a volume on an EC2 instance and copy it to Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch a new EC2 instance from an Amazon Machine image (AMI) in a new Region",
        "correct": true
      },
      {
        "id": 2,
        "text": "Launch a new EC2 instance in a new Region and copy a volume from Amazon S3 to the new",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy an Amazon Machine Image (AMI) of an EC2 instance and specify a different Region for the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon S3 and launch an EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy default, when you create an AMI from an instance, snapshots are taken of each EBS volume attached to the instance. AMIs can launch with multiple EBS volumes attached, allowing you to replicate both an instance's configuration and the state of all the EBS volumes that are attached to that instance. https://aws.amazon.com/premiumsupport/knowledge-center/create-ami-ebs-backed/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A recently acquired company is required to buikl its own infrastructure on AWS and migrate \nmultiple applications to the cloud within a month. \nEach application has approximately 50 TB of data to be transferred. \nAfter the migration is complete this company and its parent company will both require secure \nnetwork connectivity with consistent throughput from their data centers to the applications. \nA solutions architect must ensure one-time data migration and ongoing network connectivity. \nWhich solution will meet these requirements''",
    "options": [
      {
        "id": 0,
        "text": "AWS Direct Connect for both the initial transfer and ongoing connectivity",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Site-to-Site VPN for both the initial transfer and ongoing connectivity",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball for the initial transfer and AWS Direct Connect for ongoing connectivity",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Snowball for the initial transfer and AWS Site-to-Site VPN for ongoing connectivity",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\n\"Each application has approximately 50 TB of data to be transferred\" = AWS Snowball; \"secure network connectivity with consistent throughput from their data centers to the applications\" What are the benefits of using AWS Direct Connect and private network connections? In many circumstances, private network connections can reduce costs, increase bandwidth, and provide a more consistent network experience than Internet-based connections. \"more consistent network experience\", hence AWS Direct Connect. Direct Connect is better than VPN; reduced cost+increased bandwith+(remain connection or consistent network) = direct connect\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  }
]