[
  {
    "id": 0,
    "text": "A company has an API that receives real-time data from a fleet of monitoring devices. The API \nstores this data in an Amazon RDS DB instance for later analysis. The amount of data that the \nmonitoring devices send to the API fluctuates. During periods of heavy traffic, the API often \nreturns timeout errors. \nAfter an inspection of the logs, the company determines that the database is not capable of \nprocessing the volume of write traffic that comes from the API. A solutions architect must \nminimize the number of connections to the database and must ensure that data is not lost during \nperiods of heavy traffic. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the size of the DB instance to an instance type that has more available memory.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by introducing a queue between the API and the database. Amazon SQS acts as a buffer, allowing the API to quickly enqueue incoming data without waiting for the database to process it immediately. This reduces the load on the database during peak traffic, preventing timeout errors and minimizing the number of direct connections required from the API to the database at any given time. SQS also ensures that data is not lost because it stores messages durably until they are processed by a consumer (in this case, a process that writes to the RDS database).\n\n**Why option 0 is incorrect:**\nWhile increasing the DB instance size might temporarily alleviate the performance issues, it doesn't address the root cause of the problem, which is the direct and synchronous write load from the API. It's a vertical scaling approach that can become expensive and may not be sufficient to handle future traffic increases. It also doesn't minimize the number of connections; in fact, it could potentially increase them as the larger instance could handle more concurrent connections, but the API would still be directly connected and potentially overwhelmed during peak loads.\n\n**Why option 1 is incorrect:**\nModifying the DB instance to be Multi-AZ improves availability and provides failover capabilities, but it doesn't directly address the write performance bottleneck. The primary database instance will still be responsible for handling all write operations, and the synchronous replication to the standby instance will add overhead. Configuring the application to write to all instances is not a standard or supported configuration for RDS Multi-AZ deployments and would likely lead to data inconsistencies and conflicts. It does not minimize the number of connections or prevent data loss during periods of heavy traffic.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 1,
    "text": "A company manages its own Amazon EC2 instances that run MySQL databases. The company \nis manually managing replication and scaling as demand increases or decreases. The company \nneeds a new solution that simplifies the process of adding or removing compute capacity to or \nfrom its database tier as needed. The solution also must offer improved performance, scaling, \nand durability with minimal effort from operations. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Combine the databases into one larger MySQL database. Run the larger database on larger EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct solution because Amazon Aurora Serverless for Aurora MySQL provides automatic scaling of compute capacity based on application needs. It also offers improved performance and durability compared to traditional MySQL databases running on EC2, and it significantly reduces operational overhead by automating tasks such as patching, backups, and replication. The question specifically mentions MySQL, making Aurora MySQL a natural fit.\n\n**Why option 1 is incorrect:**\nWhile Amazon Aurora Serverless for Aurora PostgreSQL also offers automatic scaling, improved performance, and durability, the question specifically states that the company is currently using MySQL databases. Migrating to PostgreSQL would involve significant application code changes and database schema conversions, adding complexity and negating the requirement for minimal effort from operations.\n\n**Why option 2 is incorrect:**\nCombining databases into a single larger MySQL instance on a larger EC2 instance does not address the scaling challenges. It simply moves the problem to a larger instance. It doesn't provide automatic scaling, improved durability (single point of failure), or reduced operational overhead. Manual scaling would still be required, and performance could be negatively impacted due to resource contention.\n\n**Why option 3 is incorrect:**\nCreating an EC2 Auto Scaling group for the database tier addresses the scaling requirement to some extent, but it doesn't fully meet all the needs. While it automates the addition and removal of EC2 instances, it doesn't inherently improve performance or durability compared to the existing setup. The company would still need to manage replication, backups, patching, and other operational tasks manually. Aurora Serverless provides a more comprehensive solution with built-in performance enhancements and automated management.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "A company is concerned that two NAT instances in use will no longer be able to support the \ntraffic needed for the company’s application. A solutions architect wants to implement a solution \nthat is highly available, fault tolerant, and automatically scalable. \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Remove the two NAT instances and replace them with two NAT gateways in the same Availability",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different",
        "correct": false
      },
      {
        "id": 2,
        "text": "Remove the two NAT instances and replace them with two NAT gateways in different Availability",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements of high availability, fault tolerance, and automatic scalability. NAT Gateways are managed services by AWS and are inherently highly available within an Availability Zone. By deploying NAT Gateways in different Availability Zones, the solution becomes fault-tolerant, as the failure of one AZ will not impact the ability of instances in other AZs to access the internet. NAT Gateways also automatically scale to handle increased traffic, removing the need for manual intervention.\n\n**Why option 0 is incorrect:**\nWhile using NAT Gateways is a good approach, placing both NAT Gateways in the *same* Availability Zone negates the fault tolerance requirement. If that Availability Zone experiences an outage, the entire application loses internet connectivity.\n\n**Why option 1 is incorrect:**\nWhile Auto Scaling groups and Network Load Balancers can provide scalability and availability for NAT instances, this approach involves more management overhead than using NAT Gateways. It requires configuring and maintaining the Auto Scaling group, the Network Load Balancer, and the NAT instances themselves. NAT Gateways are managed services that handle scaling and availability automatically, making them a simpler and more efficient solution. Also, this option does not directly address fault tolerance as effectively as using NAT Gateways in different AZs.\n\n**Why option 3 is incorrect:**\nSpot Instances are cost-effective but can be terminated with short notice. Relying on Spot Instances for a critical component like NAT can lead to unpredictable disruptions in internet connectivity. This does not meet the requirement for high availability and fault tolerance. Also, this option does not provide automatic scaling.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The \napplication requires access to a database in VPC B. Both VPCs are in the same AWS account. \nWhich solution will provide the required access MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Create a DB instance security group that allows all traffic from the public IP address of the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a VPC peering connection between VPC A and VPC B.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Make the DB instance publicly accessible. Assign a public IP address to the DB instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by establishing a private network connection between VPC A and VPC B. VPC peering allows network traffic to route between the VPCs using private IP addresses, without exposing the traffic to the public internet. This is more secure than using public IP addresses or internet gateways. Security groups can then be used to further restrict access to the database instance to only the EC2 instance in VPC A.\n\n**Why option 0 is incorrect:**\nThis is incorrect because allowing all traffic from a public IP address (even if it's an Elastic IP) to a database instance's security group is a security risk. Public IP addresses are exposed to the internet, making the database vulnerable to unauthorized access and potential attacks. Security groups should be configured to allow traffic only from specific, trusted sources, and relying solely on a public IP address is not a secure practice.\n\n**Why option 2 is incorrect:**\nThis is incorrect because making the DB instance publicly accessible introduces a significant security risk. Exposing the database to the public internet increases the attack surface and makes it vulnerable to unauthorized access and data breaches. While a public IP address might provide connectivity, it compromises the security posture of the database.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while this approach could technically work, it adds unnecessary complexity and cost. Launching another EC2 instance as a proxy introduces an additional point of failure and requires managing and maintaining the proxy instance. VPC peering provides a more direct and efficient way to establish secure connectivity between the two VPCs without the overhead of a proxy instance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company runs demonstration environments for its customers on Amazon EC2 instances. Each \nenvironment is isolated in its own VPC. The company’s operations team needs to be notified \nwhen RDP or SSH access to an environment has been established.",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the EC2 instances with an IAM instance profile that has an IAM role with the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by capturing network traffic information through VPC Flow Logs. By publishing these logs to CloudWatch Logs, we can create metric filters that specifically look for traffic on ports 22 (SSH) and 3389 (RDP). When traffic is detected on these ports, the metric filter will increment a counter. An alarm can then be created based on this metric, triggering a notification to the operations team when the counter exceeds a certain threshold (e.g., one or more connections). This provides a near real-time alerting mechanism for RDP and SSH access.\n\n**Why option 0 is incorrect:**\nCloudWatch Application Insights is designed for monitoring the health and performance of applications. While it can provide insights into application behavior, it is not the most direct or efficient way to monitor network connections like RDP and SSH. Configuring it to create Systems Manager OpsItems for every RDP/SSH connection would likely generate a large number of OpsItems, making it difficult to manage and potentially increasing costs. Additionally, it requires more configuration and integration compared to using VPC Flow Logs and CloudWatch Logs.\n\n**Why option 1 is incorrect:**\nAn IAM instance profile is used to grant permissions to the EC2 instance itself, allowing it to interact with other AWS services. While proper IAM roles are essential for security, they do not directly provide a mechanism for monitoring network connections. This option does not address the core requirement of detecting and alerting on RDP/SSH access.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company is building a new web-based customer relationship management application. The \napplication will use several Amazon EC2 instances that are backed by Amazon Elastic Block \nStore (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will \nalso use an Amazon Aurora database. All data for the application must be encrypted at rest and \nin transit. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the AWS root account to log in to the AWS Management Console. Upload the company’s",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement of encrypting data at rest by using AWS KMS to encrypt both the EBS volumes attached to the EC2 instances and the Aurora database. AWS KMS provides a managed and secure way to create and control the encryption keys used to protect the data. Additionally, Aurora supports encryption at rest using KMS. This option also implicitly covers encryption in transit as HTTPS is typically configured on the ALB, and Aurora connections can be configured to use TLS.\n\n**Why option 0 is incorrect:**\nUsing AWS KMS certificates directly on the ALB is not the standard or recommended way to handle encryption in transit. The ALB uses TLS/SSL certificates, typically obtained from AWS Certificate Manager (ACM) or imported into ACM, to encrypt traffic between clients and the ALB. While KMS can be used to protect the certificates themselves, it doesn't directly provide certificates for the ALB.\n\n**Why option 1 is incorrect:**\nUsing the AWS root account for logging in and uploading TLS certificate keys is a security anti-pattern. The root account should be used only for initial setup and break-glass scenarios. Uploading TLS certificates directly to the root account is not a secure way to manage certificates. AWS Certificate Manager (ACM) is the recommended service for managing TLS certificates.\n\n**Why option 3 is incorrect:**\nBitLocker is a full disk encryption feature available in Windows operating systems. While it can encrypt data at rest on EC2 instances running Windows, it doesn't address the encryption of the Aurora database. Also, managing BitLocker keys in AWS can be complex and is not the most efficient or recommended approach compared to using AWS KMS. Importing the company's TLS certificate keys to AWS Key Management Service (AWS KMS) is a valid step for managing certificates, but BitLocker is not the best solution for the overall requirement.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A solutions architect has created a new AWS account and must secure AWS account root user \naccess. \nWhich combination of actions will accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Ensure the root user uses a strong password.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable multi-factor authentication to the root user.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store root user access keys in an encrypted Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add the root user to a group containing administrative permissions.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Apply the required permissions to the root user with an inline policy document.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a strong password is a fundamental security measure for any user account, especially the root user account, which has complete administrative privileges. A strong password makes it significantly harder for unauthorized individuals to gain access through brute-force attacks or password guessing.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while enabling multi-factor authentication (MFA) for the root user is an *essential* security best practice, the question asks for *two* actions, and only one is correct. The question states that only one answer is correct, and since option 0 is correct, option 1 must be incorrect.\n\n**Why option 2 is incorrect:**\nThis is incorrect because storing root user access keys is strongly discouraged. The root user should be used only for specific account and service management tasks that require root privileges. Access keys for the root user should never be created or stored. Using IAM users with appropriate permissions is the recommended approach.\n\n**Why option 3 is incorrect:**\nThis is incorrect because adding the root user to a group is not a valid action. The root user is a distinct entity and cannot be managed like an IAM user. IAM users are added to groups to inherit permissions, but this concept does not apply to the root user.\n\n**Why option 4 is incorrect:**\nThis is incorrect because applying permissions to the root user with an inline policy is not a standard or recommended practice. The root user inherently has all permissions. Applying an inline policy would not restrict or enhance the root user's existing capabilities.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company has a three-tier application for image sharing. The application uses an Amazon EC2 \ninstance for the front-end layer, another EC2 instance for the application layer, and a third EC2 \ninstance for a MySQL database. A solutions architect must design a scalable and highly available \nsolution that requires the least amount of change to the application. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by providing scalability and high availability for both the front-end and application tiers using Elastic Beanstalk. Elastic Beanstalk simplifies the deployment and management of web applications and services. Using load-balanced Multi-AZ environments automatically distributes traffic across multiple Availability Zones, ensuring high availability. The load balancer handles traffic distribution, and Elastic Beanstalk manages the underlying EC2 instances, scaling them up or down based on demand. This approach minimizes changes to the application code itself, as Elastic Beanstalk handles the infrastructure management.\n\n**Why option 0 is incorrect:**\nWhile using S3 for the front-end can improve scalability and reduce the load on the EC2 instance, it requires significant changes to the application architecture. The front-end would need to be refactored to serve static content from S3. Using Lambda functions for the application layer also requires a major rewrite of the application logic, as it would need to be converted into serverless functions. This option does not minimize changes to the application.\n\n**Why option 1 is incorrect:**\nWhile using load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer is a good start, it doesn't address the application layer. The question specifies that both the front-end and application layers need to be scalable and highly available. This option only addresses the front-end, leaving the application layer as a single EC2 instance, which is a single point of failure and doesn't scale well.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company wants to experiment with individual AWS accounts for its engineer team. The \ncompany wants to be notified as soon as the Amazon EC2 instance usage for a given month \nexceeds a specific threshold for each account. \nWhat should a solutions architect do to meet this requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using AWS Budgets to create a cost budget for each account. Setting the period to monthly aligns with the requirement of monitoring monthly usage. Configuring notifications allows the company to be alerted when the EC2 instance usage exceeds the specified threshold. AWS Budgets is a cost-effective service designed for this purpose.\n\n**Why option 0 is incorrect:**\nWhile Cost Explorer can generate reports, creating a daily report is not the most cost-effective solution for monthly threshold monitoring. It would require more frequent analysis and potentially lead to unnecessary overhead. Also, it does not provide built-in alerting capabilities like AWS Budgets.\n\n**Why option 1 is incorrect:**\nWhile Cost Explorer can generate monthly reports, it doesn't provide a built-in alerting mechanism to notify when a specific threshold is exceeded. It requires manual review of the report, which is not efficient or automated.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 9,
    "text": "A solutions architect needs to design a new microservice for a company’s application. Clients \nmust be able to call an HTTPS endpoint to reach the microservice. The microservice also must \nuse AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect \nwill write the logic for this microservice by using a single AWS Lambda function that is written in \nGo 1.x. \nWhich solution will deploy the function in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon API Gateway provides a managed service for creating HTTPS endpoints that can invoke Lambda functions. It also natively supports IAM authentication, allowing clients to authenticate using IAM roles or users. API Gateway handles the complexities of scaling, security, and monitoring, making it the most operationally efficient solution for exposing a Lambda function as a microservice with IAM authentication. It provides features like request validation, throttling, and caching that contribute to operational efficiency.\n\n**Why option 1 is incorrect:**\nWhile Lambda function URLs with AWS_IAM authentication are a valid option, they lack many of the features provided by API Gateway, such as request validation, throttling, custom domain names, and detailed monitoring. This makes it less operationally efficient for managing a microservice in the long run. API Gateway is a more robust and scalable solution for production environments.\n\n**Why option 2 is incorrect:**\nCloudFront with Lambda@Edge is primarily used for content delivery and edge processing. While Lambda@Edge can perform authentication, it's not the primary use case, and it adds complexity compared to using API Gateway for a simple microservice endpoint. Furthermore, managing Lambda@Edge functions can be more complex than managing Lambda functions invoked by API Gateway. Lambda@Edge is more suitable for modifying content or behavior at the edge, not for serving as the main endpoint for a microservice.\n\n**Why option 3 is incorrect:**\nCloudFront Functions are even more limited than Lambda@Edge and are designed for lightweight transformations and manipulations of HTTP requests and responses. They cannot directly integrate with IAM for authentication or execute complex business logic. This option is not suitable for the requirements of the question.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company previously migrated its data warehouse solution to AWS. The company also has an \nAWS Direct Connect connection. Corporate office users query the data warehouse using a \nvisualization tool. The average size of a query returned by the data warehouse is 50 MB and \neach webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the \ndata warehouse are not cached. \nWhich solution provides the LOWEST data transfer egress cost for the company?",
    "options": [
      {
        "id": 0,
        "text": "Host the visualization tool on premises and query the data warehouse directly over the internet.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host the visualization tool on premises and query the data warehouse directly over a Direct",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the visualization tool in the same AWS Region as the data warehouse and access it over a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by minimizing data transfer costs. Hosting the visualization tool in the same AWS Region as the data warehouse means that data transfer between the two services stays within the AWS Region. Data transfer within the same AWS Region is significantly cheaper (and often free for certain services) compared to data transfer out of the region (egress) to the internet or over Direct Connect. Since the visualization tool generates webpage data (500KB) and the data warehouse returns query results (50MB), keeping both within the same region avoids egress charges for both types of data transfer.\n\n**Why option 0 is incorrect:**\nThis is incorrect because querying the data warehouse directly over the internet incurs significant data transfer egress costs. Each 50MB query result would be charged at the standard internet data transfer rates, which are generally higher than intra-region data transfer costs. Also, the 500KB webpage data would be transferred over the internet, adding to the egress charges.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while it keeps the visualization tool and data warehouse in the same region, it doesn't explicitly state that the access is over a private network within AWS. Data transfer between AWS services in the same region is generally free or very low cost, but if the communication is routed through the internet (even within the same region), it would incur egress charges. The absence of a private network connection (like VPC peering or a private subnet) makes this option less cost-effective than option 3.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 11,
    "text": "An online learning company is migrating to the AWS Cloud. The company maintains its student \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n169 \nrecords in a PostgreSQL database. The company needs a solution in which its data is available \nand online across multiple AWS Regions at all times. \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using Amazon RDS for PostgreSQL, which is a managed service, thus reducing operational overhead. Creating a read replica in another AWS Region provides cross-region data availability and disaster recovery capabilities. Promoting the read replica to a standalone instance in the event of a regional failure allows the company to maintain data availability across multiple AWS Regions at all times. This approach offers a good balance between availability, cost, and operational overhead.\n\n**Why option 0 is incorrect:**\nRunning a PostgreSQL cluster on Amazon EC2 instances involves significant operational overhead, including patching, backups, scaling, and failure recovery. This contradicts the requirement for the least amount of operational overhead. While it can achieve multi-region availability, it requires manual configuration and management, making it a less desirable solution compared to managed services.\n\n**Why option 1 is incorrect:**\nWhile Amazon RDS for PostgreSQL with Multi-AZ provides high availability within a single AWS Region, it does not provide cross-region availability. The question specifically requires data to be available and online across multiple AWS Regions at all times. Multi-AZ is designed for fault tolerance within a single region, not for regional disasters or cross-region failover. Therefore, this option does not fully meet the requirements.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A medical research lab produces data that is related to a new study. The lab wants to make the \ndata available with minimum latency to clinics across the country for their on-premises, file-based \napplications. The data files are stored in an Amazon S3 bucket that has read-only permissions for \neach clinic. \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the files to each clinic’s on-premises applications by using AWS DataSync for processing.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic’s on-premises",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirements by deploying a Storage Gateway file gateway at each clinic. The file gateway provides a local cache of the S3 data, enabling low-latency access for the on-premises applications. It presents the S3 data as a local file share (using NFS or SMB), which is compatible with the clinics' existing file-based applications. The file gateway handles the data transfer between S3 and the local cache, ensuring that the clinics always have access to the latest data. Since the clinics have read-only permissions, the file gateway will only be used to read data from S3.\n\n**Why option 1 is incorrect:**\nWhile AWS DataSync can transfer files from S3 to on-premises locations, it is designed for bulk data transfers and synchronization, not for providing low-latency access to frequently accessed data. It would require repeated data transfers, which would introduce latency and be inefficient. Also, it involves migrating files to the clinic's applications, which may not be desirable or feasible.\n\n**Why option 2 is incorrect:**\nA Storage Gateway volume gateway presents data as block storage volumes (iSCSI), not as files. The clinics' applications are file-based, so a volume gateway would not be compatible without significant modifications to the applications. This would require the clinics to rewrite their applications to work with block storage, which is not desirable.\n\n**Why option 3 is incorrect:**\nAmazon EFS is a fully managed NFS file system service that is designed for use with EC2 instances. It cannot be directly attached to on-premises applications. While it is possible to create a VPN or Direct Connect connection and mount the EFS file system on-premises, this would introduce significant latency and complexity, and it is not the best solution for providing low-latency access to data for file-based applications.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company is using a content management system that runs on a single Amazon EC2 instance. \nThe EC2 instance contains both the web server and the database software. The company must \nmake its website platform highly available and must enable the website to scale to meet user \ndemand. \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the database to Amazon RDS, and enable automatic backups. Manually launch another",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon Aurora instance with a read replica in the same Availability",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by moving the database to Amazon Aurora, which provides high availability and scalability. The read replica in another Availability Zone ensures that the database remains available even if one Availability Zone fails. Creating an Auto Scaling group for the web server EC2 instances behind an Application Load Balancer (ALB) distributes traffic across multiple instances, providing both scalability and high availability for the web tier. The ALB also provides health checks, ensuring that only healthy instances receive traffic.\n\n**Why option 0 is incorrect:**\nWhile moving the database to Amazon RDS and enabling automatic backups is a good practice for data durability, it doesn't inherently provide high availability. Manually launching another EC2 instance does not provide automatic scaling or failover capabilities, which are essential for meeting the requirements. This option lacks automated scaling and failover for both the web and database tiers.\n\n**Why option 1 is incorrect:**\nMigrating the database to an Amazon Aurora instance with a read replica in the *same* Availability Zone improves read performance but does not provide high availability in the event of an Availability Zone failure. If the entire Availability Zone goes down, both the primary and read replica would be unavailable. This option also doesn't address the scalability and availability of the web server tier.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company is launching an application on AWS. The application uses an Application Load \nBalancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single target group. The \ninstances are in an Auto Scaling group for each environment. The company requires a \ndevelopment environment and a production environment. The production environment will have \nperiods of high traffic. \nWhich solution will configure the development environment MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Reconfigure the target group in the development environment to have only one EC2 instance as a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the ALB balancing algorithm to least outstanding requests.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Reduce the size of the EC2 instances in both environments.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Reduce the maximum number of EC2 instances in the development environment’s Auto Scaling",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the most cost-effective solution because the development environment likely doesn't need to handle production-level traffic. Reducing the maximum number of EC2 instances in the Auto Scaling group limits the number of instances that can be launched, thereby reducing the overall cost when demand increases. This directly addresses the cost optimization requirement for the development environment without impacting the production environment's ability to scale to meet high traffic demands.\n\n**Why option 0 is incorrect:**\nWhile reducing the number of EC2 instances to one in the target group might seem cost-effective, it introduces a single point of failure. If that single instance fails, the development environment becomes unavailable. The question specifies at least two instances initially, implying a need for some level of redundancy, even in development. Removing that redundancy entirely is not a good practice, especially since Auto Scaling groups are designed to maintain a desired capacity. Furthermore, the ALB requires at least one healthy instance to be registered in the target group to function correctly. While it might technically work, it's not the *most* cost-effective solution that maintains a reasonable level of availability.\n\n**Why option 1 is incorrect:**\nChanging the ALB balancing algorithm to least outstanding requests doesn't directly address the cost optimization requirement. The balancing algorithm affects how traffic is distributed among the instances, but it doesn't reduce the number of instances running or the size of the instances. Therefore, it won't significantly reduce costs. The cost is primarily driven by the number and size of EC2 instances running.\n\n**Why option 2 is incorrect:**\nReducing the size of the EC2 instances in both environments would reduce costs in both environments, but it doesn't differentiate between the development and production environments. The production environment needs to handle periods of high traffic, so reducing the instance size might negatively impact its performance and ability to scale effectively. The question asks for a solution that configures the development environment *most* cost-effectively, implying a solution that primarily targets the development environment without negatively impacting the production environment.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 15,
    "text": "A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The \nEC2 instances are in private subnets. A solutions architect implements an internet-facing \nApplication Load Balancer (ALB) and specifies the EC2 instances as the target group. However, \nthe internet traffic is not reaching the EC2 instances. \nHow should the solutions architect reconfigure the architecture to resolve this issue? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n171",
    "options": [
      {
        "id": 0,
        "text": "Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the EC2 instances to public subnets. Add a rule to the EC2 instances’ security groups to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the route tables for the EC2 instances’ subnets to send 0.0.0.0/0 traffic through the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create public subnets in each Availability Zone. Associate the public subnets with the ALB.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by placing the ALB in public subnets. An internet-facing ALB needs to be in public subnets to receive traffic from the internet. By associating the public subnets with the ALB, it can receive traffic from the internet gateway. The ALB can then forward the traffic to the EC2 instances in the private subnets, assuming the security groups and network ACLs are configured correctly to allow this communication. Creating public subnets in each Availability Zone ensures high availability for the ALB.\n\n**Why option 0 is incorrect:**\nReplacing the ALB with a Network Load Balancer (NLB) does not solve the fundamental problem of the ALB needing to be in public subnets to receive internet traffic. While an NLB can handle TCP, UDP, and TLS traffic, it still needs to be in subnets that are routable to the internet. Configuring a NAT gateway is necessary for the EC2 instances to initiate outbound traffic to the internet, but it doesn't allow the internet to initiate inbound traffic to the EC2 instances directly or the ALB.\n\n**Why option 1 is incorrect:**\nMoving the EC2 instances to public subnets is generally not a security best practice. EC2 instances that do not need to be directly accessible from the internet should reside in private subnets. Exposing the EC2 instances directly to the internet increases the attack surface and requires more stringent security measures. While adding a rule to the EC2 instances’ security groups to allow traffic from the ALB is necessary regardless of the subnet type, moving the instances to public subnets is not the correct approach.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, \nthe database support team is reporting slow reads against the DB instance and recommends \nadding a read replica. \nWhich combination of actions should a solutions architect take before implementing this change? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable binlog replication on the RDS primary node.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Choose a failover priority for the source DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow long-running transactions to complete on the source DB instance.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a global table and specify the AWS Regions where the table will be available.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Enable automatic backups on the source instance by setting the backup retention period to a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because allowing long-running transactions to complete on the source DB instance before creating a read replica is crucial for data consistency. If a long-running transaction is in progress during the replica creation, the replica might be created with incomplete or inconsistent data, leading to data discrepancies between the primary and replica. Waiting for these transactions to complete ensures a consistent snapshot for the replica.\n\n**Why option 0 is incorrect:**\nThis is incorrect because binary logging is automatically enabled for RDS MySQL instances when backups are enabled. You don't need to manually enable it before creating a read replica. The question specifies that the database is already deployed, so it's implied that backups are already configured.\n\n**Why option 1 is incorrect:**\nThis is incorrect because failover priority is relevant for Multi-AZ deployments, not read replicas. Failover priority determines which replica becomes the primary in case of a failure in a Multi-AZ setup. Read replicas are primarily for scaling read operations, not for high availability in the same way as Multi-AZ deployments.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A company runs analytics software on Amazon EC2 instances. The software accepts job \nrequests from users to process data that has been uploaded to Amazon S3. Users report that \nsome submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances \nhave a consistent CPU utilization at or near 100%. The company wants to improve system \nperformance and scale the system based on user load. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a copy of the instance. Place all instances behind an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more",
        "correct": false
      },
      {
        "id": 3,
        "text": "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the problem of high CPU utilization and lack of scalability by introducing a queue (SQS) to buffer incoming requests. This decouples the request submission from the processing, allowing the EC2 instances to process jobs at their own pace. An Auto Scaling group configured to scale based on the queue depth will automatically add or remove EC2 instances to handle the workload, ensuring that the system can scale based on user load. This prevents the EC2 instances from being overwhelmed and ensures that all submitted data is eventually processed.\n\n**Why option 0 is incorrect:**\nSimply creating a copy of the instance and placing them behind an Application Load Balancer will distribute the load, but it doesn't address the fundamental problem of the instances being CPU-bound. If the instances are already at 100% CPU utilization, adding more instances without addressing the root cause will only delay the problem, not solve it. The instances will still be overwhelmed, and the system won't scale effectively based on user load. It also doesn't decouple the request submission from the processing.\n\n**Why option 1 is incorrect:**\nCreating an S3 VPC endpoint improves security and network performance by allowing EC2 instances to access S3 without traversing the public internet. However, it does not address the CPU utilization issue or provide a mechanism for scaling based on user load. The bottleneck is the processing power of the EC2 instances, not the network connection to S3. While a VPC endpoint is a good practice, it doesn't solve the core problem in this scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 18,
    "text": "A company is implementing a shared storage solution for a media application that is hosted in the \nAWS Cloud. The company needs the ability to use SMB clients to access data. The solution must \nbe fully managed. \nWhich AWS solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by providing a fully managed Windows file server. Amazon FSx for Windows File Server is a fully managed service that supports the SMB protocol, allowing SMB clients to access the data. It eliminates the need to manage the underlying infrastructure, operating system, and file server software.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because AWS Storage Gateway volume gateway presents block storage to on-premises applications. While it can be used to store data in AWS, it doesn't directly provide SMB access. It requires additional configuration and management on the client side to expose the storage as an SMB share, failing the 'fully managed' requirement. Also, volume gateway is more suited for disaster recovery or backup scenarios, not for shared storage for a media application.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because AWS Storage Gateway tape gateway is designed for archiving data to Amazon S3 or Glacier. It emulates a tape library and is not suitable for providing shared storage access via SMB. It's specifically for backup and archival purposes, not for active media application storage.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because creating an EC2 instance and configuring a Windows file share involves significant manual configuration and management. This includes patching the operating system, managing the file server software, and ensuring high availability. This contradicts the requirement for a 'fully managed' solution. While it provides SMB access, it's not a managed service.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company’s security team requests that network traffic be captured in VPC Flow Logs. The logs \nwill be frequently accessed for 90 days and then accessed intermittently. \nWhat should a solutions architect do to meet these requirements when configuring the logs?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using Amazon S3 as the target for VPC Flow Logs. S3 is a cost-effective and scalable storage solution suitable for storing large volumes of log data. Enabling an S3 Lifecycle policy allows for the automatic transition of logs to a cheaper storage tier (like S3 Standard-IA or S3 Glacier) after 90 days, which aligns with the requirement of infrequent access after the initial period. This approach optimizes costs while ensuring the logs remain accessible when needed.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while CloudWatch can be used as a target for VPC Flow Logs, it is generally more expensive than S3 for long-term storage, especially for large volumes of data. Setting an expiration of 90 days would delete the logs entirely, failing to meet the requirement of intermittent access after 90 days. CloudWatch is better suited for real-time monitoring and alerting, not long-term archival.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Kinesis Data Streams are designed for real-time data processing and analysis, not long-term storage of log data. While Kinesis can retain data, it is not cost-effective for storing logs for 90 days, especially when intermittent access is required after that period. Kinesis is more appropriate for scenarios where immediate processing of streaming data is necessary.\n\n**Why option 2 is incorrect:**\nThis is incorrect because AWS CloudTrail is designed to track API calls made to AWS services, not to capture network traffic data like VPC Flow Logs. CloudTrail logs provide information about who did what, when, and from where within your AWS environment. It does not capture the details of network traffic flowing through your VPC.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect needs to design a system to store client case files. The files are core \ncompany assets and are important. The number of files will grow over time. \nThe files must be simultaneously accessible from multiple application servers that run on Amazon \nEC2 instances. The solution must have built-in redundancy. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Backup",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon EFS is a fully managed, scalable, elastic, shared file system that makes it simple to set up, scale, and cost-optimize file storage in the AWS Cloud. It can be mounted on multiple EC2 instances simultaneously, providing shared access. EFS is designed with built-in redundancy and durability, making it suitable for storing important company assets.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon EBS is a block storage volume that is attached to a single EC2 instance at a time. It does not natively support simultaneous access from multiple EC2 instances. While you can share EBS volumes using complex setups like clustering, it's not the simplest or most efficient solution for shared file access.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon S3 Glacier Deep Archive is a low-cost storage class designed for long-term data archiving. It is not suitable for frequently accessed files or simultaneous access from multiple EC2 instances. Retrieval times are measured in hours, making it unsuitable for this use case.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Backup is a service for centralizing and automating the backup of data across AWS services. While it can be used to back up data stored on EFS or EBS, it doesn't provide the primary storage solution with simultaneous access and built-in redundancy as required by the question.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are \nattached to an IAM group. \n \n \n \n \n \nA cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer \nbe able to perform?",
    "options": [
      {
        "id": 0,
        "text": "Deleting IAM users",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deleting directories",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deleting Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deleting logs from Amazon CloudWatch Logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because the IAM policies attached to the group must grant the permission to delete EC2 instances. IAM policies define what actions are allowed or denied on AWS resources. If Policy1 or Policy2 (or both) include a statement that allows the `ec2:TerminateInstances` action, the user will be able to delete EC2 instances. The question implies this is the only action allowed among the options.\n\n**Why option 0 is incorrect:**\nThis is incorrect because deleting IAM users requires specific IAM permissions, such as `iam:DeleteUser`. It's unlikely that the provided policies would grant this permission without being explicitly designed to do so. The question implies that the policies do not grant this permission.\n\n**Why option 1 is incorrect:**\nThis is incorrect because deleting directories is not a standard AWS action. This implies the question is referring to deleting directories within an EC2 instance or an S3 bucket. Deleting directories within an EC2 instance would depend on the permissions granted to the user on the EC2 instance itself, and deleting directories in S3 would require `s3:DeleteObject` permission. It's unlikely that the policies would grant this permission without being explicitly designed to do so. The question implies that the policies do not grant this permission.\n\n**Why option 3 is incorrect:**\nThis is incorrect because deleting logs from CloudWatch Logs requires specific CloudWatch Logs permissions, such as `logs:DeleteLogStream` or `logs:DeleteLogGroup`. It's unlikely that the provided policies would grant this permission without being explicitly designed to do so. The question implies that the policies do not grant this permission.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is reviewing a recent migration of a three-tier application to a VPC. The security team \ndiscovers that the principle of least privilege is not being applied to Amazon EC2 security group \ningress and egress rules between the application tiers. \nWhat should a solutions architect do to correct this issue?",
    "options": [
      {
        "id": 0,
        "text": "Create security group rules using the instance ID as the source or destination.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create security group rules using the security group ID as the source or destination.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create security group rules using the VPC CIDR blocks as the source or destination.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create security group rules using the subnet CIDR blocks as the source or destination.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct approach because security groups can be used as sources and destinations in security group rules. This allows you to define rules that permit traffic only between specific security groups, representing different tiers of the application. By referencing security groups instead of individual instance IDs or CIDR blocks, you can maintain a more dynamic and manageable security posture. As instances are added or removed within a tier (represented by a security group), the rules automatically apply to the new instances without requiring modification. This adheres to the principle of least privilege by only allowing traffic between the necessary tiers and nothing else.\n\n**Why option 0 is incorrect:**\nUsing instance IDs as the source or destination in security group rules is not a scalable or maintainable solution. Instance IDs are dynamic and change when instances are replaced. This would require constant updates to the security group rules, making it difficult to manage and maintain the security posture. It also doesn't align with the principle of least privilege because it ties security rules to specific instances rather than logical groups of instances.\n\n**Why option 2 is incorrect:**\nUsing VPC CIDR blocks as the source or destination is too broad and violates the principle of least privilege. It would allow traffic from any instance within the entire VPC to communicate with any other instance, which is not desirable in a three-tier application where communication should be restricted between specific tiers. This approach does not provide granular control over network traffic.\n\n**Why option 3 is incorrect:**\nUsing subnet CIDR blocks as the source or destination is also too broad and does not adhere to the principle of least privilege. While it's more restrictive than using the entire VPC CIDR block, it still allows any instance within a subnet to communicate with any other instance, regardless of the application tier. This does not provide the necessary level of isolation between tiers.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A solutions architect is implementing a document review application using an Amazon S3 bucket \nfor storage. The solution must prevent accidental deletion of the documents and ensure that all \nversions of the documents are available. Users must be able to download, modify, and upload \ndocuments. \nWhich combination of actions should be taken to meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable a read-only bucket ACL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable versioning on the bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Attach an IAM policy to the bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable MFA Delete on the bucket.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Encrypt the bucket using AWS KMS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object in the same bucket. When a user uploads a new version of a document, the previous version is retained, and a unique version ID is assigned to each version. This ensures that all versions of the documents are available, satisfying one of the key requirements.\n\n**Why option 0 is incorrect:**\nThis is incorrect because a read-only bucket ACL would prevent users from uploading or modifying documents, which contradicts the requirement that users must be able to download, modify, and upload documents.\n\n**Why option 2 is incorrect:**\nThis is incorrect because attaching an IAM policy to the bucket controls access permissions but doesn't inherently prevent accidental deletion or ensure versioning. While an IAM policy can restrict delete actions, it doesn't provide a mechanism for retaining previous versions of the documents.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while MFA Delete adds a layer of security against malicious deletions by requiring multi-factor authentication, it doesn't protect against accidental deletions by authorized users. Also, MFA Delete can only be enabled by the root account, which is not a best practice. It also doesn't address the requirement of keeping all versions of the documents.\n\n**Why option 4 is incorrect:**\nThis is incorrect because encrypting the bucket using AWS KMS focuses on data security at rest, but it doesn't address the requirements of preventing accidental deletion or ensuring that all versions of the documents are available. Encryption protects the data from unauthorized access but doesn't prevent authorized users from deleting or overwriting it.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company is building a solution that will report Amazon EC2 Auto Scaling events across all the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n175 \napplications in an AWS account. The company needs to use a serverless solution to store the \nEC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 \nto provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 \ninstance launches. \nHow should the company move the data to Amazon S3 to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because CloudWatch Metric Streams allow you to stream metrics, including Auto Scaling metrics, to destinations like S3 in near real-time. This is a serverless solution that doesn't require managing any infrastructure. It also doesn't impact EC2 instance launch speed because the data is streamed asynchronously.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because launching an EMR cluster is not a serverless solution and is overkill for simply collecting and sending Auto Scaling status data. EMR is designed for big data processing and analysis, not event collection and forwarding. It would also be significantly more complex and expensive than necessary.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because using EventBridge on a schedule would not provide near-real-time updates. Scheduled events are not triggered by the Auto Scaling events themselves, but rather at predefined intervals. This would introduce significant latency and not meet the near-real-time requirement. Also, the question asks for EC2 Auto Scaling *status data*, which is best captured by metrics, not events.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because using a bootstrap script to install Kinesis Agent on each EC2 instance would impact the EC2 instance launch speed, which is a constraint in the question. Also, this approach requires managing the Kinesis Agent on each instance, which is not a serverless solution. Furthermore, Kinesis Agent is typically used for streaming logs, not metrics.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company has an application that places hundreds of .csv files into an Amazon S3 bucket every \nhour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the \nfile to Apache Parquet format and place the output file into an S3 bucket. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format,",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by leveraging AWS Glue, a fully managed ETL service. Glue ETL jobs are designed for data transformation tasks like converting between file formats. It scales automatically to handle the volume of files and requires minimal operational overhead because AWS manages the infrastructure and execution. Glue can be triggered by S3 events, ensuring the conversion happens automatically upon file upload. This eliminates the need for manual intervention or managing custom infrastructure.\n\n**Why option 0 is incorrect:**\nUsing a Lambda function for this task would be inefficient and potentially problematic. Lambda functions have execution time limits and memory constraints, which could be exceeded when processing 1 GB files. Furthermore, managing the concurrency and scaling of hundreds of Lambda invocations per hour would add significant operational overhead. While Lambda can be triggered by S3 events, it's not the best choice for large-scale data transformation.\n\n**Why option 1 is incorrect:**\nWhile Apache Spark can handle large-scale data processing, setting up and managing a Spark cluster (e.g., using EMR) would introduce significant operational overhead. This includes managing the cluster's infrastructure, scaling it appropriately, and handling potential failures. Although Spark is powerful, it's overkill for a simple file format conversion task and requires more operational effort than AWS Glue.\n\n**Why option 2 is incorrect:**\nCreating an AWS Glue table and crawler is useful for discovering the schema of the .csv files and making them queryable, but it doesn't directly address the requirement of converting the files to Parquet format. While the crawler can infer the schema, it doesn't perform data transformation. This option is a prerequisite for using Glue ETL, but not a complete solution on its own.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company is implementing new data retention policies for all databases that run on Amazon \nRDS DB instances. The company must retain daily backups for a minimum period of 2 years. The \nbackups must be consistent and restorable. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n176 \nWhich solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution directly addresses the requirements by utilizing AWS Backup. AWS Backup is designed for centralized backup management and data retention. Creating a backup vault allows for secure storage of the backups. A backup plan can be configured to schedule daily backups of the RDS instances and define a retention period of 2 years. This ensures consistent backups are taken and stored for the required duration, and the backups are restorable.\n\n**Why option 1 is incorrect:**\nWhile configuring a backup window for RDS instances creates daily snapshots, it doesn't inherently provide a mechanism for long-term retention of 2 years. RDS snapshots are typically managed within the RDS service and may be subject to accidental deletion or lifecycle management issues if not explicitly handled. Also, simply assigning a snapshot lifecycle policy doesn't guarantee compliance with the 2-year retention requirement in a centrally managed and auditable way.\n\n**Why option 2 is incorrect:**\nConfiguring database transaction logs to be automatically backed up to Amazon CloudWatch Logs is useful for auditing and troubleshooting, but it does not provide a mechanism for creating consistent, restorable database backups. Transaction logs alone are not sufficient for restoring a database to a specific point in time. They need to be used in conjunction with a full or incremental backup.\n\n**Why option 3 is incorrect:**\nAWS DMS is primarily used for database migration and replication, not for long-term backup and retention. While DMS can replicate data to another database, it's not designed as a backup solution and doesn't provide the same level of consistency and restorability as a dedicated backup service. Also, maintaining a continuously running DMS replication task solely for backup purposes would be inefficient and costly.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company’s compliance team needs to move its file shares to AWS. The shares run on a \nWindows Server SMB file share. A self-managed on-premises Active Directory controls access to \nthe files and folders. \nThe company wants to use Amazon FSx for Windows File Server as part of the solution. The \ncompany must ensure that the on-premises Active Directory groups restrict access to the FSx for \nWindows File Server SMB compliance shares, folders, and files after the move to AWS. The \ncompany has created an FSx for Windows File Server file system. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory",
        "correct": false
      },
      {
        "id": 1,
        "text": "Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict",
        "correct": false
      },
      {
        "id": 3,
        "text": "Join the file system to the Active Directory to restrict access.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution directly addresses the requirement of using the existing on-premises Active Directory groups to control access to the FSx file share. Joining the FSx for Windows File Server file system to the Active Directory domain allows the file system to authenticate users and groups against the on-premises directory. This enables the use of existing Active Directory groups to manage permissions on the FSx file shares, folders, and files, thus maintaining the desired access control after the migration.\n\n**Why option 0 is incorrect:**\nActive Directory Connector (ADC) is used to allow AWS services to authenticate against an on-premises Active Directory. While ADC can facilitate authentication, it doesn't directly map Active Directory groups to file share permissions on FSx for Windows File Server. The FSx file system needs to be joined to the domain for native Active Directory integration and permission management. Mapping AD groups is not a function of ADC itself.\n\n**Why option 1 is incorrect:**\nTags are used for metadata and resource management, not for access control. Assigning tags to the FSx file system will not restrict access based on Active Directory groups. Tags are useful for cost allocation, automation, and organization, but they do not provide authentication or authorization capabilities.\n\n**Why option 2 is incorrect:**\nIAM service-linked roles are used to grant permissions to AWS services to perform actions on your behalf. While FSx for Windows File Server uses service-linked roles, they are not directly involved in restricting access to file shares based on Active Directory groups. IAM roles manage permissions for AWS services, not for individual users or groups accessing the file system. The access control needs to be handled through Active Directory integration.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company recently announced the deployment of its retail website to a global audience. The \nwebsite runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances \nrun in an Auto Scaling group across multiple Availability Zones. \nThe company wants to provide its customers with different versions of content based on the \ndevices that the customers use to access the website. \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudFront to cache multiple versions of the content.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a host header in a Network Load Balancer to forward traffic to different instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a Lambda@Edge function to send specific objects to users based on the User-Agent",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB).",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because CloudFront can be configured to cache multiple versions of content based on request headers, including the User-Agent header. This allows CloudFront to serve different versions of the website based on the device type, improving performance and reducing load on the origin servers.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Network Load Balancers (NLBs) operate at Layer 4 (Transport Layer) and do not inspect HTTP headers like User-Agent. NLBs are designed for high throughput and low latency, but they lack the content-based routing capabilities needed for this scenario. Host headers are used for routing traffic to different backends based on the domain name, not the device type.\n\n**Why option 2 is incorrect:**\nThis is correct because Lambda@Edge allows you to run code at CloudFront edge locations. By inspecting the User-Agent header in the Lambda@Edge function, you can modify the request to fetch the appropriate version of the content from the origin or even generate the content dynamically. This provides a flexible and efficient way to serve device-specific content.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Global Accelerator improves the performance of applications by routing traffic through the AWS global network. While it can improve latency, it doesn't provide the functionality to serve different content versions based on the User-Agent header. It simply forwards requests to the specified endpoint (in this case, an NLB), which, as explained before, cannot differentiate traffic based on the User-Agent.\n\n**Why option 4 is incorrect:**\nThis is incorrect because AWS Global Accelerator improves the performance of applications by routing traffic through the AWS global network. While it can improve latency, it doesn't provide the functionality to serve different content versions based on the User-Agent header. It simply forwards requests to the specified endpoint (in this case, an NLB), which, as explained before, cannot differentiate traffic based on the User-Agent.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions \narchitect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s \nAmazon EC2 instances. Both VPCs are in the us-east-1 Region. \nThe solutions architect must implement a solution to provide the application’s EC2 instances with \naccess to the ElastiCache cluster. \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a peering connection between the VPCs. Add a route table entry for the peering",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a peering connection between the VPCs. Add a route table entry for the peering",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because VPC peering is the most cost-effective and straightforward method for enabling communication between two VPCs within the same region. Creating a peering connection establishes a direct networking route between the VPCs. Adding a route table entry in both VPCs directs traffic destined for the other VPC's CIDR block to the peering connection. This avoids the complexity and cost associated with Transit Gateways or Transit VPCs, which are better suited for more complex networking scenarios involving multiple VPCs or cross-region connectivity.\n\n**Why option 1 is incorrect:**\nThis is incorrect because creating a Transit VPC involves deploying additional resources (EC2 instances acting as routers) and managing routing configurations, which adds complexity and cost compared to VPC peering. While a Transit VPC can provide connectivity between multiple VPCs, it's an overkill for this scenario involving only two VPCs. Transit Gateways are generally preferred over Transit VPCs, but even Transit Gateways are more expensive than VPC peering for this simple two-VPC scenario.\n\n**Why option 2 is incorrect:**\nThis is incorrect because the question is incomplete. It does not specify which route table needs to be updated. To establish bidirectional communication, route table entries must be added in both the App VPC and the Cache VPC, pointing to the peering connection as the target for the other VPC's CIDR block. Without specifying both, the option is incomplete and therefore incorrect.\n\n**Why option 3 is incorrect:**\nThis is incorrect because creating a Transit VPC involves deploying additional resources (EC2 instances acting as routers) and managing routing configurations, which adds complexity and cost compared to VPC peering. While a Transit VPC can provide connectivity between multiple VPCs, it's an overkill for this scenario involving only two VPCs. Transit Gateways are generally preferred over Transit VPCs, but even Transit Gateways are more expensive than VPC peering for this simple two-VPC scenario. The question is also incomplete. It does not specify which route table needs to be updated. To establish bidirectional communication, route table entries must be added in both the App VPC and the Cache VPC, pointing to the peering connection as the target for the other VPC's CIDR block. Without specifying both, the option is incomplete and therefore incorrect.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 30,
    "text": "A company is building an application that consists of several microservices. The company has \ndecided to use container technologies to deploy its software on AWS. The company needs a \nsolution that minimizes the amount of ongoing effort for maintenance and scaling. The company \ncannot manage additional infrastructure. \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because deploying an Amazon ECS cluster is the foundational step for running containerized applications using ECS. It provides the environment where your containers will be orchestrated and managed. While it doesn't fully address the 'no infrastructure management' requirement on its own, it's a necessary component when combined with Fargate.\n\n**Why option 1 is incorrect:**\nThis is incorrect because deploying the Kubernetes control plane on EC2 instances requires managing the underlying infrastructure (EC2 instances, networking, etc.). This contradicts the requirement of minimizing infrastructure management.\n\n**Why option 2 is incorrect:**\nThis is incorrect because deploying an ECS service with an EC2 launch type requires managing the underlying EC2 instances. This includes patching, scaling, and general maintenance, which goes against the requirement of minimizing ongoing effort and infrastructure management.\n\n**Why option 3 is incorrect:**\nThis is incorrect because deploying an ECS service with a Fargate launch type directly addresses the requirement of minimizing infrastructure management. Fargate is a serverless compute engine for containers that allows you to run containers without managing servers or clusters. It automatically scales and handles infrastructure concerns.\n\n**Why option 4 is incorrect:**\nThis is incorrect because deploying Kubernetes worker nodes on EC2 instances requires managing the underlying EC2 instances. This includes patching, scaling, and general maintenance, which goes against the requirement of minimizing ongoing effort and infrastructure management.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by \nAmazon Route 53. The company occasionally experiences a timeout error when attempting to \nbrowse the application. The networking team finds that some DNS queries return IP addresses of \nunhealthy instances, resulting in the timeout error. \nWhat should a solutions architect implement to overcome these timeout errors?",
    "options": [
      {
        "id": 0,
        "text": "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by introducing an Application Load Balancer (ALB) in front of the EC2 instances. The ALB performs health checks on the EC2 instances and only routes traffic to healthy instances. This ensures that users are not directed to unhealthy instances, resolving the timeout errors. The ALB integrates seamlessly with Route 53, allowing Route 53 to point to the ALB's DNS name. The ALB's health checks are more granular and responsive than Route 53's health checks alone, providing a more reliable solution.\n\n**Why option 0 is incorrect:**\nWhile associating health checks with a simple routing policy can provide some level of health monitoring, it doesn't guarantee that unhealthy instances will be completely removed from the DNS responses. Simple routing policy distributes traffic randomly across all records, even if some are marked as unhealthy. Route 53 might still return the IP address of an unhealthy instance, leading to timeout errors.\n\n**Why option 1 is incorrect:**\nFailover routing policy is designed for active-passive setups, where one instance is designated as the primary and another as the secondary. It's not suitable for distributing traffic across multiple active instances. While health checks are involved, the failover policy's primary purpose is to switch traffic to a standby instance when the primary fails, not to ensure all instances receiving traffic are healthy in a multi-instance environment. This doesn't solve the problem of multiple active instances, some of which are unhealthy, being returned by DNS.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "A solutions architect needs to design a highly available application consisting of web, application, \nand database tiers. HTTPS content delivery should be as close to the edge as possible, with the \nleast delivery time. \nWhich solution meets these requirements and is MOST secure?",
    "options": [
      {
        "id": 0,
        "text": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement for low latency HTTPS content delivery by using an Application Load Balancer (ALB). ALBs support HTTPS listeners, allowing for SSL/TLS termination at the load balancer. This reduces the load on the backend EC2 instances and allows for centralized certificate management. The use of multiple redundant EC2 instances behind the ALB ensures high availability. While this option doesn't explicitly mention CloudFront, the ALB itself provides some level of edge caching and distribution, making it a reasonable solution given the limited options. The question emphasizes 'as close to the edge as possible' and the ALB is a valid edge component in this context.\n\n**Why option 0 is incorrect:**\nThis option is similar to the correct answer but lacks the explicit mention of HTTPS configuration. Without HTTPS, the connection is not secure. The question specifically requires HTTPS content delivery, making this option insufficient.\n\n**Why option 1 is incorrect:**\nThis option is similar to the correct answer but lacks the explicit mention of HTTPS configuration. Without HTTPS, the connection is not secure. The question specifically requires HTTPS content delivery, making this option insufficient.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A company has a popular gaming platform running on AWS. The application is sensitive to \nlatency because latency can impact the user experience and introduce unfair advantages to \nsome players. The application is deployed in every AWS Region. It runs on Amazon EC2 \ninstances that are part of Auto Scaling groups configured behind Application Load Balancers \n(ALBs). A solutions architect needs to implement a mechanism to monitor the health of the \napplication and redirect traffic to healthy endpoints. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon DynamoDB database to serve as the data store for the application. Create",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by using AWS Global Accelerator, which provides static entry points to the application and intelligently routes traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator minimizes latency by routing users to the optimal AWS Region, improving user experience and fairness in the game. Configuring an accelerator and adding a listener for the application's port allows Global Accelerator to manage traffic distribution across the regions.\n\n**Why option 1 is incorrect:**\nWhile CloudFront can improve latency by caching content closer to users, it's primarily designed for caching static content. In this scenario, the gaming application likely involves dynamic content and real-time interactions. Using CloudFront with the ALB as the origin would add an extra layer of complexity and might not be as effective as Global Accelerator for routing traffic to the nearest healthy endpoint based on real-time network conditions. Also, CloudFront is not primarily designed for health checks and failover in the same way as Global Accelerator.\n\n**Why option 2 is incorrect:**\nUsing Amazon S3 as the origin server for a CloudFront distribution is suitable for serving static content like images or videos. However, the gaming application runs on EC2 instances and requires dynamic content delivery and real-time interactions. S3 is not designed to handle this type of workload. Therefore, this option is not appropriate.\n\n**Why option 3 is incorrect:**\nWhile DynamoDB is a fast and scalable NoSQL database, it doesn't directly address the requirement of monitoring application health and redirecting traffic to healthy endpoints. DynamoDB is a data store and not a traffic management or load balancing solution. Creating a DynamoDB database would not help in minimizing latency or ensuring high availability for the gaming application.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A company has one million users that use its mobile app. The company must analyze the data \nusage in near-real time. The company also must encrypt the data in near-real time and must \nstore the data in a centralized location in Apache Parquet format for further processing. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n180 \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements efficiently. Kinesis Data Firehose can directly ingest data from the mobile app (potentially via API Gateway). It can be configured to encrypt the data in transit and at rest. Most importantly, it can automatically convert the data to Parquet format before storing it in S3. This eliminates the need for separate processing steps for encryption and format conversion, thereby minimizing operational overhead. The AWS Glue Data Catalog integration allows for easy querying of the Parquet data in S3 using services like Athena or Redshift Spectrum.\n\n**Why option 0 is incorrect:**\nWhile Kinesis Data Streams can ingest data in near-real time and store it in S3, it doesn't natively support encryption or Parquet conversion. Implementing these features would require additional services like Lambda or Kinesis Data Analytics, increasing operational overhead. Furthermore, storing directly to S3 from Kinesis Data Streams is not a native functionality; you would need to build a custom application to read from the stream and write to S3.\n\n**Why option 1 is incorrect:**\nSimilar to option 0, Kinesis Data Streams can ingest data, but it doesn't handle encryption or Parquet conversion natively. Using EMR to process the data for encryption and Parquet conversion would introduce significant operational overhead, as EMR is a managed Hadoop framework that requires cluster management and configuration. This is far more complex than using Kinesis Data Firehose, which handles these tasks automatically.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "An ecommerce company has noticed performance degradation of its Amazon RDS based web \napplication. The performance degradation is attributed to an increase in the number of read-only \nSQL queries triggered by business analysts. A solutions architect needs to solve the problem with \nminimal changes to the existing web application. \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read replica of the primary database and have the business analysts run their queries.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the most suitable solution because it directly addresses the problem of read load on the primary database. Creating a read replica allows the business analysts to run their read-only queries against the replica, thus offloading the read operations from the primary database. This minimizes the impact on the performance of the web application and requires minimal changes to the existing application, as only the connection string for the read-only queries needs to be updated to point to the read replica.\n\n**Why option 0 is incorrect:**\nExporting the data to Amazon DynamoDB is not an ideal solution because DynamoDB is a NoSQL database, and the business analysts are already using SQL queries. This would require significant changes to the queries and potentially the data model, which violates the requirement of minimal changes to the existing web application. Additionally, exporting and keeping the data synchronized between RDS and DynamoDB adds complexity.\n\n**Why option 1 is incorrect:**\nLoading the data into Amazon ElastiCache is not a suitable solution because ElastiCache is primarily used for caching frequently accessed data to improve read performance. It is not designed to handle complex analytical queries. Furthermore, ElastiCache is an in-memory data store, and it may not be suitable for storing the entire dataset used by the business analysts. The data would also need to be refreshed regularly from the database, adding complexity. It's not designed for running arbitrary SQL queries.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A company runs an internal browser-based application. The application runs on Amazon EC2 \ninstances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto \nScaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 \ninstances during work hours, but scales down to 2 instances overnight. Staff are complaining that \nthe application is very slow when the day begins, although it runs well by mid-morning. \n \nHow should the scaling be changed to address the staff complaints and keep costs to a \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n181 \nminimum?",
    "options": [
      {
        "id": 0,
        "text": "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement a target tracking action triggered at a lower CPU threshold, and decrease the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by dynamically adjusting the number of instances based on a target metric (CPU utilization). By setting a lower CPU threshold, the Auto Scaling group will proactively scale up before the application becomes overloaded. Decreasing the cooldown period allows the Auto Scaling group to respond more quickly to changes in demand, ensuring that the application can handle the initial surge in traffic at the start of the workday. Target tracking is more responsive than scheduled actions and more cost-effective than maintaining a fixed number of instances.\n\n**Why option 0 is incorrect:**\nWhile a scheduled action would increase the instance count before the workday, it doesn't dynamically adjust based on actual load. If the load is lower than expected, resources are wasted. Also, setting the desired capacity to 20 might be too aggressive and lead to unnecessary costs if the actual demand doesn't require that many instances. It's also less responsive to unexpected changes in load.\n\n**Why option 1 is incorrect:**\nWhile step scaling can be effective, it requires more configuration and fine-tuning to determine the appropriate step adjustments for different CPU thresholds. Target tracking is generally simpler to configure and maintain, as it automatically adjusts the scaling based on the target metric. Decreasing the cooldown period is helpful, but step scaling alone is not as efficient as target tracking in this scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto \nScaling group. An Amazon RDS for Oracle instance is the application' s data layer that uses \nOracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is \ncausing the EC2 instances to become overloaded and the RDS instance to run out of storage. \nThe Auto Scaling group does not have any scaling metrics and defines the minimum healthy \ninstance count only. The company predicts that traffic will continue to increase at a steady but \nunpredictable rate before leveling off. \n \nWhat should a solutions architect do to ensure the system can automatically scale for the \nincreased traffic? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure storage Auto Scaling on the RDS for Oracle instance.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon Aurora to use Auto Scaling storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an alarm on the RDS for Oracle instance for low free storage space.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the Auto Scaling group to use the average CPU as the scaling metric.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the Auto Scaling group to use the average free memory as the scaling metric.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because RDS for Oracle supports storage auto-scaling. As the database grows and consumes more storage, storage auto-scaling automatically increases the allocated storage capacity, preventing the database from running out of space. This directly addresses the problem of the RDS instance running out of storage.\n\n**Why option 1 is incorrect:**\nThis is incorrect because migrating the database to Amazon Aurora is a significant undertaking that involves schema conversion, data migration, and application code changes. While Aurora does offer auto-scaling storage, this option is too complex and time-consuming to be considered an immediate solution to the current problem. The question requires a solution to automatically scale for increased traffic. Migration is not scaling.\n\n**Why option 2 is incorrect:**\nThis is incorrect because configuring an alarm on low free storage space will only trigger a notification when the storage is running low. It does not automatically increase the storage capacity. While monitoring is important, it doesn't solve the problem of automatic scaling.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while configuring the Auto Scaling group to use average CPU utilization as a scaling metric is a good practice for scaling the EC2 instances, it doesn't address the RDS storage issue. The question requires two solutions, one for EC2 and one for RDS.\n\n**Why option 4 is incorrect:**\nThis is incorrect because average free memory is not a reliable metric for scaling web applications. CPU utilization is a better indicator of load. Also, this option only addresses the EC2 scaling and not the RDS storage issue.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "A company provides an online service for posting video content and transcoding it for use by any \nmobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) \nStandard to collect and store the videos so that multiple Amazon EC2 Linux instances can access \nthe video content for processing. As the popularity of the service has grown over time, the \nstorage costs have become too expensive. \n \nWhich storage solution is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Storage Gateway for files to store and process the video content.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Storage Gateway for volumes to store and process the video content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by leveraging Amazon S3 for cost-effective storage of the video content. S3 is significantly cheaper than EFS, especially for infrequently accessed data. The files are temporarily moved to an Amazon EBS volume attached to the EC2 instances for processing. This allows the EC2 instances to perform the transcoding tasks efficiently with local storage, while the bulk of the data resides in the more cost-effective S3 storage. After processing, the transcoded videos can be uploaded back to S3 or another appropriate storage location. This balances cost savings with performance requirements.\n\n**Why option 0 is incorrect:**\nUsing AWS Storage Gateway for files is not the most cost-effective solution. While Storage Gateway can provide on-premises access to AWS storage, it doesn't inherently reduce the cost of storage itself. In this scenario, the video content is already in AWS, and introducing Storage Gateway adds complexity and potentially additional costs without significantly addressing the core issue of high EFS storage costs.\n\n**Why option 1 is incorrect:**\nUsing AWS Storage Gateway for volumes is not the most cost-effective solution. Storage Gateway for volumes is typically used for backing up on-premises data to AWS or providing block storage to on-premises applications. It doesn't directly address the need for cost-effective storage within AWS for video content already residing in the cloud. Furthermore, it adds complexity and overhead without providing a significant cost advantage over directly using S3.\n\n**Why option 2 is incorrect:**\nWhile EFS can be used with lifecycle policies to move infrequently accessed files to EFS Infrequent Access (EFS IA), S3 is generally more cost-effective for storing large amounts of video content, especially if the content is not frequently accessed. Moving the files to S3 provides a more significant cost reduction compared to using EFS IA alone. Also, the question states that storage costs have become too expensive, implying that even EFS IA might not be sufficient to meet the cost optimization goals.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 39,
    "text": "A company wants to create an application to store employee data in a hierarchical structured \nrelationship. The company needs a minimum-latency response to high-traffic queries for the \nemployee data and must protect any sensitive data. The company also needs to receive monthly \nemail messages if any financial information is present in the employee data. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of storing employee data in a hierarchical structure and providing low-latency access. DynamoDB, being a NoSQL database, can model hierarchical data using techniques like adjacency lists or materialized paths. Its key-value and document-oriented nature allows for fast retrieval of data based on primary keys and indexes, fulfilling the low-latency requirement. Exporting the data to S3 provides a cost-effective storage solution for archival and further analysis.\n\n**Why option 0 is incorrect:**\nAmazon Redshift is a data warehouse service optimized for analytical workloads, not for low-latency, high-traffic queries on hierarchical data. While it can store data in a structured format, it's not designed for the operational use case described. Unloading data to S3 is a common practice for Redshift, but it doesn't address the primary need for low-latency access.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 40,
    "text": "A company has an application that is backed by an Amazon DynamoDB table. The company's \ncompliance requirements specify that database backups must be taken every month, must be \navailable for 6 months, and must be retained for 7 years. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Backup plan to back up the DynamoDB table on the first day of each month.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution correctly addresses the requirements by utilizing AWS Backup. AWS Backup allows for the creation of backup plans that can be scheduled to run on a monthly basis. It also provides the ability to define retention policies, allowing backups to be available for 6 months and then retained for a total of 7 years. This automated and managed service simplifies the backup and retention process, ensuring compliance with the stated requirements.\n\n**Why option 1 is incorrect:**\nWhile creating on-demand backups is possible, it doesn't inherently address the long-term retention requirement of 7 years. On-demand backups require manual management of retention, which can be error-prone and difficult to scale. It also doesn't provide the centralized management and monitoring capabilities of AWS Backup.\n\n**Why option 2 is incorrect:**\nDeveloping a custom script using the AWS SDK to create on-demand backups introduces unnecessary complexity and overhead. It requires managing the script, scheduling its execution, and implementing the retention policy manually. This approach is less efficient and more prone to errors compared to using a managed service like AWS Backup. Furthermore, it increases the operational burden.\n\n**Why option 3 is incorrect:**\nSimilar to using the AWS SDK, using the AWS CLI to create on-demand backups requires manual management of the backup process and retention policy. Setting up an Amazon CloudWatch Events rule to trigger the CLI command provides scheduling, but it still lacks the comprehensive retention management and centralized monitoring capabilities of AWS Backup. This approach is less efficient and more complex than using AWS Backup.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company is using Amazon CloudFront with its website. The company has enabled logging on \nthe CloudFront distribution, and logs are saved in one of the company's Amazon S3 buckets. The \ncompany needs to perform advanced analyses on the logs and build visualizations. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the need for advanced analysis of CloudFront logs stored in S3. Amazon Athena is a serverless query service that allows you to use standard SQL to analyze data stored in S3. It's cost-effective and well-suited for analyzing large log files. Furthermore, Athena integrates well with visualization tools like Amazon QuickSight, enabling the creation of dashboards and reports based on the query results.\n\n**Why option 0 is incorrect:**\nThis option is a duplicate of the correct answer and therefore incorrect.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After \na routine compliance check, the company sets a standard that requires a recovery point objective \n(RPO) of less than 1 second for all its production databases. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable a Multi-AZ deployment for the DB instance.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable auto scaling for the DB instance in one Availability Zone.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the DB instance in one Availability Zone, and create multiple read replicas in a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the DB instance in one Availability Zone, and configure AWS Database Migration",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling a Multi-AZ deployment for the RDS for PostgreSQL DB instance provides automatic failover to a standby instance in a different Availability Zone. The synchronous replication to the standby instance ensures that data is continuously replicated, minimizing data loss in case of a primary instance failure. This synchronous replication is crucial for achieving an RPO of less than 1 second, as any committed transaction on the primary instance is immediately replicated to the standby before the transaction is acknowledged as complete. This ensures minimal data loss during a failover.\n\n**Why option 1 is incorrect:**\nThis is incorrect because auto scaling primarily addresses performance and availability by dynamically adjusting the compute resources allocated to the DB instance. While it can improve performance and handle increased load, it does not directly address the RPO requirement. Auto scaling does not inherently prevent data loss in the event of a failure. The database instance would still be in a single AZ, and a failure could lead to data loss depending on the last backup.\n\n**Why option 2 is incorrect:**\nThis is incorrect because read replicas are asynchronous. Data is replicated from the primary instance to the read replicas with a delay. This delay means that in the event of a primary instance failure, data loss is likely to occur, violating the RPO of less than 1 second. Read replicas are primarily used for offloading read traffic from the primary instance and improving read performance, not for minimizing data loss during a failure.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Database Migration Service (DMS) is used for migrating databases from one platform to another. It is not designed for continuous replication to achieve a very low RPO. DMS is typically used for one-time or periodic migrations, not for real-time data replication required for near-zero data loss. Using DMS for this purpose would be an overly complex and inefficient solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company runs a web application that is deployed on Amazon EC2 instances in the private \nsubnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets \ndirects web traffic to the EC2 instances. The company wants to implement new security \nmeasures to restrict inbound traffic from the ALB to the EC2 instances while preventing access \nfrom any other source inside or outside the private subnet of the EC2 instances. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a route in a route table to direct traffic from the internet to the private IP addresses of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the security group for the EC2 instances to only allow traffic that comes from the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the ALB to allow any TCP traffic on any port.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by utilizing security groups, which act as virtual firewalls at the instance level. By configuring the EC2 instances' security group to only allow traffic originating from the ALB's security group, all other traffic sources are implicitly denied. This ensures that only the ALB can communicate with the EC2 instances, fulfilling the security requirement of restricting inbound traffic to only the ALB.\n\n**Why option 0 is incorrect:**\nConfiguring a route in the route table to direct traffic to the private IP addresses of the EC2 instances is not a security measure and would not restrict traffic based on the source. Route tables control the routing of network packets, not the filtering of traffic based on source. This option would not prevent unauthorized access.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nConfiguring the ALB's security group to allow any TCP traffic on any port is a very insecure practice. The ALB's security group should only allow traffic on the necessary ports (e.g., 80, 443) from the intended sources (e.g., the internet or specific CIDR blocks). This option would not restrict access to the EC2 instances and would make the ALB itself vulnerable to attacks.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A research company runs experiments that are powered by a simulation application and a \nvisualization application. The simulation application runs on Linux and outputs intermediate data \nto an NFS share every 5 minutes. The visualization application is a Windows desktop application \nthat displays the simulation output and requires an SMB file system. \n \nThe company maintains two synchronized file systems. This strategy is causing data duplication \nand inefficient resource usage. The company needs to migrate the applications to AWS without \nmaking code changes to either application. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by migrating the simulation application to Linux EC2 instances, which can natively support NFS. The visualization application is migrated to Windows EC2 instances, which can natively support SMB. By using separate EC2 instances with their respective file system protocols, the need for synchronized file systems is eliminated, thus resolving the data duplication and inefficiency issues. No code changes are required as the applications continue to use their native file system protocols.\n\n**Why option 0 is incorrect:**\nMigrating both applications to AWS Lambda and using S3 for data exchange is incorrect because Lambda functions have limitations on execution time and temporary storage. More importantly, it would require significant code changes to both applications to read and write data to S3 instead of using NFS and SMB. The question explicitly states that no code changes are allowed.\n\n**Why option 1 is incorrect:**\nMigrating both applications to Amazon ECS and configuring file sharing would require significant changes to the applications. While ECS can run both Linux and Windows containers, it doesn't inherently solve the NFS/SMB requirement without code changes. Furthermore, sharing files between containers often involves complex configurations and can introduce performance bottlenecks. The question requires a solution without code changes.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "As part of budget planning, management wants a report of AWS billed items listed by user. The \ndata will be used to create department budgets. A solutions architect needs to determine the \nmost efficient way to obtain this report information. \n \nWhich solution meets these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n185",
    "options": [
      {
        "id": 0,
        "text": "Run a query with Amazon Athena to generate the report.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a report in Cost Explorer and download the report.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Access the bill details from the billing dashboard and download the bill.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirement by leveraging Cost Explorer's built-in reporting capabilities. Cost Explorer allows you to create custom reports, filter by various dimensions including user, and download the report in a suitable format for analysis and budget planning. It provides a user-friendly interface and pre-built features specifically designed for cost management and reporting, making it the most efficient option.\n\n**Why option 0 is incorrect:**\nWhile Amazon Athena can query cost and usage data, it requires setting up the AWS Cost and Usage Report (CUR), defining the schema, and writing SQL queries. This is a more complex and time-consuming approach compared to using Cost Explorer's built-in reporting features. It's not the most efficient way to obtain the required report.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nAWS Budgets are primarily for setting and tracking budgets and generating alerts when costs exceed defined thresholds. While Budgets can provide cost information, they don't directly generate detailed reports broken down by user. Modifying a budget to send alerts via SES doesn't fulfill the requirement of providing a report for budget planning.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company hosts its static website by using Amazon S3. The company wants to add a contact \nform to its webpage. The contact form will have dynamic server-side components for users to \ninput their name, email address, phone number, and user message. The company anticipates \nthat there will be fewer than 100 site visits each month. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL,",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing a serverless architecture. API Gateway acts as the entry point for the contact form submissions. Lambda provides the server-side logic to process the form data (e.g., sending an email or storing the data in a database). The cost is optimized because both API Gateway and Lambda have pay-per-use pricing models, making them very cost-effective for low traffic scenarios. You only pay when the API is called and the Lambda function is executed.\n\n**Why option 0 is incorrect:**\nHosting a dynamic contact form page in ECS would be significantly more expensive than a serverless approach. ECS requires running and maintaining container instances, even when there are no requests. This incurs costs for the underlying EC2 instances or Fargate tasks, regardless of the traffic volume. For only 100 visits per month, the cost of running ECS would be disproportionately high.\n\n**Why option 2 is incorrect:**\nWhile Lightsail is simpler to manage than EC2, it's still a fixed-price service. Converting the static webpage to dynamic and using client-side scripting might handle the form display, but it doesn't address the server-side processing requirement. Client-side scripting alone cannot reliably send emails or store data securely. Even if client-side scripting could handle some processing, it would expose sensitive information and be vulnerable to manipulation. Lightsail's fixed cost would also be more expensive than a serverless approach for such low traffic.\n\n**Why option 3 is incorrect:**\nCreating an EC2 instance and deploying a LAMP stack is a traditional approach, but it's not cost-effective for this scenario. A t2.micro instance would still incur costs even when idle. Managing the operating system, web server, database, and security patches would also add operational overhead. For only 100 visits per month, the cost of running and maintaining an EC2 instance would be far higher than a serverless solution.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The \nstatic website uses a database backend. The company notices that the website does not reflect \nupdates that have been made in the website's Git repository. The company checks the \ncontinuous integration and continuous delivery (CI/CD) pipeline between the Git repository and \nAmazon S3. The company verifies that the webhooks are configured properly and that the CI/CD \npipeline is sending messages that indicate successful deployments. \n \nA solutions architect needs to implement a solution that displays the updates on the website. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Add an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Invalidate the CloudFront cache.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Certificate Manager (ACM) to validate the website's SSL certificate.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by clearing the CloudFront cache. CloudFront caches content to improve performance and reduce latency. When updates are made to the S3 bucket, CloudFront may continue to serve the older cached content. Invalidating the cache forces CloudFront to retrieve the latest content from the origin (S3), ensuring that users see the updated website.\n\n**Why option 0 is incorrect:**\nAn Application Load Balancer (ALB) is used for distributing incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. Since the website is static and hosted on S3, an ALB is not needed and would not solve the caching issue. The ALB is for dynamic content and load balancing, not static content delivery from S3.\n\n**Why option 1 is incorrect:**\nAmazon ElastiCache is a caching service that can be used to improve the performance of database-driven applications. However, in this scenario, the problem is not related to the database performance but to the CloudFront cache serving outdated static content. Adding ElastiCache to the database layer would not address the issue of the website not reflecting the latest updates from the Git repository.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 48,
    "text": "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. \nThe application has three tiers: an application tier, a business tier, and a database tier with \nMicrosoft SQL Server. The company wants to use specific features of SQL Server such as native \nbackups and Data Quality Services. The company also needs to share files for processing \nbetween the tiers. \n \nHow should a solutions architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of migrating a Windows-based application with a SQL Server database while leveraging specific SQL Server features. Hosting all three tiers on EC2 instances provides the necessary control and flexibility to install and configure the application and database. Using Amazon FSx for Windows File Server directly addresses the need for file sharing between the tiers, as it's a fully managed Windows file server built on Windows Server, offering compatibility and performance for Windows-based applications. It also supports features like SMB protocol, Active Directory integration, and NTFS permissions, which are likely required for a Windows environment.\n\n**Why option 0 is incorrect:**\nWhile hosting all tiers on EC2 is a valid approach, using Amazon FSx File Gateway is not the optimal solution for file sharing in this scenario. FSx File Gateway is primarily used to provide on-premises access to file shares stored in AWS file services like Amazon S3 or Amazon FSx for Windows File Server. Since the application tiers are already in AWS, using FSx File Gateway introduces unnecessary complexity and latency. It's more suited for hybrid scenarios where on-premises systems need to access AWS-based file storage.\n\n**Why option 2 is incorrect:**\nWhile hosting the application and business tiers on EC2 is a valid approach, hosting the database tier on an unspecified service leaves the SQL Server requirements unaddressed. The question specifically mentions the need for native backups and Data Quality Services, which are best supported by running SQL Server on EC2 or RDS for SQL Server. Without specifying how the database tier is hosted, it's impossible to determine if the solution meets the application's requirements.\n\n**Why option 3 is incorrect:**\nSimilar to option 2, this option doesn't specify how the database tier is hosted, leaving the SQL Server requirements unaddressed. The question specifically mentions the need for native backups and Data Quality Services, which are best supported by running SQL Server on EC2 or RDS for SQL Server. Without specifying how the database tier is hosted, it's impossible to determine if the solution meets the application's requirements.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 49,
    "text": "A company is migrating a Linux-based web server group to AWS. The web servers must access \nfiles in a shared file store for some content. The company must not make any changes to the \napplication. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 Standard bucket with access to the web servers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by providing a network file system that can be mounted on multiple Linux-based web servers. Amazon EFS is designed for shared file storage and is compatible with Linux instances. It allows the web servers to access the files in the shared file store without any application modifications, as the file system is mounted directly to the servers.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon S3 is an object storage service, not a file system. While web servers can access objects in S3, it would require application changes to interact with the S3 API instead of a standard file system interface. The requirement states that no application changes are allowed.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while CloudFront can distribute content from S3, it is primarily a content delivery network (CDN) for caching and delivering static content closer to users. It doesn't provide a shared file system that the web servers can directly mount and access. Furthermore, using CloudFront would still require the web servers to access S3, necessitating application changes, which violates the requirement.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that \nis located in the same AWS account. \n \nWhich solution will meet these requirements in the MOST secure manner?",
    "options": [
      {
        "id": 0,
        "text": "Apply an S3 bucket policy that grants read access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Embed an access key and a secret key in the Lambda function's code to grant the required IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by assigning an IAM role to the Lambda function. This role acts as a security principal for the Lambda function, allowing it to assume temporary credentials. The IAM policy attached to the role explicitly grants the necessary `s3:GetObject` permission (or similar read-level permissions) to the specific S3 bucket. This approach adheres to the principle of least privilege, granting only the necessary permissions and avoiding the need to embed or manage long-term credentials within the Lambda function's code. It is the most secure and recommended method for granting permissions to Lambda functions.\n\n**Why option 0 is incorrect:**\nWhile an S3 bucket policy can grant access to the Lambda function, it's generally considered less secure and less manageable than using IAM roles. Bucket policies are primarily designed for cross-account access or granting access to anonymous users. Using an IAM role attached to the Lambda function provides better isolation and control over the function's permissions. It also simplifies auditing and permission management, especially in environments with many Lambda functions accessing various resources. It's also less flexible when needing to manage permissions for multiple Lambda functions.\n\n**Why option 2 is incorrect:**\nEmbedding access keys and secret keys directly into the Lambda function's code is a highly insecure practice. If the code is compromised, the credentials could be exposed, granting unauthorized access to the S3 bucket and potentially other AWS resources. This approach violates security best practices and should never be used in a production environment. Managing and rotating these credentials would also be a significant operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are \nin an Auto Scaling group that scales in response to user demand. The company wants to \noptimize cost savings without making a long-term commitment. \n \nWhich EC2 instance purchasing option should a solutions architect recommend to meet these \nrequirements?",
    "options": [
      {
        "id": 0,
        "text": "Dedicated Instances only",
        "correct": false
      },
      {
        "id": 1,
        "text": "On-Demand Instances only",
        "correct": false
      },
      {
        "id": 2,
        "text": "A mix of On-Demand Instances and Spot Instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "A mix of On-Demand Instances and Reserved Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by combining the reliability of On-Demand Instances with the cost savings of Spot Instances. On-Demand Instances provide a baseline level of capacity and availability, ensuring the application remains responsive even during periods of high demand or when Spot Instances are unavailable. Spot Instances, on the other hand, offer significant cost reductions when available, allowing the company to take advantage of unused EC2 capacity. This combination allows for cost optimization without a long-term commitment, as Spot Instances can be terminated when the price exceeds the bid or when capacity is needed elsewhere, and On-Demand Instances will continue to provide service.\n\n**Why option 0 is incorrect:**\nDedicated Instances offer hardware isolation and are typically more expensive than other instance types. While they might be suitable for compliance or security reasons, they do not directly address the requirement of cost optimization without a long-term commitment. The question emphasizes cost savings and flexibility, which Dedicated Instances do not inherently provide.\n\n**Why option 1 is incorrect:**\nWhile On-Demand Instances provide flexibility and availability, they are generally more expensive than other purchasing options like Spot Instances or Reserved Instances. Relying solely on On-Demand Instances would not be the most cost-effective solution, especially given the requirement to optimize cost savings. The question specifically asks for cost optimization, which On-Demand instances alone do not fully satisfy.\n\n**Why option 3 is incorrect:**\nReserved Instances require a commitment of 1 or 3 years, which contradicts the requirement of avoiding long-term commitments. While Reserved Instances can offer significant cost savings over On-Demand Instances, they lack the flexibility needed in this scenario where the company wants to avoid long-term contracts. The Auto Scaling group dynamically adjusts based on demand, and Reserved Instances might not align perfectly with the fluctuating capacity needs.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 52,
    "text": "A media company uses Amazon CloudFront for its publicly available streaming video content. \nThe company wants to secure the video content that is hosted in Amazon S3 by controlling who \nhas access. Some of the company's users are using a custom HTTP client that does not support \ncookies. Some of the company's users are unable to change the hardcoded URLs that they are \nusing for access. \n \nWhich services or methods will meet these requirements with the LEAST impact to the users? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n188",
    "options": [
      {
        "id": 0,
        "text": "Signed cookies",
        "correct": true
      },
      {
        "id": 1,
        "text": "Signed URLs",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS AppSync",
        "correct": false
      },
      {
        "id": 3,
        "text": "JSON Web Token (JWT)",
        "correct": false
      },
      {
        "id": 4,
        "text": "AWS Secrets Manager",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSigned cookies allow access to multiple restricted files (e.g., video segments) with a single signature. The hardcoded URLs used by some users will continue to work as long as the signed cookie is valid. While the question states that some users have custom HTTP clients that do not support cookies, the prompt asks for the solution with the *least* impact to the users. The implication is that some users *do* support cookies, and using signed cookies for those users is the least impactful way to secure content for them. The question is poorly worded because it asks for two services/methods, but only one of the provided options is correct.\n\n**Why option 1 is incorrect:**\nSigned URLs are not ideal because they expire, and the requirement states that some users cannot change the hardcoded URLs they are using. Each request would require a new signed URL, making it impractical and violating the 'no URL change' requirement.\n\n**Why option 2 is incorrect:**\nAWS AppSync is a GraphQL service for building data-driven mobile and web applications. It is not directly relevant to securing streaming video content served via CloudFront and stored in S3.\n\n**Why option 3 is incorrect:**\nJSON Web Tokens (JWT) are a standard for securely transmitting information as a JSON object. While JWTs can be used for authentication and authorization, they are not a direct replacement for CloudFront's signed URLs or cookies in this scenario. They would require significant changes to the application architecture and would not directly address the requirement of users with hardcoded URLs.\n\n**Why option 4 is incorrect:**\nAWS Secrets Manager helps you manage, retrieve, and rotate secrets. While it can be used to store the private key used to generate signed URLs or cookies, it doesn't directly address the content security requirements or the constraints related to hardcoded URLs and cookie support.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A company is preparing a new data platform that will ingest real-time streaming data from multiple \nsources. The company needs to transform the data before writing the data to Amazon S3. The \ncompany needs the ability to use SQL to query the transformed data. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon Kinesis Data Streams is designed for real-time data streaming. Kinesis Data Analytics allows for real-time data transformation using SQL. The transformed data can then be written to Amazon S3. This solution directly addresses all the requirements of the scenario.\n\n**Why option 1 is incorrect:**\nWhile Amazon MSK can handle streaming data, using AWS Glue for transformation is not ideal for real-time streaming scenarios. AWS Glue is primarily designed for batch processing and ETL operations, not real-time transformations. Kinesis Data Analytics is better suited for real-time SQL-based transformations.\n\n**Why option 2 is incorrect:**\nAWS Database Migration Service (DMS) is primarily used for migrating databases, not for ingesting real-time streaming data. While Amazon EMR can perform transformations, it's an overkill for this scenario. Kinesis Data Analytics is a more appropriate and cost-effective solution for real-time SQL-based transformations.\n\n**Why option 3 is incorrect:**\nWhile Amazon MSK can handle streaming data, using Amazon Athena directly on the raw streaming data before it's transformed is not efficient or practical. Athena is designed for querying data at rest in S3 or other data lakes. The data needs to be transformed first before it can be effectively queried with Athena. Kinesis Data Analytics is needed for the real-time transformation.\n\n**Why option 4 is incorrect:**\nWhile Kinesis Data Streams is suitable for streaming and AWS Glue can transform data, Glue is not designed for real-time transformation of streaming data. It's better suited for batch processing. Therefore, this combination doesn't efficiently address the real-time transformation requirement. Kinesis Data Analytics is better suited for this.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 54,
    "text": "A company has an on-premises volume backup solution that has reached its end of life. The \ncompany wants to use AWS as part of a new backup solution and wants to maintain local access \nto all the data while it is backed up on AWS. The company wants to ensure that the data backed \nup on AWS is automatically and securely transferred. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by providing local access to the data through the on-premises Storage Gateway appliance. The stored volume gateway asynchronously backs up the entire dataset to AWS, ensuring that the data is securely transferred and stored in AWS. The data is stored locally first, and then asynchronously backed up to AWS, satisfying the requirement of local access and automatic backup to AWS.\n\n**Why option 0 is incorrect:**\nAWS Snowball is primarily used for large-scale data migration, not for ongoing, automated backups. While it can migrate data to S3, it doesn't provide a continuous backup solution or local access after the initial migration. The question requires an ongoing backup solution, not a one-time migration.\n\n**Why option 1 is incorrect:**\nAWS Snowball Edge, like Snowball, is primarily for large-scale data migration and edge computing. While it can migrate data to S3, it doesn't provide a continuous backup solution or local access after the initial migration. The question requires an ongoing backup solution, not a one-time migration.\n\n**Why option 2 is incorrect:**\nA cached volume gateway stores only the most frequently accessed data locally, caching it for low-latency access. The entire dataset is stored in AWS S3, and only a subset is cached locally. This does not meet the requirement of maintaining local access to *all* the data. The company wants to maintain local access to *all* the data while it is backed up on AWS.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. \nTraffic must not traverse the internet. \n \nHow should a solutions architect configure access to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a private hosted zone by using Amazon Route 53.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a gateway VPC endpoint for Amazon S3 in the VPC.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the EC2 instances to use a NAT gateway to access the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating a private connection between the VPC and S3. A gateway VPC endpoint for S3 allows EC2 instances within the VPC to access S3 using AWS's internal network, bypassing the internet. This ensures that the traffic remains within the AWS infrastructure, enhancing security and reducing latency.\n\n**Why option 0 is incorrect:**\nCreating a private hosted zone in Route 53 is primarily for managing DNS records within a VPC. While it's useful for internal name resolution, it doesn't directly establish a private connection for data transfer between EC2 instances and S3. It doesn't prevent traffic from going over the internet.\n\n**Why option 2 is incorrect:**\nConfiguring EC2 instances to use a NAT gateway allows them to initiate outbound traffic to the internet. While it provides network address translation, it doesn't prevent traffic from traversing the public internet to reach S3. The traffic would still need to go through the NAT gateway and then out to the internet to reach the S3 service, which violates the requirement.\n\n**Why option 3 is incorrect:**\nEstablishing an AWS Site-to-Site VPN connection is used to connect an on-premises network to a VPC. It's not the correct solution for connecting EC2 instances within a VPC to an S3 bucket. This option is more complex and expensive than necessary, and it's designed for a different use case (connecting a remote network to AWS).",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains \npersonally identifiable information (PII). The company wants to use the data in three applications. \nOnly one of the applications needs to process the PII. The PII must be removed before the other \ntwo applications process the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object",
        "correct": true
      },
      {
        "id": 2,
        "text": "Process the data and store the transformed data in three separate Amazon S3 buckets so that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Process the data and store the transformed data in three separate Amazon DynamoDB tables so",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by storing the data in S3 and using S3 Object Lambda to transform the data on-the-fly as it's accessed. S3 Object Lambda allows you to add your own code to S3 GET, HEAD, and LIST requests to modify and return data as it is retrieved from S3. This eliminates the need to create and manage separate data copies or ETL pipelines for each application. By configuring S3 Object Lambda to remove PII for the two applications that don't need it, the data is transformed only when requested, minimizing operational overhead and storage costs. This approach is more efficient than creating and managing separate data copies or using a proxy application layer.\n\n**Why option 0 is incorrect:**\nUsing DynamoDB as the primary storage for terabytes of customer data is generally less cost-effective than using S3. Also, creating a proxy application layer to intercept and transform the data adds significant operational overhead. The proxy layer would need to be managed, scaled, and maintained, increasing complexity and cost. DynamoDB is also not ideal for large-scale data analytics or processing, which might be required by the applications.\n\n**Why option 2 is incorrect:**\nStoring the transformed data in three separate S3 buckets requires processing the data upfront and creating multiple copies. This increases storage costs and operational overhead because you need to manage the data transformation process and ensure consistency across the buckets. It also adds complexity to the data management lifecycle.\n\n**Why option 3 is incorrect:**\nStoring the transformed data in three separate DynamoDB tables suffers from the same issues as option 2, but is further compounded by the higher cost of DynamoDB compared to S3 for storing large amounts of data. Maintaining consistency across three DynamoDB tables would also be operationally complex.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A development team has launched a new application that is hosted on Amazon EC2 instances \ninside a development VPC. A solutions architect needs to create a new VPC in the same \naccount. The new VPC will be peered with the development VPC. The VPC CIDR block for the \ndevelopment VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the \nnew VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. \n \nWhat is the SMALLEST CIDR block that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "10.0.1.0/32",
        "correct": false
      },
      {
        "id": 1,
        "text": "192.168.0.0/24",
        "correct": false
      },
      {
        "id": 2,
        "text": "192.168.1.0/32",
        "correct": false
      },
      {
        "id": 3,
        "text": "10.0.1.0/24",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because 10.0.1.0/24 is a valid CIDR block that does not overlap with the existing 192.168.0.0/24 CIDR block. A /24 CIDR block provides 256 addresses, which is a reasonable size. It also adheres to the best practice of using private IP address ranges (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) for VPCs. Since the question asks for the SMALLEST CIDR block that meets the requirements, and no smaller block is valid, this is the correct answer.\n\n**Why option 0 is incorrect:**\nThis is incorrect because 10.0.1.0/32 represents a single IP address. While it doesn't overlap with the existing CIDR block, a VPC requires a range of IP addresses to function correctly. A /32 CIDR block is not a valid CIDR block for a VPC.\n\n**Why option 1 is incorrect:**\nThis is incorrect because 192.168.0.0/24 is the same CIDR block as the existing development VPC. VPC peering requires that the CIDR blocks of the peered VPCs do not overlap. Using the same CIDR block would cause routing conflicts and prevent the peering connection from being established.\n\n**Why option 2 is incorrect:**\nThis is incorrect because 192.168.1.0/32 represents a single IP address. While it doesn't overlap with the existing CIDR block, a VPC requires a range of IP addresses to function correctly. A /32 CIDR block is not a valid CIDR block for a VPC.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company deploys an application on five Amazon EC2 instances. An Application Load Balancer \n(ALB) distributes traffic to the instances by using a target group. The average CPU usage on \neach of the instances is below 10% most of the time, with occasional surges to 65%. \n \nA solutions architect needs to implement a solution to automate the scalability of the application. \nThe solution must optimize the cost of the architecture and must ensure that the application has \nenough CPU resources when surges occur. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating an EC2 Auto Scaling group that uses the existing ALB as the load balancer and the existing target group. This allows the Auto Scaling group to automatically register and deregister instances with the ALB as they are launched and terminated. By configuring scaling policies based on CPU utilization, the Auto Scaling group can dynamically adjust the number of instances to handle the surges, ensuring sufficient resources are available. Because it uses the existing ALB and target group, it minimizes changes to the existing infrastructure and is cost-effective. This approach directly addresses the need for automated scalability and cost optimization.\n\n**Why option 0 is incorrect:**\nCreating a CloudWatch alarm alone will only notify when CPU utilization exceeds a threshold. It does not automatically scale the application. While it can trigger manual intervention, it doesn't meet the requirement for automated scalability.\n\n**Why option 2 is incorrect:**\nCreating two CloudWatch alarms, one for scaling up and one for scaling down, is a valid approach for triggering scaling actions. However, it requires manual configuration of scaling actions within the alarms, which is less efficient and flexible than using an EC2 Auto Scaling group. An Auto Scaling group provides more comprehensive management of the scaling process, including instance lifecycle management and integration with the ALB.\n\n**Why option 3 is incorrect:**\nCreating an EC2 Auto Scaling group without integrating it with the existing ALB and target group would require significant reconfiguration of the application's traffic routing. The ALB would need to be manually updated with the new instances launched by the Auto Scaling group, which is not automated and would disrupt traffic flow. This solution is not practical or cost-effective.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 59,
    "text": "A company is running a critical business application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances run in an Auto Scaling group and access an \nAmazon RDS DB instance. \n \nThe design did not pass an operational review because the EC2 instances and the DB instance \nare all located in a single Availability Zone. A solutions architect must update the design to use a \nsecond Availability Zone. \n \nWhich solution will make the application highly available?",
    "options": [
      {
        "id": 0,
        "text": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by provisioning a subnet in each Availability Zone. This allows the Auto Scaling group to launch EC2 instances in both AZs, distributing the application load and ensuring that if one AZ fails, the application remains available in the other. The RDS DB instance can also be configured for Multi-AZ deployment, further enhancing availability. This is the standard and recommended approach for achieving high availability in AWS.\n\n**Why option 0 is incorrect:**\nWhile provisioning a subnet in each Availability Zone is correct, the statement that the Auto Scaling group distributes the is incomplete and doesn't provide the full picture. The Auto Scaling group needs to be configured to launch instances in both subnets to achieve high availability. This option is incomplete and less clear than option 2.\n\n**Why option 1 is incorrect:**\nSubnets cannot extend across Availability Zones. Subnets are confined to a single Availability Zone. Therefore, provisioning a subnet that extends across both Availability Zones is not a valid configuration in AWS. This option demonstrates a fundamental misunderstanding of AWS networking.\n\n**Why option 3 is incorrect:**\nSimilar to option 1, subnets cannot extend across Availability Zones. This option proposes an invalid network configuration, making it an incorrect solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-\nmillisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds \nof Amazon EC2 instances that run Amazon Linux will distribute and process the data. \n \nWhich solution will meet the performance requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume' tiering policy to ALL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for NetApp ONTAP file system. Set each volume's tiering policy to NONE.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by using Amazon S3 for storing the raw data and Amazon FSx for Lustre for high-performance processing. FSx for Lustre is designed for workloads that require fast access to data, such as machine learning, high-performance computing (HPC), and media processing. It provides sub-millisecond latencies and can deliver throughput in the GBps range, meeting the laboratory's performance requirements. S3 is suitable for storing the raw data due to its scalability and cost-effectiveness, and FSx for Lustre can be configured to import data from S3 for processing.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because while Amazon FSx for NetApp ONTAP can provide high performance, it is not optimized for the extreme throughput requirements (6 GBps) stated in the question. Also, setting the tiering policy to ALL would mean data is tiered to capacity pool, which would increase latency and reduce throughput, contradicting the requirements. FSx for NetApp ONTAP is better suited for general-purpose file storage with enterprise features.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it is identical to the correct option. There is no difference between the two.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because while setting the tiering policy to NONE on FSx for NetApp ONTAP would keep all data on the faster SSD tier, it still doesn't provide the extreme throughput (6 GBps) required by the research laboratory. FSx for Lustre is a better choice for this level of performance.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A company needs to migrate a legacy application from an on-premises data center to the AWS \nCloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a \nweek. The application's database storage continues to grow over time. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by utilizing EC2 Reserved Instances for the application layer. Reserved Instances provide significant cost savings compared to On-Demand Instances for applications that run continuously. Migrating the data storage layer to Amazon S3 Intelligent-Tiering is also cost-effective. S3 Intelligent-Tiering automatically moves data between access tiers based on changing access patterns, optimizing storage costs without performance impact. This is suitable for data that may be accessed frequently, infrequently, or rarely, and its access patterns may change over time.\n\n**Why option 0 is incorrect:**\nUsing EC2 Spot Instances for a 24/7 application is not reliable. Spot Instances can be terminated with short notice if the spot price exceeds the bid price, leading to application downtime. While S3 Intelligent-Tiering is a good choice for the data layer, the unreliability of Spot Instances makes this option unsuitable for the application layer.\n\n**Why option 1 is incorrect:**\nWhile EC2 Reserved Instances are a good choice for the application layer, using Amazon EBS for the data storage layer is not the most cost-effective solution for continuously growing data. EBS volumes are attached to specific EC2 instances and require manual scaling and management. S3 Intelligent-Tiering is a more scalable and cost-effective solution for growing data storage needs, especially when access patterns are not predictable.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A university research laboratory needs to migrate 30 TB of data from an on-premises Windows \nfile server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that \nmany other departments in the university share. \n \nThe laboratory wants to implement a data migration service that will maximize the performance of \nthe data transfer. However, the laboratory needs to be able to control the amount of bandwidth \nthat the service uses to minimize the impact on other departments. The data migration must take \nplace within the next 5 days. \n \nWhich AWS solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "AWS Snowcone",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx File Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS DataSync",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Transfer Family",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by providing a managed data transfer service specifically designed for moving data between on-premises storage and AWS storage services, including Amazon FSx for Windows File Server. It optimizes data transfer performance through parallel data streams and built-in data validation. Critically, AWS DataSync allows for bandwidth throttling, enabling the laboratory to control the amount of bandwidth used during the migration, thus minimizing the impact on other departments. Given the 30TB size and 5-day timeframe, DataSync's optimized transfer and bandwidth control features make it the most suitable option.\n\n**Why option 0 is incorrect:**\nThis option is not suitable because while it's a physical device for data transfer, it's better suited for offline data transfer when network connectivity is limited or unavailable. The scenario specifies a 1 Gbps network link, making an online data transfer solution more appropriate. Using this option would also involve shipping the device, which would likely exceed the 5-day timeframe. Furthermore, it does not directly integrate with FSx for Windows File Server.\n\n**Why option 1 is incorrect:**\nThis option is designed to provide low-latency access to data stored in Amazon S3 for on-premises applications. It does not directly support migrating data from an on-premises Windows file server to Amazon FSx for Windows File Server. It's primarily used for hybrid cloud storage scenarios where on-premises applications need to access data stored in S3. It doesn't address the initial migration requirement.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "A company wants to create a mobile app that allows users to stream slow-motion video clips on \ntheir mobile devices. Currently, the app captures video clips and uploads the video clips in raw \nformat into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. \nHowever, the videos are large in their raw format. \n \nUsers are experiencing issues with buffering and playback on mobile devices. The company \nwants to implement solutions to maximize the performance and scalability of the app while \nminimizing operational overhead. \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon CloudFront for content delivery and caching.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and",
        "correct": false
      },
      {
        "id": 4,
        "text": "Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because CloudFront is a content delivery network (CDN) that caches content closer to users, reducing latency and improving streaming performance. By caching the video files at edge locations, CloudFront minimizes the need for users to download the entire file from the S3 bucket, resulting in faster playback and reduced buffering. This also improves scalability by offloading traffic from the S3 bucket.\n\n**Why option 1 is incorrect:**\nThis is incorrect because replicating video files across AWS Regions using DataSync, while improving availability, does not directly address the buffering and playback issues caused by large file sizes. It also increases operational overhead and costs without providing the necessary performance improvements for mobile streaming.\n\n**Why option 2 is incorrect:**\nThis is correct because Elastic Transcoder converts video files into formats suitable for different devices and network conditions. By transcoding the raw video files into smaller, optimized formats, the app can deliver video clips that are better suited for mobile devices, reducing buffering and improving playback. This directly addresses the issue of large file sizes causing performance problems.\n\n**Why option 3 is incorrect:**\nThis is incorrect because deploying an Auto Scaling group of EC2 instances in Local Zones for content delivery is an overly complex and expensive solution. While Local Zones reduce latency for users in specific geographic areas, it requires significant operational overhead to manage the EC2 instances and content delivery infrastructure. CloudFront provides a managed CDN service that is more cost-effective and easier to manage.\n\n**Why option 4 is incorrect:**\nThis is incorrect because deploying an Auto Scaling group of EC2 instances to convert video files adds significant operational overhead. While it addresses the need for video conversion, it requires managing the EC2 instances, transcoding software, and scaling infrastructure. Elastic Transcoder is a managed service that simplifies video transcoding and reduces operational burden.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 64,
    "text": "A company is launching a new application deployed on an Amazon Elastic Container Service \n(Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is \nmonitoring CPU and memory usage because it is expecting high traffic to the application upon its \nlaunch. However, the company wants to reduce costs when utilization decreases. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of scaling ECS tasks based on CPU and memory utilization to reduce costs. AWS Application Auto Scaling is specifically designed for scaling resources like ECS services. Target tracking policies allow you to define a target value for a metric (e.g., CPU utilization) and Application Auto Scaling automatically adjusts the number of tasks to maintain that target. This ensures that the application scales up when utilization increases and scales down when utilization decreases, optimizing costs. Fargate already handles the underlying infrastructure scaling, so this focuses on the application layer.\n\n**Why option 0 is incorrect:**\nThis is incorrect because using EC2 Auto Scaling is not applicable to ECS Fargate. Fargate manages the underlying infrastructure, so EC2 Auto Scaling is not relevant. Scaling at certain periods based on previous traffic patterns is also not as responsive as scaling based on real-time metrics.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while a Lambda function could be used to scale ECS, it's not the most efficient or recommended approach. AWS Application Auto Scaling is a managed service specifically designed for this purpose and provides more robust features like target tracking policies. Using a Lambda function would require more custom code and management overhead.\n\n**Why option 2 is incorrect:**\nThis is incorrect because EC2 Auto Scaling is not applicable to ECS Fargate. Fargate manages the underlying infrastructure, so EC2 Auto Scaling is not relevant. ECS tasks running on Fargate do not directly use EC2 instances that can be scaled by EC2 Auto Scaling.",
    "domain": "Design Cost-Optimized Architectures"
  }
]