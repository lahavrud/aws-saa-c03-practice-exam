[
  {
    "id": 0,
    "text": "A company has an API that receives real-time data from a fleet of monitoring devices. The API \nstores this data in an Amazon RDS DB instance for later analysis. The amount of data that the \nmonitoring devices send to the API fluctuates. During periods of heavy traffic, the API often \nreturns timeout errors. \nAfter an inspection of the logs, the company determines that the database is not capable of \nprocessing the volume of write traffic that comes from the API. A solutions architect must \nminimize the number of connections to the database and must ensure that data is not lost during \nperiods of heavy traffic. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the size of the DB instance to an instance type that has more available memory.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing Amazon SQS will help minimize the number of connections to the database, as the API will write data to a queue instead of directly to the database. Additionally, using an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database will help ensure that data is not lost during periods of heavy traffic, as the queue will serve as a buffer between the API and the database.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 1,
    "text": "A company manages its own Amazon EC2 instances that run MySQL databases. The company \nis manually managing replication and scaling as demand increases or decreases. The company \nneeds a new solution that simplifies the process of adding or removing compute capacity to or \nfrom its database tier as needed. The solution also must offer improved performance, scaling, \nand durability with minimal effort from operations. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Combine the databases into one larger MySQL database. Run the larger database on larger EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/rds/aurora/serverless/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "A company is concerned that two NAT instances in use will no longer be able to support the \ntraffic needed for the company’s application. A solutions architect wants to implement a solution \nthat is highly available, fault tolerant, and automatically scalable. \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Remove the two NAT instances and replace them with two NAT gateways in the same Availability",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different",
        "correct": false
      },
      {
        "id": 2,
        "text": "Remove the two NAT instances and replace them with two NAT gateways in different Availability",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nIf you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway's Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-basics\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The \napplication requires access to a database in VPC B. Both VPCs are in the same AWS account. \nWhich solution will provide the required access MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Create a DB instance security group that allows all traffic from the public IP address of the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a VPC peering connection between VPC A and VPC B.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Make the DB instance publicly accessible. Assign a public IP address to the DB instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-connectivity-instance-subnet- vpc/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company runs demonstration environments for its customers on Amazon EC2 instances. Each \nenvironment is isolated in its own VPC. The company’s operations team needs to be notified \nwhen RDP or SSH access to an environment has been established.",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the EC2 instances with an IAM instance profile that has an IAM role with the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nEC2 Instance State-change Notifications are not the same as RDP or SSH established connection notifications. Use Amazon CloudWatch Logs to monitor SSH access to your Amazon EC2 Linux instances so that you can monitor rejected (or established) SSH connection requests and take action. https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts- to-amazon-ec2-linux-instances/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company is building a new web-based customer relationship management application. The \napplication will use several Amazon EC2 instances that are backed by Amazon Elastic Block \nStore (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will \nalso use an Amazon Aurora database. All data for the application must be encrypted at rest and \nin transit. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the AWS root account to log in to the AWS Management Console. Upload the company’s",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A solutions architect has created a new AWS account and must secure AWS account root user \naccess. \nWhich combination of actions will accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Ensure the root user uses a strong password.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable multi-factor authentication to the root user.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store root user access keys in an encrypted Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add the root user to a group containing administrative permissions.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Apply the required permissions to the root user with an inline policy document.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\n\"Enable MFA\" The AWS Account Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html \"Choose a strong password\" Changing the AWS Account Root User Password https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_change-root.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company has a three-tier application for image sharing. The application uses an Amazon EC2 \ninstance for the front-end layer, another EC2 instance for the application layer, and a third EC2 \ninstance for a MySQL database. A solutions architect must design a scalable and highly available \nsolution that requires the least amount of change to the application. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nfor \"Highly available\": Multi-AZ & for \"least amount of changes to the application\": Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company wants to experiment with individual AWS accounts for its engineer team. The \ncompany wants to be notified as soon as the Amazon EC2 instance usage for a given month \nexceeds a specific threshold for each account. \nWhat should a solutions architect do to meet this requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Budgets allows you to create budgets for your AWS accounts and set alerts when usage exceeds a certain threshold. By creating a budget for each account, specifying the period as monthly and the scope as EC2 instances, you can effectively track the EC2 usage for each account and be notified when a threshold is exceeded. This solution is the most cost-effective option as it does not require additional resources such as Amazon Athena or Amazon EventBridge. https://aws.amazon.com/getting-started/hands-on/control-your-costs-free-tier-budgets/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 9,
    "text": "A solutions architect needs to design a new microservice for a company’s application. Clients \nmust be able to call an HTTPS endpoint to reach the microservice. The microservice also must \nuse AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect \nwill write the logic for this microservice by using a single AWS Lambda function that is written in \nGo 1.x. \nWhich solution will deploy the function in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company previously migrated its data warehouse solution to AWS. The company also has an \nAWS Direct Connect connection. Corporate office users query the data warehouse using a \nvisualization tool. The average size of a query returned by the data warehouse is 50 MB and \neach webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the \ndata warehouse are not cached. \nWhich solution provides the LOWEST data transfer egress cost for the company?",
    "options": [
      {
        "id": 0,
        "text": "Host the visualization tool on premises and query the data warehouse directly over the internet.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host the visualization tool on premises and query the data warehouse directly over a Direct",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the visualization tool in the same AWS Region as the data warehouse and access it over a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://aws.amazon.com/directconnect/pricing/ https://aws.amazon.com/blogs/aws/aws-data-transfer-prices-reduced/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 11,
    "text": "An online learning company is migrating to the AWS Cloud. The company maintains its student \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n169 \nrecords in a PostgreSQL database. The company needs a solution in which its data is available \nand online across multiple AWS Regions at all times. \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A medical research lab produces data that is related to a new study. The lab wants to make the \ndata available with minimum latency to clinics across the country for their on-premises, file-based \napplications. The data files are stored in an Amazon S3 bucket that has read-only permissions for \neach clinic. \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the files to each clinic’s on-premises applications by using AWS DataSync for processing.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic’s on-premises",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Storage Gateway is a service that connects an on-premises software appliance with cloud- based storage to provide seamless and secure integration between an organization's on- premises IT environment and AWS's storage infrastructure. By deploying a file gateway as a virtual machine on each clinic's premises, the medical research lab can provide low-latency access to the data stored in the S3 bucket while maintaining read-only permissions for each clinic. This solution allows the clinics to access the data files directly from their on-premises file- based applications without the need for data transfer or migration.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company is using a content management system that runs on a single Amazon EC2 instance. \nThe EC2 instance contains both the web server and the database software. The company must \nmake its website platform highly available and must enable the website to scale to meet user \ndemand. \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the database to Amazon RDS, and enable automatic backups. Manually launch another",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon Aurora instance with a read replica in the same Availability",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis approach will provide both high availability and scalability for the website platform. By moving the database to Amazon Aurora with a read replica in another availability zone, it will provide a failover option for the database. The use of an Application Load Balancer and an Auto Scaling group across two availability zones allows for automatic scaling of the website to meet increased user demand. Additionally, creating an AMI from the original EC2 instance allows for easy replication of the instance in case of failure.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company is launching an application on AWS. The application uses an Application Load \nBalancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single target group. The \ninstances are in an Auto Scaling group for each environment. The company requires a \ndevelopment environment and a production environment. The production environment will have \nperiods of high traffic. \nWhich solution will configure the development environment MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Reconfigure the target group in the development environment to have only one EC2 instance as a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the ALB balancing algorithm to least outstanding requests.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Reduce the size of the EC2 instances in both environments.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Reduce the maximum number of EC2 instances in the development environment’s Auto Scaling",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis option will configure the development environment in the most cost-effective way as it reduces the number of instances running in the development environment and therefore reduces the cost of running the application. The development environment typically requires less resources than the production environment, and it is unlikely that the development environment will have periods of high traffic that would require a large number of instances. By reducing the maximum number of instances in the development environment's Auto Scaling group, the company can save on costs while still maintaining a functional development environment.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 15,
    "text": "A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The \nEC2 instances are in private subnets. A solutions architect implements an internet-facing \nApplication Load Balancer (ALB) and specifies the EC2 instances as the target group. However, \nthe internet traffic is not reaching the EC2 instances. \nHow should the solutions architect reconfigure the architecture to resolve this issue? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n171",
    "options": [
      {
        "id": 0,
        "text": "Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the EC2 instances to public subnets. Add a rule to the EC2 instances’ security groups to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the route tables for the EC2 instances’ subnets to send 0.0.0.0/0 traffic through the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create public subnets in each Availability Zone. Associate the public subnets with the ALB.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, \nthe database support team is reporting slow reads against the DB instance and recommends \nadding a read replica. \nWhich combination of actions should a solutions architect take before implementing this change? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable binlog replication on the RDS primary node.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Choose a failover priority for the source DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow long-running transactions to complete on the source DB instance.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a global table and specify the AWS Regions where the table will be available.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Enable automatic backups on the source instance by setting the backup retention period to a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAn active, long-running transaction can slow the process of creating the read replica. We recommend that you wait for long-running transactions to complete before creating a read replica. If you create multiple read replicas in parallel from the same source DB instance, Amazon RDS takes only one snapshot at the start of the first create action. When creating a read replica, there are a few things to consider. First, you must enable automatic backups on the source DB instance by setting the backup retention period to a value other than 0. This requirement also applies to a read replica that is the source DB instance for another read replica. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A company runs analytics software on Amazon EC2 instances. The software accepts job \nrequests from users to process data that has been uploaded to Amazon S3. Users report that \nsome submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances \nhave a consistent CPU utilization at or near 100%. The company wants to improve system \nperformance and scale the system based on user load. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a copy of the instance. Place all instances behind an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more",
        "correct": false
      },
      {
        "id": 3,
        "text": "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nBy routing incoming requests to Amazon SQS, the company can decouple the job requests from the processing instances. This allows them to scale the number of instances based on the size of the queue, providing more resources when needed. Additionally, using an Auto Scaling group based on the queue size will automatically scale the number of instances up or down depending on the workload. Updating the software to read from the queue will allow it to process the job requests in a more efficient manner, improving the performance of the system.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 18,
    "text": "A company is implementing a shared storage solution for a media application that is hosted in the \nAWS Cloud. The company needs the ability to use SMB clients to access data. The solution must \nbe fully managed. \nWhich AWS solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx has native support for Windows file system features and for the industry-standard Server Message Block (SMB) protocol to access file storage over a network. https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company’s security team requests that network traffic be captured in VPC Flow Logs. The logs \nwill be frequently accessed for 90 days and then accessed intermittently. \nWhat should a solutions architect do to meet these requirements when configuring the logs?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogsConcepts.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect needs to design a system to store client case files. The files are core \ncompany assets and are important. The number of files will grow over time. \nThe files must be simultaneously accessible from multiple application servers that run on Amazon \nEC2 instances. The solution must have built-in redundancy. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Backup",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon EFS provides a simple, scalable, fully managed file system that can be simultaneously accessed from multiple EC2 instances and provides built-in redundancy. It is optimized for multiple EC2 instances to access the same files, and it is designed to be highly available, durable, and secure. It can scale up to petabytes of data and can handle thousands of concurrent connections, and is a cost-effective solution for storing and accessing large amounts of data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are \nattached to an IAM group. \n \n \n \n \n \nA cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer \nbe able to perform?",
    "options": [
      {
        "id": 0,
        "text": "Deleting IAM users",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deleting directories",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deleting Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deleting logs from Amazon CloudWatch Logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThere is an explicit DENY on deleting directories in the second policy. So the only thing that can be deleted is EC2 instances as per the permission in the first policy.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is reviewing a recent migration of a three-tier application to a VPC. The security team \ndiscovers that the principle of least privilege is not being applied to Amazon EC2 security group \ningress and egress rules between the application tiers. \nWhat should a solutions architect do to correct this issue?",
    "options": [
      {
        "id": 0,
        "text": "Create security group rules using the instance ID as the source or destination.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create security group rules using the security group ID as the source or destination.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create security group rules using the VPC CIDR blocks as the source or destination.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create security group rules using the subnet CIDR blocks as the source or destination.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe ID of a security group (referred to here as the specified security group). For example, the current security group, a security group from the same VPC, or a security group for a peered VPC. This allows traffic based on the private IP addresses of the resources associated with the specified security group. This does not add rules from the specified security group to the current security group. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A solutions architect is implementing a document review application using an Amazon S3 bucket \nfor storage. The solution must prevent accidental deletion of the documents and ensure that all \nversions of the documents are available. Users must be able to download, modify, and upload \ndocuments. \nWhich combination of actions should be taken to meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable a read-only bucket ACL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable versioning on the bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Attach an IAM policy to the bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable MFA Delete on the bucket.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Encrypt the bucket using AWS KMS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nTo prevent or mitigate future accidental deletions, consider the following features: - Enable versioning to keep historical versions of an object. - Enable cross-region replication of objects. - Enable MFA Delete to require multi-factor authentication (MFA) when deleting an object version.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company is building a solution that will report Amazon EC2 Auto Scaling events across all the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n175 \napplications in an AWS account. The company needs to use a serverless solution to store the \nEC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 \nto provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 \ninstance launches. \nHow should the company move the data to Amazon S3 to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nYou can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. One of the use cases is Data Lake: create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric- Streams.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company has an application that places hundreds of .csv files into an Amazon S3 bucket every \nhour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the \nfile to Apache Parquet format and place the output file into an S3 bucket. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format,",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types- for-converting-data-to-apache-parquet.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company is implementing new data retention policies for all databases that run on Amazon \nRDS DB instances. The company must retain daily backups for a minimum period of 2 years. The \nbackups must be consistent and restorable. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n176 \nWhich solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company’s compliance team needs to move its file shares to AWS. The shares run on a \nWindows Server SMB file share. A self-managed on-premises Active Directory controls access to \nthe files and folders. \nThe company wants to use Amazon FSx for Windows File Server as part of the solution. The \ncompany must ensure that the on-premises Active Directory groups restrict access to the FSx for \nWindows File Server SMB compliance shares, folders, and files after the move to AWS. The \ncompany has created an FSx for Windows File Server file system. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory",
        "correct": false
      },
      {
        "id": 1,
        "text": "Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict",
        "correct": false
      },
      {
        "id": 3,
        "text": "Join the file system to the Active Directory to restrict access.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nJoining the FSx for Windows File Server file system to the on-premises Active Directory will allow the company to use the existing Active Directory groups to restrict access to the file shares, folders, and files after the move to AWS. This option allows the company to continue using their existing access controls and management structure, making the transition to AWS more seamless.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company recently announced the deployment of its retail website to a global audience. The \nwebsite runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances \nrun in an Auto Scaling group across multiple Availability Zones. \nThe company wants to provide its customers with different versions of content based on the \ndevices that the customers use to access the website. \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudFront to cache multiple versions of the content.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a host header in a Network Load Balancer to forward traffic to different instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a Lambda@Edge function to send specific objects to users based on the User-Agent",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB).",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-caching.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions \narchitect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s \nAmazon EC2 instances. Both VPCs are in the us-east-1 Region. \nThe solutions architect must implement a solution to provide the application’s EC2 instances with \naccess to the ElastiCache cluster. \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a peering connection between the VPCs. Add a route table entry for the peering",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a peering connection between the VPCs. Add a route table entry for the peering",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCreating a peering connection between the VPCs allows the application's EC2 instances to communicate with the ElastiCache cluster directly and efficiently. This is the most cost-effective solution as it does not involve creating additional resources such as a Transit VPC, and it does not incur additional costs for traffic passing through the Transit VPC. Additionally, it is also more secure as it allows you to configure a more restrictive security group rule to allow inbound connection from only the application's security group.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 30,
    "text": "A company is building an application that consists of several microservices. The company has \ndecided to use container technologies to deploy its software on AWS. The company needs a \nsolution that minimizes the amount of ongoing effort for maintenance and scaling. The company \ncannot manage additional infrastructure. \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by \nAmazon Route 53. The company occasionally experiences a timeout error when attempting to \nbrowse the application. The networking team finds that some DNS queries return IP addresses of \nunhealthy instances, resulting in the timeout error. \nWhat should a solutions architect implement to overcome these timeout errors?",
    "options": [
      {
        "id": 0,
        "text": "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAn Application Load Balancer (ALB) allows you to distribute incoming traffic across multiple backend instances, and can automatically route traffic to healthy instances while removing traffic from unhealthy instances. By using an ALB in front of the EC2 instances and routing traffic to it from Route 53, the load balancer can perform health checks on the instances and only route traffic to healthy instances, which should help to reduce or eliminate timeout errors caused by unhealthy instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "A solutions architect needs to design a highly available application consisting of web, application, \nand database tiers. HTTPS content delivery should be as close to the edge as possible, with the \nleast delivery time. \nWhich solution meets these requirements and is MOST secure?",
    "options": [
      {
        "id": 0,
        "text": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution meets the requirements for a highly available application with web, application, and database tiers, as well as providing edge-based content delivery. Additionally, it maximizes security by having the ALB in a private subnet, which limits direct access to the web servers, while still being able to serve traffic over the Internet via the public ALB. This will ensure that the web servers are not exposed to the public Internet, which reduces the attack surface and provides a secure way to access the application. https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A company has a popular gaming platform running on AWS. The application is sensitive to \nlatency because latency can impact the user experience and introduce unfair advantages to \nsome players. The application is deployed in every AWS Region. It runs on Amazon EC2 \ninstances that are part of Auto Scaling groups configured behind Application Load Balancers \n(ALBs). A solutions architect needs to implement a mechanism to monitor the health of the \napplication and redirect traffic to healthy endpoints. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon DynamoDB database to serve as the data store for the application. Create",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nWhen you have an Application Load Balancer or Network Load Balancer that includes multiple target groups, Global Accelerator considers the load balancer endpoint to be healthy only if each target group behind the load balancer has at least one healthy target. If any single target group for the load balancer has only unhealthy targets, Global Accelerator considers the endpoint to be unhealthy. https://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoint-groups-health-check- options.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A company has one million users that use its mobile app. The company must analyze the data \nusage in near-real time. The company also must encrypt the data in near-real time and must \nstore the data in a centralized location in Apache Parquet format for further processing. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n180 \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution will meet the requirements with the least operational overhead as it uses Amazon Kinesis Data Firehose, which is a fully managed service that can automatically handle the data collection, data transformation, encryption, and data storage in near-real time. Kinesis Data Firehose can automatically store the data in Amazon S3 in Apache Parquet format for further processing. Additionally, it allows you to create an Amazon Kinesis Data Analytics application to analyze the data in near real-time, with no need to manage any infrastructure or invoke any Lambda function. This way you can process a large amount of data with the least operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "An ecommerce company has noticed performance degradation of its Amazon RDS based web \napplication. The performance degradation is attributed to an increase in the number of read-only \nSQL queries triggered by business analysts. A solutions architect needs to solve the problem with \nminimal changes to the existing web application. \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read replica of the primary database and have the business analysts run their queries.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCreating a read replica of the primary RDS database will offload the read-only SQL queries from the primary database, which will help to improve the performance of the web application. Read replicas are exact copies of the primary database that can be used to handle read-only traffic, which will reduce the load on the primary database and improve the performance of the web application. This solution can be implemented with minimal changes to the existing web application, as the business analysts can continue to run their queries on the read replica without modifying the code.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A company runs an internal browser-based application. The application runs on Amazon EC2 \ninstances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto \nScaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 \ninstances during work hours, but scales down to 2 instances overnight. Staff are complaining that \nthe application is very slow when the day begins, although it runs well by mid-morning. \n \nHow should the scaling be changed to address the staff complaints and keep costs to a \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n181 \nminimum?",
    "options": [
      {
        "id": 0,
        "text": "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement a target tracking action triggered at a lower CPU threshold, and decrease the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto \nScaling group. An Amazon RDS for Oracle instance is the application' s data layer that uses \nOracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is \ncausing the EC2 instances to become overloaded and the RDS instance to run out of storage. \nThe Auto Scaling group does not have any scaling metrics and defines the minimum healthy \ninstance count only. The company predicts that traffic will continue to increase at a steady but \nunpredictable rate before leveling off. \n \nWhat should a solutions architect do to ensure the system can automatically scale for the \nincreased traffic? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure storage Auto Scaling on the RDS for Oracle instance.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon Aurora to use Auto Scaling storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an alarm on the RDS for Oracle instance for low free storage space.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the Auto Scaling group to use the average CPU as the scaling metric.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the Auto Scaling group to use the average free memory as the scaling metric.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "A company provides an online service for posting video content and transcoding it for use by any \nmobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) \nStandard to collect and store the videos so that multiple Amazon EC2 Linux instances can access \nthe video content for processing. As the popularity of the service has grown over time, the \nstorage costs have become too expensive. \n \nWhich storage solution is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Storage Gateway for files to store and process the video content.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Storage Gateway for volumes to store and process the video content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nA better solution would be to use a transcoding service like Amazon Elastic Transcoder to process the video content directly from Amazon S3. This would eliminate the need for storing the Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 39,
    "text": "A company wants to create an application to store employee data in a hierarchical structured \nrelationship. The company needs a minimum-latency response to high-traffic queries for the \nemployee data and must protect any sensitive data. The company also needs to receive monthly \nemail messages if any financial information is present in the employee data. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 40,
    "text": "A company has an application that is backed by an Amazon DynamoDB table. The company's \ncompliance requirements specify that database backups must be taken every month, must be \navailable for 6 months, and must be retained for 7 years. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Backup plan to back up the DynamoDB table on the first day of each month.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company is using Amazon CloudFront with its website. The company has enabled logging on \nthe CloudFront distribution, and logs are saved in one of the company's Amazon S3 buckets. The \ncompany needs to perform advanced analyses on the logs and build visualizations. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nQuicksite creating data visualizations. https://docs.aws.amazon.com/quicksight/latest/user/welcome.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After \na routine compliance check, the company sets a standard that requires a recovery point objective \n(RPO) of less than 1 second for all its production databases. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable a Multi-AZ deployment for the DB instance.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable auto scaling for the DB instance in one Availability Zone.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the DB instance in one Availability Zone, and create multiple read replicas in a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the DB instance in one Availability Zone, and configure AWS Database Migration",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy using Multi-AZ deployment, the company can achieve an RPO of less than 1 second because the standby instance is always in sync with the primary instance, ensuring that data changes are continuously replicated.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company runs a web application that is deployed on Amazon EC2 instances in the private \nsubnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets \ndirects web traffic to the EC2 instances. The company wants to implement new security \nmeasures to restrict inbound traffic from the ALB to the EC2 instances while preventing access \nfrom any other source inside or outside the private subnet of the EC2 instances. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a route in a route table to direct traffic from the internet to the private IP addresses of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the security group for the EC2 instances to only allow traffic that comes from the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the ALB to allow any TCP traffic on any port.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis ensures that only the traffic originating from the ALB is allowed access to the EC2 instances in the private subnet, while denying any other traffic from other sources.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A research company runs experiments that are powered by a simulation application and a \nvisualization application. The simulation application runs on Linux and outputs intermediate data \nto an NFS share every 5 minutes. The visualization application is a Windows desktop application \nthat displays the simulation output and requires an SMB file system. \n \nThe company maintains two synchronized file systems. This strategy is causing data duplication \nand inefficient resource usage. The company needs to migrate the applications to AWS without \nmaking code changes to either application. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx for NetApp ONTAP is a fully-managed shared storage service built on NetApp's popular ONTAP file system. Amazon FSx for NetApp ONTAP provides the popular features, performance, and APIs of ONTAP file systems with the agility, scalability, and simplicity of a fully managed AWS service, making it easier for customers to migrate on-premises applications that rely on NAS appliances to AWS. FSx for ONTAP file systems are similar to on-premises NetApp clusters. Within each file system that you create, you also create one or more storage virtual machines (SVMs). These are isolated file servers each with their own endpoints for NFS, SMB, and management access, as well as authentication (for both administration and end-user data access). In turn, each SVM has one or more volumes which store your data. https://aws.amazon.com/de/blogs/storage/getting-started-cloud-file-storage-with-amazon-fsx-for- netapp-ontap-using-netapp-management-tools/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "As part of budget planning, management wants a report of AWS billed items listed by user. The \ndata will be used to create department budgets. A solutions architect needs to determine the \nmost efficient way to obtain this report information. \n \nWhich solution meets these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n185",
    "options": [
      {
        "id": 0,
        "text": "Run a query with Amazon Athena to generate the report.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a report in Cost Explorer and download the report.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Access the bill details from the billing dashboard and download the bill.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company hosts its static website by using Amazon S3. The company wants to add a contact \nform to its webpage. The contact form will have dynamic server-side components for users to \ninput their name, email address, phone number, and user message. The company anticipates \nthat there will be fewer than 100 site visits each month. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL,",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites- using-aws-lambda-amazon-api-gateway-and-amazon-ses/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The \nstatic website uses a database backend. The company notices that the website does not reflect \nupdates that have been made in the website's Git repository. The company checks the \ncontinuous integration and continuous delivery (CI/CD) pipeline between the Git repository and \nAmazon S3. The company verifies that the webhooks are configured properly and that the CI/CD \npipeline is sending messages that indicate successful deployments. \n \nA solutions architect needs to implement a solution that displays the updates on the website. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Add an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Invalidate the CloudFront cache.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Certificate Manager (ACM) to validate the website's SSL certificate.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nInvalidate the CloudFront cache: The solutions architect should invalidate the CloudFront cache to ensure that the latest version of the website is being served to users. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 48,
    "text": "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. \nThe application has three tiers: an application tier, a business tier, and a database tier with \nMicrosoft SQL Server. The company wants to use specific features of SQL Server such as native \nbackups and Data Quality Services. The company also needs to share files for processing \nbetween the tiers. \n \nHow should a solutions architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nData Quality Services: If this feature is critical to your workload, consider choosing Amazon RDS Custom or Amazon EC2. https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/comparison.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 49,
    "text": "A company is migrating a Linux-based web server group to AWS. The web servers must access \nfiles in a shared file store for some content. The company must not make any changes to the \napplication. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 Standard bucket with access to the web servers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCreate an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers. To meet the requirements of providing a shared file store for Linux-based web servers without making changes to the application, using an Amazon EFS file system is the best solution. Amazon EFS is a managed NFS file system service that provides shared access to files across multiple Linux-based instances, which makes it suitable for this use case. Amazon S3 is not ideal for this scenario since it is an object storage service and not a file system, and it requires additional tools or libraries to mount the S3 bucket as a file system. Amazon CloudFront can be used to improve content delivery performance but is not necessary for this requirement. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that \nis located in the same AWS account. \n \nWhich solution will meet these requirements in the MOST secure manner?",
    "options": [
      {
        "id": 0,
        "text": "Apply an S3 bucket policy that grants read access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Embed an access key and a secret key in the Lambda function's code to grant the required IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the most secure and recommended way to provide an AWS Lambda function with access to an S3 bucket. It involves creating an IAM role that the Lambda function assumes, and attaching an IAM policy to the role that grants the necessary permissions to read from the S3 bucket. https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are \nin an Auto Scaling group that scales in response to user demand. The company wants to \noptimize cost savings without making a long-term commitment. \n \nWhich EC2 instance purchasing option should a solutions architect recommend to meet these \nrequirements?",
    "options": [
      {
        "id": 0,
        "text": "Dedicated Instances only",
        "correct": false
      },
      {
        "id": 1,
        "text": "On-Demand Instances only",
        "correct": false
      },
      {
        "id": 2,
        "text": "A mix of On-Demand Instances and Spot Instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "A mix of On-Demand Instances and Reserved Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances- groups.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 52,
    "text": "A media company uses Amazon CloudFront for its publicly available streaming video content. \nThe company wants to secure the video content that is hosted in Amazon S3 by controlling who \nhas access. Some of the company's users are using a custom HTTP client that does not support \ncookies. Some of the company's users are unable to change the hardcoded URLs that they are \nusing for access. \n \nWhich services or methods will meet these requirements with the LEAST impact to the users? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n188",
    "options": [
      {
        "id": 0,
        "text": "Signed cookies",
        "correct": true
      },
      {
        "id": 1,
        "text": "Signed URLs",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS AppSync",
        "correct": false
      },
      {
        "id": 3,
        "text": "JSON Web Token (JWT)",
        "correct": false
      },
      {
        "id": 4,
        "text": "AWS Secrets Manager",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSigned URLs are URLs that grant temporary access to an S3 object. They include a signature that verifies the authenticity of the request, as well as an expiration date that limits the time during which the URL is valid. This solution will work for users who are using custom HTTP clients that do not support cookies. Signed cookies are similar to signed URLs, but they use cookies to grant temporary access to S3 objects. This solution will work for users who are unable to change the hardcoded URLs that they are using for access. https://aws.amazon.com/blogs/media/secure-content-using-cloudfront-functions/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A company is preparing a new data platform that will ingest real-time streaming data from multiple \nsources. The company needs to transform the data before writing the data to Amazon S3. The \ncompany needs the ability to use SQL to query the transformed data. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 54,
    "text": "A company has an on-premises volume backup solution that has reached its end of life. The \ncompany wants to use AWS as part of a new backup solution and wants to maintain local access \nto all the data while it is backed up on AWS. The company wants to ensure that the data backed \nup on AWS is automatically and securely transferred. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nIn the cached mode, your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access. In the stored mode, your primary data is stored locally and your entire dataset is available for low- latency access while asynchronously backed up to AWS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. \nTraffic must not traverse the internet. \n \nHow should a solutions architect configure access to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a private hosted zone by using Amazon Route 53.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a gateway VPC endpoint for Amazon S3 in the VPC.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the EC2 instances to use a NAT gateway to access the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains \npersonally identifiable information (PII). The company wants to use the data in three applications. \nOnly one of the applications needs to process the PII. The PII must be removed before the other \ntwo applications process the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object",
        "correct": true
      },
      {
        "id": 2,
        "text": "Process the data and store the transformed data in three separate Amazon S3 buckets so that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Process the data and store the transformed data in three separate Amazon DynamoDB tables so",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon S3 Object Lambda allows you to add custom code to S3 GET requests, which means that you can modify the data before it is returned to the requesting application. In this case, you Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A development team has launched a new application that is hosted on Amazon EC2 instances \ninside a development VPC. A solutions architect needs to create a new VPC in the same \naccount. The new VPC will be peered with the development VPC. The VPC CIDR block for the \ndevelopment VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the \nnew VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. \n \nWhat is the SMALLEST CIDR block that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "10.0.1.0/32",
        "correct": false
      },
      {
        "id": 1,
        "text": "192.168.0.0/24",
        "correct": false
      },
      {
        "id": 2,
        "text": "192.168.1.0/32",
        "correct": false
      },
      {
        "id": 3,
        "text": "10.0.1.0/24",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe allowed block size is between a /28 netmask and /16 netmask. The CIDR block must not overlap with any existing CIDR block that's associated with the VPC. https://docs.aws.amazon.com/vpc/latest/userguide/configure-your-vpc.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company deploys an application on five Amazon EC2 instances. An Application Load Balancer \n(ALB) distributes traffic to the instances by using a target group. The average CPU usage on \neach of the instances is below 10% most of the time, with occasional surges to 65%. \n \nA solutions architect needs to implement a solution to automate the scalability of the application. \nThe solution must optimize the cost of the architecture and must ensure that the application has \nenough CPU resources when surges occur. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIt allows for automatic scaling based on the average CPU utilization of the EC2 instances in the target group. With the use of a target tracking scaling policy based on the ASGAverageCPUUtilization metric, the EC2 Auto Scaling group can ensure that the target value of 50% is maintained while scaling the number of instances in the group up or down as needed. This will help ensure that the application has enough CPU resources during surges without overprovisioning, thus optimizing the cost of the architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 59,
    "text": "A company is running a critical business application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances run in an Auto Scaling group and access an \nAmazon RDS DB instance. \n \nThe design did not pass an operational review because the EC2 instances and the DB instance \nare all located in a single Availability Zone. A solutions architect must update the design to use a \nsecond Availability Zone. \n \nWhich solution will make the application highly available?",
    "options": [
      {
        "id": 0,
        "text": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nA subnet must reside within a single Availability Zone. https://aws.amazon.com/vpc/faqs/#:~:text=Can%20a%20subnet%20span%20Availability,within% 20a%20single%20Availability%20Zone.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-\nmillisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds \nof Amazon EC2 instances that run Amazon Linux will distribute and process the data. \n \nWhich solution will meet the performance requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume' tiering policy to ALL.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for NetApp ONTAP file system. Set each volume's tiering policy to NONE.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreate an Amazon S3 bucket to store the raw data Create an Amazon FSx for Lustre file system that uses persistent SSD storage Select the option to import data from and export data to Amazon S3 Mount the file system on the EC2 instances. Amazon FSx for Lustre uses SSD storage for sub-millisecond latencies and up to 6 GBps throughput, and can import data from and export data to Amazon S3. Additionally, the option to select persistent SSD storage will ensure that the data is stored on the disk and not lost if the file system is stopped.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A company needs to migrate a legacy application from an on-premises data center to the AWS \nCloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a \nweek. The application's database storage continues to grow over time. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A university research laboratory needs to migrate 30 TB of data from an on-premises Windows \nfile server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that \nmany other departments in the university share. \n \nThe laboratory wants to implement a data migration service that will maximize the performance of \nthe data transfer. However, the laboratory needs to be able to control the amount of bandwidth \nthat the service uses to minimize the impact on other departments. The data migration must take \nplace within the next 5 days. \n \nWhich AWS solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "AWS Snowcone",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx File Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS DataSync",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Transfer Family",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nDataSync can be used to migrate data between on-premises Windows file servers and Amazon FSx for Windows File Server with its compatibility for Windows file systems. The laboratory needs to migrate a large amount of data (30 TB) within a relatively short timeframe (5 days) and limit the impact on other departments' network traffic. Therefore, AWS DataSync can meet these requirements by providing fast and efficient data transfer with network throttling capability to control bandwidth usage. https://docs.aws.amazon.com/datasync/latest/userguide/configure-bandwidth.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "A company wants to create a mobile app that allows users to stream slow-motion video clips on \ntheir mobile devices. Currently, the app captures video clips and uploads the video clips in raw \nformat into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. \nHowever, the videos are large in their raw format. \n \nUsers are experiencing issues with buffering and playback on mobile devices. The company \nwants to implement solutions to maximize the performance and scalability of the app while \nminimizing operational overhead. \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon CloudFront for content delivery and caching.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and",
        "correct": false
      },
      {
        "id": 4,
        "text": "Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/elastictranscoder/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 64,
    "text": "A company is launching a new application deployed on an Amazon Elastic Container Service \n(Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is \nmonitoring CPU and memory usage because it is expecting high traffic to the application upon its \nlaunch. However, the company wants to reduce costs when utilization decreases. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto- scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  }
]