[
  {
    "id": 0,
    "text": "A company uses Amazon S3 to host its static website. The company wants to add a contact form \nto the webpage. The contact form will have dynamic server-side components for users to input \ntheir name, email address, phone number, and user message. \n \nThe company expects fewer than 100 site visits each month. The contact form must notify the \ncompany by email when a customer fills out the form. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Host the dynamic contact form in Amazon Elastic Container Service (Amazon ECS). Set up",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway endpoint that returns the contact form from an AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the website by using AWS Amplify Hosting for static content and dynamic content. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the website from Amazon S3 to Amazon EC2 instances that run Windows Server. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company creates dedicated AWS accounts in AWS Organizations for its business units. \nRecently, an important notification was sent to the root user email address of a business unit \naccount instead of the assigned account owner. The company wants to ensure that all future \nnotifications can be sent to different employees based on the notification categories of billing, \noperations, or security. \n \nWhich solution will meet these requirements MOST securely? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n450",
    "options": [
      {
        "id": 0,
        "text": "Configure each AWS account to use a single email address that the company manages. Ensure",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure each AWS account to use a different email distribution list for each business unit that",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure each AWS account root user email address to be the individual company managed",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure each AWS account root user to use email aliases that go to a centralized mailbox.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company runs an ecommerce application on AWS. Amazon EC2 instances process purchases \nand store the purchase details in an Amazon Aurora PostgreSQL DB cluster. \n \nCustomers are experiencing application timeouts during times of peak usage. A solutions \narchitect needs to rearchitect the application so that the application can scale to meet peak usage \ndemands. \n \nWhich combination of actions will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure an Auto Scaling group of new EC2 instances to retry the purchases until the processing",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the application to use an Amazon ElastiCache cluster in front of the Aurora",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the application to send the purchase requests to an Amazon Simple Queue Service",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Lambda function to retry the ticket purchases until the processing is complete.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure an Amazon AP! Gateway REST API with a usage plan.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A company that uses AWS Organizations runs 150 applications across 30 different AWS \naccounts. The company used AWS Cost and Usage Report to create a new report in the \nmanagement account. The report is delivered to an Amazon S3 bucket that is replicated to a \nbucket in the data collection account. \n \nThe company's senior leadership wants to view a custom dashboard that provides NAT gateway \ncosts each day starting at the beginning of the current month. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n451",
    "options": [
      {
        "id": 0,
        "text": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure",
        "correct": false
      },
      {
        "id": 1,
        "text": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure",
        "correct": true
      },
      {
        "id": 2,
        "text": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company is hosting a high-traffic static website on Amazon S3 with an Amazon CloudFront \ndistribution that has a default TTL of 0 seconds. The company wants to implement caching to \nimprove performance for the website. However, the company also wants to ensure that stale \ncontent is not served for more than a few minutes after a deployment. \n \nWhich combination of caching methods should a solutions architect implement to meet these \nrequirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Set the CloudFront default TTL to 2 minutes.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set a default TTL of 2 minutes on the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add a Cache-Control private directive to the objects in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda@Edge function to add an Expires header to HTTP responses. Configure",
        "correct": false
      },
      {
        "id": 4,
        "text": "Add a Cache-Control max-age directive of 24 hours to the objects in Amazon S3. On deployment,",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because Amazon Neptune is a graph database service. While it provides low latency for graph-based queries, it is not the best choice for a simple leaderboard application. The data structure and query patterns of a leaderboard are better suited for key-value stores or sorted sets, which are offered by ElastiCache for Redis or DynamoDB with DAX. Neptune is optimized for complex relationship analysis, which is not a primary requirement for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "A company runs its application by using Amazon EC2 instances and AWS Lambda functions. The \nEC2 instances run in private subnets of a VPC. The Lambda functions need direct network \naccess to the EC2 instances for the application to work. \n \nThe application will run for 1 year. The number of Lambda functions that the application uses will \nincrease during the 1-year period. The company must minimize costs on all application \nresources. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to the private subnets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to new public subnets in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a Compute Savings Plan. Connect the Lambda functions to the private subnets that",
        "correct": true
      },
      {
        "id": 3,
        "text": "Purchase a Compute Savings Plan. Keep the Lambda functions in the Lambda service VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A company has deployed a multi-account strategy on AWS by using AWS Control Tower. The \ncompany has provided individual AWS accounts to each of its developers. The company wants to \nimplement controls to limit AWS resource costs that the developers incur. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Instruct each developer to tag all their resources with a tag that has a key of CostCenter and a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Budgets to establish budgets for each developer account. Set up budget alerts for",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Cost Explorer to monitor and report on costs for each developer account. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Service Catalog to allow developers to launch resources within a limited cost range.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A solutions architect is designing a three-tier web application. The architecture consists of an \ninternet-facing Application Load Balancer (ALB) and a web tier that is hosted on Amazon EC2 \ninstances in private subnets. The application tier with the business logic runs on EC2 instances in \nprivate subnets. The database tier consists of Microsoft SQL Server that runs on EC2 instances \nin private subnets. Security is a high priority for the company. \n \nWhich combination of security group configurations should the solutions architect use? (Choose \nthree.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the web tier to allow inbound HTTPS traffic from the security",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the security group for the web tier to allow outbound HTTPS traffic to 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the security group for the database tier to allow inbound Microsoft SQL Server traffic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the database tier to allow outbound HTTPS traffic and Microsoft",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the security group for the application tier to allow inbound HTTPS traffic from the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because when using bucket default settings, you specify a retention *period* (e.g., number of days or years), not a 'Retain Until Date'. The 'Retain Until Date' is calculated based on the object's creation date and the specified retention period.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company has released a new version of its production application. The company's workload \nuses Amazon EC2, AWS Lambda, AWS Fargate, and Amazon SageMaker. \n \nThe company wants to cost optimize the workload now that usage is at a steady state. The \ncompany wants to cover the most services with the fewest savings plans. \n \nWhich combination of savings plans will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Purchase an EC2 Instance Savings Plan for Amazon EC2 and SageMaker.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase a Compute Savings Plan for Amazon EC2, Lambda, and SageMaker.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a SageMaker Savings Plan.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Purchase a Compute Savings Plan for Lambda, Fargate, and Amazon EC2.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Purchase an EC2 Instance Savings Plan for Amazon EC2 and Fargate.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 9,
    "text": "A company uses a Microsoft SQL Server database. The company's applications are connected to \nthe database. The company wants to migrate to an Amazon Aurora PostgreSQL database with \nminimal changes to the application code. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to rewrite the SQL queries in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Babelfish on Aurora PostgreSQL to run the SQL queries from the applications.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database schema and data by using the AWS Schema Conversion Tool (AWS SCT)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS Proxy to connect the applications to Aurora PostgreSQL.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Database Migration Service (AWS DMS) to rewrite the SQL queries in the applications.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. It allows S3 to automatically manage the tiering of objects based on access patterns, potentially reducing storage costs for data that is not frequently accessed. This is a common and recommended practice for cost optimization.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company plans to rehost an application to Amazon EC2 instances that use Amazon Elastic \nBlock Store (Amazon EBS) as the attached storage. \n \nA solutions architect must design a solution to ensure that all newly created Amazon EBS \nvolumes are encrypted by default. The solution must also prevent the creation of unencrypted \nEBS volumes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the EC2 account attributes to always encrypt new EBS volumes.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Config. Configure the encrypted-volumes identifier. Apply the default AWS Key",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Systems Manager to create encrypted copies of the EBS volumes. Reconfigure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a customer managed key in AWS Key Management Service (AWS KMS). Configure AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "An ecommerce company wants to collect user clickstream data from the company's website for \nreal-time analysis. The website experiences fluctuating traffic patterns throughout the day. The \ncompany needs a scalable solution that can adapt to varying levels of traffic. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Firehose to capture the clickstream data. Use AWS Glue to process",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Kinesis Video Streams to capture the clickstream data. Use AWS Glue to process",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company needs to optimize its Amazon S3 storage costs for an application that generates \nmany files that cannot be recreated. Each file is approximately 5 MB and is stored in Amazon S3 \nStandard storage. \n \nThe company must store the files for 4 years before the files can be deleted. The files must be \nimmediately accessible. The files are frequently accessed in the first 30 days of object creation, \nbut they are rarely accessed after the first 30 days. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Lifecycle policy to move the files to S3 Glacier Instant Retrieval 30 days after object",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 Lifecycle policy to move the files to S3 One Zone-Infrequent Access (S3 One Zone-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 14,
    "text": "A company runs its critical storage application in the AWS Cloud. The application uses Amazon \nS3 in two AWS Regions. The company wants the application to send remote user data to the \nnearest S3 bucket with no public network congestion. The company also wants the application to \nfail over with the least amount of management of Amazon S3. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement an active-active design between the two Regions. Configure the application to use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an active-passive configuration with S3 Multi-Region Access Points. Create a global endpoint",
        "correct": false
      },
      {
        "id": 2,
        "text": "Send user data to the regional S3 endpoints closest to the user. Configure an S3 cross-account",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon S3 to use Multi-Region Access Points in an active-active configuration with a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company is migrating a data center from its on-premises location to AWS. The company has \nseveral legacy applications that are hosted on individual virtual servers. Changes to the \napplication designs cannot be made. \n \nEach individual virtual server currently runs as its own EC2 instance. A solutions architect needs \nto ensure that the applications are reliable and fault tolerant after migration to AWS. The \napplications will run on Amazon EC2 instances. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group that has a minimum of one and a maximum of one. Create an",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Backup to create an hourly backup of the EC2 instance that hosts each application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Machine Image (AMI) of each application instance. Launch two new EC2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Mitigation Hub Refactor Spaces to migrate each application off the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company wants to isolate its workloads by creating an AWS account for each workload. The \ncompany needs a solution that centrally manages networking components for the workloads. The \nsolution also must create accounts with automatic security controls (guardrails). \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Control Tower to deploy accounts. Create a networking account that has a VPC with",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Organizations to deploy accounts. Create a networking account that has a VPC with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Control Tower to deploy accounts. Deploy a VPC in each workload account. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Organizations to deploy accounts. Deploy a VPC in each workload account. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The website serves static content. Website traffic is increasing. The company wants to \nminimize the website hosting costs. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the website to an Amazon S3 bucket. Configure an Amazon CloudFront distribution for the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Move the website to an Amazon S3 bucket. Configure an Amazon ElastiCache cluster for the S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the website to AWS Amplify. Configure an ALB to resolve to the Amplify website.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the website to AWS Amplify. Configure EC2 instances to cache the website.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 18,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n458 \nA company is implementing a shared storage solution for a media application that the company \nhosts on AWS. The company needs the ability to use SMB clients to access stored data. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Storage Gateway Volume Gateway. Create a file share that uses the required",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway Tape Gateway. Configure tapes to use Amazon S3. Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server file system. Connect the application server to the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is designing its production application's disaster recovery (DR) strategy. The \napplication is backed by a MySQL database on an Amazon Aurora cluster in the us-east-1 \nRegion. The company has chosen the us-west-1 Region as its DR Region. \n \nThe company's target recovery point objective (RPO) is 5 minutes and the target recovery time \nobjective (RTO) is 20 minutes. The company wants to minimize configuration changes. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create an Aurora read replica in us-west-1 similar in size to the production application's Aurora",
        "correct": false
      },
      {
        "id": 1,
        "text": "Convert the Aurora cluster to an Aurora global database. Configure managed failover.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new Aurora cluster in us-west-1 that has Cross-Region Replication.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new Aurora cluster in us-west-1. Use AWS Database Migration Service (AWS DMS) to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A company runs a critical data analysis job each week before the first day of the work week. The \njob requires at least 1 hour to complete the analysis. The job is stateful and cannot tolerate \ninterruptions. The company needs a solution to run the job on AWS. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n459",
    "options": [
      {
        "id": 0,
        "text": "Create a container for the job. Schedule the job to run as an AWS Fargate task on an Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the job to run in an AWS Lambda function. Create a scheduled rule in Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Auto Scaling group of Amazon EC2 Spot Instances that run Amazon Linux.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS DataSync task to run the job. Configure a cron expression to run the task on a",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A company runs workloads in the AWS Cloud. The company wants to centrally collect security \ndata to assess security across the entire company and to improve workload protection. \n \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Configure a data lake in AWS Lake Formation. Use AWS Glue crawlers to ingest the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an AWS Lambda function to collect the security data in .csv format. Upload the data to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a data lake in Amazon Security Lake to collect the security data. Upload the data to an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an AWS Database Migration Service (AWS DMS) replication instance to load the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is migrating five on-premises applications to VPCs in the AWS Cloud. Each \napplication is currently deployed in isolated virtual networks on premises and should be deployed \nsimilarly in the AWS Cloud. The applications need to reach a shared services VPC. All the \napplications must be able to communicate with each other. \n \nIf the migration is successful, the company will repeat the migration process for more than 100 \napplications. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy software VPN tunnels between the application VPCs and the shared services VPC. Add",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy VPC peering connections between the application VPCs and the shared services VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Direct Connect connection between the application VPCs and the shared",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a transit gateway with associations between the transit gateway and the application VPCs",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A company wants to use Amazon Elastic Container Service (Amazon ECS) to run its on-premises \napplication in a hybrid environment. The application currently runs on containers on premises. \n \nThe company needs a single container solution that can scale in an on-premises, hybrid, or cloud \nenvironment. The company must run new application containers in the AWS Cloud and must use \na load balancer for HTTP traffic. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Application Load Balancer for cloud ECS services.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a Network Load Balancer for cloud ECS services.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an ECS cluster that uses the AWS Fargate launch type. Use Fargate for the cloud",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company is migrating its workloads to AWS. The company has sensitive and critical data in on-\npremises relational databases that run on SQL Server instances. \n \nThe company wants to use the AWS Cloud to increase security and reduce operational overhead \nfor the databases. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the databases to Amazon EC2 instances. Use an AWS Key Management Service (AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the databases to a Multi-AZ Amazon RDS for SQL Server DB instance. Use an AWS Key",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the data to an Amazon S3 bucket. Use Amazon Macie to ensure data security.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the databases to an Amazon DynamoDB table. Use Amazon CloudWatch Logs to ensure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company wants to migrate an application to AWS. The company wants to increase the \napplication's current availability. The company wants to use AWS WAF in the application's \narchitecture. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a cluster placement group that contains multiple Amazon EC2 instances that hosts the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create two Amazon EC2 instances that host the application across two Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company manages a data lake in an Amazon S3 bucket that numerous applications access. \nThe S3 bucket contains a unique prefix for each application. The company wants to restrict each \napplication to its specific prefix and to have granular control of the objects under each prefix. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create dedicated S3 access points and access point policies for each application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an S3 Batch Operations job to set the ACL permissions for each object in the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create replication",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create dedicated",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company has an application that customers use to upload images to an Amazon S3 bucket. \nEach night, the company launches an Amazon EC2 Spot Fleet that processes all the images that \nthe company received that day. The processing for each image takes 2 minutes and requires 512 \nMB of memory. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n462 \n \nA solutions architect needs to change the application to process the images when the images are \nuploaded. \n \nWhich change will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "A company wants to improve the availability and performance of its hybrid application. The \napplication consists of a stateful TCP-based workload hosted on Amazon EC2 instances in \ndifferent AWS Regions and a stateless UDP-based workload hosted on premises. \n \nWhich combination of actions should a solutions architect take to improve availability and \nperformance? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an accelerator using AWS Global Accelerator. Add the load balancers as endpoints.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure two Application Load Balancers in each Region. The first will route to the EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure a",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nProvisioned IOPS Solid State Drive - io1) is incorrect. io1 volumes are designed for I/O-intensive workloads that require sustained high performance. They are suitable for boot volumes, especially when high performance is required for the operating system and applications.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "A company runs a self-managed Microsoft SQL Server on Amazon EC2 instances and Amazon \nElastic Block Store (Amazon EBS). Daily snapshots are taken of the EBS volumes. \n \nRecently, all the company's EBS snapshots were accidentally deleted while running a snapshot \ncleaning script that deletes all expired EBS snapshots. A solutions architect needs to update the \narchitecture to prevent data loss without retaining EBS snapshots indefinitely. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n463 \n \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Change the IAM policy of the user to deny EBS snapshot deletion.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy the EBS snapshots to another AWS Region after completing the snapshots daily.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Copy EBS snapshots to Amazon S3 Standard-Infrequent Access (S3 Standard-IA).",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company wants to use an AWS CloudFormation stack for its application in a test environment. \nThe company stores the CloudFormation template in an Amazon S3 bucket that blocks public \naccess. The company wants to grant CloudFormation access to the template in the S3 bucket \nbased on specific user requests to create the test environment. The solution must follow security \nbest practices. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint for Amazon S3. Configure the CloudFormation stack to use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway REST API that has the S3 bucket as the target. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a presigned URL for the template object. Configure the CloudFormation stack to use the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Allow public access to the template object in the S3 bucket. Block the public access after the test",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company has applications that run in an organization in AWS Organizations. The company \noutsources operational support of the applications. The company needs to provide access for the \nexternal support engineers without compromising security. \n \nThe external support engineers need access to the AWS Management Console. The external \nsupport engineers also need operating system access to the company's fleet ofAmazon EC2 \ninstances that run Amazon Linux in private subnets. \n \nWhich solution will meet these requirements MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an",
        "correct": true
      },
      {
        "id": 1,
        "text": "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Confirm that all instances have a security group that allows SSH access only from the external",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a bastion host in a public subnet. Set up the bastion host security group to allow access",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company uses Amazon RDS for PostgreSQL to run its applications in the us-east-1 Region. \nThe company also uses machine learning (ML) models to forecast annual revenue based on near \nreal-time reports. The reports are generated by using the same RDS for PostgreSQL database. \nThe database performance slows during business hours. The company needs to improve \ndatabase performance. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a cross-Region read replica. Configure the reports to be generated from the read replica.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Activate Multi-AZ DB instance deployment for RDS for PostgreSQL. Configure the reports to be",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Data Migration Service (AWS DMS) to logically replicate data to a new database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a read replica in us-east-1. Configure the reports to be generated from the read replica.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "A company hosts its multi-tier, public web application in the AWS Cloud. The web application \nruns on Amazon EC2 instances, and its database runs on Amazon RDS. The company is \nanticipating a large increase in sales during an upcoming holiday weekend. A solutions architect \nneeds to build a solution to analyze the performance of the web application with a granularity of \nno more than 2 minutes. \n \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Send Amazon CloudWatch logs to Amazon Redshift. Use Amazon QuickS ght to perform further",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable detailed monitoring on all EC2 instances. Use Amazon CloudWatch metrics to perform",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function to fetch EC2 logs from Amazon CloudWatch Logs. Use Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Send EC2 logs to Amazon S3. Use Amazon Redshift to fetch logs from the S3 bucket to process",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A company runs an application that stores and shares photos. Users upload the photos to an \nAmazon S3 bucket. Every day, users upload approximately 150 photos. The company wants to \ndesign a solution that creates a thumbnail of each new photo and stores the thumbnail in a \nsecond S3 bucket. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a long-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a memory-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an S3 event notification to invoke an AWS Lambda function each time a user uploads a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure S3 Storage Lens to invoke an AWS Lambda function each time a user uploads a new",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company has stored millions of objects across multiple prefixes in an Amazon S3 bucket by \nusing the Amazon S3 Glacier Deep Archive storage class. The company needs to delete all data \nolder than 3 years except for a subset of data that must be retained. The company has identified \nthe data that must be retained and wants to implement a serverless solution. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Inventory to list all objects. Use the AWS CLI to create a script that runs on an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Batch to delete objects older than 3 years except for the data that must be retained.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Glue crawler to query objects older than 3 years. Save the manifest file of old",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable S3 Inventory. Create an AWS Lambda function to filter and delete objects. Invoke the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "A company is building an application on AWS. The application uses multiple AWS Lambda \nfunctions to retrieve sensitive data from a single Amazon S3 bucket for processing. The company \nmust ensure that only authorized Lambda functions can access the data. The solution must \ncomply with the principle of least privilege. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Grant full S3 bucket access to all Lambda functions through a shared IAM role.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Lambda functions to run within a VPC. Configure a bucket policy to grant access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create individual IAM roles for each Lambda function. Grant the IAM roles access to the S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a bucket policy granting access to the Lambda functions based on their function ARNs.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has developed a non-production application that is composed of multiple \nmicroservices for each of the company's business units. A single development team maintains all \nthe microservices. \n \nThe current architecture uses a static web frontend and a Java-based backend that contains the \napplication logic. The architecture also uses a MySQL database that the company hosts on an \nAmazon EC2 instance. \n \nThe company needs to ensure that the application is secure and available globally. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront and AWS Amplify to host the static web frontend. Refactor the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 to host the static web frontend. Refactor the microservices to use AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A video game company is deploying a new gaming application to its global users. The company \nrequires a solution that will provide near real-time reviews and rankings of the players. \n \nA solutions architect must design a solution to provide fast access to the data. The solution must \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n467 \nalso ensure the data persists on disks in the event that the company restarts the application. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. Store the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Amazon EC2 instances in multiple AWS Regions. Store the player data on the EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon ElastiCache for Redis duster. Store the player data in the ElastiCache cluster.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy an Amazon ElastiCache for Memcached duster. Store the player data in the ElastiCache",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is designing an application on AWS that processes sensitive data. The application \nstores and processes financial data for multiple customers. \n \nTo meet compliance requirements, the data for each customer must be encrypted separately at \nrest by using a secure, centralized key management solution. The company wants to use AWS \nKey Management Service (AWS KMS) to implement encryption. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Generate a unique encryption key for each customer. Store the keys in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a hardware security appliance in the AWS environment that securely stores customer-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a single AWS KMS key to encrypt all sensitive data across the application.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create separate AWS KMS keys for each customer's data that have granular access control and",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A company needs to design a resilient web application to process customer orders. The web \napplication must automatically handle increases in web traffic and application usage without \naffecting the customer experience or losing customer orders. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n468",
    "options": [
      {
        "id": 0,
        "text": "Use a NAT gateway to manage web traffic. Use Amazon EC2 Auto Scaling groups to receive,",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Network Load Balancer (NLB) to manage web traffic. Use an Application Load Balancer to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Gateway Load Balancer (GWLB) to manage web traffic. Use Amazon Elastic Container",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Application Load Balancer to manage web traffic. Use Amazon EC2 Auto Scaling groups",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company is using AWS DataSync to migrate millions of files from an on-premises system to \nAWS. The files are 10 KB in size on average. \n \nThe company wants to use Amazon S3 for file storage. For the first year after the migration, the \nfiles will be accessed once or twice and must be immediately available. After 1 year, the files \nmust be archived for at least 7 years. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use an archive tool to group the files into large objects. Use DataSync to migrate the objects.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an archive tool to group the files into large objects. Use DataSync to copy the objects to S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the destination storage class for the files as S3 Glacier Instant Retrieval. Use a lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a DataSync task to transfer the files to S3 Standard-Infrequent Access (S3 Standard-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company recently performed a lift and shift migration of its on-premises Oracle database \nworkload to run on an Amazon EC2 memory optimized Linux instance. The EC2 Linux instance \nuses a 1 TB Provisioned IOPS SSD (io1) EBS volume with 64,000 IOPS. \n \nThe database storage performance after the migration is slower than the performance of the on-\npremises database. \n \nWhich solution will improve storage performance?",
    "options": [
      {
        "id": 0,
        "text": "Add more Provisioned IOPS SSD (io1) EBS volumes. Use OS commands to create a Logical",
        "correct": true
      },
      {
        "id": 1,
        "text": "Increase the Provisioned IOPS SSD (io1) EBS volume to more than 64,000 IOPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Increase the size of the Provisioned IOPS SSD (io1) EBS volume to 2 TB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change the EC2 Linux instance to a storage optimized instance type. Do not change the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "A company is migrating from a monolithic architecture for a web application that is hosted on \nAmazon EC2 to a serverless microservices architecture. The company wants to use AWS \nservices that support an event-driven, loosely coupled architecture. The company wants to use \nthe publish/subscribe (pub/sub) pattern. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an Amazon API Gateway WebSocket API to write to a data stream in Amazon Kinesis",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon API Gateway HTTP API to invoke an AWS Lambda function that publishes",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company recently migrated a monolithic application to an Amazon EC2 instance and Amazon \nRDS. The application has tightly coupled modules. The existing design of the application gives \nthe application the ability to run on only a single EC2 instance. \n \nThe company has noticed high CPU utilization on the EC2 instance during peak usage times. The \nhigh CPU utilization corresponds to degraded performance on Amazon RDS for read requests. \nThe company wants to reduce the high CPU utilization and improve read request performance. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto",
        "correct": false
      },
      {
        "id": 1,
        "text": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an Auto Scaling group with a minimum size of 1 and maximum size of 2. Resize the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company needs to grant a team of developers access to the company's AWS resources. The \ncompany must maintain a high level of security for the resources. \n \nThe company requires an access control solution that will prevent unauthorized access to the \nsensitive data. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Share the IAM user credentials for each development team member with the rest of the team to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define IAM roles that have fine-grained permissions based on the principle of least privilege.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create IAM access keys to grant programmatic access to AWS resources. Allow only developers",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Cognito user pool. Grant developers access to AWS resources by using the user",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 0 is incorrect:**\nis incorrect because partition placement groups are primarily used for large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. While they provide fault tolerance by distributing instances across partitions, they do not offer the low latency and high throughput benefits of cluster placement groups, which are more critical for HPC applications.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company hosts a monolithic web application on an Amazon EC2 instance. Application users \nhave recently reported poor performance at specific times. Analysis of Amazon CloudWatch \nmetrics shows that CPU utilization is 100% during the periods of poor performance. \n \nThe company wants to resolve this performance issue and improve application availability. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale vertically.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Machine Image (AMI) from the web server. Reference the AMI in a new",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group and an Application Load Balancer to scale vertically.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Auto Scaling group and an Application Load Balancer to scale horizontally.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A company runs all its business applications in the AWS Cloud. The company uses AWS \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n471 \nOrganizations to manage multiple AWS accounts. \n \nA solutions architect needs to review all permissions that are granted to IAM users to determine \nwhich IAM users have more permissions than required. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Network Access Analyzer to review all access permissions in the company's AWS accounts.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS CloudWatch alarm that activates when an IAM user creates or modifies",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Identity and Access Management (IAM) Access Analyzer to review all the company's",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Inspector to find vulnerabilities in existing IAM policies.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company needs to implement a new data retention policy for regulatory compliance. As part of \nthis policy, sensitive documents that are stored in an Amazon S3 bucket must be protected from \ndeletion or modification for a fixed period of time. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Activate S3 Object Lock on the required objects and enable governance mode.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Activate S3 Object Lock on the required objects and enable compliance mode.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable versioning on the S3 bucket. Set a lifecycle policy to delete the objects after a specified",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Flexible Retrieval for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company runs its customer-facing web application on containers. The workload uses Amazon \nElastic Container Service (Amazon ECS) on AWS Fargate. The web application is resource \nintensive. \n \nThe web application needs to be available 24 hours a day, 7 days a week for customers. The \ncompany expects the application to experience short bursts of high traffic. The workload must be \nhighly available. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an ECS capacity provider with Fargate. Conduct load testing by using a third-party tool.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an ECS capacity provider with Fargate for steady state and Fargate Spot for burst",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an ECS capacity provider with Fargate Spot for steady state and Fargate for burst",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an ECS capacity provider with Fargate. Use AWS Compute Optimizer to rightsize the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 50,
    "text": "A company is building an application in the AWS Cloud. The application is hosted on Amazon \nEC2 instances behind an Application Load Balancer (ALB). The company uses Amazon Route 53 \nfor the DNS. \n \nThe company needs a managed solution with proactive engagement to detect against DDoS \nattacks. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS Config. Configure an AWS Config managed rule that detects DDoS attacks.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable AWS WAF on the ALCreate an AWS WAF web ACL with rules to detect and prevent",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the ALB access logs in an Amazon S3 bucket. Configure Amazon GuardDuty to detect and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Subscribe to AWS Shield Advanced. Configure hosted zones in Route 53. Add ALB resources as",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 1 is incorrect:**\nis incorrect because while it creates IAM policies for DynamoDB and S3, attaching them to the EC2 instance profile grants those permissions to *all* Pods running on those nodes. Kubernetes RBAC controls access *within* the cluster, not to AWS resources. This approach doesn't provide the fine-grained control needed to restrict UI Pods to DynamoDB and data-processing Pods to S3. All pods running on the node would inherit the permissions of the instance profile.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company hosts a video streaming web application in a VPC. The company uses a Network \nLoad Balancer (NLB) to handle TCP traffic for real-time data processing. There have been \nunauthorized attempts to access the application. \n \nThe company wants to improve application security with minimal architectural change to prevent \nunauthorized attempts to access the application. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement a series of AWS WAF rules directly on the NLB to filter out unauthorized traffic.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Recreate the NLB with a security group to allow only trusted IP addresses.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy a second NLB in parallel with the existing NLB configured with a strict IP address allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Shield Advanced to provide enhanced DDoS protection and prevent unauthorized",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A healthcare company is developing an AWS Lambda function that publishes notifications to an \nencrypted Amazon Simple Notification Service (Amazon SNS) topic. The notifications contain \nprotected health information (PHI). \n \nThe SNS topic uses AWS Key Management Service (AWS KMS) customer managed keys for \nencryption. The company must ensure that the application has the necessary permissions to \npublish messages securely to the SNS topic. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n473 \n \nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Create a resource policy for the SNS topic that allows the Lambda function to publish messages",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use server-side encryption with AWS KMS keys (SSE-KMS) for the SNS topic instead of",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a resource policy for the encryption key that the SNS topic uses that has the necessary",
        "correct": false
      },
      {
        "id": 3,
        "text": "Specify the Lambda function's Amazon Resource Name (ARN) in the SNS topic's resource policy.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Associate an Amazon API Gateway HTTP API with the SNS topic to control access to the topic",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A company has an employee web portal. Employees log in to the portal to view payroll details. \nThe company is developing a new system to give employees the ability to upload scanned \ndocuments for reimbursement. The company runs a program to extract text-based data from the \ndocuments and attach the extracted information to each employee's reimbursement IDs for \nprocessing. \n \nThe employee web portal requires 100% uptime. The document extract program runs infrequently \nthroughout the day on an on-demand basis. The company wants to build a scalable and cost-\neffective new system that will require minimal changes to the existing web portal. The company \ndoes not want to make any code changes. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Run Amazon EC2 On-Demand Instances in an Auto Scaling group for the web portal. Use an",
        "correct": true
      },
      {
        "id": 1,
        "text": "Run Amazon EC2 Spot Instances in an Auto Scaling group for the web portal. Run the document",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a Savings Plan to run the web portal and the document extract program. Run the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket to host the web portal. Use Amazon API Gateway and an AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 54,
    "text": "A media company has a multi-account AWS environment in the us-east-1 Region. The company \nhas an Amazon Simple Notification Service (Amazon SNS) topic in a production account that \npublishes performance metrics. The company has an AWS Lambda function in an administrator \naccount to process and analyze log data. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n474 \nThe Lambda function that is in the administrator account must be invoked by messages from the \nSNS topic that is in the production account when significant metrics are reported. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM resource policy for the Lambda function that allows Amazon SNS to invoke the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Implement an Amazon Simple Queue Service (Amazon SQS) queue in the administrator account",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM policy for the SNS topic that allows the Lambda function to subscribe to the topic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon EventBridge rule in the production account to capture the SNS topic notifications.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Store performance metrics in an Amazon S3 bucket in the production account. Use Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company is migrating an application from an on-premises location to Amazon Elastic \nKubernetes Service (Amazon EKS). The company must use a custom subnet for pods that are in \nthe company's VPC to comply with requirements. The company also needs to ensure that the \npods can communicate securely within the pods' VPC. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Transit Gateway to directly manage custom subnet configurations for the pods in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Direct Connect connection from the company's on-premises IP address ranges to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the Amazon VPC CNI plugin for Kubernetes. Define custom subnets in the VPC cluster for",
        "correct": true
      },
      {
        "id": 3,
        "text": "Implement a Kubernetes network policy that has pod anti-affinity rules to restrict pod placement to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A company hosts an ecommerce application that stores all data in a single Amazon RDS for \nMySQL DB instance that is fully managed by AWS. The company needs to mitigate the risk of a \nsingle point of failure. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Modify the RDS DB instance to use a Multi-AZ deployment. Apply the changes during the next",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the current database to a new Amazon DynamoDB Multi-AZ deployment. Use AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new RDS DB instance in a Multi-AZ deployment. Manually restore the data from the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the DB instance in an Amazon EC2 Auto Scaling group with a minimum group size of",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 57,
    "text": "A company has multiple Microsoft Windows SMB file servers and Linux NFS file servers for file \nsharing in an on-premises environment. As part of the company's AWS migration plan, the \ncompany wants to consolidate the file servers in the AWS Cloud. \n \nThe company needs a managed AWS storage service that supports both NFS and SMB access. \nThe solution must be able to share between protocols. The solution must have redundancy at the \nAvailability Zone level. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon FSx for NetApp ONTAP for storage. Configure multi-protocol access.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create two Amazon EC2 instances. Use one EC2 instance for Windows SMB file server access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon FSx for NetApp ONTAP for SMB access. Use Amazon FSx for Lustre for NFS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 storage. Access Amazon S3 through an Amazon S3 File Gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A software company needs to upgrade a critical web application. The application currently runs \non a single Amazon EC2 instance that the company hosts in a public subnet. The EC2 instance \nruns a MySQL database. The application's DNS records are published in an Amazon Route 53 \nzone. \n \nA solutions architect must reconfigure the application to be scalable and highly available. The \nsolutions architect must also reduce MySQL read latency. \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Launch a second EC2 instance in a second AWS Region. Use a Route 53 failover routing policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon Aurora MySQL cluster. Create the primary DB instance and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple AWS",
        "correct": false
      },
      {
        "id": 4,
        "text": "Migrate the database to an Amazon Aurora MySQL cluster with cross-Region read replicas.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A company runs thousands of AWS Lambda functions. The company needs a solution to \nsecurely store sensitive information that all the Lambda functions use. The solution must also \nmanage the automatic rotation of the sensitive information. \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create HTTP security headers by using Lambda@Edge to retrieve and create sensitive",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Lambda layer that retrieves sensitive information",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store sensitive information in AWS Secrets Manager",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store sensitive information in AWS Systems Manager Parameter Store",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a Lambda consumer with dedicated throughput to retrieve sensitive information and create",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company has an internal application that runs on Amazon EC2 instances in an Auto Scaling \ngroup. The EC2 instances are compute optimized and use Amazon Elastic Block Store (Amazon \nEBS) volumes. \n \nThe company wants to identify cost optimizations across the EC2 instances, the Auto Scaling \ngroup, and the EBS volumes. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Cost and Usage Report. Search the report for cost recommendations for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create new Amazon CloudWatch billing alerts. Check the alert statuses for cost",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances, the Auto",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 61,
    "text": "A company is running a media store across multiple Amazon EC2 instances distributed across \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n477 \nmultiple Availability Zones in a single VPC. The company wants a high-performing solution to \nshare data between all the EC2 instances, and prefers to keep the data within the VPC only. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket and call the service APIs from each instance's application",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket and configure all instances to access it as a mounted volume",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon Elastic Block Store (Amazon EBS) volume and mount it across all",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Elastic File System (Amazon EFS) file system and mount it across all",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company uses an Amazon RDS for MySQL instance. To prepare for end-of-year processing, \nthe company added a read replica to accommodate extra read-only queries from the company's \nreporting tool. The read replica CPU usage was 60% and the primary instance CPU usage was \n60%. \n \nAfter end-of-year activities are complete, the read replica has a constant 25% CPU usage. The \nprimary instance still has a constant 60% CPU usage. The company wants to rightsize the \ndatabase and still provide enough performance for future growth. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Delete the read replica Do not make changes to the primary instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Resize the read replica to a smaller instance size Do not make changes to the primary instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Resize the read replica to a larger instance size Resize the primary instance to a smaller instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Delete the read replica Resize the primary instance to a larger instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "A company is migrating its databases to Amazon RDS for PostgreSQL. The company is migrating \nits applications to Amazon EC2 instances. The company wants to optimize costs for long-running \nworkloads. \n \nWhich solution will meet this requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use On-Demand Instances for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase Reserved Instances for a 1 year term with the No Upfront option for the Amazon RDS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase Reserved Instances for a 1 year term with the Partial Upfront option for the Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase Reserved Instances for a 3 year term with the All Upfront option for the Amazon RDS",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A company is using an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The company \nmust ensure that Kubernetes service accounts in the EKS cluster have secure and granular \naccess to specific AWS resources by using IAM roles for service accounts (IRSA). \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM policy that defines the required permissions Attach the policy directly to the IAM",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement network policies within the EKS cluster to prevent Kubernetes service accounts from",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the EKS cluster's IAM role to include permissions for each Kubernetes service account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Define an IAM role that includes the necessary permissions. Annotate the Kubernetes service",
        "correct": true
      },
      {
        "id": 4,
        "text": "Set up a trust relationship between the IAM roles for the service accounts and an OpenID",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  }
]