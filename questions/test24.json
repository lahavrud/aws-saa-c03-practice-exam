[
  {
    "id": 0,
    "text": "A company uses Amazon S3 to host its static website. The company wants to add a contact form \nto the webpage. The contact form will have dynamic server-side components for users to input \ntheir name, email address, phone number, and user message. \n \nThe company expects fewer than 100 site visits each month. The contact form must notify the \ncompany by email when a customer fills out the form. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Host the dynamic contact form in Amazon Elastic Container Service (Amazon ECS). Set up",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway endpoint that returns the contact form from an AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the website by using AWS Amplify Hosting for static content and dynamic content. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the website from Amazon S3 to Amazon EC2 instances that run Windows Server. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company creates dedicated AWS accounts in AWS Organizations for its business units. \nRecently, an important notification was sent to the root user email address of a business unit \naccount instead of the assigned account owner. The company wants to ensure that all future \nnotifications can be sent to different employees based on the notification categories of billing, \noperations, or security. \n \nWhich solution will meet these requirements MOST securely? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n450",
    "options": [
      {
        "id": 0,
        "text": "Configure each AWS account to use a single email address that the company manages. Ensure",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure each AWS account to use a different email distribution list for each business unit that",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure each AWS account root user email address to be the individual company managed",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure each AWS account root user to use email aliases that go to a centralized mailbox.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company runs an ecommerce application on AWS. Amazon EC2 instances process purchases \nand store the purchase details in an Amazon Aurora PostgreSQL DB cluster. \n \nCustomers are experiencing application timeouts during times of peak usage. A solutions \narchitect needs to rearchitect the application so that the application can scale to meet peak usage \ndemands. \n \nWhich combination of actions will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure an Auto Scaling group of new EC2 instances to retry the purchases until the processing",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the application to use an Amazon ElastiCache cluster in front of the Aurora",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the application to send the purchase requests to an Amazon Simple Queue Service",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Lambda function to retry the ticket purchases until the processing is complete.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure an Amazon AP! Gateway REST API with a usage plan.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A company that uses AWS Organizations runs 150 applications across 30 different AWS \naccounts. The company used AWS Cost and Usage Report to create a new report in the \nmanagement account. The report is delivered to an Amazon S3 bucket that is replicated to a \nbucket in the data collection account. \n \nThe company's senior leadership wants to view a custom dashboard that provides NAT gateway \ncosts each day starting at the beginning of the current month. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n451",
    "options": [
      {
        "id": 0,
        "text": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure",
        "correct": false
      },
      {
        "id": 1,
        "text": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure",
        "correct": true
      },
      {
        "id": 2,
        "text": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company is hosting a high-traffic static website on Amazon S3 with an Amazon CloudFront \ndistribution that has a default TTL of 0 seconds. The company wants to implement caching to \nimprove performance for the website. However, the company also wants to ensure that stale \ncontent is not served for more than a few minutes after a deployment. \n \nWhich combination of caching methods should a solutions architect implement to meet these \nrequirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Set the CloudFront default TTL to 2 minutes.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set a default TTL of 2 minutes on the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add a Cache-Control private directive to the objects in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda@Edge function to add an Expires header to HTTP responses. Configure",
        "correct": false
      },
      {
        "id": 4,
        "text": "Add a Cache-Control max-age directive of 24 hours to the objects in Amazon S3. On deployment,",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "A company runs its application by using Amazon EC2 instances and AWS Lambda functions. The \nEC2 instances run in private subnets of a VPC. The Lambda functions need direct network \naccess to the EC2 instances for the application to work. \n \nThe application will run for 1 year. The number of Lambda functions that the application uses will \nincrease during the 1-year period. The company must minimize costs on all application \nresources. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to the private subnets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to new public subnets in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a Compute Savings Plan. Connect the Lambda functions to the private subnets that",
        "correct": true
      },
      {
        "id": 3,
        "text": "Purchase a Compute Savings Plan. Keep the Lambda functions in the Lambda service VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCompute Savings Plan: This plan offers significant discounts on Lambda functions compared to Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A company has deployed a multi-account strategy on AWS by using AWS Control Tower. The \ncompany has provided individual AWS accounts to each of its developers. The company wants to \nimplement controls to limit AWS resource costs that the developers incur. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Instruct each developer to tag all their resources with a tag that has a key of CostCenter and a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Budgets to establish budgets for each developer account. Set up budget alerts for",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Cost Explorer to monitor and report on costs for each developer account. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Service Catalog to allow developers to launch resources within a limited cost range.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nTaking into consideration that AWS Budgets is allowing to will inform you that you exceeded budged and execute actions like for example IAM actions to prevent running new resources in cloud, I think this option is a good and resonable move. In case of need budged can be always increased and \"chains\" disabled. https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A solutions architect is designing a three-tier web application. The architecture consists of an \ninternet-facing Application Load Balancer (ALB) and a web tier that is hosted on Amazon EC2 \ninstances in private subnets. The application tier with the business logic runs on EC2 instances in \nprivate subnets. The database tier consists of Microsoft SQL Server that runs on EC2 instances \nin private subnets. Security is a high priority for the company. \n \nWhich combination of security group configurations should the solutions architect use? (Choose \nthree.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the web tier to allow inbound HTTPS traffic from the security",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the security group for the web tier to allow outbound HTTPS traffic to 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the security group for the database tier to allow inbound Microsoft SQL Server traffic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the database tier to allow outbound HTTPS traffic and Microsoft",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the security group for the application tier to allow inbound HTTPS traffic from the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSecurity Group is protecting instances, it's statefull. by defoult is allowing for outgoing traffic but not incomming. hence we need to allow for inboud traffic. path looks like below ALB >>HTTPS>> WEB tier >>HTTPS>> Application >>SQL traffic>> SQL DB hence we need allow for incoming https traffic on web tier then incomming http on app tier and on the end for incomming sql traffic on DB tier\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company has released a new version of its production application. The company's workload \nuses Amazon EC2, AWS Lambda, AWS Fargate, and Amazon SageMaker. \n \nThe company wants to cost optimize the workload now that usage is at a steady state. The \ncompany wants to cover the most services with the fewest savings plans. \n \nWhich combination of savings plans will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Purchase an EC2 Instance Savings Plan for Amazon EC2 and SageMaker.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase a Compute Savings Plan for Amazon EC2, Lambda, and SageMaker.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a SageMaker Savings Plan.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Purchase a Compute Savings Plan for Lambda, Fargate, and Amazon EC2.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Purchase an EC2 Instance Savings Plan for Amazon EC2 and Fargate.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/savingsplans/ml-pricing/ https://aws.amazon.com/savingsplans/compute-pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 9,
    "text": "A company uses a Microsoft SQL Server database. The company's applications are connected to \nthe database. The company wants to migrate to an Amazon Aurora PostgreSQL database with \nminimal changes to the application code. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to rewrite the SQL queries in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Babelfish on Aurora PostgreSQL to run the SQL queries from the applications.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database schema and data by using the AWS Schema Conversion Tool (AWS SCT)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS Proxy to connect the applications to Aurora PostgreSQL.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Database Migration Service (AWS DMS) to rewrite the SQL queries in the applications.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nDMS will allow for DATABASE migration and use AWS Schema Conversion Tool (AWS SCT) to create some or all of the target tables, indexes, views, triggers, and so on. https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html To minimalize amount of code which need to me changes we need to use babelfish https://aws.amazon.com/rds/aurora/babelfish/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company plans to rehost an application to Amazon EC2 instances that use Amazon Elastic \nBlock Store (Amazon EBS) as the attached storage. \n \nA solutions architect must design a solution to ensure that all newly created Amazon EBS \nvolumes are encrypted by default. The solution must also prevent the creation of unencrypted \nEBS volumes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the EC2 account attributes to always encrypt new EBS volumes.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Config. Configure the encrypted-volumes identifier. Apply the default AWS Key",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Systems Manager to create encrypted copies of the EBS volumes. Reconfigure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a customer managed key in AWS Key Management Service (AWS KMS). Configure AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://repost.aws/knowledge-center/ebs-automatic-encryption\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "An ecommerce company wants to collect user clickstream data from the company's website for \nreal-time analysis. The website experiences fluctuating traffic patterns throughout the day. The \ncompany needs a scalable solution that can adapt to varying levels of traffic. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Firehose to capture the clickstream data. Use AWS Glue to process",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Kinesis Video Streams to capture the clickstream data. Use AWS Glue to process",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nApache Flink (previously known as Amazon Kinesis Data Analytics) seems to not allowing sent data directly to Lambda... Glue is allowing to integrate data from couple of sources in to one. https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A global company runs its workloads on AWS. The company's application uses Amazon S3 \nbuckets across AWS Regions for sensitive data storage and analysis. The company stores \nmillions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that \nare not versioning-enabled. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an AWS CloudTrail event that has a rule to identify all S3 buckets that are not versioning-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nS3 Sorage Lens \"can also identify buckets that aren't following data-protection best practices, such as using S3 Replication or S3 Versioning. \" https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_basics_metrics_recomm endations.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company needs to optimize its Amazon S3 storage costs for an application that generates \nmany files that cannot be recreated. Each file is approximately 5 MB and is stored in Amazon S3 \nStandard storage. \n \nThe company must store the files for 4 years before the files can be deleted. The files must be \nimmediately accessible. The files are frequently accessed in the first 30 days of object creation, \nbut they are rarely accessed after the first 30 days. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Lifecycle policy to move the files to S3 Glacier Instant Retrieval 30 days after object",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 Lifecycle policy to move the files to S3 One Zone-Infrequent Access (S3 One Zone-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nRequirements: - frequently accessed for 30 days - lower cost Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 14,
    "text": "A company runs its critical storage application in the AWS Cloud. The application uses Amazon \nS3 in two AWS Regions. The company wants the application to send remote user data to the \nnearest S3 bucket with no public network congestion. The company also wants the application to \nfail over with the least amount of management of Amazon S3. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement an active-active design between the two Regions. Configure the application to use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an active-passive configuration with S3 Multi-Region Access Points. Create a global endpoint",
        "correct": false
      },
      {
        "id": 2,
        "text": "Send user data to the regional S3 endpoints closest to the user. Configure an S3 cross-account",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon S3 to use Multi-Region Access Points in an active-active configuration with a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUsing a Multi-region Accesspoint in an Active-Active setup will send data to the closest Region, without accessing the internet: \"send remote user data to the nearest S3 bucket with no public network congestion\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A company is migrating a data center from its on-premises location to AWS. The company has \nseveral legacy applications that are hosted on individual virtual servers. Changes to the \napplication designs cannot be made. \n \nEach individual virtual server currently runs as its own EC2 instance. A solutions architect needs \nto ensure that the applications are reliable and fault tolerant after migration to AWS. The \napplications will run on Amazon EC2 instances. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group that has a minimum of one and a maximum of one. Create an",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Backup to create an hourly backup of the EC2 instance that hosts each application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Machine Image (AMI) of each application instance. Launch two new EC2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Mitigation Hub Refactor Spaces to migrate each application off the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company wants to isolate its workloads by creating an AWS account for each workload. The \ncompany needs a solution that centrally manages networking components for the workloads. The \nsolution also must create accounts with automatic security controls (guardrails). \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Control Tower to deploy accounts. Create a networking account that has a VPC with",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Organizations to deploy accounts. Create a networking account that has a VPC with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Control Tower to deploy accounts. Deploy a VPC in each workload account. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Organizations to deploy accounts. Deploy a VPC in each workload account. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Control Tower provides a pre-packaged set of guardrails (policies) and blueprints (best- practice configurations) to ensure that the environment complies with security and compliance standards. It’s designed to simplify the process of creating and managing a multi-account AWS environment while maintaining security and compliance. https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The website serves static content. Website traffic is increasing. The company wants to \nminimize the website hosting costs. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the website to an Amazon S3 bucket. Configure an Amazon CloudFront distribution for the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Move the website to an Amazon S3 bucket. Configure an Amazon ElastiCache cluster for the S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the website to AWS Amplify. Configure an ALB to resolve to the Amplify website.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the website to AWS Amplify. Configure EC2 instances to cache the website.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon CloudFront: Uses the durable storage of Amazon Simple Storage Service (Amazon S3) - This solution creates an Amazon S3 bucket to host your static website’s content. To update your website, just upload your new files to the S3 bucket. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/getting-started-secure- static-website-cloudformation-template.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 18,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n458 \nA company is implementing a shared storage solution for a media application that the company \nhosts on AWS. The company needs the ability to use SMB clients to access stored data. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Storage Gateway Volume Gateway. Create a file share that uses the required",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway Tape Gateway. Configure tapes to use Amazon S3. Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server file system. Connect the application server to the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://aws.amazon.com/fsx/windows/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is designing its production application's disaster recovery (DR) strategy. The \napplication is backed by a MySQL database on an Amazon Aurora cluster in the us-east-1 \nRegion. The company has chosen the us-west-1 Region as its DR Region. \n \nThe company's target recovery point objective (RPO) is 5 minutes and the target recovery time \nobjective (RTO) is 20 minutes. The company wants to minimize configuration changes. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create an Aurora read replica in us-west-1 similar in size to the production application's Aurora",
        "correct": false
      },
      {
        "id": 1,
        "text": "Convert the Aurora cluster to an Aurora global database. Configure managed failover.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new Aurora cluster in us-west-1 that has Cross-Region Replication.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new Aurora cluster in us-west-1. Use AWS Database Migration Service (AWS DMS) to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCross-Region disaster recovery If your primary Region suffers a performance degradation or outage, you can promote one of the secondary Regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute, even in the event of a complete Regional outage. This provides your application with an effective recovery point objective (RPO) of 1 second and a recovery time objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan. https://aws.amazon.com/rds/aurora/global-database/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A company runs a critical data analysis job each week before the first day of the work week. The \njob requires at least 1 hour to complete the analysis. The job is stateful and cannot tolerate \ninterruptions. The company needs a solution to run the job on AWS. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n459",
    "options": [
      {
        "id": 0,
        "text": "Create a container for the job. Schedule the job to run as an AWS Fargate task on an Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the job to run in an AWS Lambda function. Create a scheduled rule in Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Auto Scaling group of Amazon EC2 Spot Instances that run Amazon Linux.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS DataSync task to run the job. Configure a cron expression to run the task on a",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A company runs workloads in the AWS Cloud. The company wants to centrally collect security \ndata to assess security across the entire company and to improve workload protection. \n \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Configure a data lake in AWS Lake Formation. Use AWS Glue crawlers to ingest the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an AWS Lambda function to collect the security data in .csv format. Upload the data to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a data lake in Amazon Security Lake to collect the security data. Upload the data to an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an AWS Database Migration Service (AWS DMS) replication instance to load the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon Security Lake automatically centralizes security data from AWS environments, you can get a more complete understanding of your security data across your entire organization. You can also improve the protection.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is migrating five on-premises applications to VPCs in the AWS Cloud. Each \napplication is currently deployed in isolated virtual networks on premises and should be deployed \nsimilarly in the AWS Cloud. The applications need to reach a shared services VPC. All the \napplications must be able to communicate with each other. \n \nIf the migration is successful, the company will repeat the migration process for more than 100 \napplications. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy software VPN tunnels between the application VPCs and the shared services VPC. Add",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy VPC peering connections between the application VPCs and the shared services VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Direct Connect connection between the application VPCs and the shared",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a transit gateway with associations between the transit gateway and the application VPCs",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nIt will allow for inter-VPC communication for all 5 applications/VPC, reach shared resource/VPC and in the future it will be easy to allow for inter-communication between even 100 VPCs (applications). https://aws.amazon.com/transit-gateway/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A company wants to use Amazon Elastic Container Service (Amazon ECS) to run its on-premises \napplication in a hybrid environment. The application currently runs on containers on premises. \n \nThe company needs a single container solution that can scale in an on-premises, hybrid, or cloud \nenvironment. The company must run new application containers in the AWS Cloud and must use \na load balancer for HTTP traffic. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Application Load Balancer for cloud ECS services.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a Network Load Balancer for cloud ECS services.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an ECS cluster that uses the AWS Fargate launch type. Use Fargate for the cloud",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nWe need to load-balance HTTP traffic hence Application Load Balancer is needed. Because Customer want to use container solution we need to use ECS with Fargate which will lunch cloud applications. To run on-premises applications in containers we need to use ECS Anyware. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company is migrating its workloads to AWS. The company has sensitive and critical data in on-\npremises relational databases that run on SQL Server instances. \n \nThe company wants to use the AWS Cloud to increase security and reduce operational overhead \nfor the databases. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the databases to Amazon EC2 instances. Use an AWS Key Management Service (AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the databases to a Multi-AZ Amazon RDS for SQL Server DB instance. Use an AWS Key",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the data to an Amazon S3 bucket. Use Amazon Macie to ensure data security.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the databases to an Amazon DynamoDB table. Use Amazon CloudWatch Logs to ensure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company wants to migrate an application to AWS. The company wants to increase the \napplication's current availability. The company wants to use AWS WAF in the application's \narchitecture. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a cluster placement group that contains multiple Amazon EC2 instances that hosts the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create two Amazon EC2 instances that host the application across two Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company manages a data lake in an Amazon S3 bucket that numerous applications access. \nThe S3 bucket contains a unique prefix for each application. The company wants to restrict each \napplication to its specific prefix and to have granular control of the objects under each prefix. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create dedicated S3 access points and access point policies for each application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an S3 Batch Operations job to set the ACL permissions for each object in the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create replication",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create dedicated",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-policies.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company has an application that customers use to upload images to an Amazon S3 bucket. \nEach night, the company launches an Amazon EC2 Spot Fleet that processes all the images that \nthe company received that day. The processing for each image takes 2 minutes and requires 512 \nMB of memory. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n462 \n \nA solutions architect needs to change the application to process the images when the images are \nuploaded. \n \nWhich change will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nWhen using SQS we will be sure that all images will be processed and hence to process we need 2 min and 512 MB of memory (Lambda is allowing upto 15 min and upto10K MB) Lambda should be perfect scalable solution which will allow for almost in real time image processing.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "A company wants to improve the availability and performance of its hybrid application. The \napplication consists of a stateful TCP-based workload hosted on Amazon EC2 instances in \ndifferent AWS Regions and a stateless UDP-based workload hosted on premises. \n \nWhich combination of actions should a solutions architect take to improve availability and \nperformance? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an accelerator using AWS Global Accelerator. Add the load balancers as endpoints.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure two Application Load Balancers in each Region. The first will route to the EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure a",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "A company runs a self-managed Microsoft SQL Server on Amazon EC2 instances and Amazon \nElastic Block Store (Amazon EBS). Daily snapshots are taken of the EBS volumes. \n \nRecently, all the company's EBS snapshots were accidentally deleted while running a snapshot \ncleaning script that deletes all expired EBS snapshots. A solutions architect needs to update the \narchitecture to prevent data loss without retaining EBS snapshots indefinitely. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n463 \n \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Change the IAM policy of the user to deny EBS snapshot deletion.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy the EBS snapshots to another AWS Region after completing the snapshots daily.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Copy EBS snapshots to Amazon S3 Standard-Infrequent Access (S3 Standard-IA).",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/blogs/aws/new-recycle-bin-for-ebs-snapshots/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company wants to use an AWS CloudFormation stack for its application in a test environment. \nThe company stores the CloudFormation template in an Amazon S3 bucket that blocks public \naccess. The company wants to grant CloudFormation access to the template in the S3 bucket \nbased on specific user requests to create the test environment. The solution must follow security \nbest practices. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint for Amazon S3. Configure the CloudFormation stack to use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway REST API that has the S3 bucket as the target. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a presigned URL for the template object. Configure the CloudFormation stack to use the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Allow public access to the template object in the S3 bucket. Block the public access after the test",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company has applications that run in an organization in AWS Organizations. The company \noutsources operational support of the applications. The company needs to provide access for the \nexternal support engineers without compromising security. \n \nThe external support engineers need access to the AWS Management Console. The external \nsupport engineers also need operating system access to the company's fleet ofAmazon EC2 \ninstances that run Amazon Linux in private subnets. \n \nWhich solution will meet these requirements MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an",
        "correct": true
      },
      {
        "id": 1,
        "text": "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Confirm that all instances have a security group that allows SSH access only from the external",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a bastion host in a public subnet. Set up the bastion host security group to allow access",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSystems Manager Session Manager allows secure, auditable, and controlled access to your EC2 instances without needing to open SSH ports or manage SSH keys, reducing the attack surface. Local IAM user credentials are less secure and harder to manage at scale compared to using IAM Identity Center.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company uses Amazon RDS for PostgreSQL to run its applications in the us-east-1 Region. \nThe company also uses machine learning (ML) models to forecast annual revenue based on near \nreal-time reports. The reports are generated by using the same RDS for PostgreSQL database. \nThe database performance slows during business hours. The company needs to improve \ndatabase performance. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a cross-Region read replica. Configure the reports to be generated from the read replica.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Activate Multi-AZ DB instance deployment for RDS for PostgreSQL. Configure the reports to be",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Data Migration Service (AWS DMS) to logically replicate data to a new database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a read replica in us-east-1. Configure the reports to be generated from the read replica.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nRead replicas are typically less expensive than setting up a cross-Region replica or activating Multi-AZ deployments. You only pay for the additional read replica, without the overhead costs associated with cross-Region data transfer or maintaining a synchronous standby in Multi-AZ setups.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "A company hosts its multi-tier, public web application in the AWS Cloud. The web application \nruns on Amazon EC2 instances, and its database runs on Amazon RDS. The company is \nanticipating a large increase in sales during an upcoming holiday weekend. A solutions architect \nneeds to build a solution to analyze the performance of the web application with a granularity of \nno more than 2 minutes. \n \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Send Amazon CloudWatch logs to Amazon Redshift. Use Amazon QuickS ght to perform further",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable detailed monitoring on all EC2 instances. Use Amazon CloudWatch metrics to perform",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function to fetch EC2 logs from Amazon CloudWatch Logs. Use Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Send EC2 logs to Amazon S3. Use Amazon Redshift to fetch logs from the S3 bucket to process",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A company runs an application that stores and shares photos. Users upload the photos to an \nAmazon S3 bucket. Every day, users upload approximately 150 photos. The company wants to \ndesign a solution that creates a thumbnail of each new photo and stores the thumbnail in a \nsecond S3 bucket. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a long-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a memory-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an S3 event notification to invoke an AWS Lambda function each time a user uploads a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure S3 Storage Lens to invoke an AWS Lambda function each time a user uploads a new",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company has stored millions of objects across multiple prefixes in an Amazon S3 bucket by \nusing the Amazon S3 Glacier Deep Archive storage class. The company needs to delete all data \nolder than 3 years except for a subset of data that must be retained. The company has identified \nthe data that must be retained and wants to implement a serverless solution. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Inventory to list all objects. Use the AWS CLI to create a script that runs on an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Batch to delete objects older than 3 years except for the data that must be retained.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Glue crawler to query objects older than 3 years. Save the manifest file of old",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable S3 Inventory. Create an AWS Lambda function to filter and delete objects. Invoke the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "A company is building an application on AWS. The application uses multiple AWS Lambda \nfunctions to retrieve sensitive data from a single Amazon S3 bucket for processing. The company \nmust ensure that only authorized Lambda functions can access the data. The solution must \ncomply with the principle of least privilege. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Grant full S3 bucket access to all Lambda functions through a shared IAM role.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Lambda functions to run within a VPC. Configure a bucket policy to grant access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create individual IAM roles for each Lambda function. Grant the IAM roles access to the S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a bucket policy granting access to the Lambda functions based on their function ARNs.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has developed a non-production application that is composed of multiple \nmicroservices for each of the company's business units. A single development team maintains all \nthe microservices. \n \nThe current architecture uses a static web frontend and a Java-based backend that contains the \napplication logic. The architecture also uses a MySQL database that the company hosts on an \nAmazon EC2 instance. \n \nThe company needs to ensure that the application is secure and available globally. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront and AWS Amplify to host the static web frontend. Refactor the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 to host the static web frontend. Refactor the microservices to use AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A video game company is deploying a new gaming application to its global users. The company \nrequires a solution that will provide near real-time reviews and rankings of the players. \n \nA solutions architect must design a solution to provide fast access to the data. The solution must \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n467 \nalso ensure the data persists on disks in the event that the company restarts the application. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. Store the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Amazon EC2 instances in multiple AWS Regions. Store the player data on the EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon ElastiCache for Redis duster. Store the player data in the ElastiCache cluster.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy an Amazon ElastiCache for Memcached duster. Store the player data in the ElastiCache",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon ElastiCache for Redis provides in-memory caching which ensures low latency and high throughput, perfect for near real-time access to player reviews and rankings. Redis supports data persistence by snapshotting data to disk (RDB snapshots) and appending changes to a log (AOF), ensuring that the data is not lost even if the application restarts.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is designing an application on AWS that processes sensitive data. The application \nstores and processes financial data for multiple customers. \n \nTo meet compliance requirements, the data for each customer must be encrypted separately at \nrest by using a secure, centralized key management solution. The company wants to use AWS \nKey Management Service (AWS KMS) to implement encryption. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Generate a unique encryption key for each customer. Store the keys in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a hardware security appliance in the AWS environment that securely stores customer-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a single AWS KMS key to encrypt all sensitive data across the application.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create separate AWS KMS keys for each customer's data that have granular access control and",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nWhile enabling server-side encryption in S3 can manage encryption, it does not offer the same level of control and auditing as AWS KMS. Managing individual keys manually in S3 would also increase operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A company needs to design a resilient web application to process customer orders. The web \napplication must automatically handle increases in web traffic and application usage without \naffecting the customer experience or losing customer orders. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n468",
    "options": [
      {
        "id": 0,
        "text": "Use a NAT gateway to manage web traffic. Use Amazon EC2 Auto Scaling groups to receive,",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Network Load Balancer (NLB) to manage web traffic. Use an Application Load Balancer to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Gateway Load Balancer (GWLB) to manage web traffic. Use Amazon Elastic Container",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Application Load Balancer to manage web traffic. Use Amazon EC2 Auto Scaling groups",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company is using AWS DataSync to migrate millions of files from an on-premises system to \nAWS. The files are 10 KB in size on average. \n \nThe company wants to use Amazon S3 for file storage. For the first year after the migration, the \nfiles will be accessed once or twice and must be immediately available. After 1 year, the files \nmust be archived for at least 7 years. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use an archive tool to group the files into large objects. Use DataSync to migrate the objects.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an archive tool to group the files into large objects. Use DataSync to copy the objects to S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the destination storage class for the files as S3 Glacier Instant Retrieval. Use a lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a DataSync task to transfer the files to S3 Standard-Infrequent Access (S3 Standard-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company recently performed a lift and shift migration of its on-premises Oracle database \nworkload to run on an Amazon EC2 memory optimized Linux instance. The EC2 Linux instance \nuses a 1 TB Provisioned IOPS SSD (io1) EBS volume with 64,000 IOPS. \n \nThe database storage performance after the migration is slower than the performance of the on-\npremises database. \n \nWhich solution will improve storage performance?",
    "options": [
      {
        "id": 0,
        "text": "Add more Provisioned IOPS SSD (io1) EBS volumes. Use OS commands to create a Logical",
        "correct": true
      },
      {
        "id": 1,
        "text": "Increase the Provisioned IOPS SSD (io1) EBS volume to more than 64,000 IOPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Increase the size of the Provisioned IOPS SSD (io1) EBS volume to 2 TB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change the EC2 Linux instance to a storage optimized instance type. Do not change the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe maximum provisioned IOPS for io1 is 64000 and hence you can achieve higher aggregate performance by adding more io1 volumes.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "A company is migrating from a monolithic architecture for a web application that is hosted on \nAmazon EC2 to a serverless microservices architecture. The company wants to use AWS \nservices that support an event-driven, loosely coupled architecture. The company wants to use \nthe publish/subscribe (pub/sub) pattern. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an Amazon API Gateway WebSocket API to write to a data stream in Amazon Kinesis",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon API Gateway HTTP API to invoke an AWS Lambda function that publishes",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company recently migrated a monolithic application to an Amazon EC2 instance and Amazon \nRDS. The application has tightly coupled modules. The existing design of the application gives \nthe application the ability to run on only a single EC2 instance. \n \nThe company has noticed high CPU utilization on the EC2 instance during peak usage times. The \nhigh CPU utilization corresponds to degraded performance on Amazon RDS for read requests. \nThe company wants to reduce the high CPU utilization and improve read request performance. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto",
        "correct": false
      },
      {
        "id": 1,
        "text": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an Auto Scaling group with a minimum size of 1 and maximum size of 2. Resize the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis approach addresses both the high CPU utilization on the EC2 instance and the degraded read performance on the RDS instance effectively.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company needs to grant a team of developers access to the company's AWS resources. The \ncompany must maintain a high level of security for the resources. \n \nThe company requires an access control solution that will prevent unauthorized access to the \nsensitive data. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Share the IAM user credentials for each development team member with the rest of the team to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define IAM roles that have fine-grained permissions based on the principle of least privilege.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create IAM access keys to grant programmatic access to AWS resources. Allow only developers",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Cognito user pool. Grant developers access to AWS resources by using the user",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company hosts a monolithic web application on an Amazon EC2 instance. Application users \nhave recently reported poor performance at specific times. Analysis of Amazon CloudWatch \nmetrics shows that CPU utilization is 100% during the periods of poor performance. \n \nThe company wants to resolve this performance issue and improve application availability. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale vertically.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Machine Image (AMI) from the web server. Reference the AMI in a new",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group and an Application Load Balancer to scale vertically.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Auto Scaling group and an Application Load Balancer to scale horizontally.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A company runs all its business applications in the AWS Cloud. The company uses AWS \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n471 \nOrganizations to manage multiple AWS accounts. \n \nA solutions architect needs to review all permissions that are granted to IAM users to determine \nwhich IAM users have more permissions than required. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Network Access Analyzer to review all access permissions in the company's AWS accounts.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS CloudWatch alarm that activates when an IAM user creates or modifies",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Identity and Access Management (IAM) Access Analyzer to review all the company's",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Inspector to find vulnerabilities in existing IAM policies.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company needs to implement a new data retention policy for regulatory compliance. As part of \nthis policy, sensitive documents that are stored in an Amazon S3 bucket must be protected from \ndeletion or modification for a fixed period of time. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Activate S3 Object Lock on the required objects and enable governance mode.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Activate S3 Object Lock on the required objects and enable compliance mode.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable versioning on the S3 bucket. Set a lifecycle policy to delete the objects after a specified",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Flexible Retrieval for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company runs its customer-facing web application on containers. The workload uses Amazon \nElastic Container Service (Amazon ECS) on AWS Fargate. The web application is resource \nintensive. \n \nThe web application needs to be available 24 hours a day, 7 days a week for customers. The \ncompany expects the application to experience short bursts of high traffic. The workload must be \nhighly available. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an ECS capacity provider with Fargate. Conduct load testing by using a third-party tool.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an ECS capacity provider with Fargate for steady state and Fargate Spot for burst",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an ECS capacity provider with Fargate Spot for steady state and Fargate for burst",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an ECS capacity provider with Fargate. Use AWS Compute Optimizer to rightsize the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis combination leverages the cost benefits of Fargate Spot for burst traffic while ensuring steady performance with regular Fargate instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 50,
    "text": "A company is building an application in the AWS Cloud. The application is hosted on Amazon \nEC2 instances behind an Application Load Balancer (ALB). The company uses Amazon Route 53 \nfor the DNS. \n \nThe company needs a managed solution with proactive engagement to detect against DDoS \nattacks. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS Config. Configure an AWS Config managed rule that detects DDoS attacks.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable AWS WAF on the ALCreate an AWS WAF web ACL with rules to detect and prevent",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the ALB access logs in an Amazon S3 bucket. Configure Amazon GuardDuty to detect and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Subscribe to AWS Shield Advanced. Configure hosted zones in Route 53. Add ALB resources as",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company hosts a video streaming web application in a VPC. The company uses a Network \nLoad Balancer (NLB) to handle TCP traffic for real-time data processing. There have been \nunauthorized attempts to access the application. \n \nThe company wants to improve application security with minimal architectural change to prevent \nunauthorized attempts to access the application. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement a series of AWS WAF rules directly on the NLB to filter out unauthorized traffic.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Recreate the NLB with a security group to allow only trusted IP addresses.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy a second NLB in parallel with the existing NLB configured with a strict IP address allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Shield Advanced to provide enhanced DDoS protection and prevent unauthorized",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A healthcare company is developing an AWS Lambda function that publishes notifications to an \nencrypted Amazon Simple Notification Service (Amazon SNS) topic. The notifications contain \nprotected health information (PHI). \n \nThe SNS topic uses AWS Key Management Service (AWS KMS) customer managed keys for \nencryption. The company must ensure that the application has the necessary permissions to \npublish messages securely to the SNS topic. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n473 \n \nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Create a resource policy for the SNS topic that allows the Lambda function to publish messages",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use server-side encryption with AWS KMS keys (SSE-KMS) for the SNS topic instead of",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a resource policy for the encryption key that the SNS topic uses that has the necessary",
        "correct": false
      },
      {
        "id": 3,
        "text": "Specify the Lambda function's Amazon Resource Name (ARN) in the SNS topic's resource policy.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Associate an Amazon API Gateway HTTP API with the SNS topic to control access to the topic",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A company has an employee web portal. Employees log in to the portal to view payroll details. \nThe company is developing a new system to give employees the ability to upload scanned \ndocuments for reimbursement. The company runs a program to extract text-based data from the \ndocuments and attach the extracted information to each employee's reimbursement IDs for \nprocessing. \n \nThe employee web portal requires 100% uptime. The document extract program runs infrequently \nthroughout the day on an on-demand basis. The company wants to build a scalable and cost-\neffective new system that will require minimal changes to the existing web portal. The company \ndoes not want to make any code changes. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Run Amazon EC2 On-Demand Instances in an Auto Scaling group for the web portal. Use an",
        "correct": true
      },
      {
        "id": 1,
        "text": "Run Amazon EC2 Spot Instances in an Auto Scaling group for the web portal. Run the document",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a Savings Plan to run the web portal and the document extract program. Run the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket to host the web portal. Use Amazon API Gateway and an AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 54,
    "text": "A media company has a multi-account AWS environment in the us-east-1 Region. The company \nhas an Amazon Simple Notification Service (Amazon SNS) topic in a production account that \npublishes performance metrics. The company has an AWS Lambda function in an administrator \naccount to process and analyze log data. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n474 \nThe Lambda function that is in the administrator account must be invoked by messages from the \nSNS topic that is in the production account when significant metrics are reported. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM resource policy for the Lambda function that allows Amazon SNS to invoke the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Implement an Amazon Simple Queue Service (Amazon SQS) queue in the administrator account",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM policy for the SNS topic that allows the Lambda function to subscribe to the topic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon EventBridge rule in the production account to capture the SNS topic notifications.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Store performance metrics in an Amazon S3 bucket in the production account. Use Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company is migrating an application from an on-premises location to Amazon Elastic \nKubernetes Service (Amazon EKS). The company must use a custom subnet for pods that are in \nthe company's VPC to comply with requirements. The company also needs to ensure that the \npods can communicate securely within the pods' VPC. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Transit Gateway to directly manage custom subnet configurations for the pods in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Direct Connect connection from the company's on-premises IP address ranges to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the Amazon VPC CNI plugin for Kubernetes. Define custom subnets in the VPC cluster for",
        "correct": true
      },
      {
        "id": 3,
        "text": "Implement a Kubernetes network policy that has pod anti-affinity rules to restrict pod placement to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe Amazon VPC Container Network Interface (CNI) plugin is the default network plugin for Amazon EKS. It allows Kubernetes pods to receive IP addresses from a VPC's subnet and enables pods to communicate securely within the VPC as if they were native VPC resources.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A company hosts an ecommerce application that stores all data in a single Amazon RDS for \nMySQL DB instance that is fully managed by AWS. The company needs to mitigate the risk of a \nsingle point of failure. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Modify the RDS DB instance to use a Multi-AZ deployment. Apply the changes during the next",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the current database to a new Amazon DynamoDB Multi-AZ deployment. Use AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new RDS DB instance in a Multi-AZ deployment. Manually restore the data from the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the DB instance in an Amazon EC2 Auto Scaling group with a minimum group size of",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 57,
    "text": "A company has multiple Microsoft Windows SMB file servers and Linux NFS file servers for file \nsharing in an on-premises environment. As part of the company's AWS migration plan, the \ncompany wants to consolidate the file servers in the AWS Cloud. \n \nThe company needs a managed AWS storage service that supports both NFS and SMB access. \nThe solution must be able to share between protocols. The solution must have redundancy at the \nAvailability Zone level. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon FSx for NetApp ONTAP for storage. Configure multi-protocol access.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create two Amazon EC2 instances. Use one EC2 instance for Windows SMB file server access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon FSx for NetApp ONTAP for SMB access. Use Amazon FSx for Lustre for NFS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 storage. Access Amazon S3 through an Amazon S3 File Gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A software company needs to upgrade a critical web application. The application currently runs \non a single Amazon EC2 instance that the company hosts in a public subnet. The EC2 instance \nruns a MySQL database. The application's DNS records are published in an Amazon Route 53 \nzone. \n \nA solutions architect must reconfigure the application to be scalable and highly available. The \nsolutions architect must also reduce MySQL read latency. \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Launch a second EC2 instance in a second AWS Region. Use a Route 53 failover routing policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon Aurora MySQL cluster. Create the primary DB instance and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple AWS",
        "correct": false
      },
      {
        "id": 4,
        "text": "Migrate the database to an Amazon Aurora MySQL cluster with cross-Region read replicas.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A company runs thousands of AWS Lambda functions. The company needs a solution to \nsecurely store sensitive information that all the Lambda functions use. The solution must also \nmanage the automatic rotation of the sensitive information. \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create HTTP security headers by using Lambda@Edge to retrieve and create sensitive",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Lambda layer that retrieves sensitive information",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store sensitive information in AWS Secrets Manager",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store sensitive information in AWS Systems Manager Parameter Store",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a Lambda consumer with dedicated throughput to retrieve sensitive information and create",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Secrets Manager securely stores sensitive information and provides automatic rotation of secrets, reducing the need for manual management. Using a Lambda layer allows multiple Lambda functions to access the sensitive information stored in Secrets Manager without needing to duplicate retrieval logic in each function. This approach centralizes the retrieval process and reduces operational complexity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company has an internal application that runs on Amazon EC2 instances in an Auto Scaling \ngroup. The EC2 instances are compute optimized and use Amazon Elastic Block Store (Amazon \nEBS) volumes. \n \nThe company wants to identify cost optimizations across the EC2 instances, the Auto Scaling \ngroup, and the EBS volumes. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Cost and Usage Report. Search the report for cost recommendations for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create new Amazon CloudWatch billing alerts. Check the alert statuses for cost",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances, the Auto",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 61,
    "text": "A company is running a media store across multiple Amazon EC2 instances distributed across \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n477 \nmultiple Availability Zones in a single VPC. The company wants a high-performing solution to \nshare data between all the EC2 instances, and prefers to keep the data within the VPC only. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket and call the service APIs from each instance's application",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket and configure all instances to access it as a mounted volume",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon Elastic Block Store (Amazon EBS) volume and mount it across all",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Elastic File System (Amazon EFS) file system and mount it across all",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company uses an Amazon RDS for MySQL instance. To prepare for end-of-year processing, \nthe company added a read replica to accommodate extra read-only queries from the company's \nreporting tool. The read replica CPU usage was 60% and the primary instance CPU usage was \n60%. \n \nAfter end-of-year activities are complete, the read replica has a constant 25% CPU usage. The \nprimary instance still has a constant 60% CPU usage. The company wants to rightsize the \ndatabase and still provide enough performance for future growth. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Delete the read replica Do not make changes to the primary instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Resize the read replica to a smaller instance size Do not make changes to the primary instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Resize the read replica to a larger instance size Resize the primary instance to a smaller instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Delete the read replica Resize the primary instance to a larger instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "A company is migrating its databases to Amazon RDS for PostgreSQL. The company is migrating \nits applications to Amazon EC2 instances. The company wants to optimize costs for long-running \nworkloads. \n \nWhich solution will meet this requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use On-Demand Instances for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase Reserved Instances for a 1 year term with the No Upfront option for the Amazon RDS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase Reserved Instances for a 1 year term with the Partial Upfront option for the Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase Reserved Instances for a 3 year term with the All Upfront option for the Amazon RDS",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A company is using an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The company \nmust ensure that Kubernetes service accounts in the EKS cluster have secure and granular \naccess to specific AWS resources by using IAM roles for service accounts (IRSA). \n \nWhich combination of solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM policy that defines the required permissions Attach the policy directly to the IAM",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement network policies within the EKS cluster to prevent Kubernetes service accounts from",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the EKS cluster's IAM role to include permissions for each Kubernetes service account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Define an IAM role that includes the necessary permissions. Annotate the Kubernetes service",
        "correct": true
      },
      {
        "id": 4,
        "text": "Set up a trust relationship between the IAM roles for the service accounts and an OpenID",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  }
]