[
  {
    "id": 1,
    "text": "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      },
      {
        "id": 1,
        "text": "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements",
        "correct": true
      },
      {
        "id": 2,
        "text": "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      },
      {
        "id": 3,
        "text": "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements",
        "correct": true
      },
      {
        "id": 4,
        "text": "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "The correct answers are: Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements, Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
    "options": [
      {
        "id": 0,
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
    "options": [
      {
        "id": 0,
        "text": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS",
        "correct": true
      },
      {
        "id": 1,
        "text": "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Different versions of a single object can have different retention modes and periods",
        "correct": true
      },
      {
        "id": 1,
        "text": "The bucket default settings will override any explicit retention mode or period you request on an object version",
        "correct": false
      },
      {
        "id": 2,
        "text": "You cannot place a retention period on an object version through a bucket default setting",
        "correct": false
      },
      {
        "id": 3,
        "text": "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version",
        "correct": true
      },
      {
        "id": 4,
        "text": "When you use bucket default settings, you specify a Retain Until Date for the object version",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "The correct answers are: Different versions of a single object can have different retention modes and periods, When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The bucket default settings will override any explicit retention mode or period you request on an object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You cannot place a retention period on an object version through a bucket default setting: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- When you use bucket default settings, you specify a Retain Until Date for the object version: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Glue",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon EMR",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon FSx for Lustre",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon S3 Intelligent-Tiering => Amazon S3 Standard",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard-IA => Amazon S3 One Zone-IA",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 One Zone-IA => Amazon S3 Standard-IA",
        "correct": true
      },
      {
        "id": 4,
        "text": "Amazon S3 Standard => Amazon S3 Intelligent-Tiering",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "The correct answers are: Amazon S3 Intelligent-Tiering => Amazon S3 Standard, Amazon S3 One Zone-IA => Amazon S3 Standard-IA\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard-IA => Amazon S3 One Zone-IA: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard => Amazon S3 Intelligent-Tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the security group on the Application Load Balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure the security group for the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the security group on the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
    "options": [
      {
        "id": 0,
        "text": "Leverage Amazon API Gateway with AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics",
        "correct": true
      },
      {
        "id": 2,
        "text": "Leverage Amazon QuickSight with Amazon Redshift",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage Amazon Athena with Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Leverage Amazon API Gateway with Amazon Kinesis Data Analytics\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway with AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon QuickSight with Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon Athena with Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 14,
    "text": "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Encrypt the access keys and save them on Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a strong password for the AWS account root user",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Multi Factor Authentication (MFA) for the AWS account root user account",
        "correct": true
      },
      {
        "id": 3,
        "text": "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create AWS account root user access keys and share those keys only with the business owner",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2
    ],
    "explanation": "The correct answers are: Create a strong password for the AWS account root user, Enable Multi Factor Authentication (MFA) for the AWS account root user account\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Encrypt the access keys and save them on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create AWS account root user access keys and share those keys only with the business owner: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
    "options": [
      {
        "id": 0,
        "text": "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
    "options": [
      {
        "id": 0,
        "text": "Grant maximum privileges to avoid assigning privileges again",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use user credentials to provide access specific permissions for Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a minimum number of accounts and share these account credentials among employees",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users",
        "correct": true
      },
      {
        "id": 4,
        "text": "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "The correct answers are: Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users, Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Grant maximum privileges to avoid assigning privileges again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use user credentials to provide access specific permissions for Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a minimum number of accounts and share these account credentials among employees: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
    "options": [
      {
        "id": 0,
        "text": "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals",
        "correct": true
      },
      {
        "id": 1,
        "text": "Only root user should have full database access in the organization",
        "correct": false
      },
      {
        "id": 2,
        "text": "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur",
        "correct": false
      },
      {
        "id": 3,
        "text": "Remove full database access for all IAM users in the organization",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance",
        "correct": true
      },
      {
        "id": 1,
        "text": "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched",
        "correct": false
      },
      {
        "id": 2,
        "text": "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "The correct answers are: Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance, As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon SQS standard queue to process the messages",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS standard queue to process the messages: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
    "options": [
      {
        "id": 0,
        "text": "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Upload the compressed file using multipart upload",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the compressed file in a single operation",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file using multipart upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Upload the compressed file in a single operation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
    "options": [
      {
        "id": 0,
        "text": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "The correct answers are: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution, Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection",
        "correct": true
      },
      {
        "id": 1,
        "text": "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region",
        "correct": false
      },
      {
        "id": 2,
        "text": "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region",
        "correct": false
      },
      {
        "id": 3,
        "text": "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 26,
    "text": "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
    "options": [
      {
        "id": 0,
        "text": "The engineering team needs to provision more servers running the Amazon SNS service",
        "correct": false
      },
      {
        "id": 1,
        "text": "The engineering team needs to provision more servers running the AWS Lambda service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The engineering team needs to provision more servers running the Amazon SNS service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The engineering team needs to provision more servers running the AWS Lambda service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 28,
    "text": "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "General Purpose Solid State Drive (gp2)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Throughput Optimized Hard disk drive (st1)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Instance Store",
        "correct": false
      },
      {
        "id": 3,
        "text": "Cold Hard disk drive (sc1)",
        "correct": true
      },
      {
        "id": 4,
        "text": "Provisioned IOPS Solid state drive (io1)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "The correct answers are: Throughput Optimized Hard disk drive (st1), Cold Hard disk drive (sc1)\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- General Purpose Solid State Drive (gp2): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Instance Store: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provisioned IOPS Solid state drive (io1): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
    "options": [
      {
        "id": 0,
        "text": "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      },
      {
        "id": 2,
        "text": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": true
      },
      {
        "id": 3,
        "text": "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Host-based Routing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Path-based Routing",
        "correct": true
      },
      {
        "id": 2,
        "text": "HTTP header-based routing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Query string parameter-based routing",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Path-based Routing\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Host-based Routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- HTTP header-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Query string parameter-based routing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 31,
    "text": "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier",
        "correct": true
      },
      {
        "id": 2,
        "text": "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda function to run the workflow processes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 on-demand instances to run the workflow processes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 reserved instances to run the workflow processes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 spot instances to run the workflow processes",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use Amazon EC2 spot instances to run the workflow processes\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Lambda function to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances to run the workflow processes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 33,
    "text": "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "The correct answers are: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed, Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
    "options": [
      {
        "id": 0,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 35,
    "text": "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service",
        "correct": true
      },
      {
        "id": 2,
        "text": "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again",
        "correct": false
      },
      {
        "id": 3,
        "text": "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy",
        "correct": false
      },
      {
        "id": 4,
        "text": "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "The correct answers are: Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service, Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
    "options": [
      {
        "id": 0,
        "text": "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests",
        "correct": true
      },
      {
        "id": 1,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used",
        "correct": false
      },
      {
        "id": 2,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
        "correct": false
      },
      {
        "id": 3,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 38,
    "text": "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block",
        "correct": false
      },
      {
        "id": 2,
        "text": "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
    "options": [
      {
        "id": 0,
        "text": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 2,
        "text": "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 41,
    "text": "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use Amazon Aurora Global Database to replicate data across regions for compatibility",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "The correct answers are: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data, Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Aurora Global Database to replicate data across regions for compatibility: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use VPC security groups to control the network traffic to and from your file system",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use an IAM policy to control access for clients who can mount your file system with the required permissions",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "The correct answers are: Use VPC security groups to control the network traffic to and from your file system, Use an IAM policy to control access for clients who can mount your file system with the required permissions\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
    "options": [
      {
        "id": 0,
        "text": "Establish a process to get managerial approval for deleting Amazon S3 objects",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable versioning on the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 4,
        "text": "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "The correct answers are: Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket, Enable versioning on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
    "options": [
      {
        "id": 0,
        "text": "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively",
        "correct": false
      },
      {
        "id": 1,
        "text": "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures",
        "correct": false
      },
      {
        "id": 2,
        "text": "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements",
        "correct": false
      },
      {
        "id": 3,
        "text": "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 46,
    "text": "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
    "options": [
      {
        "id": 0,
        "text": "The junior scientist does not need to pay any transfer charges for the image upload",
        "correct": true
      },
      {
        "id": 1,
        "text": "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload",
        "correct": false
      },
      {
        "id": 2,
        "text": "The junior scientist only needs to pay S3TA transfer charges for the image upload",
        "correct": false
      },
      {
        "id": 3,
        "text": "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: The junior scientist does not need to pay any transfer charges for the image upload\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay S3TA transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The junior scientist only needs to pay Amazon S3 transfer charges for the image upload: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Amazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon S3 Glacier Deep Archive: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Standard: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "It is not possible to access cross-account resources",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment",
        "correct": true
      },
      {
        "id": 2,
        "text": "Both IAM roles and IAM users can be used interchangeably for cross-account access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 instances with Amazon EFS mount points",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 instances with access to Amazon S3 based storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Instance Store based Amazon EC2 instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use Instance Store based Amazon EC2 instances\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with Amazon EFS mount points: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 instances with access to Amazon S3 based storage: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
    "options": [
      {
        "id": 0,
        "text": "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods",
        "correct": true
      },
      {
        "id": 3,
        "text": "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
    "options": [
      {
        "id": 0,
        "text": "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the intermediary query results in Amazon S3 Standard storage class",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Store the intermediary query results in Amazon S3 Standard storage class\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 54,
    "text": "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point",
        "correct": true
      },
      {
        "id": 1,
        "text": "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers",
        "correct": true
      },
      {
        "id": 1,
        "text": "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use Amazon CloudFront with a custom origin pointing to the on-premises servers\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "AWS Global Accelerator",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Elastic Load Balancing (ELB)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Route 53",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon CloudFront",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: AWS Global Accelerator\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- AWS Elastic Load Balancing (ELB): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Route 53: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 3,
        "text": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 1,
        "text": "Microsoft SQL Server on AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon FSx for Lustre",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Amazon FSx for Windows File Server\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Microsoft SQL Server on AWS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Lustre: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume",
        "correct": false
      },
      {
        "id": 2,
        "text": "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required",
        "correct": true
      },
      {
        "id": 3,
        "text": "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks: SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.\n- Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume: SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect to establish a connection between the data center and AWS Cloud: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
    "options": [
      {
        "id": 0,
        "text": "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B",
        "correct": true
      },
      {
        "id": 1,
        "text": "1 Amazon EC2 instance and 2 AMIs exist in Region B",
        "correct": false
      },
      {
        "id": 2,
        "text": "1 Amazon EC2 instance and 1 AMI exist in Region B",
        "correct": false
      },
      {
        "id": 3,
        "text": "1 Amazon EC2 instance and 1 snapshot exist in Region B",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: 1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- 1 Amazon EC2 instance and 2 AMIs exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 AMI exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 1 Amazon EC2 instance and 1 snapshot exist in Region B: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 62,
    "text": "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
    "options": [
      {
        "id": 0,
        "text": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
    "options": [
      {
        "id": 0,
        "text": "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "correct": false
      },
      {
        "id": 1,
        "text": "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "correct": true
      },
      {
        "id": 2,
        "text": "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "correct": false
      },
      {
        "id": 3,
        "text": "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- { \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 65,
    "text": "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
    "options": [
      {
        "id": 0,
        "text": "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations",
        "correct": true
      },
      {
        "id": 2,
        "text": "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  }
]