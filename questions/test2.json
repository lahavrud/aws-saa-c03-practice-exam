[
  {
    "id": 1,
    "text": "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances. As a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-ec2",
      "amazon-guardduty",
      "amazon-inspector",
      "amazon-s3",
      "best-practice-employ-specialized-security-services-for-specific-tasks-(guardduty-for-threat-detection,-inspector-for-vulnerability-assessment).",
      "best-practice-follow-the-principle-of-least-privilege-when-granting-permissions-to-aws-services.",
      "best-practice-regularly-review-and-remediate-security-findings-from-guardduty-and-inspector.",
      "security-best-practices",
      "threat-detection",
      "vulnerability-assessment"
    ]
  },
  {
    "id": 2,
    "text": "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-s3",
      "best-practice-choose-the-appropriate-s3-storage-class-based-on-access-frequency,-data-durability-requirements,-and-cost-considerations.",
      "best-practice-consider-s3-one-zone-ia-for-data-that-is-infrequently-accessed-and-can-be-easily-recreated.",
      "best-practice-use-s3-lifecycle-policies-to-manage-object-storage-costs-based-on-access-patterns.",
      "s3-lifecycle-policies",
      "s3-storage-classes-(s3-standard,-s3-standard-ia,-s3-one-zone-ia)",
      "storage-cost-optimization"
    ]
  },
  {
    "id": 3,
    "text": "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom. Which of the following is the BEST solution for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-s3",
      "auditing",
      "aws-cloudtrail",
      "aws-key-management-service-(kms)",
      "best-practice-encrypt-sensitive-data-at-rest",
      "best-practice-implement-auditing-for-security-and-compliance",
      "best-practice-leverage-aws-managed-services-to-reduce-operational-overhead",
      "best-practice-use-kms-for-key-management-and-auditing",
      "data-security",
      "encryption-at-rest",
      "server-side-encryption-(sse)",
      "sse-c",
      "sse-kms",
      "sse-s3"
    ]
  },
  {
    "id": 4,
    "text": "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home. As a solutions architect, which of the following solutions would you recommend? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      },
      {
        "id": 1,
        "text": "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements",
        "correct": true
      },
      {
        "id": 2,
        "text": "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      },
      {
        "id": 3,
        "text": "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements",
        "correct": true
      },
      {
        "id": 4,
        "text": "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 3 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 4 is incorrect:**\nis incorrect because Amazon Neptune is a graph database service. While it provides low latency for graph-based queries, it is not the best choice for a simple leaderboard application. The data structure and query patterns of a leaderboard are better suited for key-value stores or sorted sets, which are offered by ElastiCache for Redis or DynamoDB with DAX. Neptune is optimized for complex relationship analysis, which is not a primary requirement for this scenario.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-dynamodb",
      "amazon-elasticache-for-redis",
      "amazon-neptune",
      "amazon-rds-for-aurora",
      "best-practice-choose-the-right-database-for-the-workload.",
      "best-practice-design-for-high-availability-and-fault-tolerance.",
      "best-practice-optimize-database-performance-for-real-time-applications.",
      "best-practice-use-in-memory-caching-for-low-latency-read-operations.",
      "caching",
      "dynamodb-accelerator-(dax)",
      "high-availability",
      "in-memory-data-store",
      "low-latency",
      "real-time-processing"
    ]
  },
  {
    "id": 5,
    "text": "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
    "options": [
      {
        "id": 0,
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-cloudfront",
      "amazon-dynamodb",
      "amazon-dynamodb-accelerator-(dax)",
      "amazon-elasticache-memcached",
      "amazon-elasticache-redis",
      "amazon-s3",
      "application-load-balancer",
      "auto-scaling-group",
      "best-practice-choose-the-right-caching-solution-for-the-specific-use-case-(e.g.,-dax-for-dynamodb,-cloudfront-for-s3).",
      "best-practice-consider-the-trade-offs-between-different-caching-strategies-(e.g.,-dax-vs.-elasticache-redis).",
      "best-practice-optimize-read-performance-by-caching-frequently-accessed-data.",
      "best-practice-use-a-cdn-to-cache-static-content-and-reduce-load-on-the-origin-server.",
      "best-practice-use-caching-to-improve-application-performance-and-reduce-latency.",
      "caching",
      "content-delivery-network-(cdn)",
      "rest-api"
    ]
  },
  {
    "id": 6,
    "text": "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What is the correct order of the storage charges incurred for the test file on these three storage types?",
    "options": [
      {
        "id": 0,
        "text": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS",
        "correct": true
      },
      {
        "id": 1,
        "text": "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-ebs",
      "amazon-efs",
      "amazon-s3",
      "best-practice-choose-the-appropriate-storage-service-based-on-cost,-performance,-and-access-patterns.",
      "best-practice-consider-s3-for-infrequently-accessed-data-or-data-that-doesn't-require-high-performance.",
      "best-practice-right-size-ebs-volumes-to-avoid-unnecessary-costs.",
      "best-practice-understand-the-pricing-models-of-different-aws-services-before-deploying-resources.",
      "best-practice-use-storage-lifecycle-policies-to-move-data-to-cheaper-storage-tiers-when-appropriate.",
      "pay-as-you-go-pricing",
      "pricing-models",
      "provisioned-storage",
      "storage-classes"
    ]
  },
  {
    "id": 7,
    "text": "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected. Which of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Different versions of a single object can have different retention modes and periods",
        "correct": true
      },
      {
        "id": 1,
        "text": "The bucket default settings will override any explicit retention mode or period you request on an object version",
        "correct": false
      },
      {
        "id": 2,
        "text": "You cannot place a retention period on an object version through a bucket default setting",
        "correct": false
      },
      {
        "id": 3,
        "text": "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version",
        "correct": true
      },
      {
        "id": 4,
        "text": "When you use bucket default settings, you specify a Retain Until Date for the object version",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 3 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 4 is incorrect:**\nis incorrect because when using bucket default settings, you specify a retention *period* (e.g., number of days or years), not a 'Retain Until Date'. The 'Retain Until Date' is calculated based on the object's creation date and the specified retention period.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-s3",
      "best-practice-explicitly-define-retention-periods-for-objects-when-specific-requirements-exist,-overriding-bucket-default-settings.",
      "best-practice-regularly-review-and-audit-retention-policies-to-ensure-they-align-with-compliance-requirements.",
      "best-practice-understand-the-difference-between-governance-and-compliance-retention-modes.",
      "best-practice-use-s3-object-lock-to-enforce-retention-policies-for-compliance-and-data-governance.",
      "bucket-default-settings",
      "object-versions",
      "retain-until-date",
      "retention-modes-(governance,-compliance)",
      "retention-periods",
      "s3-object-lock"
    ]
  },
  {
    "id": 8,
    "text": "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Glue",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon EMR",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon FSx for Lustre",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-fsx-for-lustre",
      "amazon-s3",
      "aws-glue",
      "best-practice-choose-the-right-storage-solution-based-on-data-access-patterns.",
      "best-practice-consider-high-performance-file-systems-for-compute-intensive-workloads.",
      "best-practice-use-tiered-storage-to-optimize-cost-and-performance.",
      "high-performance-computing-(hpc)",
      "tiered-storage"
    ]
  },
  {
    "id": 9,
    "text": "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3. Can you spot the INVALID lifecycle transitions from the options below? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon S3 Intelligent-Tiering => Amazon S3 Standard",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard-IA => Amazon S3 One Zone-IA",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 One Zone-IA => Amazon S3 Standard-IA",
        "correct": true
      },
      {
        "id": 4,
        "text": "Amazon S3 Standard => Amazon S3 Intelligent-Tiering",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 3 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. It allows S3 to automatically manage the tiering of objects based on access patterns, potentially reducing storage costs for data that is not frequently accessed. This is a common and recommended practice for cost optimization.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-s3",
      "best-practice-choose-the-appropriate-s3-storage-class-based-on-access-patterns-and-data-durability-requirements.",
      "best-practice-consider-using-s3-intelligent-tiering-to-automatically-optimize-storage-costs.",
      "best-practice-understand-the-limitations-of-s3-storage-class-transitions.",
      "best-practice-use-s3-lifecycle-policies-to-manage-object-storage-costs.",
      "cost-optimization",
      "data-availability",
      "data-durability",
      "s3-intelligent-tiering",
      "s3-lifecycle-policies",
      "s3-one-zone-ia",
      "s3-standard",
      "s3-standard-ia",
      "s3-storage-classes"
    ]
  },
  {
    "id": 10,
    "text": "A software company has a globally distributed team of developers, that requires secure and compliant access to AWS environments. The company manages multiple AWS accounts under AWS Organizations and uses an on-premises Microsoft Active Directory for user authentication. To simplify access control and identity governance across projects and accounts, the company wants a centrally managed solution that integrates with their existing infrastructure. The solution should require the least amount of ongoing operational management. Which approach best meets the company’s requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Directory Service for Microsoft Active Directory in AWS. Establish a trust relationship with the on-premises Active Directory. Use IAM roles linked to AD groups to control access to AWS resources",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Control Tower to enable account access for developers. Create AWS IAM roles in each member account and manually assign permissions. Instruct developers to assume roles across accounts using the AWS CLI",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an open-source identity provider (IdP) on Amazon EC2. Synchronize it with the on-premises Active Directory and use SAML to federate access to AWS accounts. Assign IAM roles to federated users based on SAML assertions",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Directory Service AD Connector to connect AWS to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Use permission sets to assign access to AWS accounts and resources based on Active Directory group membership",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.",
    "domain": "Design Secure Architectures",
    "tags": [
      "aws-control-tower",
      "aws-directory-service",
      "aws-directory-service-ad-connector",
      "aws-directory-service-for-microsoft-active-directory",
      "aws-iam-identity-center-(successor-to-aws-sso)",
      "aws-organizations",
      "best-practice-automate-access-control",
      "best-practice-centralized-identity-management",
      "best-practice-federated-access",
      "best-practice-integrate-with-existing-identity-providers",
      "best-practice-least-privilege",
      "best-practice-use-managed-services",
      "iam-roles",
      "permission-sets",
      "saml"
    ]
  },
  {
    "id": 11,
    "text": "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company. Which configuration should be used to meet this changed requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the security group on the Application Load Balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-cloudfront",
      "amazon-elastic-compute-cloud-(ec2)",
      "amazon-virtual-private-cloud-(vpc)",
      "application-load-balancer-(alb)",
      "aws-web-application-firewall-(waf)",
      "best-practice-centralize-security-policies-for-easier-management-and-enforcement.",
      "best-practice-choose-the-right-aws-service-for-the-specific-task-(e.g.,-waf-for-application-level-filtering,-cloudfront-for-content-delivery).",
      "best-practice-implement-security-at-multiple-layers-(defense-in-depth).",
      "best-practice-use-aws-waf-for-application-level-security-and-protection-against-common-web-exploits.",
      "geo-restriction",
      "security-groups"
    ]
  },
  {
    "id": 12,
    "text": "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis. Which of the following options addresses the given use case?",
    "options": [
      {
        "id": 0,
        "text": "Leverage Amazon API Gateway with AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics",
        "correct": true
      },
      {
        "id": 2,
        "text": "Leverage Amazon QuickSight with Amazon Redshift",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage Amazon Athena with Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-api-gateway",
      "amazon-athena",
      "amazon-kinesis-data-analytics",
      "amazon-quicksight",
      "amazon-redshift",
      "amazon-s3",
      "aws-lambda",
      "best-practice-choose-the-right-tool-for-the-job-(kinesis-data-analytics-for-real-time-streaming-data)",
      "best-practice-consider-real-time-data-processing-requirements",
      "best-practice-design-for-scalability-and-performance",
      "best-practice-use-api-gateway-for-creating-rest-apis",
      "data-streaming",
      "real-time-data-processing",
      "rest-api"
    ]
  },
  {
    "id": 13,
    "text": "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3–based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code. Which AWS solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue Studio’s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-appflow",
      "amazon-athena",
      "amazon-s3",
      "apache-parquet",
      "aws-glue-data-catalog",
      "aws-glue-databrew",
      "aws-iam",
      "best-practice-choose-services-that-facilitate-easy-sharing-and-reuse-of-transformation-logic.",
      "best-practice-implement-data-lineage-tracking-for-auditing-and-troubleshooting.",
      "best-practice-leverage-visual-interfaces-to-improve-collaboration-and-reduce-the-need-for-coding.",
      "best-practice-use-purpose-built-services-for-specific-tasks-(e.g.,-databrew-for-data-preparation).",
      "best-practice-utilize-data-profiling-to-understand-data-quality-and-identify-potential-issues."
    ]
  },
  {
    "id": 14,
    "text": "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Encrypt the access keys and save them on Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a strong password for the AWS account root user",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Multi Factor Authentication (MFA) for the AWS account root user account",
        "correct": true
      },
      {
        "id": 3,
        "text": "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create AWS account root user access keys and share those keys only with the business owner",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 2 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.\n\n**Why option 4 is incorrect:**\nis incorrect because creating and sharing root user access keys is a dangerous practice. Root user access keys grant full administrative access to the AWS account. Sharing these keys, even with the business owner, significantly increases the risk of unauthorized access and potential security breaches. It is strongly recommended to avoid creating root user access keys whenever possible. Instead, use IAM users and roles with least privilege access.",
    "domain": "Design Secure Architectures",
    "tags": [
      "access-keys",
      "aws-account-root-user",
      "best-practice-avoid-creating-access-keys-for-the-aws-account-root-user.",
      "best-practice-do-not-share-aws-account-root-user-credentials.",
      "best-practice-enable-multi-factor-authentication-(mfa)-for-the-aws-account-root-user.",
      "best-practice-regularly-review-and-rotate-iam-user-credentials.",
      "best-practice-use-a-strong-password-for-the-aws-account-root-user.",
      "best-practice-use-iam-users-and-roles-with-least-privilege-access-instead-of-the-root-user.",
      "identity-and-access-management-(iam)",
      "least-privilege-principle",
      "multi-factor-authentication-(mfa)",
      "s3-(simple-storage-service)"
    ]
  },
  {
    "id": 15,
    "text": "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
    "options": [
      {
        "id": 0,
        "text": "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-rds",
      "amazon-rds-custom",
      "availability-zones",
      "best-practice-balance-customization-needs-with-managed-service-benefits",
      "best-practice-choose-the-right-database-service-based-on-requirements-(managed-vs.-self-managed)",
      "best-practice-minimize-operational-overhead-by-leveraging-managed-services",
      "best-practice-use-multi-az-deployments-for-high-availability",
      "multi-az-deployment",
      "oracle-database"
    ]
  },
  {
    "id": 16,
    "text": "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM). As an AWS Certified Solutions Architect – Associate, which best practices would you recommend (Select two)?",
    "options": [
      {
        "id": 0,
        "text": "Grant maximum privileges to avoid assigning privileges again",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use user credentials to provide access specific permissions for Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a minimum number of accounts and share these account credentials among employees",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users",
        "correct": true
      },
      {
        "id": 4,
        "text": "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 4 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures",
    "tags": [
      "aws-cloudtrail",
      "aws-iam",
      "aws-mfa",
      "best-practice-apply-the-principle-of-least-privilege",
      "best-practice-avoid-sharing-account-credentials",
      "best-practice-enable-mfa-for-privileged-users",
      "best-practice-monitor-iam-activity-with-cloudtrail",
      "best-practice-use-iam-roles-for-ec2-instances",
      "iam-roles",
      "principle-of-least-privilege"
    ]
  },
  {
    "id": 17,
    "text": "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
    "options": [
      {
        "id": 0,
        "text": "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals",
        "correct": true
      },
      {
        "id": 1,
        "text": "Only root user should have full database access in the organization",
        "correct": false
      },
      {
        "id": 2,
        "text": "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur",
        "correct": false
      },
      {
        "id": 3,
        "text": "Remove full database access for all IAM users in the organization",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-dynamodb",
      "best-practice-automate-security-controls-and-processes.",
      "best-practice-avoid-using-the-root-user-for-day-to-day-tasks.",
      "best-practice-implement-the-principle-of-least-privilege.",
      "best-practice-regularly-review-and-audit-iam-permissions.",
      "best-practice-use-iam-roles-for-applications-and-services.",
      "best-practice-use-permissions-boundaries-to-limit-the-maximum-permissions-of-iam-principals.",
      "iam",
      "iam-policies",
      "iam-roles",
      "iam-users",
      "least-privilege",
      "permissions-boundaries"
    ]
  },
  {
    "id": 18,
    "text": "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within theus-east-1region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check. Can you identify the correct outcomes for these events? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance",
        "correct": true
      },
      {
        "id": 1,
        "text": "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched",
        "correct": false
      },
      {
        "id": 2,
        "text": "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 2 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.\n\n**Why option 4 is incorrect:**\nis incorrect because Auto Scaling will terminate the unhealthy instance first (triggered by health check failure) and then launch a new instance to maintain desired capacity. The order of operations is important.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-ec2",
      "application-load-balancer-(alb)",
      "auto-scaling-groups-(asg)",
      "availability-zones-(az)",
      "best-practice-configure-auto-scaling-groups-to-maintain-desired-capacity.",
      "best-practice-distribute-instances-across-multiple-availability-zones-for-high-availability.",
      "best-practice-monitor-auto-scaling-group-events-and-metrics-to-ensure-proper-operation.",
      "best-practice-understand-auto-scaling's-rebalancing-behavior-to-avoid-unexpected-capacity-changes.",
      "best-practice-use-health-checks-to-automatically-detect-and-replace-unhealthy-instances.",
      "desired-capacity",
      "health-checks",
      "instance-launch",
      "instance-termination",
      "rebalancing",
      "scaling-policies"
    ]
  },
  {
    "id": 19,
    "text": "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead. Which solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-api-gateway",
      "amazon-comprehend",
      "amazon-dynamodb",
      "amazon-eventbridge",
      "amazon-lookout-for-vision",
      "amazon-s3",
      "amazon-sagemaker",
      "amazon-sns",
      "amazon-transcribe",
      "aws-lambda",
      "best-practice-avoid-unnecessary-complexity-by-using-direct-integrations-where-possible-(e.g.,-s3-event-triggers-instead-of-eventbridge).",
      "best-practice-choose-managed-services-to-minimize-operational-overhead.",
      "best-practice-optimize-for-cost-by-using-pay-per-use-services-like-lambda-and-comprehend.",
      "best-practice-select-the-most-appropriate-service-for-the-specific-task-(e.g.,-comprehend-for-nlp,-not-lookout-for-vision).",
      "best-practice-use-event-driven-architectures-for-automation.",
      "s3-event-notifications"
    ]
  },
  {
    "id": 20,
    "text": "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order. Which of the following options can be used to implement this system?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon SQS standard queue to process the messages",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-sqs",
      "aws-cost-optimization",
      "best-practice-choose-the-appropriate-queue-type-(fifo-or-standard)-based-on-application-requirements-(message-ordering-vs.-throughput).",
      "best-practice-design-for-high-availability-by-leveraging-sqs's-inherent-redundancy.",
      "best-practice-optimize-costs-by-minimizing-the-number-of-api-calls-to-sqs.",
      "best-practice-understand-the-throughput-limits-of-sqs-queue-types-and-design-accordingly.",
      "best-practice-use-message-batching-to-increase-throughput-and-reduce-costs-when-using-sqs-fifo-queues.",
      "high-availability",
      "message-batching",
      "message-ordering",
      "queue-throughput",
      "sqs-fifo-queues",
      "sqs-standard-queues"
    ]
  },
  {
    "id": 21,
    "text": "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The company’s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features — but without migrating any data or compute resources to the cloud. Which AWS solution will best meet the company’s requirements for modernization while ensuring that all data remains on premises?",
    "options": [
      {
        "id": 0,
        "text": "Install an AWS Outposts rack in the company’s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowball’s offline data transfer features. Use the Snowball console to orchestrate workloads in batches",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the company’s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-cloudwatch",
      "amazon-ecs",
      "amazon-eks",
      "amazon-eks-anywhere",
      "aws-direct-connect",
      "aws-iam",
      "aws-local-zones",
      "aws-outposts",
      "aws-snowball-edge",
      "best-practice-choose-the-right-compute-service-based-on-data-residency-requirements.",
      "best-practice-leverage-aws-managed-services-for-operational-efficiency.",
      "best-practice-prioritize-data-security-and-compliance-when-designing-hybrid-architectures.",
      "best-practice-use-aws-outposts-for-running-aws-services-on-premises.",
      "kubernetes"
    ]
  },
  {
    "id": 22,
    "text": "A data analytics company measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes. Which of the following is the fastest way to upload the daily compressed file into Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Upload the compressed file using multipart upload",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the compressed file in a single operation",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-ec2",
      "amazon-s3",
      "amazon-s3-transfer-acceleration-(s3ta)",
      "best-practice-avoid-unnecessary-intermediaries-in-the-data-transfer-path-to-minimize-latency.",
      "best-practice-choose-the-most-efficient-and-secure-method-for-transferring-data-to-s3.",
      "best-practice-consider-using-s3-transfer-acceleration-for-faster-uploads-from-geographically-distant-locations.",
      "best-practice-use-multipart-upload-for-large-files-to-improve-throughput-and-resilience.",
      "multipart-upload"
    ]
  },
  {
    "id": 23,
    "text": "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture. As a Solutions Architect, which of the following will you suggest for the company?",
    "options": [
      {
        "id": 0,
        "text": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-ec2",
      "application-load-balancer-(alb)",
      "auto-scaling-group-(asg)",
      "availability-zones-(azs)",
      "best-practice-avoid-using-elastic-ip-addresses-directly-on-ec2-instances-behind-a-load-balancer.",
      "best-practice-choose-the-appropriate-load-balancer-type-based-on-the-application's-needs-(alb-for-http/https,-nlb-for-tcp/udp).",
      "best-practice-distribute-instances-across-multiple-availability-zones-for-fault-tolerance.",
      "best-practice-use-application-load-balancers-for-http/https-traffic-and-content-based-routing.",
      "best-practice-use-auto-scaling-groups-to-maintain-the-desired-capacity-and-ensure-high-availability.",
      "best-practice-use-load-balancers-for-distributing-traffic-across-multiple-instances.",
      "content-based-routing",
      "elastic-ip-address-(eip)",
      "high-availability",
      "network-load-balancer-(nlb)"
    ]
  },
  {
    "id": 24,
    "text": "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches. Which of the following options would allow the company to enforce these streaming restrictions? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 1 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.\n\n**Why option 4 is incorrect:**\nis incorrect because latency-based routing policy routes traffic to the resource with the lowest latency for the user. While it can improve performance, it doesn't directly address the requirement of restricting access based on geographic location.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-cloudfront",
      "amazon-route-53",
      "best-practice-choose-the-appropriate-route-53-routing-policy-based-on-the-specific-requirements-(e.g.,-geolocation,-failover,-weighted).",
      "best-practice-implement-geo-restriction-to-comply-with-licensing-agreements-and-distribution-rights.",
      "best-practice-secure-content-delivery-using-appropriate-security-measures,-such-as-geo-restriction-and-signed-urls/cookies.",
      "best-practice-use-cloudfront-for-content-delivery-to-improve-performance-and-reduce-latency.",
      "best-practice-use-route-53-geolocation-routing-to-direct-users-to-appropriate-resources-based-on-their-location.",
      "content-delivery-network-(cdn)",
      "geo-restriction",
      "geolocation-routing-policy",
      "routing-policies"
    ]
  },
  {
    "id": 25,
    "text": "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created inus-east-1region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet. As a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection",
        "correct": true
      },
      {
        "id": 1,
        "text": "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region",
        "correct": false
      },
      {
        "id": 2,
        "text": "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region",
        "correct": false
      },
      {
        "id": 3,
        "text": "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design High-Performing Architectures",
    "tags": []
  },
  {
    "id": 26,
    "text": "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack. Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zone–IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-efs",
      "amazon-fsx-for-windows-file-server",
      "amazon-s3",
      "amazon-s3-glacier",
      "amazon-s3-glacier-deep-archive",
      "aws-backup",
      "aws-datasync",
      "aws-storage-gateway-(file-gateway,-volume-gateway)",
      "best-practice-choose-the-most-cost-effective-storage-solution-based-on-access-patterns.",
      "best-practice-consider-nfs-compatibility-when-migrating-nfs-based-workloads-to-the-cloud.",
      "best-practice-leverage-s3-lifecycle-policies-for-automated-tiering-of-data.",
      "best-practice-minimize-on-premises-infrastructure-when-migrating-to-the-cloud.",
      "best-practice-use-the-appropriate-aws-storage-gateway-type-based-on-the-specific-requirements.",
      "nfs",
      "s3-lifecycle-policies",
      "smb-protocol"
    ]
  },
  {
    "id": 27,
    "text": "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website. As a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
    "options": [
      {
        "id": 0,
        "text": "The engineering team needs to provision more servers running the Amazon SNS service",
        "correct": false
      },
      {
        "id": 1,
        "text": "The engineering team needs to provision more servers running the AWS Lambda service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-sns",
      "aws-lambda",
      "best-practice-design-for-scalability-and-resilience",
      "best-practice-monitor-lambda-function-execution-metrics-(e.g.,-invocations,-errors,-throttles)",
      "best-practice-set-appropriate-lambda-concurrency-limits",
      "best-practice-use-serverless-services-for-event-driven-architectures",
      "lambda-concurrency-limits",
      "scalability",
      "serverless-computing",
      "throttling"
    ]
  },
  {
    "id": 28,
    "text": "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume. Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "General Purpose Solid State Drive (gp2)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Throughput Optimized Hard disk drive (st1)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Instance Store",
        "correct": false
      },
      {
        "id": 3,
        "text": "Cold Hard disk drive (sc1)",
        "correct": true
      },
      {
        "id": 4,
        "text": "Provisioned IOPS Solid state drive (io1)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 3 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nProvisioned IOPS Solid State Drive - io1) is incorrect. io1 volumes are designed for I/O-intensive workloads that require sustained high performance. They are suitable for boot volumes, especially when high performance is required for the operating system and applications.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-ebs",
      "amazon-ec2",
      "best-practice-choose-the-appropriate-ebs-volume-type-based-on-the-workload-requirements.",
      "best-practice-consider-the-i/o-requirements-of-the-operating-system-and-applications-when-selecting-a-boot-volume-type.",
      "best-practice-use-hdd-backed-volumes-(st1,-sc1)-for-infrequently-accessed-data-and-large-sequential-workloads.",
      "best-practice-use-ssd-backed-volumes-(gp2,-io1,-io2)-for-boot-volumes-to-ensure-good-performance.",
      "boot-volumes",
      "ebs-volume-types-(gp2,-st1,-sc1,-io1)",
      "storage-performance"
    ]
  },
  {
    "id": 29,
    "text": "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. Given these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
    "options": [
      {
        "id": 0,
        "text": "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      },
      {
        "id": 2,
        "text": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": true
      },
      {
        "id": 3,
        "text": "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-dynamodb",
      "amazon-ec2",
      "amazon-kinesis-data-firehose",
      "amazon-simple-queue-service-(sqs)",
      "auto-scaling",
      "aws-lambda",
      "best-practice-choose-the-right-data-ingestion-service-based-on-the-specific-requirements-of-the-application-(e.g.,-kinesis-for-real-time-streaming,-sqs-for-asynchronous-processing).",
      "best-practice-decouple-services-using-message-queues-to-improve-scalability-and-resilience.",
      "best-practice-leverage-auto-scaling-capabilities-of-aws-services-to-handle-varying-workloads.",
      "best-practice-use-lambda-functions-for-event-driven-processing-and-data-transformation.",
      "best-practice-use-serverless-architectures-whenever-possible-to-reduce-operational-overhead.",
      "data-streaming",
      "message-queues",
      "serverless-computing"
    ]
  },
  {
    "id": 30,
    "text": "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice. Which of the following features of Application Load Balancers can be used for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Host-based Routing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Path-based Routing",
        "correct": true
      },
      {
        "id": 2,
        "text": "HTTP header-based routing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Query string parameter-based routing",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "application-load-balancer-(alb)",
      "best-practice-choose-the-most-appropriate-routing-method-based-on-the-application's-requirements-(path-based,-host-based,-etc.).",
      "best-practice-design-microservices-with-well-defined-apis-and-url-structures-to-facilitate-routing.",
      "best-practice-use-application-load-balancers-for-routing-traffic-to-microservices.",
      "best-practice-use-target-groups-to-manage-the-backend-instances-for-each-microservice.",
      "host-based-routing",
      "http-headers",
      "microservices",
      "path-based-routing",
      "target-groups"
    ]
  },
  {
    "id": 31,
    "text": "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data. Which design choice best satisfies these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier",
        "correct": true
      },
      {
        "id": 2,
        "text": "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-api-gateway",
      "amazon-eventbridge",
      "amazon-sqs",
      "asynchronous-processing",
      "aws-lambda",
      "best-practice-avoid-direct-synchronous-invocation-of-lambda-functions-from-api-gateway-for-long-running-or-critical-processes.",
      "best-practice-choose-the-appropriate-queue-type-(standard-or-fifo)-based-on-application-requirements.",
      "best-practice-design-for-scalability-and-reliability-in-high-traffic-applications.",
      "best-practice-use-fifo-queues-when-message-order-and-exactly-once-processing-are-critical.",
      "best-practice-use-message-queues-to-decouple-application-components.",
      "exactly-once-processing",
      "message-queuing",
      "sqs-fifo-queues",
      "sqs-standard-queues"
    ]
  },
  {
    "id": 32,
    "text": "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times. Which is the most cost-effective solution to build a solution for the workflow?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda function to run the workflow processes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 on-demand instances to run the workflow processes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 reserved instances to run the workflow processes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 spot instances to run the workflow processes",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-ec2",
      "aws-lambda",
      "best-practice-choose-the-right-ec2-instance-type-based-on-workload-requirements.",
      "best-practice-consider-lambda-execution-time-limits-when-designing-serverless-applications.",
      "best-practice-implement-fault-tolerance-mechanisms-in-applications-that-run-on-spot-instances.",
      "best-practice-monitor-spot-instance-pricing-and-availability-to-minimize-disruptions.",
      "best-practice-utilize-spot-instances-for-fault-tolerant-and-interruptible-workloads-to-optimize-costs.",
      "cost-optimization",
      "ec2-on-demand-instances",
      "ec2-reserved-instances",
      "ec2-spot-instances",
      "fault-tolerance"
    ]
  },
  {
    "id": 33,
    "text": "A digital event-ticketing platform hosts its core transaction-processing service on AWS. The service runs on Amazon EC2 instances and stores finalized transactions in an Amazon Aurora PostgreSQL database. During periods of high user activity - such as flash ticket sales or holiday promotions - the application begins timing out, causing failed or delayed purchases. A solutions architect has been asked to redesign the backend for scalability and cost-efficiency, without reengineering the database layer. Which combination of actions will meet these goals in the most cost-effective and scalable manner? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use an Amazon ElastiCache cluster to cache database queries. Configure the application to store purchase transactions in the cache before writing to the database",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 3 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-api-gateway",
      "amazon-aurora-postgresql",
      "amazon-ec2",
      "amazon-elasticache",
      "amazon-rds-proxy",
      "amazon-sqs",
      "auto-scaling",
      "best-practice-avoid-throttling-requests-unless-absolutely-necessary,-as-it-impacts-user-experience.",
      "best-practice-optimize-database-queries-and-use-caching-to-improve-performance.",
      "best-practice-use-asynchronous-processing-with-sqs-to-decouple-components-and-improve-resilience.",
      "best-practice-use-auto-scaling-to-scale-application-servers-based-on-demand.",
      "best-practice-use-rds-proxy-to-manage-database-connections-and-improve-scalability.",
      "read-replicas"
    ]
  },
  {
    "id": 34,
    "text": "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway. Which of the following would you identify as correct?",
    "options": [
      {
        "id": 0,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-api-gateway",
      "best-practice-choose-the-appropriate-api-type-(restful-or-websocket)-based-on-the-application's-communication-requirements-(stateless-or-stateful).",
      "best-practice-design-apis-that-are-scalable,-secure,-and-easy-to-use.",
      "best-practice-use-restful-apis-for-stateless-interactions-where-each-request-is-independent.",
      "best-practice-use-websocket-apis-for-real-time,-bidirectional-communication-where-maintaining-a-persistent-connection-is-necessary.",
      "full-duplex-communication",
      "restful-apis",
      "stateful-communication",
      "stateless-communication",
      "websocket-apis"
    ]
  },
  {
    "id": 35,
    "text": "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs thegamestable to be accessible globally but needs theusersandgames_playedtables to be regional only. How would you implement this with minimal application refactoring?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-aurora",
      "amazon-aurora-global-database",
      "amazon-dynamodb",
      "amazon-dynamodb-global-tables",
      "best-practice-choose-the-right-database-for-the-workload-(aurora-for-transactional,-dynamodb-for-key-value/document)",
      "best-practice-consider-data-residency-requirements-when-designing-global-applications",
      "best-practice-minimize-application-refactoring-when-migrating-or-expanding-infrastructure",
      "best-practice-use-aurora-global-database-for-low-latency-global-reads",
      "database-replication",
      "global-data-distribution",
      "regional-data-residency"
    ]
  },
  {
    "id": 36,
    "text": "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately. As a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service",
        "correct": true
      },
      {
        "id": 2,
        "text": "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again",
        "correct": false
      },
      {
        "id": 3,
        "text": "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy",
        "correct": false
      },
      {
        "id": 4,
        "text": "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 4 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-ec2",
      "amazon-machine-image-(ami)",
      "auto-scaling-groups",
      "auto-scaling-processes",
      "best-practice-avoid-unnecessary-resource-creation",
      "best-practice-leverage-auto-scaling-features-for-efficient-instance-management",
      "best-practice-minimize-downtime-during-maintenance",
      "best-practice-use-the-least-disruptive-method-for-maintenance-tasks",
      "health-checks",
      "scaling-policies",
      "standby-state"
    ]
  },
  {
    "id": 37,
    "text": "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
    "options": [
      {
        "id": 0,
        "text": "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests",
        "correct": true
      },
      {
        "id": 1,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used",
        "correct": false
      },
      {
        "id": 2,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
        "correct": false
      },
      {
        "id": 3,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-elastic-container-service-(ecs)",
      "best-practice-choose-the-appropriate-ecs-launch-type-based-on-your-application-requirements-and-cost-optimization-goals.",
      "best-practice-consider-using-fargate-for-serverless-container-deployments.",
      "best-practice-monitor-resource-utilization-to-optimize-costs.",
      "best-practice-understand-the-pricing-models-for-different-aws-services.",
      "best-practice-use-ec2-launch-type-when-you-need-more-control-over-the-underlying-infrastructure-or-have-specific-ec2-instance-requirements.",
      "containerization",
      "docker",
      "ebs-volumes",
      "ec2-instances",
      "ecs-ec2-launch-type",
      "ecs-fargate-launch-type",
      "memory",
      "vcpu"
    ]
  },
  {
    "id": 38,
    "text": "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partner’s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partner’s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the company’s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up VPC peering between the company’s VPC and the partner’s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database. Modify the RDS subnet route tables to allow access from the company’s CIDR block",
        "correct": false
      },
      {
        "id": 2,
        "text": "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research company’s VPC",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a client VPN endpoint in the company’s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partner’s VPC and route RDS traffic via this connection",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the company’s CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-rds",
      "amazon-vpc",
      "aws-privatelink",
      "aws-transit-gateway",
      "best-practice-avoid-unnecessary-complexity-in-network-architectures.",
      "best-practice-implement-the-principle-of-least-privilege-when-configuring-security-groups-and-route-tables.",
      "best-practice-minimize-the-use-of-public-ip-addresses-for-database-access.",
      "best-practice-prioritize-security-when-designing-solutions-for-accessing-sensitive-data.",
      "best-practice-use-aws-privatelink-for-secure-and-private-connectivity-between-vpcs.",
      "network-load-balancer-(nlb)",
      "route-tables",
      "security-groups",
      "vpc-peering"
    ]
  },
  {
    "id": 39,
    "text": "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas. Which of the following correctly summarizes these capabilities for the given database?",
    "options": [
      {
        "id": 0,
        "text": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 2,
        "text": "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-rds",
      "asynchronous-replication",
      "availability-zones-(azs)",
      "best-practice-choose-the-appropriate-replication-method-(synchronous-or-asynchronous)-based-on-the-application's-requirements-for-data-consistency-and-performance.",
      "best-practice-consider-cross-region-read-replicas-for-disaster-recovery-purposes.",
      "best-practice-use-multi-az-deployments-for-high-availability-and-failover-capabilities.",
      "best-practice-use-read-replicas-to-offload-read-traffic-from-the-primary-database.",
      "multi-az-deployment",
      "read-replicas",
      "regions",
      "synchronous-replication"
    ]
  },
  {
    "id": 40,
    "text": "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead. Which solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-athena",
      "amazon-aurora",
      "amazon-rds-for-postgresql",
      "amazon-redshift",
      "amazon-redshift-ml",
      "amazon-redshift-serverless",
      "amazon-s3",
      "amazon-sagemaker",
      "aws-glue",
      "best-practice-choose-the-right-data-warehouse-solution-based-on-scale,-performance,-and-analytical-requirements.",
      "best-practice-leverage-sql-based-ml-capabilities-for-ease-of-use-and-integration-with-existing-data-warehouse-workflows.",
      "best-practice-optimize-data-transformation-and-loading-processes-for-efficiency-and-performance.",
      "best-practice-use-mpp-databases-for-large-scale-analytical-workloads.",
      "best-practice-use-serverless-services-to-minimize-operational-overhead.",
      "data-lake",
      "data-warehouse",
      "etl-(extract,-transform,-load)",
      "mpp-(massively-parallel-processing)",
      "serverless-computing"
    ]
  },
  {
    "id": 41,
    "text": "A healthcare company is developing a secure internal web portal hosted on AWS. The application must communicate with legacy systems that reside in the company's on-premises data centers. These data centers are connected to AWS via a site-to-site VPN. The company uses Amazon Route 53 as its DNS solution and requires the application to resolve private DNS records for the on-premises services from within its Amazon VPC. What is the MOST secure and appropriate way to meet these DNS resolution requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Route 53 Resolver inbound endpoint and create a DNS forwarding rule. Enable recursive DNS resolution in the VPC to access on-premises services",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Route 53 Resolver outbound endpoint. Define a forwarding rule that routes DNS queries for on-premises domains to the on-premises DNS server. Associate the rule with the VPC",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Route 53 private hosted zone for the on-premises domain. Associate the hosted zone with the VPC to allow the application to resolve DNS names of the on-premises services",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a hybrid connectivity gateway and attach the on-premises DNS servers to Route 53 as authoritative zones for internal domains",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-route-53",
      "best-practice-avoid-replicating-dns-records-between-on-premises-and-aws-environments.",
      "best-practice-leverage-existing-on-premises-dns-infrastructure-when-possible.",
      "best-practice-minimize-the-exposure-of-internal-dns-information-to-external-networks.",
      "best-practice-use-private-hosted-zones-for-internal-dns-resolution-within-a-vpc.",
      "best-practice-use-route-53-resolver-outbound-endpoints-and-forwarding-rules-for-hybrid-dns-resolution.",
      "hybrid-cloud",
      "private-hosted-zone",
      "route-53-resolver",
      "route-53-resolver-forwarding-rules",
      "route-53-resolver-outbound-endpoint",
      "site-to-site-vpn",
      "virtual-private-cloud-(vpc)"
    ]
  },
  {
    "id": 42,
    "text": "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic. Which combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use Amazon Aurora Global Database to replicate data across regions for compatibility",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 3 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-aurora-global-database",
      "amazon-aurora-postgresql",
      "aws-database-migration-service-(aws-dms)",
      "aws-glue",
      "aws-schema-conversion-tool-(aws-sct)",
      "babelfish-for-aurora-postgresql",
      "best-practice-choose-the-right-database-migration-strategy-based-on-application-requirements.",
      "best-practice-consider-babelfish-for-aurora-postgresql-to-reduce-application-refactoring-when-migrating-from-sql-server.",
      "best-practice-leverage-aws-dms-for-data-migration.",
      "best-practice-minimize-application-downtime-during-database-migration.",
      "best-practice-optimize-database-performance-after-migration.",
      "best-practice-use-aws-sct-to-assess-and-convert-database-schemas."
    ]
  },
  {
    "id": 43,
    "text": "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be accessed by multiple Amazon EC2 instances. As an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use VPC security groups to control the network traffic to and from your file system",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use an IAM policy to control access for clients who can mount your file system with the required permissions",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 4 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-ec2",
      "amazon-elastic-file-system-(efs)",
      "amazon-guardduty",
      "best-practice-avoid-using-root-account-credentials.",
      "best-practice-implement-defense-in-depth-by-using-multiple-layers-of-security.",
      "best-practice-use-iam-roles-for-ec2-instances-to-access-aws-services.",
      "best-practice-use-security-groups-to-control-network-traffic-to-and-from-ec2-instances-and-efs-mount-targets.",
      "best-practice-use-the-principle-of-least-privilege-when-granting-permissions.",
      "iam-policies",
      "iam-roles",
      "network-acls",
      "vpc-security-groups"
    ]
  },
  {
    "id": 44,
    "text": "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
    "options": [
      {
        "id": 0,
        "text": "Establish a process to get managerial approval for deleting Amazon S3 objects",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable versioning on the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 4,
        "text": "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 3 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.\n\n**Why option 4 is incorrect:**\nis incorrect because there is no built-in configuration in the S3 console to require additional confirmation for deletion. While custom solutions could be built, this is not a standard or readily available feature and does not provide the same level of protection as MFA Delete or Versioning.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-s3",
      "amazon-sns",
      "best-practice-enable-s3-versioning-for-data-protection-and-recovery.",
      "best-practice-implement-strong-iam-policies-to-control-access-to-s3-resources.",
      "best-practice-regularly-review-and-audit-s3-bucket-configurations.",
      "best-practice-use-mfa-delete-to-protect-against-accidental-or-malicious-deletion.",
      "iam-permissions",
      "s3-mfa-delete",
      "s3-versioning"
    ]
  },
  {
    "id": 45,
    "text": "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites. Which of the following Amazon EC2 instance topologies should this application be deployed on?",
    "options": [
      {
        "id": 0,
        "text": "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively",
        "correct": false
      },
      {
        "id": 1,
        "text": "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures",
        "correct": false
      },
      {
        "id": 2,
        "text": "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements",
        "correct": false
      },
      {
        "id": 3,
        "text": "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 0 is incorrect:**\nis incorrect because partition placement groups are primarily used for large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. While they provide fault tolerance by distributing instances across partitions, they do not offer the low latency and high throughput benefits of cluster placement groups, which are more critical for HPC applications.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-ec2",
      "availability-zones",
      "best-practice-consider-the-trade-offs-between-availability-and-performance-when-choosing-a-placement-group-strategy.",
      "best-practice-for-hpc-applications-requiring-low-latency-and-high-throughput,-use-cluster-placement-groups.",
      "best-practice-understand-the-specific-requirements-of-your-workload-to-select-the-appropriate-ec2-instance-type-and-placement-group.",
      "high-performance-computing-(hpc)",
      "placement-groups-(cluster,-partition,-spread)"
    ]
  },
  {
    "id": 46,
    "text": "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?",
    "options": [
      {
        "id": 0,
        "text": "The junior scientist does not need to pay any transfer charges for the image upload",
        "correct": true
      },
      {
        "id": 1,
        "text": "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload",
        "correct": false
      },
      {
        "id": 2,
        "text": "The junior scientist only needs to pay S3TA transfer charges for the image upload",
        "correct": false
      },
      {
        "id": 3,
        "text": "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-s3",
      "amazon-s3-transfer-acceleration-(s3ta)",
      "best-practice-understand-the-pricing-model-of-aws-services-before-using-them.",
      "best-practice-use-s3-transfer-acceleration-when-appropriate-for-faster-uploads,-but-be-aware-of-the-'no-acceleration,-no-charge'-policy.",
      "s3-pricing"
    ]
  },
  {
    "id": 47,
    "text": "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency. As an AWS Certified Solutions Architect – Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves objects between access tiers based on access patterns, but it has monitoring and automation fees that can make it less cost-effective than S3 Standard-IA for data accessed only twice a year. The infrequent access charges and monitoring fees can outweigh the benefits for this use case.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-s3",
      "aws-step-functions",
      "best-practice-choose-the-appropriate-s3-storage-class-based-on-access-patterns-and-cost-requirements.",
      "best-practice-consider-data-lifecycle-policies-to-automatically-transition-data-to-lower-cost-storage-classes-as-it-ages.",
      "best-practice-optimize-storage-costs-by-analyzing-access-patterns-and-selecting-the-most-cost-effective-storage-option.",
      "best-practice-use-aws-step-functions-to-orchestrate-complex-workflows,-including-data-processing-and-report-generation.",
      "data-lifecycle-management",
      "s3-storage-classes-(standard,-standard-ia,-glacier-deep-archive,-intelligent-tiering)",
      "storage-cost-optimization"
    ]
  },
  {
    "id": 48,
    "text": "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "It is not possible to access cross-account resources",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment",
        "correct": true
      },
      {
        "id": 2,
        "text": "Both IAM roles and IAM users can be used interchangeably for cross-account access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Secure Architectures",
    "tags": [
      "assumerole-api",
      "aws-cli",
      "aws-sdk",
      "best-practice-avoid-sharing-iam-user-credentials",
      "best-practice-implement-strong-authentication-and-authorization",
      "best-practice-principle-of-least-privilege",
      "best-practice-regularly-review-and-audit-iam-policies",
      "best-practice-use-iam-roles-for-delegation",
      "best-practice-use-temporary-security-credentials-whenever-possible",
      "cross-account-access",
      "iam-(identity-and-access-management)",
      "iam-policies",
      "iam-roles",
      "iam-users",
      "temporary-security-credentials",
      "trust-policies"
    ]
  },
  {
    "id": 49,
    "text": "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset. Which of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 instances with Amazon EFS mount points",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 instances with access to Amazon S3 based storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Instance Store based Amazon EC2 instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-ebs",
      "amazon-ec2",
      "amazon-efs",
      "amazon-s3",
      "best-practice-choose-the-right-storage-option-based-on-performance,-cost,-and-durability-requirements.",
      "best-practice-leverage-application-level-resilience-to-reduce-infrastructure-costs.",
      "best-practice-optimize-for-cost-when-durability-is-handled-by-the-application.",
      "best-practice-understand-the-trade-offs-between-different-storage-options.",
      "high-i/o-performance",
      "instance-store",
      "storage-options"
    ]
  },
  {
    "id": 50,
    "text": "An enterprise runs a microservices-based application on Amazon EKS, deployed on EC2 worker nodes. The application includes a frontend UI service that interacts with Amazon DynamoDB and a data-processing service that stores and retrieves files from Amazon S3. The organization needs to strictly enforce least privilege access: the UI Pods must access only DynamoDB, and the data-processing Pods must access only S3. Which solution will best enforce these access controls within the EKS cluster?",
    "options": [
      {
        "id": 0,
        "text": "Create one Kubernetes service account shared across all Pods. Attach a single IAM role to this account with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create IAM policies for DynamoDB and S3 access, and attach both to the EC2 instance profile used by the EKS nodes. Use Kubernetes role-based access control (RBAC) to control service-level permissions within the cluster",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create separate Kubernetes service accounts for the UI and data services. Use IAM Roles for Service Accounts (IRSA) to map each service account to an IAM role with only the required permissions. Assign DynamoDB access to the UI Pods and S3 access to the data Pods",
        "correct": true
      },
      {
        "id": 3,
        "text": "Attach an IAM policy directly to each Pod using Kubernetes annotations. Assign the S3 policy to data-service Pods and the DynamoDB policy to UI Pods",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 1 is incorrect:**\nis incorrect because while it creates IAM policies for DynamoDB and S3, attaching them to the EC2 instance profile grants those permissions to *all* Pods running on those nodes. Kubernetes RBAC controls access *within* the cluster, not to AWS resources. This approach doesn't provide the fine-grained control needed to restrict UI Pods to DynamoDB and data-processing Pods to S3. All pods running on the node would inherit the permissions of the instance profile.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-dynamodb",
      "amazon-eks",
      "amazon-s3",
      "aws-sts",
      "best-practice-avoid-attaching-broad-iam-policies-to-ec2-instance-profiles-when-more-granular-control-is-needed",
      "best-practice-isolate-workloads-with-different-security-requirements",
      "best-practice-principle-of-least-privilege",
      "best-practice-use-iam-roles-for-service-accounts-(irsa)-for-fine-grained-access-control-in-eks",
      "iam-policies",
      "iam-roles",
      "iam-roles-for-service-accounts-(irsa)",
      "kubernetes-pods",
      "kubernetes-service-accounts"
    ]
  },
  {
    "id": 51,
    "text": "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline. Which of the following is the MOST cost-effective strategy for storing this intermediary query data?",
    "options": [
      {
        "id": 0,
        "text": "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the intermediary query results in Amazon S3 Standard storage class",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 1 is incorrect:**\nstoring the data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect for similar reasons as S3 Standard-IA. While it offers even lower storage costs, it also has higher retrieval costs and a 30-day minimum storage duration charge. More importantly, S3 One Zone-IA stores data in a single Availability Zone, which makes it less durable than S3 Standard. While durability isn't explicitly mentioned as a primary concern, it's generally a good practice to use S3 Standard unless there's a very specific reason to use a lower-durability option. The frequent access pattern and the 30-day minimum storage duration make this option less cost-effective. Also, the loss of an AZ could impact the analytics pipeline.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-s3",
      "availability-zones",
      "best-practice-balance-cost-and-performance-when-designing-storage-solutions.",
      "best-practice-choose-the-appropriate-s3-storage-class-based-on-access-patterns-and-storage-duration.",
      "best-practice-consider-retrieval-costs-when-evaluating-storage-class-options,-especially-for-frequently-accessed-data.",
      "best-practice-optimize-storage-costs-by-using-lifecycle-policies-to-move-data-to-lower-cost-storage-classes-when-appropriate.",
      "best-practice-prioritize-data-durability-unless-there's-a-specific-reason-to-accept-lower-durability.",
      "big-data-analytics",
      "cost-optimization",
      "data-lake",
      "data-retrieval-costs",
      "s3-storage-classes-(standard,-standard-ia,-one-zone-ia,-glacier-instant-retrieval)",
      "storage-duration"
    ]
  },
  {
    "id": 52,
    "text": "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs. Which of the following represents the best solution for the given scenario?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-athena",
      "amazon-cloudwatch",
      "amazon-kinesis",
      "amazon-quicksight",
      "amazon-sns",
      "aws-cloudtrail",
      "aws-lambda",
      "aws-trusted-advisor",
      "best-practice-automate-incident-response-processes.",
      "best-practice-choose-the-most-cost-effective-and-efficient-solution-for-the-given-requirements.",
      "best-practice-implement-monitoring-and-alerting-for-critical-events.",
      "best-practice-use-appropriate-tools-for-real-time-vs.-historical-analysis.",
      "best-practice-use-cloudwatch-metric-filters-to-extract-specific-data-from-logs.",
      "cloudwatch-alarms",
      "cloudwatch-metric-filters"
    ]
  },
  {
    "id": 53,
    "text": "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances. As a solutions architect, what would you recommend so that the application runs near its peak performance state?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 0 is incorrect:**\nis incorrect because simple scaling policies react to alarms based on thresholds. While you *could* use simple scaling, it requires manual configuration of the scaling adjustment (how many instances to add or remove) and doesn't automatically adjust to maintain a target value. It's less sophisticated and requires more manual intervention than target tracking.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-cloudwatch",
      "amazon-ec2",
      "application-load-balancer-(alb)",
      "auto-scaling-group",
      "auto-scaling-policies-(target-tracking,-simple-scaling,-step-scaling)",
      "best-practice-automate-scaling-actions-to-respond-to-changes-in-workload.",
      "best-practice-choose-the-appropriate-scaling-policy-based-on-the-application's-requirements.",
      "best-practice-monitor-application-performance-using-cloudwatch-metrics.",
      "best-practice-use-target-tracking-scaling-policies-for-maintaining-a-specific-metric-at-a-target-value.",
      "cpu-utilization"
    ]
  },
  {
    "id": 54,
    "text": "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead. Which solution best meets these requirements in the MOST cost-effective way?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point",
        "correct": true
      },
      {
        "id": 1,
        "text": "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon S3 bucket in the research partner’s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Secure Architectures",
    "tags": [
      "amazon-api-gateway",
      "amazon-efs",
      "amazon-s3",
      "aws-datasync",
      "aws-lambda",
      "best-practice-choose-the-most-efficient-data-access-method-for-the-workload",
      "best-practice-follow-the-principle-of-least-privilege-when-granting-permissions",
      "best-practice-leverage-managed-services-for-scalability-and-cost-efficiency",
      "best-practice-minimize-data-duplication",
      "best-practice-use-resource-based-policies-for-cross-account-access",
      "efs-access-points",
      "efs-resource-policies",
      "iam",
      "shared-vpc",
      "vpc-peering"
    ]
  },
  {
    "id": 55,
    "text": "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed. What would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers",
        "correct": true
      },
      {
        "id": 1,
        "text": "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-cloudfront",
      "amazon-route-53",
      "amazon-s3",
      "best-practice-cache-static-content-to-reduce-latency.",
      "best-practice-choose-the-appropriate-aws-service-based-on-the-specific-requirements-of-the-application.",
      "best-practice-use-a-cdn-to-improve-website-performance-for-users-in-different-geographic-locations.",
      "content-delivery-network-(cdn)",
      "custom-origin",
      "geo-proximity-routing",
      "s3-cross-region-replication-(s3-crr)"
    ]
  },
  {
    "id": 56,
    "text": "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service. Which of the following AWS services represents the best solution for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "AWS Global Accelerator",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Elastic Load Balancing (ELB)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Route 53",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon CloudFront",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-cloudfront",
      "amazon-route-53",
      "aws-elastic-load-balancing-(elb)",
      "aws-global-accelerator",
      "best-practice-choose-the-appropriate-load-balancer-type-based-on-the-application's-protocol-(udp,-tcp,-http).",
      "best-practice-design-for-regional-failover-to-ensure-business-continuity-in-case-of-aws-region-outages.",
      "best-practice-leverage-health-checks-to-automatically-redirect-traffic-to-healthy-endpoints.",
      "best-practice-use-global-accelerator-for-global-applications-requiring-high-availability-and-low-latency.",
      "best-practice-use-static-ip-addresses-for-simplified-dns-configuration-and-improved-reliability.",
      "domain-name-system-(dns)",
      "global-traffic-management",
      "health-checks",
      "regional-failover",
      "static-ip-addresses",
      "user-datagram-protocol-(udp)"
    ]
  },
  {
    "id": 57,
    "text": "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution. Which of the following will you recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 3,
        "text": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SNS is a publish/subscribe messaging service that does not guarantee message order. While Lambda can process the updates, SNS is not the right choice for ordered processing. Also, running a SQL database on an EC2 instance increases management overhead compared to a managed database service like DynamoDB or RDS.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-dynamodb",
      "amazon-ec2",
      "amazon-kinesis-data-streams",
      "amazon-rds",
      "amazon-sns",
      "amazon-sqs",
      "auto-scaling",
      "aws-lambda",
      "best-practice-choose-the-appropriate-data-store-based-on-requirements-(dynamodb-for-high-scalability-and-availability).",
      "best-practice-design-for-scalability-to-handle-traffic-spikes.",
      "best-practice-use-managed-services-to-minimize-operational-overhead.",
      "best-practice-use-serverless-compute-(lambda)-for-event-driven-processing.",
      "best-practice-use-streaming-services-(kinesis)-for-ordered-data-processing."
    ]
  },
  {
    "id": 58,
    "text": "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoft’s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS. Which of the following AWS services can facilitate the migration of these workloads?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 1,
        "text": "Microsoft SQL Server on AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon FSx for Lustre",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-fsx-for-lustre",
      "amazon-fsx-for-windows-file-server",
      "aws-directory-service-for-microsoft-active-directory-(aws-managed-microsoft-ad)",
      "best-practice-choose-the-right-storage-service-based-on-the-application's-requirements.",
      "best-practice-consider-compatibility-with-existing-on-premises-infrastructure-when-migrating-to-the-cloud.",
      "best-practice-leverage-managed-services-to-reduce-operational-overhead.",
      "best-practice-use-a-hybrid-cloud-approach-to-gradually-migrate-workloads-to-the-cloud.",
      "best-practice-utilize-services-that-natively-support-existing-on-premises-technologies-to-simplify-migration-and-integration.",
      "dfs-namespaces",
      "dfs-replication",
      "hybrid-cloud",
      "microsoft-dfs",
      "microsoft-sql-server-on-aws",
      "smb-protocol"
    ]
  },
  {
    "id": 59,
    "text": "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers. Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the company’s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume",
        "correct": false
      },
      {
        "id": 2,
        "text": "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required",
        "correct": true
      },
      {
        "id": 3,
        "text": "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-api-gateway",
      "amazon-ecs",
      "amazon-eks",
      "amazon-eventbridge",
      "amazon-lightsail",
      "amazon-sqs",
      "aws-fargate",
      "aws-lambda",
      "best-practice-choose-the-appropriate-compute-service-based-on-workload-characteristics-(e.g.,-lambda-for-short-lived-tasks,-ecs-fargate-for-long-running-applications).",
      "best-practice-design-for-scalability-and-elasticity-to-handle-varying-workloads.",
      "best-practice-employ-serverless-technologies-like-lambda-for-lightweight-processing-tasks.",
      "best-practice-leverage-api-gateway-for-secure-and-scalable-api-endpoints.",
      "best-practice-use-fully-managed-services-to-minimize-operational-overhead.",
      "best-practice-utilize-containerization-and-orchestration-(ecs/eks)-for-scalable-and-resilient-applications.",
      "kubernetes"
    ]
  },
  {
    "id": 60,
    "text": "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. As a solutions architect, which of the following solutions would you recommend to the company?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design Secure Architectures",
    "tags": [
      "aws-direct-connect",
      "aws-site-to-site-vpn",
      "aws-transit-gateway",
      "best-practice-choose-the-appropriate-connectivity-solution-based-on-requirements-(latency,-throughput,-security)",
      "best-practice-consider-operational-overhead-when-selecting-a-solution",
      "best-practice-encrypt-data-in-transit",
      "best-practice-use-dedicated-connections-for-low-latency-and-high-throughput",
      "encryption",
      "network-connectivity",
      "virtual-private-network-(vpn)"
    ]
  },
  {
    "id": 61,
    "text": "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B. At this point in time, what entities exist in Region B?",
    "options": [
      {
        "id": 0,
        "text": "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B",
        "correct": true
      },
      {
        "id": 1,
        "text": "1 Amazon EC2 instance and 2 AMIs exist in Region B",
        "correct": false
      },
      {
        "id": 2,
        "text": "1 Amazon EC2 instance and 1 AMI exist in Region B",
        "correct": false
      },
      {
        "id": 3,
        "text": "1 Amazon EC2 instance and 1 snapshot exist in Region B",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Resilient Architectures",
    "tags": [
      "amazon-ebs-snapshot",
      "amazon-ec2",
      "amazon-machine-image-(ami)",
      "ami-copying",
      "aws-regions",
      "best-practice-copy-amis-to-other-regions-for-disaster-recovery-and-to-launch-instances-closer-to-users.",
      "best-practice-understand-the-relationship-between-amis-and-snapshots.",
      "best-practice-use-amis-to-create-consistent-and-reproducible-environments."
    ]
  },
  {
    "id": 62,
    "text": "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload. As a solutions architect, which of the following steps would you recommend to implement the solution?",
    "options": [
      {
        "id": 0,
        "text": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures",
    "tags": [
      "amazon-ec2",
      "auto-scaling-group-(asg)",
      "best-practice-choose-the-appropriate-scaling-policy-based-on-the-workload-characteristics-(scheduled-vs.-reactive).",
      "best-practice-set-the-desired-capacity-of-an-auto-scaling-group-to-manage-the-number-of-instances.",
      "best-practice-use-scheduled-actions-for-predictable-scaling-events.",
      "desired-capacity",
      "maximum-capacity",
      "minimum-capacity",
      "scheduled-actions",
      "simple-scaling-policy",
      "target-tracking-scaling-policy"
    ]
  },
  {
    "id": 63,
    "text": "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 3 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.\n\n**Why option 4 is incorrect:**\nOption 4: Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3. While VPN connections provide secure communication, they often introduce overhead and latency, potentially slowing down upload speeds compared to using the public internet with S3 Transfer Acceleration. Also, setting up and managing multiple VPN connections across different regions can be complex and costly. It's not the most cost-effective solution for improving S3 upload speeds.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-s3",
      "amazon-s3-transfer-acceleration",
      "aws-direct-connect",
      "aws-global-accelerator",
      "aws-site-to-site-vpn",
      "best-practice-choose-the-most-cost-effective-solution-based-on-the-specific-requirements.",
      "best-practice-consider-network-latency-when-designing-applications-that-involve-data-transfer-across-regions.",
      "best-practice-use-multipart-uploads-for-large-files-and-improved-resilience.",
      "best-practice-use-s3-transfer-acceleration-for-faster-uploads-from-geographically-dispersed-locations.",
      "cloudfront-edge-locations",
      "multipart-upload"
    ]
  },
  {
    "id": 64,
    "text": "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege. Which statement should a solutions architect add to the policy to address this issue?",
    "options": [
      {
        "id": 0,
        "text": "{ \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "correct": false
      },
      {
        "id": 1,
        "text": "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "correct": true
      },
      {
        "id": 2,
        "text": "{ \"Action\": [ \"s3:*Object\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket/*\" ], \"Effect\": \"Allow\" }",
        "correct": false
      },
      {
        "id": 3,
        "text": "{ \"Action\": [ \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::example-bucket*\" ], \"Effect\": \"Allow\" }",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.",
    "domain": "Design Secure Architectures",
    "tags": [
      "best-practice-define-resource-arns-precisely-to-limit-the-scope-of-permissions.",
      "best-practice-principle-of-least-privilege:-grant-only-the-permissions-required-to-perform-a-task.",
      "best-practice-regularly-review-and-refine-iam-policies-to-ensure-they-remain-aligned-with-security-best-practices.",
      "best-practice-use-specific-iam-actions-instead-of-wildcards-whenever-possible.",
      "iam-(identity-and-access-management)",
      "iam-actions",
      "iam-arns-(amazon-resource-names)",
      "iam-policies",
      "iam-resources",
      "s3-(simple-storage-service)",
      "s3-bucket-permissions"
    ]
  },
  {
    "id": 65,
    "text": "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second. Which of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
    "options": [
      {
        "id": 0,
        "text": "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations",
        "correct": true
      },
      {
        "id": 2,
        "text": "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the correct answer. S3 automatically partitions data to scale and handle high request rates. However, if object keys are sequentially named (e.g., using timestamps or sequential IDs), all requests might be directed to a single partition, leading to throttling. Using customer-specific prefixes distributes the load across multiple partitions within the same bucket, effectively increasing the aggregate request rate the service can handle. This approach is resource-efficient because it avoids creating a large number of buckets, which can add management overhead and potentially increase costs. It's also cost-optimal because it leverages S3's built-in scalability features without requiring significant changes to the underlying storage infrastructure.\n\n**Why option 0 is incorrect:**\nis incorrect because creating a new S3 bucket for each customer is not resource-efficient or cost-optimal. While it would solve the request rate issue, managing a large number of buckets can become complex and expensive. S3 has limits on the number of buckets per account, and managing permissions, lifecycle policies, and other configurations across thousands of buckets would be a significant operational burden. It also introduces unnecessary overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon EFS is not designed for this use case. EFS is a network file system suitable for applications that require shared file storage across multiple EC2 instances. It's generally more expensive than S3 for storing large amounts of data and is not optimized for high-volume object storage and retrieval. EFS also has different performance characteristics and is not a direct replacement for S3 in this scenario. It doesn't address the core problem of high request rates to a single storage location.\n\n**Why option 3 is incorrect:**\nis incorrect because creating a new S3 bucket for each day's data might improve scalability to some extent, but it's not as efficient or cost-optimal as using prefixes. It still requires managing multiple buckets, albeit fewer than creating a bucket per customer. Also, it might not fully address the issue if a single day's data still generates a high request rate. The prefix approach is more granular and adaptable to varying customer activity levels.",
    "domain": "Design Cost-Optimized Architectures",
    "tags": [
      "amazon-efs",
      "amazon-s3",
      "best-practice-avoid-sequential-naming-of-s3-object-keys.",
      "best-practice-choose-the-appropriate-storage-service-based-on-the-application's-requirements-(s3-for-object-storage,-efs-for-shared-file-systems).",
      "best-practice-optimize-for-cost-and-performance-when-designing-storage-solutions.",
      "best-practice-use-prefixes-in-s3-object-keys-to-distribute-the-load-across-multiple-partitions.",
      "cost-optimization",
      "s3-bucket",
      "s3-object-key-naming",
      "s3-partitioning",
      "s3-request-rate",
      "scalability"
    ]
  }
]