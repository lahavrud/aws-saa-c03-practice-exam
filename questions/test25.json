[
  {
    "id": 0,
    "text": "A company regularly uploads confidential data to Amazon S3 buckets for analysis. \n \nThe company's security policies mandate that the objects must be encrypted at rest. The \ncompany must automatically rotate the encryption key every year. The company must be able to \ntrack key rotation by using AWS CloudTrail. The company also must minimize costs for the \nencryption key. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use server-side encryption with AWS KMS keys (SSE-KMS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use server-side encryption with customer managed AWS KMS keys",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A company has migrated several applications to AWS in the past 3 months. The company wants \nto know the breakdown of costs for each of these applications. The company wants to receive a \nregular report that includes this information. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Budgets to download data for the past 3 months into a .csv file. Look up the desired",
        "correct": false
      },
      {
        "id": 1,
        "text": "Load AWS Cost and Usage Reports into an Amazon RDS DB instance. Run SQL queries to get",
        "correct": false
      },
      {
        "id": 2,
        "text": "Tag all the AWS resources with a key for cost and a value of the application's name. Activate cost",
        "correct": true
      },
      {
        "id": 3,
        "text": "Tag all the AWS resources with a key for cost and a value of the application's name. Use the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 2,
    "text": "An ecommerce company is preparing to deploy a web application on AWS to ensure continuous \nservice for customers. The architecture includes a web application that the company hosts on \nAmazon EC2 instances, a relational database in Amazon RDS, and static assets that the \ncompany stores in Amazon S3. \n \nThe company wants to design a robust and resilient architecture for the application. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon EC2 instances in a single Availability Zone. Deploy an RDS DB instance in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy Amazon EC2 instances in a single Availability Zone. Deploy an RDS DB instance in a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Lambda functions to serve the web application. Use Amazon Aurora Serverless v2 for",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "An ecommerce company runs several internal applications in multiple AWS accounts. The \ncompany uses AWS Organizations to manage its AWS accounts. \n \nA security appliance in the company's networking account must inspect interactions between \napplications across AWS accounts. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a Network Load Balancer (NLB) in the networking account to send traffic to the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Application Load Balancer (ALB) in the application accounts to send traffic directly to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a Gateway Load Balancer (GWLB) in the networking account to send traffic to the security",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy an interface VPC endpoint in the application accounts to send traffic directly to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company runs its production workload on an Amazon Aurora MySQL DB cluster that includes \nsix Aurora Replicas. The company wants near-real-time reporting queries from one of its \ndepartments to be automatically distributed across three of the Aurora Replicas. Those three \nreplicas have a different compute and memory specification from the rest of the DB cluster. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create and use a custom endpoint for the workload",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a three-node cluster clone and use the reader endpoint",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use any of the instance endpoints for the selected three nodes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the reader endpoint to automatically distribute the read-only workload",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "A company runs a Node js function on a server in its on-premises data center. The data center \nstores data in a PostgreSQL database. The company stores the credentials in a connection string \nin an environment variable on the server. The company wants to migrate its application to AWS \nand to replace the Node.js application server with AWS Lambda. The company also wants to \nmigrate to Amazon RDS for PostgreSQL and to ensure that the database credentials are securely \nmanaged. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the database credentials as a parameter in AWS Systems Manager Parameter Store",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the database credentials as a secret in AWS Secrets Manager. Configure Secrets Manager",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the database credentials as an encrypted Lambda environment variable. Write a custom",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the database credentials as a key in AWS Key Management Service (AWS KMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A company wants to replicate existing and ongoing data changes from an on-premises Oracle \ndatabase to Amazon RDS for Oracle. The amount of data to replicate varies throughout each \nday. The company wants to use AWS Database Migration Service (AWS DMS) for data \nreplication. The solution must allocate only the capacity that the replication instance requires. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the AWS DMS replication instance with a Multi-AZ deployment to provision instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS DMS Serverless replication task to analyze and replicate the data while",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 Auto Scaling to scale the size of the AWS DMS replication instance up or down",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision AWS DMS replication capacity by using Amazon Elastic Container Service (Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A company has a multi-tier web application. The application's internal service components are \ndeployed on Amazon EC2 instances. The internal service components need to access third-party \nsoftware as a service (SaaS) APIs that are hosted on AWS. \n \nThe company needs to provide secure and private connectivity from the application's internal \nservices to the third-party SaaS application. The company needs to ensure that there is minimal \npublic internet exposure. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement an AWS Site-to-Site VPN to establish a secure connection with the third-party SaaS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Transit Gateway to manage and route traffic between the application's VPC and the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS PrivateLink to allow only outbound traffic from the VPC without enabling the third-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS PrivateLink to create a private connection between the application's VPC and the third-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A solutions architect needs to connect a company's corporate network to its VPC to allow on-\npremises access to its AWS resources. The solution must provide encryption of all traffic between \nthe corporate network and the VPC at the network layer and the session layer. The solution also \nmust provide security controls to prevent unrestricted access between AWS and the on-premises \nsystems. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Direct Connect to connect to the VPC. Configure the VPC route tables to allow",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM policy to allow access to the AWS Management Console only from a defined set of",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Site-to-Site VPN to connect to the VPConfigure route table entries to direct traffic",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure AWS Transit Gateway to connect to the VPC. Configure route table entries to direct",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A company has a custom application with embedded credentials that retrieves information from a \ndatabase in an Amazon RDS for MySQL DB cluster. The company needs to make the application \nmore secure with minimal programming effort. The company has created credentials on the RDS \nfor MySQL database for the application user. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the credentials in AWS Key Management Service (AWS KMS). Create keys in AWS KMS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the credentials in encrypted local storage. Configure the application to load the database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the credentials in AWS Secrets Manager. Configure the application to load the database",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the credentials in AWS Systems Manager Parameter Store. Configure the application to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company wants to move its application to a serverless solution. The serverless solution needs \nto analyze existing data and new data by using SQL. The company stores the data in an Amazon \nS3 bucket. The data must be encrypted at rest and replicated to a different AWS Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a new S3 bucket that uses server-side encryption with AWS KMS multi-Region keys",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new S3 bucket that uses server-side encryption with Amazon S3 managed keys (SSE-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side encryption",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "A company has a web application that has thousands of users. The application uses 8-10 user-\nuploaded images to generate AI images. Users can download the generated AI images once \nevery 6 hours. The company also has a premium user option that gives users the ability to \ndownload the generated AI images anytime. \n \nThe company uses the user-uploaded images to run AI model training twice a year. The company \nneeds a storage solution to store the images. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Move uploaded images to Amazon S3 Glacier Deep Archive. Move premium user-generated AI",
        "correct": true
      },
      {
        "id": 1,
        "text": "Move uploaded images to Amazon S3 Glacier Deep Archive Move all generated AI images to S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move all",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 12,
    "text": "A company is developing machine learning (ML) models on AWS. The company is developing the \nML models as independent microservices. The microservices fetch approximately 1 GB of model \ndata from Amazon S3 at startup and load the data into memory. Users access the ML models \nthrough an asynchronous API. Users can send a request or a batch of requests. \n \nThe company provides the ML models to hundreds of users. The usage patterns for the models \nare irregular. Some models are not used for days or weeks. Other models receive batches of \nthousands of requests at a time. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Direct the requests from the API to a Network Load Balancer (NLB). Deploy the ML models as",
        "correct": false
      },
      {
        "id": 1,
        "text": "Direct the requests from the API to an Application Load Balancer (ALB). Deploy the ML models",
        "correct": false
      },
      {
        "id": 2,
        "text": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an \nApplication Load Balancer (ALB). The application stores data in an Amazon Aurora MySQL DB \ncluster. \n \nThe company needs to create a disaster recovery (DR) solution. The acceptable recovery time for \nthe DR solution is up to 30 minutes. The DR solution does not need to support customer usage \nwhen the primary infrastructure is healthy. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the DR infrastructure in a second AWS Region with an ALB and an Auto Scaling group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the DR infrastructure in a second AWS Region with an ALUpdate the Auto Scaling group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Back up the Aurora MySQL DB cluster data by using AWS Backup. Deploy the DR infrastructure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Back up the infrastructure configuration by using AWS Backup. Use the backup to create the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company is migrating its data processing application to the AWS Cloud. The application \nprocesses several short-lived batch jobs that cannot be disrupted. Data is generated after each \nbatch job is completed. The data is accessed for 30 days and retained for 2 years. \n \nThe company wants to keep the cost of running the application in the AWS Cloud as low as \npossible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the data processing application to Amazon EC2 Spot Instances. Store the data in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the data processing application to Amazon EC2 On-Demand Instances. Store the data in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon EC2 Spot Instances to run the batch jobs. Store the data in Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy Amazon EC2 On-Demand Instances to run the batch jobs. Store the data in Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company needs to design a hybrid network architecture. The company's workloads are \ncurrently stored in the AWS Cloud and in on-premises data centers. The workloads require \nsingle-digit latencies to communicate. The company uses an AWS Transit Gateway transit \ngateway to connect multiple VPCs. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Establish an AWS Site-to-Site VPN connection to each VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Associate an AWS Direct Connect gateway with the transit gateway that is attached to the VPCs.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Establish an AWS Site-to-Site VPN connection to an AWS Direct Connect gateway.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Establish an AWS Direct Connect connection. Create a transit virtual interface (VIF) to a Direct",
        "correct": false
      },
      {
        "id": 4,
        "text": "Associate AWS Site-to-Site VPN connections with the transit gateway that is attached to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 16,
    "text": "A global ecommerce company runs its critical workloads on AWS. The workloads use an Amazon \nRDS for PostgreSQL DB instance that is configured for a Multi-AZ deployment. \n \nCustomers have reported application timeouts when the company undergoes database failovers. \nThe company needs a resilient solution to reduce failover time. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon RDS Proxy. Assign the proxy to the DB instance.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a read replica for the DB instance. Move the read traffic to the read replica.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Performance Insights. Monitor the CPU load to identify the timeouts.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Take regular automatic snapshots. Copy the automatic snapshots to multiple AWS Regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A company has multiple Amazon RDS DB instances that run in a development AWS account. All \nthe instances have tags to identify them as development resources. The company needs the \ndevelopment DB instances to run on a schedule only during business hours. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch alarm to identify RDS instances that need to be stopped. Create",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Trusted Advisor report to identify RDS instances to be started and stopped.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create AWS Systems Manager State Manager associations to start and stop the RDS instances.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge rule that invokes AWS Lambda functions to start and stop the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 18,
    "text": "A consumer survey company has gathered data for several years from a specific geographic \nregion. The company stores this data in an Amazon S3 bucket in an AWS Region. \n \nThe company has started to share this data with a marketing firm in a new geographic region. \nThe company has granted the firm's AWS account access to the S3 bucket. The company wants \nto minimize the data transfer costs when the marketing firm requests data from the S3 bucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Requester Pays feature on the company's S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure S3 Cross-Region Replication (CRR) from the company's S3 bucket to one of the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Resource Access Manager to share the S3 bucket with the marketing firm AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the company's S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company uses AWS to host its public ecommerce website. The website uses an AWS Global \nAccelerator accelerator for traffic from the internet. The Global Accelerator accelerator forwards \nthe traffic to an Application Load Balancer (ALB) that is the entry point for an Auto Scaling group. \n \nThe company recently identified a DDoS attack on the website. The company needs a solution to \nmitigate future attacks. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS WAF web ACL for the Global Accelerator accelerator to block traffic by using",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an AWS Lambda function to read the ALB metrics to block attacks by updating a VPC",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS WAF web ACL on the ALB to block traffic by using rate-based rules",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon CloudFront distribution in front of the Global Accelerator accelerator",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 20,
    "text": "A company uses an Amazon DynamoDB table to store data that the company receives from \ndevices. The DynamoDB table supports a customer-facing website to display recent activity on \ncustomer devices. The company configured the table with provisioned throughput for writes and \nreads. \n \nThe company wants to calculate performance metrics for customer device data on a daily basis. \nThe solution must have minimal effect on the table's provisioned read and write capacity. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon Athena SQL query with the Amazon Athena DynamoDB connector to calculate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Glue job with the AWS Glue DynamoDB export connector to calculate performance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Amazon Redshift COPY command to calculate performance metrics on a recurring",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon EMR job with an Apache Hive external table to calculate performance metrics on",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect is designing the cloud architecture for a new stateless application that will be \ndeployed on AWS. The solutions architect created an Amazon Machine Image (AMI) and launch \ntemplate for the application. \n \nBased on the number of jobs that need to be processed, the processing must run in parallel while \nadding and removing application Amazon EC2 instances as needed. The application must be \nloosely coupled. The job items must be durably stored. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to send the jobs that need to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue to hold the jobs that need to be",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue to hold the jobs that need to be",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to send the jobs that need to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A global ecommerce company uses a monolithic architecture. The company needs a solution to \nmanage the increasing volume of product data. The solution must be scalable and have a \nmodular service architecture. The company needs to maintain its structured database schemas. \nThe company also needs a storage solution to store product data and product images. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon EC2 instance in an Auto Scaling group to deploy a containerized application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Lambda functions to manage the existing monolithic application. Use Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with an Amazon EC2 deployment to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to deploy a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A company is migrating an application from an on-premises environment to AWS. The application \nwill store sensitive data in Amazon S3. The company must encrypt the data before storing the \ndata in Amazon S3. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Encrypt the data by using client-side encryption with customer managed keys.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Encrypt the data by using server-side encryption with AWS KMS keys (SSE-KMS).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Encrypt the data by using server-side encryption with customer-provided keys (SSE-C).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the data by using client-side encryption with Amazon S3 managed keys.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "A company wants to create an Amazon EMR cluster that multiple teams will use. The company \nwants to ensure that each team's big data workloads can access only the AWS services that \neach team needs to interact with. The company does not want the workloads to have access to \nInstance Metadata Service Version 2 (IMDSv2) on the cluster's underlying EC2 instances. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure interface VPC endpoints for each AWS service that the teams need. Use the required",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create EMR runtime roles. Configure the cluster to use the runtime roles. Use the runtime roles to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an EC2 IAM instance profile that has the required permissions for each team. Use the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an EMR security configuration that has the EnableApplicationScopedIAMRole option set",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A solutions architect is designing an application that helps users fill out and submit registration \nforms. The solutions architect plans to use a two-tier architecture that includes a web application \nserver tier and a worker tier. \n \nThe application needs to process submitted forms quickly. The application needs to process each \nform exactly once. The solution must ensure that no data is lost. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) FIFO queue between the web application",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an Amazon API Gateway HTTP API between the web application server tier and the worker",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) standard queue between the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Step Functions workflow. Create a synchronous workflow between the web",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A finance company uses an on-premises search application to collect streaming data from \nvarious producers. The application provides real-time updates to search and visualization \nfeatures. \n \nThe company is planning to migrate to AWS and wants to use an AWS native solution. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 instances to ingest and process the data streams to Amazon S3 buckets tor",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EMR to ingest and process the data streams to Amazon Redshift for storage. Use",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) to ingest and process the data streams",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Kinesis Data Streams to ingest and process the data streams to Amazon",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 27,
    "text": "A company currently runs an on-premises application that usesASP.NET on Linux machines. The \napplication is resource-intensive and serves customers directly. \n \nThe company wants to modernize the application to .NET. The company wants to run the \napplication on containers and to scale based on Amazon CloudWatch metrics. The company also \nwants to reduce the time spent on operational maintenance activities. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS App2Container to containerize the application. Use an AWS CloudFormation template",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS App2Container to containerize the application. Use an AWS CloudFormation template",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS App Runner to containerize the application. Use App Runner to deploy the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS App Runner to containerize the application. Use App Runner to deploy the application",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A company is designing a new internal web application in the AWS Cloud. The new application \nmust securely retrieve and store multiple employee usernames and passwords from an AWS \nmanaged service. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the employee credentials in AWS Systems Manager Parameter Store. Use AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the employee credentials in AWS Secrets Manager. Use AWS CloudFormation and AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the employee credentials in AWS Systems Manager Parameter Store. Use AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the employee credentials in AWS Secrets Manager. Use AWS CloudFormation and the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "A company that is in the ap-northeast-1 Region has a fleet of thousands of AWS Outposts \nservers. The company has deployed the servers at remote locations around the world. All the \nservers regularly download new software versions that consist of 100 files. There is significant \nlatency before all servers run the new software versions. \n \nThe company must reduce the deployment latency for new software versions. \n \nWhich solution will meet this requirement with the LEAST operational overhead? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n491",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket in ap-northeast-1. Set up an Amazon CloudFront distribution in ap-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket in ap-northeast-1. Create a second S3 bucket in the us-east-1",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket in ap-northeast-1. Configure Amazon S3 Transfer Acceleration.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket in ap-northeast-1. Set up an Amazon CloudFront distribution.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company currently runs an on-premises stock trading application by using Microsoft Windows \nServer. The company wants to migrate the application to the AWS Cloud. \n \nThe company needs to design a highly available solution that provides low-latency access to \nblock storage across multiple Availability Zones. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Windows Server cluster that spans two Availability Zones on Amazon EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a Windows Server cluster that spans two Availability Zones on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application on Amazon EC2 instances in two Availability Zones. Configure one EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application on Amazon EC2 instances in two Availability Zones. Configure one EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company is designing a web application with an internet-facing Application Load Balancer \n(ALB). \n \nThe company needs the ALB to receive HTTPS web traffic from the public internet. The ALB \nmust send only HTTPS traffic to the web application servers hosted on the Amazon EC2 \ninstances on port 443. The ALB must perform a health check of the web application servers over \nHTTPS on port 8443. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n492 \nWhich combination of configurations of the security group that is associated with the ALB will \nmeet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Allow HTTPS inbound traffic from 0.0.0.0/0 for port 443.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Allow all outbound traffic to 0.0.0.0/0 for port 443.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow HTTPS outbound traffic to the web application instances for port 443.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Allow HTTPS inbound traffic from the web application instances for port 443.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Allow HTTPS outbound traffic to the web application instances for the health check on port 8443.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company hosts an application on AWS. The application gives users the ability to upload photos \nand store the photos in an Amazon S3 bucket. The company wants to use Amazon CloudFront \nand a custom domain name to upload the photo files to the S3 bucket in the eu-west-1 Region. \n \nWhich solution will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Certificate Manager (ACM) to create a public certificate in the us-east-1 Region. Use",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Certificate Manager (ACM) to create a public certificate in eu-west-1. Use the certificate",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon S3 to allow uploads from CloudFront. Configure S3 Transfer Acceleration.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon S3 to allow uploads from CloudFront origin access control (OAC).",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure Amazon S3 to allow uploads from CloudFront. Configure an Amazon S3 website",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "A weather forecasting company collects temperature readings from various sensors on a \ncontinuous basis. An existing data ingestion process collects the readings and aggregates the \nreadings into larger Apache Parquet files. Then the process encrypts the files by using client-side \nencryption with KMS managed keys (CSE-KMS). Finally, the process writes the files to an \nAmazon S3 bucket with separate prefixes for each calendar day. \n \nThe company wants to run occasional SQL queries on the data to take sample moving averages \nfor a specific calendar day. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Athena to read the encrypted files. Run SQL queries on the data directly in",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon S3 Select to run SQL queries on the data directly in Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Redshift to read the encrypted files. Use Redshift Spectrum and Redshift",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon EMR Serverless to read the encrypted files. Use Apache SparkSQL to run",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 34,
    "text": "A company is implementing a new application on AWS. The company will run the application on \nmultiple Amazon EC2 instances across multiple Availability Zones within multiple AWS Regions. \nThe application will be available through the internet. Users will access the application from \naround the world. \n \nThe company wants to ensure that each user who accesses the application is sent to the EC2 \ninstances that are closest to the user's location. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement an Amazon Route 53 geolocation routing policy. Use an internet-facing Application",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement an Amazon Route 53 geoproximity routing policy. Use an internet-facing Network Load",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement an Amazon Route 53 multivalue answer routing policy. Use an internet-facing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an Amazon Route 53 weighted routing policy. Use an internet-facing Network Load",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A financial services company plans to launch a new application on AWS to handle sensitive \nfinancial transactions. The company will deploy the application on Amazon EC2 instances. The \ncompany will use Amazon RDS for MySQL as the database. The company's security policies \nmandate that data must be encrypted at rest and in transit. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement third-party application-level data encryption before storing data in Amazon RDS for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company is migrating its on-premises Oracle database to an Amazon RDS for Oracle \ndatabase. The company needs to retain data for 90 days to meet regulatory requirements. The \ncompany must also be able to restore the database to a specific point in time for up to 14 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon RDS automated backups. Set the retention period to 90 days.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon RDS manual snapshot every day. Delete manual snapshots that are older",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the Amazon Aurora Clone feature for Oracle to create a point-in-time restore. Delete clones",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a backup plan that has a retention period of 90 days by using AWS Backup for Amazon",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A company is developing a new application that uses a relational database to store user data and \napplication configurations. The company expects the application to have steady user growth. The \ncompany expects the database usage to be variable and read-heavy, with occasional writes. \n \nThe company wants to cost-optimize the database solution. The company wants to use an AWS \nmanaged database solution that will provide the necessary performance. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the database on Amazon RDS. Use Provisioned IOPS SSD storage to ensure consistent",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the database on Amazon Aurora Serverless to automatically scale the database capacity",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the database on Amazon DynamoDB. Use on-demand capacity mode to automatically",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the database on Amazon RDS. Use magnetic storage and use read replicas to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 38,
    "text": "A company hosts its application on several Amazon EC2 instances inside a VPC. The company \ncreates a dedicated Amazon S3 bucket for each customer to store their relevant information in \nAmazon S3. \n \nThe company wants to ensure that the application running on EC2 instances can securely access \nonly the S3 buckets that belong to the company's AWS account. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway endpoint for Amazon S3 that is attached to the VPC. Update the IAM instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a NAT gateway in a public subnet with a security group that allows access to only Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a gateway endpoint for Amazon S3 that is attached to the VPUpdate the IAM instance",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a NAT Gateway in a public subnet. Update route tables to use the NAT Gateway. Assign",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is building a cloud-based application on AWS that will handle sensitive customer \ndata. The application uses Amazon RDS for the database, Amazon S3 for object storage, and S3 \nEvent Notifications that invoke AWS Lambda for serverless processing. \n \nThe company uses AWS IAM Identity Center to manage user credentials. The development, \ntesting, and operations teams need secure access to Amazon RDS and Amazon S3 while \nensuring the confidentiality of sensitive customer data. The solution must comply with the \nprinciple of least privilege. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use IAM roles with least privilege to grant all the teams access. Assign IAM roles to each team",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable IAM Identity Center with an Identity Center directory. Create and configure permission",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create individual IAM users for each member in all the teams with role-based permissions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Organizations to create separate accounts for each team. Implement cross-account",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A company has an Amazon S3 bucket that contains sensitive data files. The company has an \napplication that runs on virtual machines in an on-premises data center. The company currently \nuses AWS IAM Identity Center. \n \nThe application requires temporary access to files in the S3 bucket. The company wants to grant \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n496 \nthe application secure access to the files in the S3 bucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket policy that permits access to the bucket from the public IP address range of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use IAM Roles Anywhere to obtain security credentials in IAM Identity Center that grant access to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Install the AWS CLI on the virtual machine. Configure the AWS CLI with access keys from an IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user and policy that grants access to the bucket. Store the access key and secret",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company hosts its core network services, including directory services and DNS, in its on-\npremises data center. The data center is connected to the AWS Cloud using AWS Direct Connect \n(DX). Additional AWS accounts are planned that will require quick, cost-effective, and consistent \naccess to these network services. \n \nWhat should a solutions architect implement to meet these requirements with the LEAST amount \nof operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a DX connection in each new account. Route the network traffic to the on-premises",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure VPC endpoints in the DX VPC for all required services. Route the network traffic to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPN connection between each new account and the DX VPRoute the network traffic to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Transit Gateway between the accounts. Assign DX to the transit gateway and",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company hosts its main public web application in one AWS Region across multiple Availability \nZones. The application uses an Amazon EC2 Auto Scaling group and an Application Load \nBalancer (ALB). \n \nA web development team needs a cost-optimized compute solution to improve the company's \nability to serve dynamic content globally to millions of customers. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution. Configure the existing ALB as the origin.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 to serve traffic to the ALB and EC2 instances based on the geographic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket with public read access enabled. Migrate the web application to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Direct Connect to directly serve content from the web application to the location of each",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company is testing an application that runs on an Amazon EC2 Linux instance. A single 500 \nGB Amazon Elastic Block Store (Amazon EBS) General Purpose SSO (gp2) volume is attached \nto the EC2 instance. \n \nThe company will deploy the application on multiple EC2 instances in an Auto Scaling group. All \ninstances require access to the data that is stored in the EBS volume. The company needs a \nhighly available and resilient solution that does not introduce significant changes to the \napplication's code. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an EC2 instance that uses NFS server software. Attach a single 500 GB gp2 EBS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an Amazon FSx for Windows File Server file system. Configure the file system as an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an EC2 instance with two 250 GB Provisioned IOPS SSD EBS volumes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an Amazon Elastic File System (Amazon EFS) file system. Configure the file system to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company recently launched a new application for its customers. The application runs on \nmultiple Amazon EC2 instances across two Availability Zones. End users use TCP to \ncommunicate with the application. \n \nThe application must be highly available and must automatically scale as the number of users \nincreases. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n498",
    "options": [
      {
        "id": 0,
        "text": "Add a Network Load Balancer in front of the EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure an Auto Scaling group for the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Application Load Balancer in front of the EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Manually add more EC2 instances for the application.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Add a Gateway Load Balancer in front of the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "A company is designing the architecture for a new mobile app that uses the AWS Cloud. The \ncompany uses organizational units (OUs) in AWS Organizations to manage its accounts. The \ncompany wants to tag Amazon EC2 instances with data sensitivity by using values of sensitive \nand nonsensitive. IAM identities must not be able to delete a tag or create instances without a \ntag. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "In Organizations, create a new tag policy that specifies the data sensitivity tag key and the",
        "correct": true
      },
      {
        "id": 1,
        "text": "In Organizations, create a new service control policy (SCP) that specifies the data sensitivity tag",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a tag policy to deny running instances when a tag key is not specified. Create another tag",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a service control policy (SCP) to deny creating instances when a tag key is not specified.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an AWS Config rule to check if EC2 instances use the data sensitivity tag and the specified",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A company runs database workloads on AWS that are the backend for the company's customer \nportals. The company runs a Multi-AZ database cluster on Amazon RDS for PostgreSQL. \n \nThe company needs to implement a 30-day backup retention policy. The company currently has \nboth automated RDS backups and manual RDS backups. The company wants to maintain both \ntypes of existing RDS backups that are less than 30 days old. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the RDS backup retention policy to 30 days for automated backups by using AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Disable RDS automated backups. Delete automated backups and manual backups that are older",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the RDS backup retention policy to 30 days for automated backups. Manually delete",
        "correct": true
      },
      {
        "id": 3,
        "text": "Disable RDS automated backups. Delete automated backups and manual backups that are older",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 48,
    "text": "A company is planning to migrate a legacy application to AWS. The application currently uses \nNFS to communicate to an on-premises storage solution to store application data. The application \ncannot be modified to use any other communication protocols other than NFS for this purpose. \n \nWhich storage solution should a solutions architect recommend for use after the migration?",
    "options": [
      {
        "id": 0,
        "text": "AWS DataSync",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon EMR File System (Amazon EMRFS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 49,
    "text": "A company uses GPS trackers to document the migration patterns of thousands of sea turtles. \nThe trackers check every 5 minutes to see if a turtle has moved more than 100 yards (91.4 \nmeters). If a turtle has moved, its tracker sends the new coordinates to a web application running \non three Amazon EC2 instances that are in multiple Availability Zones in one AWS Region. \n \nRecently, the web application was overwhelmed while processing an unexpected volume of \ntracker data. Data was lost with no way to replay the events. A solutions architect must prevent \nthis problem from happening again and needs a solution with the least operational overhead. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket to store the data. Configure the application to scan for new data in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway endpoint to handle transmitted location coordinates. Use an AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming data.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon DynamoDB table to store transmitted location coordinates. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 50,
    "text": "A company's software development team needs an Amazon RDS Multi-AZ cluster. The RDS \ncluster will serve as a backend for a desktop client that is deployed on premises. The desktop \nclient requires direct connectivity to the RDS cluster. \n \nThe company must give the development team the ability to connect to the cluster by using the \nclient when the team is in the office. \n \nWhich solution provides the required connectivity MOST securely? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n500",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Use AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use AWS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use RDS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A streaming media company is rebuilding its infrastructure to accommodate increasing demand \nfor video content that users consume daily. \n \nThe company needs to process terabyte-sized videos to block some content in the videos. Video \nprocessing can take up to 20 minutes. \n \nThe company needs a solution that will scale with demand and remain cost-effective. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda functions to process videos. Store video metadata in Amazon DynamoDB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) and AWS Fargate to implement",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a containerized video processing application on Amazon Elastic Kubernetes Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 0 is incorrect:**\nis incorrect because simple scaling policies react to alarms based on thresholds. While you *could* use simple scaling, it requires manual configuration of the scaling adjustment (how many instances to add or remove) and doesn't automatically adjust to maintain a target value. It's less sophisticated and requires more manual intervention than target tracking.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 54,
    "text": "A company runs an on-premises application on a Kubernetes cluster. The company recently \nadded millions of new customers. The company's existing on-premises infrastructure is unable to \nhandle the large number of new customers. The company needs to migrate the on-premises \napplication to the AWS Cloud. \n \nThe company will migrate to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The \ncompany does not want to manage the underlying compute infrastructure for the new architecture \non AWS. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use a self-managed node to supply compute capacity. Deploy the application to the new EKS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use managed node groups to supply compute capacity. Deploy the application to the new EKS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Fargate to supply compute capacity. Create a Fargate profile. Use the Fargate profile to",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use managed node groups with Karpenter to supply compute capacity. Deploy the application to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company is launching a new application that requires a structured database to store user \nprofiles, application settings, and transactional data. The database must be scalable with \napplication traffic and must offer backups. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a self-managed database on Amazon EC2 instances by using open source software. Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS. Use on-demand capacity mode for the database with General Purpose SSD",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Aurora Serverless for the database. Use serverless capacity scaling. Configure",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy a self-managed NoSQL database on Amazon EC2 instances. Use Reserved Instances for",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "A company runs its legacy web application on AWS. The web application server runs on an \nAmazon EC2 instance in the public subnet of a VPC. The web application server collects images \nfrom customers and stores the image files in a locally attached Amazon Elastic Block Store \n(Amazon EBS) volume. The image files are uploaded every night to an Amazon S3 bucket for \nbackup. \n \nA solutions architect discovers that the image files are being uploaded to Amazon S3 through the \npublic endpoint. The solutions architect needs to ensure that traffic to Amazon S3 does not use \nthe public endpoint. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint for the S3 bucket that has the necessary permissions for the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Move the S3 bucket inside the VPC. Configure the subnet route table to access the S3 bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 access point for the Amazon EC2 instance inside the VPConfigure the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Direct Connect connection between the VPC that has the Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A company is creating a prototype of an ecommerce website on AWS. The website consists of an \nApplication Load Balancer, an Auto Scaling group of Amazon EC2 instances for web servers, and \nan Amazon RDS for MySQL DB instance that runs with the Single-AZ configuration. \n \nThe website is slow to respond during searches of the product catalog. The product catalog is a \ngroup of tables in the MySQL database that the company does not update frequently. A solutions \narchitect has determined that the CPU utilization on the DB instance is high when product catalog \nsearches occur. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n503 \nWhat should the solutions architect recommend to improve the performance of the website during \nsearches of the product catalog?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the product catalog to an Amazon Redshift database. Use the COPY command to load the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement an Amazon ElastiCache for Redis cluster to cache the product catalog. Use lazy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Add an additional scaling policy to the Auto Scaling group to launch additional EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on the Multi-AZ configuration for the DB instance. Configure the EC2 instances to throttle the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SNS is a publish/subscribe messaging service that does not guarantee message order. While Lambda can process the updates, SNS is not the right choice for ordered processing. Also, running a SQL database on an EC2 instance increases management overhead compared to a managed database service like DynamoDB or RDS.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A company currently stores 5 TB of data in on-premises block storage systems. The company's \ncurrent storage solution provides limited space for additional data. The company runs \napplications on premises that must be able to retrieve frequently accessed data with low latency. \nThe company requires a cloud-based storage solution. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 File Gateway. Integrate S3 File Gateway with the on-premises applications to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Storage Gateway Volume Gateway with cached volumes as iSCSI targets.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an AWS Storage Gateway Volume Gateway with stored volumes as iSCSI targets.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Storage Gateway Tape Gateway. Integrate Tape Gateway with the on-premises",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company operates a food delivery service. Because of recent growth, the company's order \nprocessing system is experiencing scaling problems during peak traffic hours. The current \narchitecture includes Amazon EC2 instances in an Auto Scaling group that collect orders from an \napplication. A second group of EC2 instances in an Auto Scaling group fulfills the orders. \n \nThe order collection process occurs quickly, but the order fulfillment process can take longer. \nData must not be lost because of a scaling event. \n \nA solutions architect must ensure that the order collection process and the order fulfillment \nprocess can both scale adequately during peak traffic hours. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "An online gaming company is transitioning user data storage to Amazon DynamoDB to support \nthe company's growing user base. The current architecture includes DynamoDB tables that \ncontain user profiles, achievements, and in-game transactions. \n \nThe company needs to design a robust, continuously available, and resilient DynamoDB \narchitecture to maintain a seamless gaming experience for users. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create DynamoDB tables in a single AWS Region. Use on-demand capacity mode. Use global",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use DynamoDB Accelerator (DAX) to cache frequently accessed data. Deploy tables in a single",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create DynamoDB tables in multiple AWS Regions. Use on-demand capacity mode. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use DynamoDB global tables for automatic multi-Region replication. Deploy tables in multiple",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 61,
    "text": "A company runs its media rendering application on premises. The company wants to reduce \nstorage costs and has moved all data to Amazon S3. The on-premises rendering application \nneeds low-latency access to storage. \n \nThe company needs to design a storage solution for the application. The storage solution must \nmaintain the desired application performance. \n \nWhich storage solution will meet these requirements in the MOST cost-effective way?",
    "options": [
      {
        "id": 0,
        "text": "Use Mountpoint for Amazon S3 to access the data in Amazon S3 for the on-premises application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon S3 File Gateway to provide storage for the on-premises application.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Copy the data from Amazon S3 to Amazon FSx for Windows File Server. Configure an Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an on-premises file server. Use the Amazon S3 API to connect to S3 storage. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A company hosts its enterprise resource planning (ERP) system in the us-east-1 Region. The \nsystem runs on Amazon EC2 instances. Customers use a public API that is hosted on the EC2 \ninstances to exchange information with the ERP system. International customers report slow API \nresponse times from their data centers. \n \nWhich solution will improve response times for the international customers MOST cost-\neffectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Direct Connect connection that has a public virtual interface (VIF) to provide",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon CloudFront distribution in front of the API. Configure the CachingOptimized",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Global Accelerator. Configure listeners for the necessary ports. Configure endpoint",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Site-to-Site VPN to establish dedicated VPN tunnels between Regions and customer",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 63,
    "text": "A company tracks customer satisfaction by using surveys that the company hosts on its website. \nThe surveys sometimes reach thousands of customers every hour. Survey results are currently \nsent in email messages to the company so company employees can manually review results and \nassess customer sentiment. \n \nThe company wants to automate the customer survey process. Survey results must be available \nfor the previous 12 months. \n \nWhich solution will meet these requirements in the MOST scalable way?",
    "options": [
      {
        "id": 0,
        "text": "Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Send the survey results data to an API that is running on an Amazon EC2 instance. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write the survey results data to an Amazon S3 bucket. Use S3 Event Notifications to invoke an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 64,
    "text": "A company uses AWS Systems Manager for routine management and patching of Amazon EC2 \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n506 \ninstances. The EC2 instances are in an IP address type target group behind an Application Load \nBalancer (ALB). \n \nNew security protocols require the company to remove EC2 instances from service during a \npatch. When the company attempts to follow the security protocol during the next patch, the \ncompany receives errors during the patching window. \n \nWhich combination of solutions will resolve the errors? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Change the target type of the target group from IP address type to instance type.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Continue to use the existing Systems Manager document without changes because it is already",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement the AWSEC2-PatchLoadBalanacerInstance Systems Manager Automation document to",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Systems Manager Maintenance Windows to automatically remove the instances from service",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure Systems Manager State Manager to remove the instances from service and manage the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  }
]