[
  {
    "id": 0,
    "text": "A company hosts a data lake on Amazon S3. The data lake ingests data in Apache Parquet \nformat from various data sources. The company uses multiple transformation steps to prepare the \ningested data. The steps include filtering of anomalies, normalizing of data to standard date and \ntime values, and generation of aggregates for analyses. \n \nThe company must store the transformed data in S3 buckets that data analysts access. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n423 \ncompany needs a prebuilt solution for data transformation that does not require code. The \nsolution must provide data lineage and data profiling. The company needs to share the data \ntransformation steps with employees throughout the company. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS Glue Studio visual canvas to transform the data. Share the transformation",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon EMR Serverless to transform the data. Share the transformation steps with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Glue DataBrew to transform the data. Share the transformation steps with",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create Amazon Athena tables for the data. Write Athena SQL queries to transform the data.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A solutions architect runs a web application on multiple Amazon EC2 instances that are in \nindividual target groups behind an Application Load Balancer (ALB). Users can reach the \napplication through a public website. \n \nThe solutions architect wants to allow engineers to use a development version of the website to \naccess one specific development EC2 instance to test new features for the application. The \nsolutions architect wants to use an Amazon Route 53 hosted zone to give the engineers access \nto the development instance. The solution must automatically route to the development instance \neven if the development instance is replaced. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an A Record for the development website that has the value set to the ALB. Create a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Recreate the development instance with a public IP address. Create an A Record for the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an A Record for the development website that has the value set to the ALB. Create a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place all the instances in the same target group. Create an A Record for the development",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company runs a container application on a Kubernetes cluster in the company's data center. \nThe application uses Advanced Message Queuing Protocol (AMQP) to communicate with a \nmessage queue. The data center cannot scale fast enough to meet the company's expanding \nbusiness needs. The company wants to migrate the workloads to AWS. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n424",
    "options": [
      {
        "id": 0,
        "text": "Migrate the container application to Amazon Elastic Container Service (Amazon ECS). Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the container application to Amazon Elastic Kubernetes Service (Amazon EKS). Use",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use highly available Amazon EC2 instances to run the application. Use Amazon MQ to retrieve",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Lambda functions to run the application. Use Amazon Simple Queue Service (Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "An online gaming company hosts its platform on Amazon EC2 instances behind Network Load \nBalancers (NLBs) across multiple AWS Regions. The NLBs can route requests to targets over the \ninternet. The company wants to improve the customer playing experience by reducing end-to-end \nload time for its global customer base. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Application Load Balancers (ALBs) in each Region to replace the existing NLBs. Register",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon Route 53 to route equally weighted traffic to the NLBs in each Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create additional NLBs and EC2 instances in other Regions where the company has large",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a standard accelerator in AWS Global Accelerator. Configure the existing NLBs as target",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 4,
    "text": "A company has an on-premises application that uses SFTP to collect financial data from multiple \nvendors. The company is migrating to the AWS Cloud. The company has created an application \nthat uses Amazon S3 APIs to upload files from vendors. \n \nSome vendors run their systems on legacy applications that do not support S3 APIs. The vendors \nwant to continue to use SFTP-based applications to upload data. The company wants to use \nmanaged services for the needs of the vendors that use legacy applications. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Database Migration Service (AWS DMS) instance to replicate data from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Transfer Family endpoint for vendors that use legacy applications.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an Amazon EC2 instance to run an SFTP server. Instruct the vendors that use legacy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon S3 File Gateway for vendors that use legacy applications to upload files to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 5,
    "text": "A marketing team wants to build a campaign for an upcoming multi-sport event. The team has \nnews reports from the past five years in PDF format. The team needs a solution to extract \ninsights about the content and the sentiment of the news reports. The solution must use Amazon \nTextract to process the news reports. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Provide the extracted insights to Amazon Athena for analysis. Store the extracted insights and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the extracted insights in an Amazon DynamoDB table. Use Amazon SageMaker to build a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provide the extracted insights to Amazon Comprehend for analysis. Save the analysis to an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the extracted insights in an Amazon S3 bucket. Use Amazon QuickSight to visualize and",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company's application runs on Amazon EC2 instances that are in multiple Availability Zones. \nThe application needs to ingest real-time data from third-party applications. \n \nThe company needs a data ingestion solution that places the ingested raw data in an Amazon S3 \nbucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Kinesis data streams for data ingestion. Create Amazon Kinesis Data Firehose",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create database migration tasks in AWS Database Migration Service (AWS DMS). Specify",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create and configure AWS DataSync agents on the EC2 instances. Configure DataSync tasks to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Direct Connect connection to the application for data ingestion. Create Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 7,
    "text": "A company's application is receiving data from multiple data sources. The size of the data varies \nand is expected to increase over time. The current maximum size is 700 KB. The data volume \nand data size continue to grow as more data sources are added. \n \nThe company decides to use Amazon DynamoDB as the primary database for the application. A \nsolutions architect needs to identify a solution that handles the large data sizes. \n \nWhich solution will meet these requirements in the MOST operationally efficient way? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n426",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to filter the data that exceeds DynamoDB item size limits. Store",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the large data as objects in an Amazon S3 bucket. In a DynamoDB table, create an item",
        "correct": true
      },
      {
        "id": 2,
        "text": "Split all incoming large data into a collection of items that have the same partition key. Write the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that uses gzip compression to compress the large objects as",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company is migrating a legacy application from an on-premises data center to AWS. The \napplication relies on hundreds of cron jobs that run between 1 and 20 minutes on different \nrecurring schedules throughout the day. \n \nThe company wants a solution to schedule and run the cron jobs on AWS with minimal \nrefactoring. The solution must support running the cron jobs in response to an event in the future. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a container image for the cron jobs. Use AWS Batch on Amazon Elastic Container Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a container image for the cron jobs. Create a workflow in AWS Step Functions that uses a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company uses Salesforce. The company needs to load existing data and ongoing data \nchanges from Salesforce to Amazon Redshift for analysis. The company does not want the data \nto travel over the public internet. \n \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Establish a VPN connection from the VPC to Salesforce. Use AWS Glue DataBrew to transfer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Establish an AWS Direct Connect connection from the VPC to Salesforce. Use AWS Glue",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS PrivateLink connection in the VPC to Salesforce. Use Amazon AppFlow to",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a VPC peering connection to Salesforce. Use Amazon AppFlow to transfer data.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company recently migrated its application to AWS. The application runs on Amazon EC2 Linux \ninstances in an Auto Scaling group across multiple Availability Zones. The application stores data \nin an Amazon Elastic File System (Amazon EFS) file system that uses EFS Standard-Infrequent \nAccess storage. The application indexes the company's files. The index is stored in an Amazon \nRDS database. \n \nThe company needs to optimize storage costs with some application and services changes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket that uses an Intelligent-Tiering lifecycle policy. Copy all files to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy Amazon FSx for Windows File Server file shares. Update the application to use CIFS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon FSx for OpenZFS file system shares. Update the application to use the new",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket that uses S3 Glacier Flexible Retrieval. Copy all files to the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 11,
    "text": "A robotics company is designing a solution for medical surgery. The robots will use advanced \nsensors, cameras, and AI algorithms to perceive their environment and to complete surgeries. \n \nThe company needs a public load balancer in the AWS Cloud that will ensure seamless \ncommunication with backend services. The load balancer must be capable of routing traffic based \non the query strings to different target groups. The traffic must also be encrypted. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a Network Load Balancer with a certificate attached from AWS Certificate Manager (ACM).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Gateway Load Balancer. Import a generated certificate in AWS Identity and Access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Application Load Balancer with a certificate attached from AWS Certificate Manager",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use a Network Load Balancer. Import a generated certificate in AWS Identity and Access",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company has an application that runs on a single Amazon EC2 instance. The application uses \na MySQL database that runs on the same EC2 instance. The company needs a highly available \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n428 \nand automatically scalable solution to handle increased traffic. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the application to EC2 instances that are configured as a target group behind an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy the application to EC2 instances that are configured as a target group behind an",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company is planning to migrate data to an Amazon S3 bucket. The data must be encrypted at \nrest within the S3 bucket. The encryption key must be rotated automatically every year. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the data to the S3 bucket. Use server-side encryption with Amazon S3 managed keys",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use customer key material to encrypt the data. Migrate the data to the S3 bucket. Create an AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company is migrating applications from an on-premises Microsoft Active Directory that the \ncompany manages to AWS. The company deploys the applications in multiple AWS accounts. \nThe company uses AWS Organizations to manage the accounts centrally. \n \nThe company's security team needs a single sign-on solution across all the company's AWS \naccounts. The company must continue to manage users and groups that are in the on-premises \nActive Directory. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n429",
    "options": [
      {
        "id": 0,
        "text": "Create an Enterprise Edition Active Directory in AWS Directory Service for Microsoft Active",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable AWS IAM Identity Center. Configure a two-way forest trust relationship to connect the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Directory Service and create a two-way trust relationship with the company's self-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an identity provider (IdP) on Amazon EC2. Link the IdP as an identity source within AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company is planning to deploy its application on an Amazon Aurora PostgreSQL Serverless v2 \ncluster. The application will receive large amounts of traffic. The company wants to optimize the \nstorage performance of the cluster as the load on the application increases. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the cluster to use the Aurora Standard storage configuration.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the cluster storage type as Provisioned IOPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the cluster storage type as General Purpose.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the cluster to use the Aurora I/O-Optimized storage configuration.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 16,
    "text": "A financial services company that runs on AWS has designed its security controls to meet \nindustry standards. The industry standards include the National Institute of Standards and \nTechnology (NIST) and the Payment Card Industry Data Security Standard (PCI DSS). \n \nThe company's third-party auditors need proof that the designed controls have been implemented \nand are functioning correctly. The company has hundreds of AWS accounts in a single \norganization in AWS Organizations. The company needs to monitor the current state of the \ncontrols across accounts. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Designate one account as the Amazon Inspector delegated administrator account from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Designate one account as the Amazon GuardDuty delegated administrator account from the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS CloudTrail organization trail in the Organizations management account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Designate one account as the AWS Security Hub delegated administrator account from the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company uses an Amazon S3 bucket as its data lake storage platform. The S3 bucket contains \na massive amount of data that is accessed randomly by multiple teams and hundreds of \napplications. The company wants to reduce the S3 storage costs and provide immediate \navailability for frequently accessed objects. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Lifecycle rule to transition objects to the S3 Intelligent-Tiering storage class.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store objects in Amazon S3 Glacier. Use S3 Select to provide applications with access to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use data from S3 storage class analysis to create S3 Lifecycle rules to automatically transition",
        "correct": false
      },
      {
        "id": 3,
        "text": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company has 5 TB of datasets. The datasets consist of 1 million user profiles and 10 million \nconnections. The user profiles have connections as many-to-many relationships. The company \nneeds a performance efficient way to find mutual connections up to five levels. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon S3 bucket to store the datasets. Use Amazon Athena to perform SQL JOIN",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Neptune to store the datasets with edges and vertices. Query the data to find",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Amazon S3 bucket to store the datasets. Use Amazon QuickSight to visualize",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS to store the datasets with multiple tables. Perform SQL JOIN queries to find",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 19,
    "text": "A company needs a secure connection between its on-premises environment and AWS. This \nconnection does not need high bandwidth and will handle a small amount of traffic. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n431 \nconnection should be set up quickly. \n \nWhat is the MOST cost-effective method to establish this type of connection?",
    "options": [
      {
        "id": 0,
        "text": "Implement a client VPN.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement AWS Direct Connect.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement a bastion host on Amazon EC2.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an AWS Site-to-Site VPN connection.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "A company has an on-premises SFTP file transfer solution. The company is migrating to the AWS \nCloud to scale the file transfer solution and to optimize costs by using Amazon S3. The \ncompany's employees will use their credentials for the on-premises Microsoft Active Directory \n(AD) to access the new solution. The company wants to keep the current authentication and file \naccess mechanisms. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 File Gateway. Create SMB file shares on the file gateway that use the existing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Auto Scaling group with Amazon EC2 instances to run an SFTP solution. Configure",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Transfer Family server with SFTP endpoints. Choose the AWS Directory Service",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Transfer Family SFTP endpoint. Configure the endpoint to use the AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A company is designing an event-driven order processing system. Each order requires multiple \nvalidation steps after the order is created. An idempotent AWS Lambda function performs each \nvalidation step. Each validation step is independent from the other validation steps. Individual \nvalidation steps need only a subset of the order event information. \n \nThe company wants to ensure that each validation step Lambda function has access to only the \ninformation from the order event that the function requires. The components of the order \nprocessing system should be loosely coupled to accommodate future business changes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue for each validation step. Create",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the validation step",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge event bus. Create an event rule for each validation step.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a new Lambda function",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is migrating a three-tier application to AWS. The application requires a MySQL \ndatabase. In the past, the application users reported poor application performance when creating \nnew entries. These performance issues were caused by users generating different real-time \nreports from the application during working hours. \n \nWhich solution will improve the performance of the application when it is moved to AWS?",
    "options": [
      {
        "id": 0,
        "text": "Import the data into an Amazon DynamoDB table with provisioned capacity. Refactor the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create the database on a compute optimized Amazon EC2 instance. Ensure compute resources",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas. Configure the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster. Configure the application to use the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company is expanding a secure on-premises network to the AWS Cloud by using an AWS \nDirect Connect connection. The on-premises network has no direct internet access. An \napplication that runs on the on-premises network needs to use an Amazon S3 bucket. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a public virtual interface (VIF). Route the AWS traffic over the public VIF.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPC and a NAT gateway. Route the AWS traffic from the on-premises network to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC and an Amazon S3 interface endpoint. Route the AWS traffic from the on-premises",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a VPC peering connection between the on-premises network and Direct Connect. Route",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A company serves its website by using an Auto Scaling group of Amazon EC2 instances in a \nsingle AWS Region. The website does not require a database. \n \nThe company is expanding, and the company's engineering team deploys the website to a \nsecond Region. The company wants to distribute traffic across both Regions to accommodate \ngrowth and for disaster recovery purposes. The solution should not serve traffic from a Region in \nwhich the website is unhealthy. \n \nWhich policy or resource should the company use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "An Amazon Route 53 simple routing policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "An Amazon Route 53 multivalue answer routing policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "An Application Load Balancer in one Region with a target group that specifies the EC2 instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "An Application Load Balancer in one Region with a target group that specifies the IP addresses of",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "A company runs its applications on Amazon EC2 instances that are backed by Amazon Elastic \nBlock Store (Amazon EBS). The EC2 instances run the most recent Amazon Linux release. The \napplications are experiencing availability issues when the company's employees store and \nretrieve files that are 25 GB or larger. The company needs a solution that does not require the \ncompany to transfer files between EC2 instances. The files must be available across many EC2 \ninstances and across multiple Availability Zones. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate all the files to an Amazon S3 bucket. Instruct the employees to access the files from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Take a snapshot of the existing EBS volume. Mount the snapshot as an EBS volume across the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Mount an Amazon Elastic File System (Amazon EFS) file system across all the EC2 instances.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Machine Image (AMI) from the EC2 instances. Configure new EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company is running a highly sensitive application on Amazon EC2 backed by an Amazon RDS \ndatabase. Compliance regulations mandate that all personally identifiable information (PII) be \nencrypted at rest. \n \nWhich solution should a solutions architect recommend to meet this requirement with the LEAST \namount of changes to the infrastructure? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n434",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Certificate Manager to generate certificates. Use the certificates to encrypt the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS CloudHSM, generate encryption keys, and use the keys to encrypt database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure SSL encryption using AWS Key Management Service (AWS KMS) keys to encrypt",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon Elastic Block Store (Amazon EBS) encryption and Amazon RDS encryption",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company runs an AWS Lambda function in private subnets in a VPC. The subnets have a \ndefault route to the internet through an Amazon EC2 NAT instance. The Lambda function \nprocesses input data and saves its output as an object to Amazon S3. \n \nIntermittently, the Lambda function times out while trying to upload the object because of \nsaturated traffic on the NAT instance's network. The company wants to access Amazon S3 \nwithout traversing the internet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Replace the EC2 NAT instance with an AWS managed NAT gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the size of the EC2 NAT instance in the VPC to a network optimized instance type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a gateway endpoint for Amazon S3 in the VPUpdate the route tables of the subnets",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a transit gateway. Place transit gateway attachments in the private subnets where the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A news company that has reporters all over the world is hosting its broadcast system on AWS. \nThe reporters send live broadcasts to the broadcast system. The reporters use software on their \nphones to send live streams through the Real Time Messaging Protocol (RTMP). \n \nA solutions architect must design a solution that gives the reporters the ability to send the highest \nquality streams. The solution must provide accelerated TCP connections back to the broadcast \nsystem. \n \nWhat should the solutions architect use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon CloudFront",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Global Accelerator",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Client VPN",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 instances and AWS Elastic IP addresses",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n435 \nA company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) to run \nits self-managed database. The company has 350 TB of data spread across all EBS volumes. \nThe company takes daily EBS snapshots and keeps the snapshots for 1 month. The daily change \nrate is 5% of the EBS volumes. \n \nBecause of new regulations, the company needs to keep the monthly snapshots for 7 years. The \ncompany needs to change its backup strategy to comply with the new regulations and to ensure \nthat data is available with minimal administrative effort. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Copy the monthly",
        "correct": true
      },
      {
        "id": 1,
        "text": "Continue with the current EBS snapshot policy. Add a new policy to move the monthly snapshot",
        "correct": false
      },
      {
        "id": 2,
        "text": "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Keep the monthly",
        "correct": false
      },
      {
        "id": 3,
        "text": "Keep the daily snapshot in the EBS snapshot standard tier. Use EBS direct APIs to take",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 30,
    "text": "A company runs an application on several Amazon EC2 instances that store persistent data on \nan Amazon Elastic File System (Amazon EFS) file system. The company needs to replicate the \ndata to another AWS Region by using an AWS managed service solution. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use the EFS-to-EFS backup solution to replicate the data to an EFS file system in another",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run a nightly script to copy data from the EFS file system to an Amazon S3 bucket. Enable S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC in another Region. Establish a cross-Region VPC peer. Run a nightly rsync to copy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Backup to create a backup plan with a rule that takes a daily backup and replicates it to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "An ecommerce company is migrating its on-premises workload to the AWS Cloud. The workload \ncurrently consists of a web application and a backend Microsoft SQL database for storage. \n \nThe company expects a high volume of customers during a promotional event. The new \ninfrastructure in the AWS Cloud must be highly available and scalable. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the web application to two Amazon EC2 instances across two Availability Zones behind",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the web application to an Amazon EC2 instance that runs in an Auto Scaling group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the web application to Amazon EC2 instances that run in an Auto Scaling group across",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the web application to three Amazon EC2 instances across three Availability Zones",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A company has 15 employees. The company stores employee start dates in an Amazon \nDynamoDB table. The company wants to send an email message to each employee on the day \nof the employee's work anniversary. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a script that scans the DynamoDB table and uses Amazon Simple Notification Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a script that scans the DynamoDB table and uses Amazon Simple Queue Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company's application is running on Amazon EC2 instances within an Auto Scaling group \nbehind an Elastic Load Balancing (ELB) load balancer. Based on the application's history, the \ncompany anticipates a spike in traffic during a holiday each year. A solutions architect must \ndesign a strategy to ensure that the Auto Scaling group proactively increases capacity to \nminimize any performance impact on application users. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a recurring scheduled action to scale up the Auto Scaling group before the expected",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A company runs its application on Oracle Database Enterprise Edition. The company needs to \nmigrate the application and the database to AWS. The company can use the Bring Your Own \nLicense (BYOL) model while migrating to AWS. The application uses third-party database \nfeatures that require privileged access. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n438 \nA solutions architect must design a solution for the database migration. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon RDS for Oracle by using native tools. Replace the third-party",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon RDS Custom for Oracle by using native tools. Customize the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon RDS for PostgreSQL by using AWS Database Migration Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A large international university has deployed all of its compute services in the AWS Cloud. These \nservices include Amazon EC2, Amazon RDS, and Amazon DynamoDB. The university currently \nrelies on many custom scripts to back up its infrastructure. However, the university wants to \ncentralize management and automate data backups as much as possible by using AWS native \noptions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use third-party backup software with an AWS Storage Gateway tape gateway virtual tape library.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Backup to configure and monitor all backups for the services in use.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Config to set lifecycle management to take snapshots of all data sources on a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager State Manager to manage the configuration and monitoring of",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "A company wants to build a map of its IT infrastructure to identify and enforce policies on \nresources that pose security risks. The company's security team must be able to query data in \nthe IT infrastructure map and quickly identify security risks. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon RDS to store the data. Use SQL to query the data to identify security risks.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Neptune to store the data. Use SPARQL to query the data to identify security risks.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Redshift to store the data. Use SQL to query the data to identify security risks.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon DynamoDB to store the data. Use PartiQL to query the data to identify security",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A large company wants to provide its globally located developers separate, limited size, managed \nPostgreSQL databases for development purposes. The databases will be low volume. The \ndevelopers need the databases only when they are actively working. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Give the developers the ability to launch separate Amazon Aurora instances. Set up a process to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Develop an AWS Service Catalog product that enforces size restrictions for launching Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon Aurora Serverless cluster. Develop an AWS Service Catalog product to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Monitor AWS Trusted Advisor checks for idle Amazon RDS databases. Create a process to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 40,
    "text": "A company is building a web application that serves a content management system. The content \nmanagement system runs on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. Users \nare constantly adding and updating files, blogs, and other website assets in the content \nmanagement system. \n \nA solutions architect must implement a solution in which all the EC2 instances share up-to-date \nwebsite content with the least possible lag time. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the EC2 user data in the Auto Scaling group lifecycle policy to copy the website assets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy the website assets to an Amazon Elastic File System (Amazon EFS) file system. Configure",
        "correct": true
      },
      {
        "id": 2,
        "text": "Copy the website assets to an Amazon S3 bucket. Ensure that each EC2 instance downloads the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Restore an Amazon Elastic Block Store (Amazon EBS) snapshot with the website assets. Attach",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company's web application consists of multiple Amazon EC2 instances that run behind an \nApplication Load Balancer in a VPC. An Amazon RDS for MySQL DB instance contains the data. \nThe company needs the ability to automatically detect and respond to suspicious or unexpected \nbehavior in its AWS environment. The company already has added AWS WAF to its architecture. \n \nWhat should a solutions architect do next to protect against threats?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty to perform threat detection. Configure Amazon EventBridge to filter for",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Firewall Manager to perform threat detection. Configure Amazon EventBridge to filter",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Inspector to perform threat detection and to update the AWS WAF rules. Create a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Macie to perform threat detection and to update the AWS WAF rules. Create a VPC",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. \nThe company does not want to use the default domain name for the distribution. Instead, the \ncompany wants to use a different domain name for the distribution. \n \nWhich solution will deploy the certificate without incurring any additional costs? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n441",
    "options": [
      {
        "id": 0,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company creates operations data and stores the data in an Amazon S3 bucket. For the \ncompany's annual audit, an external consultant needs to access an annual report that is stored in \nthe S3 bucket. The external consultant needs to access the report for 7 days. \n \nThe company must implement a solution to allow the external consultant access to only the \nreport. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a new S3 bucket that is configured to host a public static website. Migrate the operations",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable public access to the S3 bucket for 7 days. Remove access to the S3 bucket when the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user that has access to the report in the S3 bucket. Provide the access keys to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Generate a presigned URL that has the required access to the location of the report on the S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company plans to run a high performance computing (HPC) workload on Amazon EC2 \nInstances. The workload requires low-latency network performance and high network throughput \nwith tightly coupled node-to-node communication. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the EC2 instances to be part of a cluster placement group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch the EC2 instances with Dedicated Instance tenancy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch the EC2 instances as Spot Instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an On-Demand Capacity Reservation when the EC2 instances are launched.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 46,
    "text": "A company has primary and secondary data centers that are 500 miles (804.7 km) apart and \ninterconnected with high-speed fiber-optic cable. The company needs a highly available and \nsecure network connection between its data centers and a VPC on AWS for a mission-critical \nworkload. A solutions architect must choose a connection solution that provides maximum \nresiliency. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Two AWS Direct Connect connections from the primary data center terminating at two Direct",
        "correct": false
      },
      {
        "id": 1,
        "text": "A single AWS Direct Connect connection from each of the primary and secondary data centers",
        "correct": false
      },
      {
        "id": 2,
        "text": "Two AWS Direct Connect connections from each of the primary and secondary data centers",
        "correct": true
      },
      {
        "id": 3,
        "text": "A single AWS Direct Connect connection from each of the primary and secondary data centers",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company runs several Amazon RDS for Oracle On-Demand DB instances that have high \nutilization. The RDS DB instances run in member accounts that are in an organization in AWS \nOrganizations. \n \nThe company's finance team has access to the organization's management account and member \naccounts. The finance team wants to find ways to optimize costs by using AWS Trusted Advisor. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use the Trusted Advisor recommendations in the management account.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the Trusted Advisor recommendations in the member accounts where the RDS DB instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Review the Trusted Advisor checks for Amazon RDS Reserved Instance Optimization.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Review the Trusted Advisor checks for Amazon RDS Idle DB Instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Review the Trusted Advisor checks for compute optimization. Crosscheck the results by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 48,
    "text": "A solutions architect is creating an application. The application will run on Amazon EC2 instances \nin private subnets across multiple Availability Zones in a VPC. The EC2 instances will frequently \naccess large files that contain confidential information. These files are stored in Amazon S3 \nbuckets for processing. The solutions architect must optimize the network architecture to \nminimize data transfer costs. \n \nWhat should the solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n443",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the private subnets,",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a single NAT gateway in a public subnet. In the route tables for the private subnets, add a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS PrivateLink interface endpoint for Amazon S3 in the VPIn the route tables for the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create one NAT gateway for each Availability Zone in public subnets. In each of the route tables",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company wants to relocate its on-premises MySQL database to AWS. The database accepts \nregular imports from a client-facing application, which causes a high volume of write operations. \nThe company is concerned that the amount of traffic might be causing performance issues within \nthe application. \n \nHow should a solutions architect design the architecture on AWS?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage. Monitor",
        "correct": true
      },
      {
        "id": 1,
        "text": "Provision an Amazon RDS for MySQL DB instance with General Purpose SSD storage. Place an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an Amazon DocumentDB (with MongoDB compatibility) instance with a memory",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an Amazon Elastic File System (Amazon EFS) file system in General Purpose",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 50,
    "text": "A company runs an application in the AWS Cloud that generates sensitive archival data files. The \ncompany wants to rearchitect the application's data storage. The company wants to encrypt the \ndata files and to ensure that third parties do not have access to the data before the data is \nencrypted and sent to AWS. The company has already created an Amazon S3 bucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the S3 bucket to use client-side encryption with an Amazon S3 managed encryption",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the S3 bucket to use dual-layer server-side encryption with AWS KMS keys (SSE-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the application to use client-side encryption with a key stored in AWS Key Management",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 1 is incorrect:**\nis incorrect because while it creates IAM policies for DynamoDB and S3, attaching them to the EC2 instance profile grants those permissions to *all* Pods running on those nodes. Kubernetes RBAC controls access *within* the cluster, not to AWS resources. This approach doesn't provide the fine-grained control needed to restrict UI Pods to DynamoDB and data-processing Pods to S3. All pods running on the node would inherit the permissions of the instance profile.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company uses Amazon RDS with default backup settings for its database tier. The company \nneeds to make a daily backup of the database to meet regulatory requirements. The company \nmust retain the backups for 30 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Write an AWS Lambda function to create an RDS snapshot every day.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the RDS database to have a retention period of 30 days for automated backups.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Systems Manager Maintenance Windows to modify the RDS backup retention period.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a manual snapshot every day by using the AWS CLI. Modify the RDS backup retention",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "A company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. \nDuring peak usage hours when multiple users access and read the data, the monitoring system \nshows degradation of database performance for the write queries. The company wants to \nincrease the scalability of the application to meet peak usage demands. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a second Aurora DB cluster. Configure a copy job to replicate the users' data to the new",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon DynamoDB Accelerator (DAX) cluster in front of the existing Aurora DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Aurora read replica in the existing Aurora DB cluster. Update the application to use the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Redshift cluster. Copy the users' data to the Redshift cluster. Update the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company's near-real-time streaming application is running on AWS. As the data is ingested, a \njob runs on the data and takes 30 minutes to complete. The workload frequently experiences high \nlatency due to large amounts of incoming data. A solutions architect needs to design a scalable \nand serverless solution to enhance performance. \n \nWhich combination of steps should the solutions architect take? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Firehose to ingest the data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Lambda with AWS Step Functions to process the data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to ingest the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 instances in an Auto Scaling group to process the data.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Fargate with Amazon Elastic Container Service (Amazon ECS) to process the data.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 54,
    "text": "A company runs a web application on multiple Amazon EC2 instances in a VPC. The application \nneeds to write sensitive data to an Amazon S3 bucket. The data cannot be sent over the public \ninternet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint for Amazon S3. Create a route in the VPC route table to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an internal Network Load Balancer that has the S3 bucket as the target.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the S3 bucket inside the VPCreate a route in the VPC route table to the bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Direct Connect connection between the VPC and an S3 regional endpoint.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company runs its production workload on Amazon EC2 instances with Amazon Elastic Block \nStore (Amazon EBS) volumes. A solutions architect needs to analyze the current EBS volume \ncost and to recommend optimizations. The recommendations need to include estimated monthly \nsaving opportunities. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Inspector reporting to generate EBS volume recommendations for optimization.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Systems Manager reporting to determine EBS volume recommendations for",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudWatch metrics reporting to determine EBS volume recommendations for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Compute Optimizer to generate EBS volume recommendations for optimization.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "A global company runs its workloads on AWS. The company's application uses Amazon S3 \nbuckets across AWS Regions for sensitive data storage and analysis. The company stores \nmillions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that \nare not versioning-enabled. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A company wants to enhance its ecommerce order-processing application that is deployed on \nAWS. The application must process each order exactly once without affecting the customer \nexperience during unpredictable traffic surges. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Put all the orders in the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Publish all the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a flow by using Amazon AppFlow. Send the orders to the flow. Configure an AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS X-Ray in the application to track the order requests. Configure the application to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company has two AWS accounts: Production and Development. The company needs to push \ncode changes in the Development account to the Production account. In the alpha phase, only \ntwo senior developers on the development team need access to the Production account. In the \nbeta phase, more developers will need access to perform testing. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create two policy documents by using the AWS Management Console in each account. Assign",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role in the Development account. Grant the IAM role access to the Production",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM role in the Production account. Define a trust policy that specifies the Development",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM group in the Production account. Add the group as a principal in a trust policy that",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company wants to restrict access to the content of its web application. The company needs to \nprotect the content by using authorization techniques that are available on AWS. The company \nalso wants to implement a serverless architecture for authorization and authentication that has \nlow login latency. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n447 \nThe solution must integrate with the web application and serve web content globally. The \napplication currently has a small user base, but the company expects the application's user base \nto increase. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Cognito for authentication. Implement Lambda@Edge for authorization.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Cognito for authentication. Implement AWS Lambda for authorization. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A development team uses multiple AWS accounts for its development, staging, and production \nenvironments. Team members have been launching large Amazon EC2 instances that are \nunderutilized. A solutions architect must prevent large instances from being launched in all \naccounts. \n \nHow can the solutions architect meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Update the IAM policies to deny the launch of large EC2 instances. Apply the policies to all users.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define a resource in AWS Resource Access Manager that prevents the launch of large EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM role in each account that denies the launch of large EC2 instances. Grant the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an organization in AWS Organizations in the management account with the default policy.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company has migrated a fleet of hundreds of on-premises virtual machines (VMs) to Amazon \nEC2 instances. The instances run a diverse fleet of Windows Server versions along with several \nLinux distributions. The company wants a solution that will automate inventory and updates of the \noperating systems. The company also needs a summary of common vulnerabilities of each \ninstance for regular monthly reviews. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Configure AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Deploy Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Shield Advanced, and configure monthly reports. Deploy AWS Config to automate",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon GuardDuty in the account to monitor all EC2 instances. Deploy AWS Config to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 62,
    "text": "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 \ninstances in an Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer. The \napplication connects to an Amazon DynamoDB table. \n \nFor disaster recovery (DR) purposes, the company wants to ensure that the application is \navailable from another AWS Region with minimal downtime. \n \nWhich solution will meet these requirements with the LEAST downtime?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS CloudFormation template to create EC2 instances, ELBs, and DynamoDB tables",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS CloudFormation template to create EC2 instances and an ELB to be launched",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 63,
    "text": "A company runs an application on Amazon EC2 instances in a private subnet. The application \nneeds to store and retrieve data in Amazon S3 buckets. According to regulatory requirements, \nthe data must not travel across the public internet. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a NAT gateway to access the S3 buckets.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Storage Gateway to access the S3 buckets.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an S3 interface endpoint to access the S3 buckets.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an S3 gateway endpoint to access the S3 buckets.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n449 \nA company hosts an application on Amazon EC2 instances that run in a single Availability Zone. \nThe application is accessible by using the transport layer of the Open Systems Interconnection \n(OSI) model. The company needs the application architecture to have high availability. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure new EC2 instances in a different Availability Zone. Use Amazon Route 53 to route",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a Network Load Balancer in front of the EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a Network Load Balancer for TCP traffic to the instances. Configure an Application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group for the EC2 instances. Configure the Auto Scaling group to use",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon CloudWatch alarm. Configure the alarm to restart EC2 instances that",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  }
]