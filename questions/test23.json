[
  {
    "id": 0,
    "text": "A company hosts a data lake on Amazon S3. The data lake ingests data in Apache Parquet \nformat from various data sources. The company uses multiple transformation steps to prepare the \ningested data. The steps include filtering of anomalies, normalizing of data to standard date and \ntime values, and generation of aggregates for analyses. \n \nThe company must store the transformed data in S3 buckets that data analysts access. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n423 \ncompany needs a prebuilt solution for data transformation that does not require code. The \nsolution must provide data lineage and data profiling. The company needs to share the data \ntransformation steps with employees throughout the company. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS Glue Studio visual canvas to transform the data. Share the transformation",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon EMR Serverless to transform the data. Share the transformation steps with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Glue DataBrew to transform the data. Share the transformation steps with",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create Amazon Athena tables for the data. Write Athena SQL queries to transform the data.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A solutions architect runs a web application on multiple Amazon EC2 instances that are in \nindividual target groups behind an Application Load Balancer (ALB). Users can reach the \napplication through a public website. \n \nThe solutions architect wants to allow engineers to use a development version of the website to \naccess one specific development EC2 instance to test new features for the application. The \nsolutions architect wants to use an Amazon Route 53 hosted zone to give the engineers access \nto the development instance. The solution must automatically route to the development instance \neven if the development instance is replaced. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an A Record for the development website that has the value set to the ALB. Create a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Recreate the development instance with a public IP address. Create an A Record for the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an A Record for the development website that has the value set to the ALB. Create a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place all the instances in the same target group. Create an A Record for the development",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company runs a container application on a Kubernetes cluster in the company's data center. \nThe application uses Advanced Message Queuing Protocol (AMQP) to communicate with a \nmessage queue. The data center cannot scale fast enough to meet the company's expanding \nbusiness needs. The company wants to migrate the workloads to AWS. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n424",
    "options": [
      {
        "id": 0,
        "text": "Migrate the container application to Amazon Elastic Container Service (Amazon ECS). Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the container application to Amazon Elastic Kubernetes Service (Amazon EKS). Use",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use highly available Amazon EC2 instances to run the application. Use Amazon MQ to retrieve",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Lambda functions to run the application. Use Amazon Simple Queue Service (Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "An online gaming company hosts its platform on Amazon EC2 instances behind Network Load \nBalancers (NLBs) across multiple AWS Regions. The NLBs can route requests to targets over the \ninternet. The company wants to improve the customer playing experience by reducing end-to-end \nload time for its global customer base. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Application Load Balancers (ALBs) in each Region to replace the existing NLBs. Register",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon Route 53 to route equally weighted traffic to the NLBs in each Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create additional NLBs and EC2 instances in other Regions where the company has large",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a standard accelerator in AWS Global Accelerator. Configure the existing NLBs as target",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 4,
    "text": "A company has an on-premises application that uses SFTP to collect financial data from multiple \nvendors. The company is migrating to the AWS Cloud. The company has created an application \nthat uses Amazon S3 APIs to upload files from vendors. \n \nSome vendors run their systems on legacy applications that do not support S3 APIs. The vendors \nwant to continue to use SFTP-based applications to upload data. The company wants to use \nmanaged services for the needs of the vendors that use legacy applications. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Database Migration Service (AWS DMS) instance to replicate data from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Transfer Family endpoint for vendors that use legacy applications.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an Amazon EC2 instance to run an SFTP server. Instruct the vendors that use legacy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon S3 File Gateway for vendors that use legacy applications to upload files to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A marketing team wants to build a campaign for an upcoming multi-sport event. The team has \nnews reports from the past five years in PDF format. The team needs a solution to extract \ninsights about the content and the sentiment of the news reports. The solution must use Amazon \nTextract to process the news reports. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Provide the extracted insights to Amazon Athena for analysis. Store the extracted insights and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the extracted insights in an Amazon DynamoDB table. Use Amazon SageMaker to build a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provide the extracted insights to Amazon Comprehend for analysis. Save the analysis to an",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the extracted insights in an Amazon S3 bucket. Use Amazon QuickSight to visualize and",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A company's application runs on Amazon EC2 instances that are in multiple Availability Zones. \nThe application needs to ingest real-time data from third-party applications. \n \nThe company needs a data ingestion solution that places the ingested raw data in an Amazon S3 \nbucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Kinesis data streams for data ingestion. Create Amazon Kinesis Data Firehose",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create database migration tasks in AWS Database Migration Service (AWS DMS). Specify",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create and configure AWS DataSync agents on the EC2 instances. Configure DataSync tasks to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Direct Connect connection to the application for data ingestion. Create Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 7,
    "text": "A company's application is receiving data from multiple data sources. The size of the data varies \nand is expected to increase over time. The current maximum size is 700 KB. The data volume \nand data size continue to grow as more data sources are added. \n \nThe company decides to use Amazon DynamoDB as the primary database for the application. A \nsolutions architect needs to identify a solution that handles the large data sizes. \n \nWhich solution will meet these requirements in the MOST operationally efficient way? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n426",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to filter the data that exceeds DynamoDB item size limits. Store",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the large data as objects in an Amazon S3 bucket. In a DynamoDB table, create an item",
        "correct": true
      },
      {
        "id": 2,
        "text": "Split all incoming large data into a collection of items that have the same partition key. Write the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that uses gzip compression to compress the large objects as",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company is migrating a legacy application from an on-premises data center to AWS. The \napplication relies on hundreds of cron jobs that run between 1 and 20 minutes on different \nrecurring schedules throughout the day. \n \nThe company wants a solution to schedule and run the cron jobs on AWS with minimal \nrefactoring. The solution must support running the cron jobs in response to an event in the future. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a container image for the cron jobs. Use AWS Batch on Amazon Elastic Container Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a container image for the cron jobs. Create a workflow in AWS Step Functions that uses a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/blogs/containers/migrate-cron-jobs-to-event-driven-architectures-using- amazon-elastic-container-service-and-amazon-eventbridge/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company uses Salesforce. The company needs to load existing data and ongoing data \nchanges from Salesforce to Amazon Redshift for analysis. The company does not want the data \nto travel over the public internet. \n \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Establish a VPN connection from the VPC to Salesforce. Use AWS Glue DataBrew to transfer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Establish an AWS Direct Connect connection from the VPC to Salesforce. Use AWS Glue",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS PrivateLink connection in the VPC to Salesforce. Use Amazon AppFlow to",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a VPC peering connection to Salesforce. Use Amazon AppFlow to transfer data.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/connect/latest/adminguide/integrate-salesforce-tasks.html https://docs.aws.amazon.com/connect/latest/adminguide/vpc-interface-endpoints.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company recently migrated its application to AWS. The application runs on Amazon EC2 Linux \ninstances in an Auto Scaling group across multiple Availability Zones. The application stores data \nin an Amazon Elastic File System (Amazon EFS) file system that uses EFS Standard-Infrequent \nAccess storage. The application indexes the company's files. The index is stored in an Amazon \nRDS database. \n \nThe company needs to optimize storage costs with some application and services changes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket that uses an Intelligent-Tiering lifecycle policy. Copy all files to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy Amazon FSx for Windows File Server file shares. Update the application to use CIFS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon FSx for OpenZFS file system shares. Update the application to use the new",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket that uses S3 Glacier Flexible Retrieval. Copy all files to the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 11,
    "text": "A robotics company is designing a solution for medical surgery. The robots will use advanced \nsensors, cameras, and AI algorithms to perceive their environment and to complete surgeries. \n \nThe company needs a public load balancer in the AWS Cloud that will ensure seamless \ncommunication with backend services. The load balancer must be capable of routing traffic based \non the query strings to different target groups. The traffic must also be encrypted. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a Network Load Balancer with a certificate attached from AWS Certificate Manager (ACM).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Gateway Load Balancer. Import a generated certificate in AWS Identity and Access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Application Load Balancer with a certificate attached from AWS Certificate Manager",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use a Network Load Balancer. Import a generated certificate in AWS Identity and Access",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company has an application that runs on a single Amazon EC2 instance. The application uses \na MySQL database that runs on the same EC2 instance. The company needs a highly available \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n428 \nand automatically scalable solution to handle increased traffic. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the application to EC2 instances that are configured as a target group behind an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy the application to EC2 instances that are configured as a target group behind an",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nTarget groups are just a group of Ec2 instances. Target groups are closely associated with ELB and not ASG. We can just use ELB and Target groups to route requests to EC2 instances. With this setup, there is no autoscaling which means instances cannot be added or removed when your load increases/decreases.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company is planning to migrate data to an Amazon S3 bucket. The data must be encrypted at \nrest within the S3 bucket. The encryption key must be rotated automatically every year. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the data to the S3 bucket. Use server-side encryption with Amazon S3 managed keys",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use customer key material to encrypt the data. Migrate the data to the S3 bucket. Create an AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company is migrating applications from an on-premises Microsoft Active Directory that the \ncompany manages to AWS. The company deploys the applications in multiple AWS accounts. \nThe company uses AWS Organizations to manage the accounts centrally. \n \nThe company's security team needs a single sign-on solution across all the company's AWS \naccounts. The company must continue to manage users and groups that are in the on-premises \nActive Directory. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n429",
    "options": [
      {
        "id": 0,
        "text": "Create an Enterprise Edition Active Directory in AWS Directory Service for Microsoft Active",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable AWS IAM Identity Center. Configure a two-way forest trust relationship to connect the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Directory Service and create a two-way trust relationship with the company's self-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an identity provider (IdP) on Amazon EC2. Link the IdP as an identity source within AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company is planning to deploy its application on an Amazon Aurora PostgreSQL Serverless v2 \ncluster. The application will receive large amounts of traffic. The company wants to optimize the \nstorage performance of the cluster as the load on the application increases. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the cluster to use the Aurora Standard storage configuration.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the cluster storage type as Provisioned IOPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the cluster storage type as General Purpose.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the cluster to use the Aurora I/O-Optimized storage configuration.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 16,
    "text": "A financial services company that runs on AWS has designed its security controls to meet \nindustry standards. The industry standards include the National Institute of Standards and \nTechnology (NIST) and the Payment Card Industry Data Security Standard (PCI DSS). \n \nThe company's third-party auditors need proof that the designed controls have been implemented \nand are functioning correctly. The company has hundreds of AWS accounts in a single \norganization in AWS Organizations. The company needs to monitor the current state of the \ncontrols across accounts. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Designate one account as the Amazon Inspector delegated administrator account from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Designate one account as the Amazon GuardDuty delegated administrator account from the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS CloudTrail organization trail in the Organizations management account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Designate one account as the AWS Security Hub delegated administrator account from the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company uses an Amazon S3 bucket as its data lake storage platform. The S3 bucket contains \na massive amount of data that is accessed randomly by multiple teams and hundreds of \napplications. The company wants to reduce the S3 storage costs and provide immediate \navailability for frequently accessed objects. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Lifecycle rule to transition objects to the S3 Intelligent-Tiering storage class.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store objects in Amazon S3 Glacier. Use S3 Select to provide applications with access to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use data from S3 storage class analysis to create S3 Lifecycle rules to automatically transition",
        "correct": false
      },
      {
        "id": 3,
        "text": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-managing.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company has 5 TB of datasets. The datasets consist of 1 million user profiles and 10 million \nconnections. The user profiles have connections as many-to-many relationships. The company \nneeds a performance efficient way to find mutual connections up to five levels. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon S3 bucket to store the datasets. Use Amazon Athena to perform SQL JOIN",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Neptune to store the datasets with edges and vertices. Query the data to find",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Amazon S3 bucket to store the datasets. Use Amazon QuickSight to visualize",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS to store the datasets with multiple tables. Perform SQL JOIN queries to find",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/neptune/latest/userguide/notebooks-visualization.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 19,
    "text": "A company needs a secure connection between its on-premises environment and AWS. This \nconnection does not need high bandwidth and will handle a small amount of traffic. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n431 \nconnection should be set up quickly. \n \nWhat is the MOST cost-effective method to establish this type of connection?",
    "options": [
      {
        "id": 0,
        "text": "Implement a client VPN.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement AWS Direct Connect.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement a bastion host on Amazon EC2.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an AWS Site-to-Site VPN connection.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "A company has an on-premises SFTP file transfer solution. The company is migrating to the AWS \nCloud to scale the file transfer solution and to optimize costs by using Amazon S3. The \ncompany's employees will use their credentials for the on-premises Microsoft Active Directory \n(AD) to access the new solution. The company wants to keep the current authentication and file \naccess mechanisms. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 File Gateway. Create SMB file shares on the file gateway that use the existing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Auto Scaling group with Amazon EC2 instances to run an SFTP solution. Configure",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Transfer Family server with SFTP endpoints. Choose the AWS Directory Service",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Transfer Family SFTP endpoint. Configure the endpoint to use the AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A company is designing an event-driven order processing system. Each order requires multiple \nvalidation steps after the order is created. An idempotent AWS Lambda function performs each \nvalidation step. Each validation step is independent from the other validation steps. Individual \nvalidation steps need only a subset of the order event information. \n \nThe company wants to ensure that each validation step Lambda function has access to only the \ninformation from the order event that the function requires. The components of the order \nprocessing system should be loosely coupled to accommodate future business changes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue for each validation step. Create",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the validation step",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge event bus. Create an event rule for each validation step.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a new Lambda function",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is migrating a three-tier application to AWS. The application requires a MySQL \ndatabase. In the past, the application users reported poor application performance when creating \nnew entries. These performance issues were caused by users generating different real-time \nreports from the application during working hours. \n \nWhich solution will improve the performance of the application when it is moved to AWS?",
    "options": [
      {
        "id": 0,
        "text": "Import the data into an Amazon DynamoDB table with provisioned capacity. Refactor the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create the database on a compute optimized Amazon EC2 instance. Ensure compute resources",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas. Configure the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster. Configure the application to use the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company is expanding a secure on-premises network to the AWS Cloud by using an AWS \nDirect Connect connection. The on-premises network has no direct internet access. An \napplication that runs on the on-premises network needs to use an Amazon S3 bucket. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a public virtual interface (VIF). Route the AWS traffic over the public VIF.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPC and a NAT gateway. Route the AWS traffic from the on-premises network to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC and an Amazon S3 interface endpoint. Route the AWS traffic from the on-premises",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a VPC peering connection between the on-premises network and Direct Connect. Route",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A company serves its website by using an Auto Scaling group of Amazon EC2 instances in a \nsingle AWS Region. The website does not require a database. \n \nThe company is expanding, and the company's engineering team deploys the website to a \nsecond Region. The company wants to distribute traffic across both Regions to accommodate \ngrowth and for disaster recovery purposes. The solution should not serve traffic from a Region in \nwhich the website is unhealthy. \n \nWhich policy or resource should the company use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "An Amazon Route 53 simple routing policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "An Amazon Route 53 multivalue answer routing policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "An Application Load Balancer in one Region with a target group that specifies the EC2 instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "An Application Load Balancer in one Region with a target group that specifies the IP addresses of",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-multivalue.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "A company runs its applications on Amazon EC2 instances that are backed by Amazon Elastic \nBlock Store (Amazon EBS). The EC2 instances run the most recent Amazon Linux release. The \napplications are experiencing availability issues when the company's employees store and \nretrieve files that are 25 GB or larger. The company needs a solution that does not require the \ncompany to transfer files between EC2 instances. The files must be available across many EC2 \ninstances and across multiple Availability Zones. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate all the files to an Amazon S3 bucket. Instruct the employees to access the files from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Take a snapshot of the existing EBS volume. Mount the snapshot as an EBS volume across the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Mount an Amazon Elastic File System (Amazon EFS) file system across all the EC2 instances.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Machine Image (AMI) from the EC2 instances. Configure new EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company is running a highly sensitive application on Amazon EC2 backed by an Amazon RDS \ndatabase. Compliance regulations mandate that all personally identifiable information (PII) be \nencrypted at rest. \n \nWhich solution should a solutions architect recommend to meet this requirement with the LEAST \namount of changes to the infrastructure? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n434",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Certificate Manager to generate certificates. Use the certificates to encrypt the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS CloudHSM, generate encryption keys, and use the keys to encrypt database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure SSL encryption using AWS Key Management Service (AWS KMS) keys to encrypt",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon Elastic Block Store (Amazon EBS) encryption and Amazon RDS encryption",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company runs an AWS Lambda function in private subnets in a VPC. The subnets have a \ndefault route to the internet through an Amazon EC2 NAT instance. The Lambda function \nprocesses input data and saves its output as an object to Amazon S3. \n \nIntermittently, the Lambda function times out while trying to upload the object because of \nsaturated traffic on the NAT instance's network. The company wants to access Amazon S3 \nwithout traversing the internet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Replace the EC2 NAT instance with an AWS managed NAT gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the size of the EC2 NAT instance in the VPC to a network optimized instance type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a gateway endpoint for Amazon S3 in the VPUpdate the route tables of the subnets",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a transit gateway. Place transit gateway attachments in the private subnets where the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A news company that has reporters all over the world is hosting its broadcast system on AWS. \nThe reporters send live broadcasts to the broadcast system. The reporters use software on their \nphones to send live streams through the Real Time Messaging Protocol (RTMP). \n \nA solutions architect must design a solution that gives the reporters the ability to send the highest \nquality streams. The solution must provide accelerated TCP connections back to the broadcast \nsystem. \n \nWhat should the solutions architect use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon CloudFront",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Global Accelerator",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Client VPN",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 instances and AWS Elastic IP addresses",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n435 \nA company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) to run \nits self-managed database. The company has 350 TB of data spread across all EBS volumes. \nThe company takes daily EBS snapshots and keeps the snapshots for 1 month. The daily change \nrate is 5% of the EBS volumes. \n \nBecause of new regulations, the company needs to keep the monthly snapshots for 7 years. The \ncompany needs to change its backup strategy to comply with the new regulations and to ensure \nthat data is available with minimal administrative effort. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Copy the monthly",
        "correct": true
      },
      {
        "id": 1,
        "text": "Continue with the current EBS snapshot policy. Add a new policy to move the monthly snapshot",
        "correct": false
      },
      {
        "id": 2,
        "text": "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Keep the monthly",
        "correct": false
      },
      {
        "id": 3,
        "text": "Keep the daily snapshot in the EBS snapshot standard tier. Use EBS direct APIs to take",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 30,
    "text": "A company runs an application on several Amazon EC2 instances that store persistent data on \nan Amazon Elastic File System (Amazon EFS) file system. The company needs to replicate the \ndata to another AWS Region by using an AWS managed service solution. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use the EFS-to-EFS backup solution to replicate the data to an EFS file system in another",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run a nightly script to copy data from the EFS file system to an Amazon S3 bucket. Enable S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC in another Region. Establish a cross-Region VPC peer. Run a nightly rsync to copy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Backup to create a backup plan with a rule that takes a daily backup and replicates it to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "An ecommerce company is migrating its on-premises workload to the AWS Cloud. The workload \ncurrently consists of a web application and a backend Microsoft SQL database for storage. \n \nThe company expects a high volume of customers during a promotional event. The new \ninfrastructure in the AWS Cloud must be highly available and scalable. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the web application to two Amazon EC2 instances across two Availability Zones behind",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the web application to an Amazon EC2 instance that runs in an Auto Scaling group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the web application to Amazon EC2 instances that run in an Auto Scaling group across",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the web application to three Amazon EC2 instances across three Availability Zones",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A company has 15 employees. The company stores employee start dates in an Amazon \nDynamoDB table. The company wants to send an email message to each employee on the day \nof the employee's work anniversary. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a script that scans the DynamoDB table and uses Amazon Simple Notification Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a script that scans the DynamoDB table and uses Amazon Simple Queue Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nSNS for sending mails Lambda to scan the database + send the message to the SNS topic. Using a script on a EC2 will add maintenance on both the EC2 and the script + cronjobs are not reliable and can be hard to monitor properly.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company's application is running on Amazon EC2 instances within an Auto Scaling group \nbehind an Elastic Load Balancing (ELB) load balancer. Based on the application's history, the \ncompany anticipates a spike in traffic during a holiday each year. A solutions architect must \ndesign a strategy to ensure that the Auto Scaling group proactively increases capacity to \nminimize any performance impact on application users. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a recurring scheduled action to scale up the Auto Scaling group before the expected",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A company runs its application on Oracle Database Enterprise Edition. The company needs to \nmigrate the application and the database to AWS. The company can use the Bring Your Own \nLicense (BYOL) model while migrating to AWS. The application uses third-party database \nfeatures that require privileged access. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n438 \nA solutions architect must design a solution for the database migration. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon RDS for Oracle by using native tools. Replace the third-party",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon RDS Custom for Oracle by using native tools. Customize the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon RDS for PostgreSQL by using AWS Database Migration Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A large international university has deployed all of its compute services in the AWS Cloud. These \nservices include Amazon EC2, Amazon RDS, and Amazon DynamoDB. The university currently \nrelies on many custom scripts to back up its infrastructure. However, the university wants to \ncentralize management and automate data backups as much as possible by using AWS native \noptions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use third-party backup software with an AWS Storage Gateway tape gateway virtual tape library.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Backup to configure and monitor all backups for the services in use.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Config to set lifecycle management to take snapshots of all data sources on a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager State Manager to manage the configuration and monitoring of",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "A company wants to build a map of its IT infrastructure to identify and enforce policies on \nresources that pose security risks. The company's security team must be able to query data in \nthe IT infrastructure map and quickly identify security risks. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon RDS to store the data. Use SQL to query the data to identify security risks.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Neptune to store the data. Use SPARQL to query the data to identify security risks.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Redshift to store the data. Use SQL to query the data to identify security risks.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon DynamoDB to store the data. Use PartiQL to query the data to identify security",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon Neptune with SPARQL, a query language for graph databases, allows the security team to easily query the data in the IT infrastructure map to identify security risks. SPARQL is specifically designed for querying graph data and allows for complex queries to traverse relationships between resources efficiently. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A large company wants to provide its globally located developers separate, limited size, managed \nPostgreSQL databases for development purposes. The databases will be low volume. The \ndevelopers need the databases only when they are actively working. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Give the developers the ability to launch separate Amazon Aurora instances. Set up a process to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Develop an AWS Service Catalog product that enforces size restrictions for launching Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon Aurora Serverless cluster. Develop an AWS Service Catalog product to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Monitor AWS Trusted Advisor checks for idle Amazon RDS databases. Create a process to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nWith AWS Service Catalog, you can meet your compliance requirements while making sure your customers can quickly deploy the cloud resources they need.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 40,
    "text": "A company is building a web application that serves a content management system. The content \nmanagement system runs on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. Users \nare constantly adding and updating files, blogs, and other website assets in the content \nmanagement system. \n \nA solutions architect must implement a solution in which all the EC2 instances share up-to-date \nwebsite content with the least possible lag time. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the EC2 user data in the Auto Scaling group lifecycle policy to copy the website assets",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy the website assets to an Amazon Elastic File System (Amazon EFS) file system. Configure",
        "correct": true
      },
      {
        "id": 2,
        "text": "Copy the website assets to an Amazon S3 bucket. Ensure that each EC2 instance downloads the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Restore an Amazon Elastic Block Store (Amazon EBS) snapshot with the website assets. Attach",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company's web application consists of multiple Amazon EC2 instances that run behind an \nApplication Load Balancer in a VPC. An Amazon RDS for MySQL DB instance contains the data. \nThe company needs the ability to automatically detect and respond to suspicious or unexpected \nbehavior in its AWS environment. The company already has added AWS WAF to its architecture. \n \nWhat should a solutions architect do next to protect against threats?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty to perform threat detection. Configure Amazon EventBridge to filter for",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Firewall Manager to perform threat detection. Configure Amazon EventBridge to filter",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Inspector to perform threat detection and to update the AWS WAF rules. Create a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Macie to perform threat detection and to update the AWS WAF rules. Create a VPC",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. \nThe company does not want to use the default domain name for the distribution. Instead, the \ncompany wants to use a different domain name for the distribution. \n \nWhich solution will deploy the certificate without incurring any additional costs? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n441",
    "options": [
      {
        "id": 0,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/certificate-manager/pricing/ AWS Certificate Manager Pricing Public SSL/TLS certificates provisioned through AWS Certificate Manager are free. You pay only for the AWS resources you create to run your application. If you manage AWS Private Certificate Authority (CA) through ACM, refer to the AWS Private CA Pricing page for more details and examples.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company creates operations data and stores the data in an Amazon S3 bucket. For the \ncompany's annual audit, an external consultant needs to access an annual report that is stored in \nthe S3 bucket. The external consultant needs to access the report for 7 days. \n \nThe company must implement a solution to allow the external consultant access to only the \nreport. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a new S3 bucket that is configured to host a public static website. Migrate the operations",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable public access to the S3 bucket for 7 days. Remove access to the S3 bucket when the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user that has access to the report in the S3 bucket. Provide the access keys to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Generate a presigned URL that has the required access to the location of the report on the S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company plans to run a high performance computing (HPC) workload on Amazon EC2 \nInstances. The workload requires low-latency network performance and high network throughput \nwith tightly coupled node-to-node communication. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the EC2 instances to be part of a cluster placement group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch the EC2 instances with Dedicated Instance tenancy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch the EC2 instances as Spot Instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an On-Demand Capacity Reservation when the EC2 instances are launched.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 46,
    "text": "A company has primary and secondary data centers that are 500 miles (804.7 km) apart and \ninterconnected with high-speed fiber-optic cable. The company needs a highly available and \nsecure network connection between its data centers and a VPC on AWS for a mission-critical \nworkload. A solutions architect must choose a connection solution that provides maximum \nresiliency. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Two AWS Direct Connect connections from the primary data center terminating at two Direct",
        "correct": false
      },
      {
        "id": 1,
        "text": "A single AWS Direct Connect connection from each of the primary and secondary data centers",
        "correct": false
      },
      {
        "id": 2,
        "text": "Two AWS Direct Connect connections from each of the primary and secondary data centers",
        "correct": true
      },
      {
        "id": 3,
        "text": "A single AWS Direct Connect connection from each of the primary and secondary data centers",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company runs several Amazon RDS for Oracle On-Demand DB instances that have high \nutilization. The RDS DB instances run in member accounts that are in an organization in AWS \nOrganizations. \n \nThe company's finance team has access to the organization's management account and member \naccounts. The finance team wants to find ways to optimize costs by using AWS Trusted Advisor. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use the Trusted Advisor recommendations in the management account.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the Trusted Advisor recommendations in the member accounts where the RDS DB instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Review the Trusted Advisor checks for Amazon RDS Reserved Instance Optimization.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Review the Trusted Advisor checks for Amazon RDS Idle DB Instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Review the Trusted Advisor checks for compute optimization. Crosscheck the results by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/awssupport/latest/user/organizational-view.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 48,
    "text": "A solutions architect is creating an application. The application will run on Amazon EC2 instances \nin private subnets across multiple Availability Zones in a VPC. The EC2 instances will frequently \naccess large files that contain confidential information. These files are stored in Amazon S3 \nbuckets for processing. The solutions architect must optimize the network architecture to \nminimize data transfer costs. \n \nWhat should the solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n443",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the private subnets,",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a single NAT gateway in a public subnet. In the route tables for the private subnets, add a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS PrivateLink interface endpoint for Amazon S3 in the VPIn the route tables for the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create one NAT gateway for each Availability Zone in public subnets. In each of the route tables",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company wants to relocate its on-premises MySQL database to AWS. The database accepts \nregular imports from a client-facing application, which causes a high volume of write operations. \nThe company is concerned that the amount of traffic might be causing performance issues within \nthe application. \n \nHow should a solutions architect design the architecture on AWS?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage. Monitor",
        "correct": true
      },
      {
        "id": 1,
        "text": "Provision an Amazon RDS for MySQL DB instance with General Purpose SSD storage. Place an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an Amazon DocumentDB (with MongoDB compatibility) instance with a memory",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an Amazon Elastic File System (Amazon EFS) file system in General Purpose",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 50,
    "text": "A company runs an application in the AWS Cloud that generates sensitive archival data files. The \ncompany wants to rearchitect the application's data storage. The company wants to encrypt the \ndata files and to ensure that third parties do not have access to the data before the data is \nencrypted and sent to AWS. The company has already created an Amazon S3 bucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the S3 bucket to use client-side encryption with an Amazon S3 managed encryption",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the S3 bucket to use dual-layer server-side encryption with AWS KMS keys (SSE-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the application to use client-side encryption with a key stored in AWS Key Management",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company uses Amazon RDS with default backup settings for its database tier. The company \nneeds to make a daily backup of the database to meet regulatory requirements. The company \nmust retain the backups for 30 days. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Write an AWS Lambda function to create an RDS snapshot every day.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the RDS database to have a retention period of 30 days for automated backups.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Systems Manager Maintenance Windows to modify the RDS backup retention period.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a manual snapshot every day by using the AWS CLI. Modify the RDS backup retention",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "A company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. \nDuring peak usage hours when multiple users access and read the data, the monitoring system \nshows degradation of database performance for the write queries. The company wants to \nincrease the scalability of the application to meet peak usage demands. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a second Aurora DB cluster. Configure a copy job to replicate the users' data to the new",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon DynamoDB Accelerator (DAX) cluster in front of the existing Aurora DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Aurora read replica in the existing Aurora DB cluster. Update the application to use the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Redshift cluster. Copy the users' data to the Redshift cluster. Update the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company's near-real-time streaming application is running on AWS. As the data is ingested, a \njob runs on the data and takes 30 minutes to complete. The workload frequently experiences high \nlatency due to large amounts of incoming data. A solutions architect needs to design a scalable \nand serverless solution to enhance performance. \n \nWhich combination of steps should the solutions architect take? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Firehose to ingest the data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Lambda with AWS Step Functions to process the data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to ingest the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 instances in an Auto Scaling group to process the data.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Fargate with Amazon Elastic Container Service (Amazon ECS) to process the data.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 54,
    "text": "A company runs a web application on multiple Amazon EC2 instances in a VPC. The application \nneeds to write sensitive data to an Amazon S3 bucket. The data cannot be sent over the public \ninternet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint for Amazon S3. Create a route in the VPC route table to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an internal Network Load Balancer that has the S3 bucket as the target.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the S3 bucket inside the VPCreate a route in the VPC route table to the bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Direct Connect connection between the VPC and an S3 regional endpoint.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company runs its production workload on Amazon EC2 instances with Amazon Elastic Block \nStore (Amazon EBS) volumes. A solutions architect needs to analyze the current EBS volume \ncost and to recommend optimizations. The recommendations need to include estimated monthly \nsaving opportunities. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Inspector reporting to generate EBS volume recommendations for optimization.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Systems Manager reporting to determine EBS volume recommendations for",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudWatch metrics reporting to determine EBS volume recommendations for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Compute Optimizer to generate EBS volume recommendations for optimization.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Compute Optimizer helps avoid overprovisioning and underprovisioning four types of AWS resources - Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volumes, Amazon Elastic Container Service (ECS) services on AWS Fargate, and AWS Lambda functions - based on your utilization data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "A global company runs its workloads on AWS. The company's application uses Amazon S3 \nbuckets across AWS Regions for sensitive data storage and analysis. The company stores \nmillions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that \nare not versioning-enabled. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/blogs/aws/s3-storage-lens/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A company wants to enhance its ecommerce order-processing application that is deployed on \nAWS. The application must process each order exactly once without affecting the customer \nexperience during unpredictable traffic surges. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Put all the orders in the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Publish all the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a flow by using Amazon AppFlow. Send the orders to the flow. Configure an AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS X-Ray in the application to track the order requests. Configure the application to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company has two AWS accounts: Production and Development. The company needs to push \ncode changes in the Development account to the Production account. In the alpha phase, only \ntwo senior developers on the development team need access to the Production account. In the \nbeta phase, more developers will need access to perform testing. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create two policy documents by using the AWS Management Console in each account. Assign",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role in the Development account. Grant the IAM role access to the Production",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM role in the Production account. Define a trust policy that specifies the Development",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM group in the Production account. Add the group as a principal in a trust policy that",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company wants to restrict access to the content of its web application. The company needs to \nprotect the content by using authorization techniques that are available on AWS. The company \nalso wants to implement a serverless architecture for authorization and authentication that has \nlow login latency. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n447 \nThe solution must integrate with the web application and serve web content globally. The \napplication currently has a small user base, but the company expects the application's user base \nto increase. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Cognito for authentication. Implement Lambda@Edge for authorization.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Cognito for authentication. Implement AWS Lambda for authorization. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A development team uses multiple AWS accounts for its development, staging, and production \nenvironments. Team members have been launching large Amazon EC2 instances that are \nunderutilized. A solutions architect must prevent large instances from being launched in all \naccounts. \n \nHow can the solutions architect meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Update the IAM policies to deny the launch of large EC2 instances. Apply the policies to all users.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define a resource in AWS Resource Access Manager that prevents the launch of large EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM role in each account that denies the launch of large EC2 instances. Grant the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an organization in AWS Organizations in the management account with the default policy.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company has migrated a fleet of hundreds of on-premises virtual machines (VMs) to Amazon \nEC2 instances. The instances run a diverse fleet of Windows Server versions along with several \nLinux distributions. The company wants a solution that will automate inventory and updates of the \noperating systems. The company also needs a summary of common vulnerabilities of each \ninstance for regular monthly reviews. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Configure AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Deploy Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Shield Advanced, and configure monthly reports. Deploy AWS Config to automate",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon GuardDuty in the account to monitor all EC2 instances. Deploy AWS Config to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 62,
    "text": "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 \ninstances in an Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer. The \napplication connects to an Amazon DynamoDB table. \n \nFor disaster recovery (DR) purposes, the company wants to ensure that the application is \navailable from another AWS Region with minimal downtime. \n \nWhich solution will meet these requirements with the LEAST downtime?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS CloudFormation template to create EC2 instances, ELBs, and DynamoDB tables",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS CloudFormation template to create EC2 instances and an ELB to be launched",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCreate an Auto Scaling group and an ELB in the DR Region, configuring the DynamoDB table as a global table, and setting up DNS failover to the new ELB. This approach allows for quick failover since the infrastructure is already in place and only DNS needs to be updated to redirect traffic.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 63,
    "text": "A company runs an application on Amazon EC2 instances in a private subnet. The application \nneeds to store and retrieve data in Amazon S3 buckets. According to regulatory requirements, \nthe data must not travel across the public internet. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a NAT gateway to access the S3 buckets.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Storage Gateway to access the S3 buckets.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an S3 interface endpoint to access the S3 buckets.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an S3 gateway endpoint to access the S3 buckets.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n449 \nA company hosts an application on Amazon EC2 instances that run in a single Availability Zone. \nThe application is accessible by using the transport layer of the Open Systems Interconnection \n(OSI) model. The company needs the application architecture to have high availability. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure new EC2 instances in a different Availability Zone. Use Amazon Route 53 to route",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a Network Load Balancer in front of the EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a Network Load Balancer for TCP traffic to the instances. Configure an Application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group for the EC2 instances. Configure the Auto Scaling group to use",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon CloudWatch alarm. Configure the alarm to restart EC2 instances that",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  }
]