[
  {
    "id": 0,
    "text": "A company needs to optimize the cost of its Amazon EC2 instances. The company also needs to \nchange the type and family of its EC2 instances every 2-3 months. \n \nWhat should the company do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Purchase Partial Upfront Reserved Instances for a 3-year term.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase a No Upfront Compute Savings Plan for a 1-year term.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Purchase All Upfront Reserved Instances for a 1-year term.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase an All Upfront EC2 Instance Savings Plan for a 1-year term.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nEC2 Instance Savings Plans give you the flexibility to change your usage between instances WITHIN a family in that region. https://aws.amazon.com/savingsplans/compute-pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A solutions architect needs to review a company's Amazon S3 buckets to discover personally \nidentifiable information (PII). The company stores the PII data in the us-east-1 Region and us-\nwest-2 Region. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n316",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Macie in each Region. Create a job to analyze the data that is in Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS Security Hub for all Regions. Create an AWS Config rule to analyze the data that",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Inspector to analyze the data that is in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon GuardDuty to analyze the data that is in Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon Macie is designed specifically for discovering and classifying sensitive data like PII in S3. This makes it the optimal service to use. Macie can be enabled directly in the required Regions rather than enabling it across all Regions which is unnecessary. This minimizes overhead. Macie can be set up to automatically scan the specified S3 buckets on a schedule. No need to create separate jobs. Security Hub is for security monitoring across AWS accounts, not specific for PII discovery. More overhead than needed. Inspector and GuardDuty are not built for PII discovery in S3 buckets. They provide broader security capabilities.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company's SAP application has a backend SQL Server database in an on-premises \nenvironment. The company wants to migrate its on-premises application and database server to \nAWS. The company needs an instance type that meets the high demands of its SAP database. \nOn-premises performance data shows that both the SAP application and the database have high \nmemory utilization. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use the compute optimized instance family for the application. Use the memory optimized",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the storage optimized instance family for both the application and the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the memory optimized instance family for both the application and the database.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the high performance computing (HPC) optimized instance family for the application. Use the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nSince both the app and database have high memory needs, the memory optimized family like R5 instances meet those requirements well. Using the same instance family simplifies management and operations, rather than mixing instance types. Compute optimized instances may not provide enough memory for the SAP app's needs. Storage optimized is overkill for the database's compute and memory needs. HPC is overprovisioned for the SAP app.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "A company runs an application in a VPC with public and private subnets. The VPC extends \nacross multiple Availability Zones. The application runs on Amazon EC2 instances in private \nsubnets. The application uses an Amazon Simple Queue Service (Amazon SQS) queue. \n \nA solutions architect needs to design a secure solution to establish a connection between the \nEC2 instances and the SQS queue. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n317 \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private",
        "correct": true
      },
      {
        "id": 1,
        "text": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement a gateway endpoint for Amazon SQS. Add a NAT gateway to the private subnets.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAn interface VPC endpoint is a private way to connect to AWS services without having to expose your VPC to the public internet. This is the most secure way to connect to Amazon SQS from the private subnets. Configuring the endpoint to use the private subnets ensures that the traffic between the EC2 instances and the SQS queue is only within the VPC. This helps to protect the traffic from being intercepted by a malicious actor. Adding a security group to the endpoint that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets further restricts the traffic to only the authorized sources. This helps to prevent unauthorized access to the SQS queue.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A solutions architect is using an AWS CloudFormation template to deploy a three-tier web \napplication. The web application consists of a web tier and an application tier that stores and \nretrieves user data in Amazon DynamoDB tables. The web and application tiers are hosted on \nAmazon EC2 instances, and the database tier is not publicly accessible. The application EC2 \ninstances need to access the DynamoDB tables without exposing API credentials in the template. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role to read the DynamoDB tables. Associate the role with the application",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role that has the required permissions to read and write from the DynamoDB",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the parameter section in the AWS CloudFormation template to have the user input access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user in the AWS CloudFormation template that has the required permissions to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A solutions architect manages an analytics application. The application stores large amounts of \nsemistructured data in an Amazon S3 bucket. The solutions architect wants to use parallel data \nprocessing to process the data more quickly. The solutions architect also wants to use \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n318 \ninformation that is stored in an Amazon Redshift database to enrich the data. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon Redshift data to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUse Amazon EMR to process the semi-structured data in Amazon S3. EMR provides a managed Hadoop framework optimized for processing large datasets in S3. EMR supports parallel data processing across multiple nodes to speed up the processing. EMR can integrate directly with Amazon Redshift using the EMR-Redshift integration. This allows querying the Redshift data from EMR and joining it with the S3 data. This enables enriching the semi-structured S3 data with the information stored in Redshift.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A company has two VPCs that are located in the us-west-2 Region within the same AWS \naccount. The company needs to allow network traffic between these VPCs. Approximately 500 \nGB of data transfer will occur between the VPCs each month. \n \nWhat is the MOST cost-effective solution to connect these VPCs?",
    "options": [
      {
        "id": 0,
        "text": "Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nVPC peering provides private connectivity between VPCs without using public IP space. Data transferred between peered VPCs is free as long as they are in the same region. 500 GB/month inter-VPC data transfer fits within peering free tier. Transit Gateway (Option A) incurs hourly charges plus data transfer fees. More costly than peering. Site-to-Site VPN (Option B) incurs hourly charges and data transfer fees. More expensive than peering. Direct Connect (Option D) has high hourly charges and would be overkill for this use case.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company hosts multiple applications on AWS for different product lines. The applications use \ndifferent compute resources, including Amazon EC2 instances and Application Load Balancers. \nThe applications run in different AWS accounts under the same organization in AWS \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n319 \nOrganizations across multiple AWS Regions. Teams for each product line have tagged each \ncompute resource in the individual accounts. \n \nThe company wants more details about the cost for each product line from the consolidated \nbilling feature in Organizations. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Select a specific AWS generated tag in the AWS Billing console.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Select a specific user-defined tag in the AWS Billing console.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Select a specific user-defined tag in the AWS Resource Groups console.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Activate the selected tag from each AWS account.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Activate the selected tag from the Organizations management account.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUser-defined tags were created by each product team to identify resources. Selecting the relevant tag in the Billing console will group costs. The tag must be activated from the Organizations management account to consolidate billing across all accounts. AWS generated tags are predefined by AWS and won't align to product lines. Resource Groups (Option C) helps manage resources but not billing. Activating the tag from each account (Option D) is not needed since Organizations centralizes billing.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 8,
    "text": "A company's solutions architect is designing an AWS multi-account solution that uses AWS \nOrganizations. The solutions architect has organized the company's accounts into organizational \nunits (OUs). \n \nThe solutions architect needs a solution that will identify any changes to the OU hierarchy. The \nsolution also needs to notify the company's operations team of any changes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Provision the AWS accounts by using AWS Control Tower. Use account drift notifications to",
        "correct": true
      },
      {
        "id": 1,
        "text": "Provision the AWS accounts by using AWS Control Tower. Use AWS Config aggregated rules to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Service Catalog to create accounts in Organizations. Use an AWS CloudTrail",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS CloudFormation templates to create accounts in Organizations. Use the drift detection",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe key advantages you highlight of Control Tower are convincing: Fully managed service simplifies multi-account setup. Built-in account drift notifications detect OU changes automatically. More scalable and less complex than Config rules or CloudTrail. Better security and compliance guardrails than custom options. Lower operational overhead compared to other solution Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company's website handles millions of requests each day, and the number of requests \ncontinues to increase. A solutions architect needs to improve the response time of the web \napplication. The solutions architect determines that the application needs to decrease latency \nwhen retrieving product details from the Amazon DynamoDB table. \n \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Set up a DynamoDB Accelerator (DAX) cluster. Route all read requests through DAX.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up Amazon ElastiCache for Redis between the DynamoDB table and the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon ElastiCache for Memcached between the DynamoDB table and the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon DynamoDB Streams on the table, and have AWS Lambda read from the table",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nDAX provides a DynamoDB-compatible caching layer to reduce read latency. It is purpose-built for accelerating DynamoDB workloads. Using DAX requires minimal application changes - only read requests are routed through it. DAX handles caching logic automatically without needing complex integration code. ElastiCache Redis/Memcached (Options B/C) require more integration work to sync DynamoDB data. Using Lambda and Streams to populate ElastiCache (Option D) is a complex event-driven approach requiring ongoing maintenance. DAX plugs in seamlessly to accelerate DynamoDB with very little operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A solutions architect needs to ensure that API calls to Amazon DynamoDB from Amazon EC2 \ninstances in a VPC do not travel across the internet. \n \nWhich combination of steps should the solutions architect take to meet this requirement? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a route table entry for the endpoint.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a gateway endpoint for DynamoDB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an interface endpoint for Amazon EC2.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an elastic network interface for the endpoint in each of the subnets of the VPC.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a security group entry in the endpoint's security group to provide access.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-ddb.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company runs its applications on both Amazon Elastic Kubernetes Service (Amazon EKS) \nclusters and on-premises Kubernetes clusters. The company wants to view all clusters and \nworkloads from a central location. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n321",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch Container Insights to collect and group the cluster information.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EKS Connector to register and connect all Kubernetes clusters.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Systems Manager to collect and view the cluster information.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EKS Anywhere as the primary cluster to view the other clusters with native",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nYou can use Amazon EKS Connector to register and connect any conformant Kubernetes cluster to AWS and visualize it in the Amazon EKS console. After a cluster is connected, you can see the status, configuration, and workloads for that cluster in the Amazon EKS console. https://docs.aws.amazon.com/eks/latest/userguide/eks-connector.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company is building an ecommerce application and needs to store sensitive customer \ninformation. The company needs to give customers the ability to complete purchase transactions \non the website. The company also needs to ensure that sensitive customer data is protected, \neven from database administrators. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store sensitive data in an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store sensitive data in Amazon RDS for MySQL. Use AWS Key Management Service (AWS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store sensitive data in Amazon S3. Use AWS Key Management Service (AWS KMS) server-side",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store sensitive data in Amazon FSx for Windows Server. Mount the file share on application",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nRDS MySQL provides a fully managed database service well suited for an ecommerce application. AWS KMS client-side encryption allows encrypting sensitive data before it hits the database. The data remains encrypted at rest. This protects sensitive customer data from database admins and privileged users. EBS encryption (Option A) protects data at rest but not in use. IAM roles don't prevent admin access. S3 (Option C) encrypts data at rest on the server side. Bucket policies don't restrict admin access. FSx file permissions (Option D) don't prevent admin access to unencrypted data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company has an on-premises MySQL database that handles transactional data. The company \nis migrating the database to the AWS Cloud. The migrated database must maintain compatibility \nwith the company's applications that use the database. The migrated database also must scale \nautomatically during periods of increased demand. \n \nWhich migration solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n322",
    "options": [
      {
        "id": 0,
        "text": "Use native MySQL tools to migrate the database to Amazon RDS for MySQL. Configure elastic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon Redshift by using the mysqldump utility. Turn on Auto Scaling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Aurora.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nDMS provides an easy migration path from MySQL to Aurora while minimizing downtime. Aurora is a MySQL-compatible relational database service that will maintain compatibility with the company's applications. Aurora Auto Scaling allows the database to automatically scale up and down based on demand to handle increased workloads. RDS MySQL (Option A) does not scale as well as the Aurora architecture. Redshift (Option B) is for analytics, not transactional data, and may not be compatible. DynamoDB (Option D) is a NoSQL datastore and lacks MySQL compatibility.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company runs multiple Amazon EC2 Linux instances in a VPC across two Availability Zones. \nThe instances host applications that use a hierarchical directory structure. The applications need \nto read and write rapidly and concurrently to shared storage. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket. Allow access from all the EC2 instances in the VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a file system on a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create file systems on Amazon Elastic Block Store (Amazon EBS) volumes that are attached to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nHow is Amazon EFS different than Amazon S3? Amazon EFS provides shared access to data using a traditional file sharing permissions model and hierarchical directory structure via the NFSv4 protocol. Applications that access data using a standard file system interface provided through the operating system can use Amazon EFS to take advantage of the scalability and reliability of file storage in the cloud without writing any new code or adjusting applications. Amazon S3 is an object storage platform that uses a simple API for storing and accessing data. Applications that do not require a file system structure and are designed to work with object storage can use Amazon S3 as a massively scalable, durable, low-cost object storage solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A solutions architect is designing a workload that will store hourly energy consumption by \nbusiness tenants in a building. The sensors will feed a database through HTTP requests that will \nadd up usage for each tenant. The solutions architect must use managed services when possible. \nThe workload will receive more features in the future as the solutions architect adds independent \ncomponents. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n323 \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors,",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors,",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A solutions architect is designing the storage architecture for a new web application used for \nstoring and viewing engineering drawings. All application components will be deployed on the \nAWS infrastructure. \n \nThe application design must support caching to minimize the amount of time that users wait for \nthe engineering drawings to load. The application must be able to store petabytes of data. \n \nWhich combination of storage and caching should the solutions architect use?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 with Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon S3 Glacier with Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway with Amazon ElastiCache",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "An Amazon EventBridge rule targets a third-party API. The third-party API has not received any \nincoming traffic. A solutions architect needs to determine whether the rule conditions are being \nmet and if the rule's target is being invoked. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Check for metrics in Amazon CloudWatch in the namespace for AWS/Events.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Review events in the Amazon Simple Queue Service (Amazon SQS) dead-letter queue.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Check for the events in Amazon CloudWatch Logs.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Check the trails in AWS CloudTrail for the EventBridge events.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A company has a large workload that runs every Friday evening. The workload runs on Amazon \nEC2 instances that are in two Availability Zones in the us-east-1 Region. Normally, the company \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n324 \nmust run no more than two instances at all times. However, the company wants to scale up to six \ninstances each Friday to handle a regularly repeating increased workload. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a reminder in Amazon EventBridge to scale the instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto Scaling group that has a scheduled action.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group that uses manual scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group that uses automatic scaling.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is creating a REST API. The company has strict requirements for the use of TLS. The \ncompany requires TLSv1.3 on the API endpoints. The company also requires a specific public \nthird-party certificate authority (CA) to sign the TLS certificate. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a local machine to create a certificate that is signed by the third-party CImport the certificate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) to create a certificate that is signed by the third-party CA.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Certificate Manager (ACM) is a service that lets you easily provision, manage, and deploy SSL/TLS certificates for use with AWS services and your internal resources. By creating a certificate in ACM that is signed by the third-party CA, the company can meet its requirement for a specific public third-party CA to sign the TLS certificate.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A company runs an application on AWS. The application receives inconsistent amounts of usage. \nThe application uses AWS Direct Connect to connect to an on-premises MySQL-compatible \ndatabase. The on-premises database consistently uses a minimum of 2 GiB of memory. \n \nThe company wants to migrate the on-premises database to a managed AWS service. The \ncompany wants to use auto scaling capabilities to manage unexpected workload increases. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon DynamoDB database with default read and write capacity settings.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an Amazon Aurora database with a minimum capacity of 1 Aurora capacity unit (ACU).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an Amazon Aurora Serverless v2 database with a minimum capacity of 1 Aurora",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision an Amazon RDS for MySQL database with 2 GiB of memory.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAurora Serverless v2 provides auto-scaling so the database can handle inconsistent workloads and spikes automatically without admin intervention. It can scale down to zero when not in use to minimize costs. The minimum 1 ACU capacity is sufficient to replace the on-prem 2 GiB database based on the info given. Serverless capabilities reduce admin overhead for capacity management. DynamoDB lacks MySQL compatibility and requires more hands-on management. RDS and provisioned Aurora require manually resizing instances to scale, increasing admin overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A company wants to use an event-driven programming model with AWS Lambda. The company \nwants to reduce startup latency for Lambda functions that run on Java 11. The company does not \nhave strict latency requirements for the applications. The company wants to reduce cold starts \nand outlier latencies when a function scales up. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure Lambda provisioned concurrency.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the timeout of the Lambda functions.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Increase the memory of the Lambda functions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Lambda SnapStart.1",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nLambda SnapStart for Java can improve startup performance for latency-sensitive applications by up to 10x at no extra cost, typically with no changes to your function code. https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 22,
    "text": "A financial services company launched a new application that uses an Amazon RDS for MySQL \ndatabase. The company uses the application to track stock market trends. The company needs to \noperate the application for only 2 hours at the end of each week. The company needs to optimize \nthe cost of running the database. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the existing RDS for MySQL database to an Aurora Serverless v2 MySQL database",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the existing RDS for MySQL database to an Aurora MySQL database cluster.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the existing RDS for MySQL database to an Amazon EC2 instance that runs MySQL.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the existing RDS for MySQL database to an Amazon Elastic Container Service (Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAurora Serverless v2 scales compute capacity automatically based on actual usage, down to zero when not in use. This minimizes costs for intermittent usage. Since it only runs for 2 hours per week, the application is ideal for a serverless architecture like Aurora Serverless. Aurora Serverless v2 charges per second when the database is active, unlike RDS which charges hourly. Aurora Serverless provides higher availability than self-managed MySQL on EC2 or ECS. Using reserved EC2 instances or ECS still incurs charges when not in use versus the fine-grained scaling of serverless. Standard Aurora clusters have a minimum capacity unlike the auto-scaling serverless architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A company deploys its applications on Amazon Elastic Kubernetes Service (Amazon EKS) \nbehind an Application Load Balancer in an AWS Region. The application needs to store data in a \nPostgreSQL database engine. The company wants the data in the database to be highly \navailable. The company also needs increased capacity for read workloads. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon DynamoDB database table configured with global tables.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon RDS database with Multi-AZ deployments.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon RDS database with Multi-AZ DB cluster deployment.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon RDS database configured with cross-Region read replicas.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nDB cluster deployment can scale read workloads by adding read replicas. This provides increased capacity for read workloads without impacting the write workload.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company is building a RESTful serverless web application on AWS by using Amazon API \nGateway and AWS Lambda. The users of this web application will be geographically distributed, \nand the company wants to reduce the latency of API requests to these users. \n \nWhich type of endpoint should a solutions architect use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Private endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "Regional endpoint",
        "correct": false
      },
      {
        "id": 2,
        "text": "Interface VPC endpoint",
        "correct": false
      },
      {
        "id": 3,
        "text": "Edge-optimized endpoint",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAn edge-optimized API endpoint typically routes requests to the nearest CloudFront Point of Presence (POP), which could help in cases where your clients are geographically distributed. This is the default endpoint type for API Gateway REST APIs. https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint- types.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company uses an Amazon CloudFront distribution to serve content pages for its website. The \ncompany needs to ensure that clients use a TLS certificate when accessing the company's \nwebsite. The company wants to automate the creation and renewal of the TLS certificates. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Use a CloudFront security policy to create a certificate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a CloudFront origin access control (OAC) to create a certificate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) to create a certificate. Use DNS validation for the domain.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Certificate Manager (ACM) to create a certificate. Use email validation for the domain.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Certificate Manager (ACM) provides free public TLS/SSL certificates and handles certificate renewals automatically. Using DNS validation with ACM is operationally efficient since it automatically makes changes to Route 53 rather than requiring manual validation steps. ACM integrates natively with CloudFront distributions for delivering HTTPS content. CloudFront security policies and origin access controls do not issue TLS certificates. Email validation requires manual steps to approve the domain validation emails for each renewal.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company deployed a serverless application that uses Amazon DynamoDB as a database layer. \nThe application has experienced a large increase in users. The company wants to improve \ndatabase response time from milliseconds to microseconds and to cache requests to the \ndatabase. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use DynamoDB Accelerator (DAX).",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon Redshift.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to Amazon RDS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Redis.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement - from milliseconds to microseconds - even at millions of requests per second.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 27,
    "text": "A company runs an application that uses Amazon RDS for PostgreSQL. The application receives \ntraffic only on weekdays during business hours. The company wants to optimize costs and \nreduce operational overhead based on this usage. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use the Instance Scheduler on AWS to configure start and stop schedules.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Turn off automatic backups. Create weekly manual snapshots of the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a custom AWS Lambda function to start and stop the database based on minimum CPU",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase All Upfront reserved DB instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe Instance Scheduler on AWS solution automates the starting and stopping of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) instances. This solution helps reduce operational costs by stopping resources that are not in use and starting them when they are needed. The cost savings can be significant if you leave all of your instances running at full utilization continuously. https://aws.amazon.com/solutions/implementations/instance-scheduler-on-aws/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "A company uses locally attached storage to run a latency-sensitive application on premises. The \ncompany is using a lift and shift method to move the application to the AWS Cloud. The company \ndoes not want to change the application architecture. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for Lustre",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 29,
    "text": "A company runs a stateful production application on Amazon EC2 instances. The application \nrequires at least two EC2 instances to always be running. \n \nA solutions architect needs to design a highly available and fault-tolerant architecture for the \napplication. The solutions architect creates an Auto Scaling group of EC2 instances. \n \nWhich set of additional steps should the solutions architect take to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set the Auto Scaling group's minimum capacity to two. Deploy one On-Demand Instance in one",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set the Auto Scaling group's minimum capacity to two. Deploy four Spot Instances in one",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy setting the Auto Scaling group's minimum capacity to four, the architect ensures that there are always at least two running instances. Deploying two On-Demand Instances in each of two Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 30,
    "text": "An ecommerce company uses Amazon Route 53 as its DNS provider. The company hosts its \nwebsite on premises and in the AWS Cloud. The company's on-premises data center is near the \nus-west-1 Region. The company uses the eu-central-1 Region to host the website. The company \nwants to minimize load time for the website as much as possible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up a geolocation routing policy. Send the traffic that is near us-west-1 to the on-premises data",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up a simple routing policy that routes all traffic that is near eu-central-1 to eu-central-1 and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a latency routing policy. Associate the policy with us-west-1.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a weighted routing policy. Split the traffic evenly between eu-central-1 and the on-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A company has 5 PB of archived data on physical tapes. The company needs to preserve the \ndata on the tapes for another 10 years for compliance purposes. The company wants to migrate \nto AWS in the next 6 months. The data center that stores the tapes has a 1 Gbps uplink internet \nconnectivity. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Read the data from the tapes on premises. Stage the data in a local NFS storage. Use AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an on-premises backup application to read the data from the tapes and to write directly to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Order multiple AWS Snowball devices that have Tape Gateway. Copy the physical tapes to virtual",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an on-premises Tape Gateway. Create virtual tapes in the AWS Cloud. Use backup",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 32,
    "text": "A company is deploying an application that processes large quantities of data in parallel. The \ncompany plans to use Amazon EC2 instances for the workload. The network architecture must be \nconfigurable to prevent groups of nodes from sharing the same underlying hardware. \n \nWhich networking solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Run the EC2 instances in a spread placement group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Group the EC2 instances in separate accounts.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the EC2 instances with dedicated tenancy.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the EC2 instances with shared tenancy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nConfiguring the EC2 instances with dedicated tenancy ensures that each instance will run on isolated, single-tenant hardware. This meets the requirement to prevent groups of nodes from sharing underlying hardware. A spread placement group only provides isolation at the Availability Zone level. Instances could still share hardware within an AZ.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 \ncapacity in a failover AWS Region. Business requirements state that the DR strategy must meet \ncapacity in the failover Region. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Purchase On-Demand Instances in the failover Region.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase an EC2 Savings Plan in the failover Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase regional Reserved Instances in the failover Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase a Capacity Reservation in the failover Region.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company has five organizational units (OUs) as part of its organization in AWS Organizations. \nEach OU correlates to the five businesses that the company owns. The company's research and \ndevelopment (R&D) business is separating from the company and will need its own organization. \nA solutions architect creates a separate new management account for this purpose. \n \nWhat should the solutions architect do next in the new management account?",
    "options": [
      {
        "id": 0,
        "text": "Have the R&D AWS account be part of both organizations during the transition.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Invite the R&D AWS account to be part of the new organization after the R&D AWS account has",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new R&D AWS account in the new organization. Migrate resources from the prior R&D",
        "correct": false
      },
      {
        "id": 3,
        "text": "Have the R&D AWS account join the new organization. Make the new management account a",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/mt/migrating-accounts-between-aws-organizations-with- consolidated-billing-to-all-features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 35,
    "text": "A company is designing a solution to capture customer activity in different web applications to \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n331 \nprocess analytics and make predictions. Customer activity in the web applications is \nunpredictable and can increase suddenly. The company requires a solution that integrates with \nother web applications. The solution must include an authorization step for security purposes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda- authorizer.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances \nthat run Microsoft SQL Server Enterprise Edition. The company's current recovery point objective \n(RPO) and recovery time objective (RTO) are 24 hours. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a cross-Region read replica and promote the read replica to the primary instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy automatic snapshots to another Region every 24 hours.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon RDS creates and saves automated backups of your DB instance or Multi-AZ DB cluster during the backup window of your DB instance. RDS creates a storage volume snapshot of your DB instance, backing up the entire DB instance and not just individual databases. RDS saves the automated backups of your DB instance according to the backup retention period that you specify. If necessary, you can recover your DB instance to any point in time during the backup retention period.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 37,
    "text": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an \nApplication Load Balancer that has sticky sessions enabled. The web server currently hosts the \nuser session state. The company wants to ensure high availability and avoid user session state \nloss in the event of a web server outage. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n332",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon ElastiCache for Memcached instance to store the session data. Update the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon ElastiCache for Redis to store the session state. Update the application to use",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an AWS Storage Gateway cached volume to store session data. Update the application to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS to store the session state. Update the application to use Amazon RDS to store",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nElastiCache Redis provides in-memory caching that can deliver microsecond latency for session data. Redis supports replication and multi-AZ which can provide high availability for the cache. The application can be updated to store session data in ElastiCache Redis rather than locally on the web servers. If a web server fails, the user can be routed via the load balancer to another web server which can retrieve their session data from the highly available ElastiCache Redis cluster.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A company migrated a MySQL database from the company's on-premises data center to an \nAmazon RDS for MySQL DB instance. The company sized the RDS DB instance to meet the \ncompany's average daily workload. Once a month, the database performs slowly when the \ncompany runs queries for a report. The company wants to have the ability to run reports and \nmaintain the performance of the daily workloads. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a read replica of the database. Direct the queries to the read replica.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a backup of the database. Restore the backup to another DB instance. Direct the queries",
        "correct": false
      },
      {
        "id": 2,
        "text": "Export the data to Amazon S3. Use Amazon Athena to query the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Resize the DB instance to accommodate the additional workload.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A company runs a container application by using Amazon Elastic Kubernetes Service (Amazon \nEKS). The application includes microservices that manage customers and place orders. The \ncompany needs to route incoming requests to the appropriate microservices. \n \nWhich solution will meet this requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Load Balancer Controller to provision a Network Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the AWS Load Balancer Controller to provision an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an AWS Lambda function to connect the requests to Amazon EKS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway to connect the requests to Amazon EKS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAPI Gateway provides an entry point to your microservices. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 40,
    "text": "A company uses AWS and sells access to copyrighted images. The company's global customer \nbase needs to be able to access these images quickly. The company must deny access to users \nfrom specific countries. The company wants to minimize costs as much as possible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 to store the images. Turn on multi-factor authentication (MFA) and public bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 to store the images. Create an IAM user for each customer. Add the users to a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 instances that are behind Application Load Balancers (ALBs) to store the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A solutions architect is designing a highly available Amazon ElastiCache for Redis based \nsolution. The solutions architect needs to ensure that failures do not result in performance \ndegradation or loss of data locally and within an AWS Region. The solution needs to provide high \navailability at the node level and at the Region level. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Multi-AZ Redis replication groups with shards that contain multiple nodes.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Multi-AZ Redis cluster with more than one read replica in the replication group.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Redis shards that contain multiple nodes with Auto Scaling turned on.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "A company plans to migrate to AWS and use Amazon EC2 On-Demand Instances for its \napplication. During the migration testing phase, a technical team observes that the application \ntakes a long time to launch and load memory to become fully productive. \n \nWhich solution will reduce the launch time of the application during the next testing phase?",
    "options": [
      {
        "id": 0,
        "text": "Launch two or more EC2 On-Demand Instances. Turn on auto scaling features and make the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch EC2 Spot Instances to support the application and to scale the application so it is",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2 Auto Scaling",
        "correct": true
      },
      {
        "id": 3,
        "text": "Launch EC2 On-Demand Instances with Capacity Reservations. Start additional EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nWith Amazon EC2 hibernation enabled, you can maintain your EC2 instances in a \"pre-warmed\" state so these can get to a productive state faster.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A company's applications run on Amazon EC2 instances in Auto Scaling groups. The company \nnotices that its applications experience sudden traffic increases on random days of the week. The \ncompany wants to maintain application performance during sudden traffic increases. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use manual scaling to change the size of the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use predictive scaling to change the size of the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use dynamic scaling to change the size of the Auto Scaling group.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use schedule scaling to change the size of the Auto Scaling group.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nDynamic Scaling - This is yet another type of Auto Scaling in which the number of EC2 instances is changed automatically depending on the signals received. Dynamic Scaling is a good choice when there is a high volume of unpredictable traffic.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "An ecommerce application uses a PostgreSQL database that runs on an Amazon EC2 instance. \nDuring a monthly sales event, database usage increases and causes database connection issues \nfor the application. The traffic is unpredictable for subsequent monthly sales events, which \nimpacts the sales forecast. The company needs to maintain performance when there is an \nunpredictable increase in traffic. \n \nWhich solution resolves this issue in the MOST cost-effective way?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the PostgreSQL database to Amazon Aurora Serverless v2.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable auto scaling for the PostgreSQL database on the EC2 instance to accommodate",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a larger instance type.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the PostgreSQL database to Amazon Redshift to accommodate increased usage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAurora Serverless v2 got autoscaling, highly available and cheaper when compared to the other options.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 45,
    "text": "A company hosts an internal serverless application on AWS by using Amazon API Gateway and \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n335 \nAWS Lambda. The company's employees report issues with high latency when they begin using \nthe application each day. The company wants to reduce latency. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the API Gateway throttling limit.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a scheduled scaling to increase Lambda provisioned concurrency before employees begin",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudWatch alarm to initiate a Lambda function as a target for the alarm at",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase the Lambda function memory.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/compute/scheduling-aws-lambda-provisioned-concurrency-for- recurring-peak-usage/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A research company uses on-premises devices to generate data for analysis. The company \nwants to use the AWS Cloud to analyze the data. The devices generate .csv files and support \nwriting the data to an SMB file share. Company analysts must be able to use SQL commands to \nquery the data. The analysts will run queries periodically throughout the day. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Storage Gateway on premises in Amazon S3 File Gateway mode.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an AWS Storage Gateway on premises in Amazon FSx File Gateway made.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an AWS Glue crawler to create a table based on the data that is in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon EMR cluster with EMR File System (EMRFS) to query the data that is in",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up an Amazon Redshift cluster to query the data that is in Amazon S3. Provide access to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon- s3/ https://aws.amazon.com/storagegateway/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A company wants to use Amazon Elastic Container Service (Amazon ECS) clusters and Amazon \nRDS DB instances to build and run a payment processing application. The company will run the \napplication in its on-premises data center for compliance purposes. \n \nA solutions architect wants to use AWS Outposts as part of the solution. The solutions architect is \nworking with the company's operational team to build the application. \n \nWhich activities are the responsibility of the company's operational team? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Providing resilient power and network connectivity to the Outposts racks",
        "correct": true
      },
      {
        "id": 1,
        "text": "Managing the virtualization hypervisor, storage systems, and the AWS services that run on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Physical security and access controls of the data center environment",
        "correct": false
      },
      {
        "id": 3,
        "text": "Availability of the Outposts infrastructure including the power supplies, servers, and networking",
        "correct": false
      },
      {
        "id": 4,
        "text": "Physical maintenance of Outposts components",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-outposts-high-availability-design/aws- outposts-high-availability-design.html With Outposts, you are responsible for providing resilient power and network connectivity to the Outpost racks to meet your availability requirements for workloads running on Outposts. You are responsible for the physical security and access controls of the data center environment. You must provide sufficient power, space, and cooling to keep the Outpost operational and network connections to connect the Outpost back to the Region. Since Outpost capacity is finite and determined by the size and number of racks AWS installs at your site, you must decide how much EC2, EBS, and S3 on Outposts capacity you need to run your initial workloads, accommodate future growth, and to provide extra capacity to mitigate server failures and maintenance events.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company is planning to migrate a TCP-based application into the company's VPC. The \napplication is publicly accessible on a nonstandard TCP port through a hardware appliance in the \ncompany's data center. This public endpoint can process up to 3 million requests per second with \nlow latency. The company requires the same level of performance for the new public endpoint in \nAWS. \n \nWhat should a solutions architect recommend to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly accessible over the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an Application Load Balancer (ALB). Configure the ALB to be publicly accessible over the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon CloudFront distribution that listens on the TCP port that the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon API Gateway API that is configured with the TCP port that the application",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSince the company requires the same level of performance for the new public endpoint in AWS. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n337 \ncompany wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a DB snapshot of the RDS for PostgreSQL DB instance to populate a new Aurora",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use data import from Amazon S3 to migrate the database to an Aurora PostgreSQL DB cluster.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the pg_dump utility to back up the RDS for PostgreSQL database. Restore the backup to a",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating .html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 50,
    "text": "A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon \nElastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 \ninstance can be recovered after a disaster. \n \nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "options": [
      {
        "id": 0,
        "text": "Take a snapshot of the EBS storage that is attached to each EC2 instance. Create an AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Take a snapshot of the EBS storage that is attached to each EC2 instance. Use AWS Elastic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function to take a snapshot of the EBS storage that is attached to each",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Backup automates backup of resources like EBS volumes. It allows defining backup policies for groups of resources. This removes the need to manually create backups for each resource. The AWS Backup API and CLI allow programmatic control of backup plans and restores. This enables restoring hundreds of EC2 instances programmatically after a disaster instead of manually. AWS Backup handles cleanup of old backups based on policies to minimize storage costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company recently migrated to the AWS Cloud. The company wants a serverless solution for \nlarge-scale parallel on-demand processing of a semistructured dataset. The data consists of logs, \nmedia files, sales transactions, and IoT sensor data that is stored in Amazon S3. The company \nwants the solution to process thousands of items in the dataset in parallel. \n \nWhich solution will meet these requirements with the MOST operational efficiency? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n338",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Step Functions Map state in Inline mode to process the data in parallel.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Glue to process the data in parallel.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use several AWS Lambda functions to process the data in parallel.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Step Functions allows you to orchestrate and scale distributed processing using the Map state. The Map state can process items in a large dataset in parallel by distributing the work across multiple resources. Using the Map state in Distributed mode will automatically handle the parallel processing and scaling. Step Functions will add more workers to process the data as needed. Step Functions is serverless so there are no servers to manage. It will scale up and down automatically based on demand.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a \n500 Mbps uplink to the internet. Other on-premises applications share the uplink. The company \ncan use 80% of the internet bandwidth for this one-time migration task. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS DataSync to migrate the data to Amazon S3 and to automatically verify the data.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use rsync to transfer the data directly to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS CLI and multiple copy processes to send the data directly to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company has several on-premises Internet Small Computer Systems Interface (ISCSI) network \nstorage servers. The company wants to reduce the number of these servers by moving to the \nAWS Cloud. A solutions architect must provide low-latency access to frequently used data and \nreduce the dependency on on-premises servers with a minimal number of infrastructure changes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon S3 File Gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon Elastic Block Store (Amazon EBS) storage with backups to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway volume gateway that is configured with stored volumes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/storagegateway/latest/vgw/WhatIsStorageGateway.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A solutions architect is designing an application that will allow business users to upload objects to \nAmazon S3. The solution needs to maximize object durability. Objects also must be readily \navailable at any time and for any length of time. Users will access objects frequently within the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n339 \nfirst 30 days after the objects are uploaded, but users are much less likely to access objects that \nare older than 30 days. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 One",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store all the objects in S3 Intelligent-Tiering with an S3 Lifecycle rule to transition the objects to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company has migrated a two-tier application from its on-premises data center to the AWS \nCloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General \nPurpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application is designed to \nprocess and store documents in the database as binary large objects (blobs) with an average \ndocument size of 6 MB. \n \nThe database size has grown over time, reducing the performance and increasing the cost of \nstorage. The company must improve the database performance and needs a solution that is \nhighly available and resilient. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Reduce the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the RDS DB instance size. Increase the storage capacity to 24 TiChange the storage",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon DynamoDB table. Update the application to use DynamoDB. Use AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "A company has an application that serves clients that are deployed in more than 20.000 retail \nstorefront locations around the world. The application consists of backend web services that are \nexposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind \nan Application Load Balancer (ALB). The retail locations communicate with the web application \nover the public internet. The company allows each retail location to register the IP address that \nthe retail location has been allocated by its local ISP. \n \nThe company's security team recommends to increase the security of the application endpoint by \nrestricting access to only the IP addresses registered by the retail locations. \n \nWhat should a solutions architect do to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n340",
    "options": [
      {
        "id": 0,
        "text": "Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the network ACL on the subnet that contains the public interface of the ALB. Update",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAssociate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company is building a data analysis platform on AWS by using AWS Lake Formation. The \nplatform will ingest data from different sources such as Amazon S3 and Amazon RDS. The \ncompany needs a secure solution to prevent access to portions of the data that contain sensitive \ninformation. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that includes permissions to access Lake Formation tables.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create data filters to implement row-level security and cell-level security.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function that removes sensitive information before Lake Formation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that periodically queries and removes sensitive information from",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nLake Formation data filters allow restricting access to rows or cells in data tables based on conditions. This allows preventing access to sensitive data. Data filters are implemented within Lake Formation and do not require additional coding or Lambda functions. Lambda functions to pre-process data or purge tables would require ongoing development and maintenance. IAM roles only provide user-level permissions, not row or cell level security. Data filters give granular access control over Lake Formation data with minimal configuration, avoiding complex custom code.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A company deploys Amazon EC2 instances that run in a VPC. The EC2 instances load source \ndata into Amazon S3 buckets so that the data can be processed in the future. According to \ncompliance laws, the data must not be transmitted over the public internet. Servers in the \ncompany's on-premises data center will consume the output from an application that runs on the \nEC2 instances. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an interface VPC endpoint for Amazon EC2. Create an AWS Site-to-Site VPN connection",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an AWS Transit Gateway connection from the VPC to the S3 buckets. Create an AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up proxy EC2 instances that have routes to NAT gateways. Configure the proxy EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nGateway VPC Endpoint = no internet to access S3. Direct Connect = secure access to VPC.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company has an application with a REST-based interface that allows data to be received in \nnear-real time from a third-party vendor. Once received, the application processes and stores the \ndata for further analysis. The application is running on Amazon EC2 instances. \n \nThe third-party vendor has received many 503 Service Unavailable Errors when sending data to \nthe application. When the data volume spikes, the compute capacity reaches its maximum limit \nand the application is unable to process all requests. \n \nWhich design should a solutions architect recommend to provide a more scalable solution?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Repackage the application as a container. Deploy the application using Amazon Elastic Container",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nKinesis Data Streams provides an auto-scaling stream that can handle large amounts of streaming data ingestion and throughput. This removes the bottlenecks around receiving the data. AWS Lambda can process and store the data in a scalable serverless manner, avoiding EC2 capacity limits. API Gateway adds API management capabilities but does not improve the underlying scalability of the EC2 application. SNS is for event publishing/notifications, not large scale data ingestion. ECS still relies on EC2 capacity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company has an application that runs on Amazon EC2 instances in a private subnet. The \napplication needs to process sensitive information from an Amazon S3 bucket. The application \nmust not use the internet to connect to the S3 bucket. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an internet gateway. Update the S3 bucket policy to allow access from the internet",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a VPN connection. Update the S3 bucket policy to allow access from the VPN",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a NAT gateway. Update the S3 bucket policy to allow access from the NAT gateway.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container \napplication. The EKS cluster stores sensitive information in the Kubernetes secrets object. The \ncompany wants to ensure that the information is encrypted. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use the container application to encrypt the information by using AWS Key Management Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement an AWS Lambda function to encrypt the information by using AWS Key Management",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/about-aws/whats-new/2020/03/amazon-eks-adds-envelope-encryption- for-secrets-with-aws-kms/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company is designing a new multi-tier web application that consists of the following \ncomponents: \n \n- Web and application servers that run on Amazon EC2 instances as part \nof Auto Scaling groups \n- An Amazon RDS DB instance for data storage \n \nA solutions architect needs to limit access to the application servers so that only the web servers \ncan access them. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS PrivateLink in front of the application servers. Configure the network ACL to allow",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a VPC endpoint in front of the application servers. Configure the security group to allow",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a Network Load Balancer with a target group that contains the application servers' Auto",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Application Load Balancer with a target group that contains the application servers'",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAn Application Load Balancer (ALB) allows directing traffic to the application servers and provides access control via security groups. Security groups act as a firewall at the instance level and can control access to the application servers from the web servers. Network ACLs work at the subnet level and are less flexible for security groups for instance-level access control. VPC endpoints are used to provide private access to AWS services, not for access between EC2 instances. AWS PrivateLink provides private connectivity between VPCs, which is not required in this single VPC scenario.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company runs a critical, customer-facing application on Amazon Elastic Kubernetes Service \n(Amazon EKS). The application has a microservices architecture. The company needs to \nimplement a solution that collects, aggregates, and summarizes metrics and logs from the \napplication in a centralized location. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run AWS App Mesh in the existing EKS cluster. View the metrics and logs in the App Mesh",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS CloudTrail to capture data events. Query CloudTrail by using Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon CloudWatch Application Insights facilitates observability for your applications and underlying AWS resources. It helps you set up the best monitors for your application resources to continuously analyze data for signs of problems with your applications.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 64,
    "text": "A company has deployed its newest product on AWS. The product runs in an Auto Scaling group \nbehind a Network Load Balancer. The company stores the product's objects in an Amazon S3 \nbucket. \n \nThe company recently experienced malicious attacks against its systems. The company needs a \nsolution that continuously monitors for malicious activity in the AWS account, workloads, and \naccess patterns to the S3 bucket. The solution must also report suspicious activity and display \nthe information on a dashboard. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n344",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Macie to monitor and report findings to AWS Config.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon Inspector to monitor and report findings to AWS CloudTrail.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure AWS Config to monitor and report findings to Amazon EventBridge.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. It analyzes AWS CloudTrail, VPC Flow Logs, and DNS logs. GuardDuty can detect threats like instance or S3 bucket compromise, malicious IP addresses, or unusual API calls. Findings can be sent to AWS Security Hub which provides a centralized security dashboard and alerts. Amazon Macie and Amazon Inspector do not monitor the breadth of activity that GuardDuty does. They focus more on data security and application vulnerabilities respectively. AWS Config monitors for resource configuration changes, not malicious activity.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  }
]