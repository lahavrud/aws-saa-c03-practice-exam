[
  {
    "id": 1,
    "text": "An Internet of Things (IoT) company would like to have a streaming system that performs real-time analytics on the ingested IoT data. Once the analytics is done, the company would like to send notifications back to the mobile applications of the IoT device owners. As a solutions architect, which of the following AWS technologies would you recommend to send these notifications to the mobile applications?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Kinesis with Amazon Simple Email Service (Amazon SES)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Amazon Kinesis with Amazon Simple Notification Service (Amazon SNS)\n\nAmazon Kinesis is designed for real-time streaming data processing, making it ideal for IoT data ingestion and analytics. It can handle high-throughput, real-time data streams and process them for analytics. Amazon SNS provides push-based notifications to mobile applications, which is exactly what's needed to notify device owners. This combination allows for real-time data processing followed by immediate notification delivery to mobile apps.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS) with Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Kinesis with Amazon Simple Email Service (Amazon SES): SES is for email notifications, not mobile push notifications. Use SNS for mobile notifications.\n- Amazon Kinesis with Amazon Simple Queue Service (Amazon SQS): SQS is pull-based and not ideal for push notifications to mobile applications. SNS is designed for push notifications.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible. As an AWS Certified Solutions Architect – Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.\n- Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days: Standard-IA is more expensive than One Zone-IA. For re-creatable assets, One Zone-IA provides better cost optimization.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances. Which of the following options would allow the engineering team to provision the instances for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": true
      },
      {
        "id": 1,
        "text": "You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": false
      },
      {
        "id": 2,
        "text": "You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": false
      },
      {
        "id": 3,
        "text": "You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost\n\nLaunch templates are the recommended and modern way to configure Auto Scaling groups with mixed instance types and purchasing options. They support both On-Demand and Spot Instances across multiple instance types, allowing for cost optimization while maintaining performance. Launch configurations are legacy and don't support mixed instance types or Spot Instances with the same flexibility. You cannot use launch configurations for mixed instance types with Spot Instances - only launch templates support this feature.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application workflow. As a solutions architect, what would you recommend to provide a long term resolution for this issue?",
    "options": [
      {
        "id": 0,
        "text": "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed",
        "correct": true
      },
      {
        "id": 2,
        "text": "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct instance type: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- No need to modify the launch configuration. Just modify the Auto Scaling group to use more number of existing instance types. More instances may offset the loss of performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Modify the launch configuration to use the correct instance type and continue to use the existing Auto Scaling group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Copy data from the source bucket to the destination bucket using the aws S3 sync command",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Snowball Edge device to copy the data from one Region to another Region",
        "correct": false
      },
      {
        "id": 4,
        "text": "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "The correct answers are: Copy data from the source bucket to the destination bucket using the aws S3 sync command, Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nThe AWS CLI 'aws s3 sync' command efficiently copies objects between S3 buckets, including cross-region transfers. It's ideal for one-time bulk transfers and handles large datasets efficiently. For petabyte-scale data, it can leverage parallel transfers and retry logic.\n\nS3 Batch Replication can copy existing objects between buckets in different regions. After the one-time copy is complete, you can delete the replication configuration. This is useful for one-time migrations or data transfers.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console: S3 Transfer Acceleration optimizes client-to-S3 transfers, not bucket-to-bucket transfers.\n- Use AWS Snowball Edge device to copy the data from one Region to another Region: Snowball is for on-premises to AWS transfers, not for S3 bucket-to-bucket transfers within AWS.\n- Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A cybersecurity company uses a fleet of Amazon EC2 instances to run a proprietary application. The infrastructure maintenance group at the company wants to be notified via an email whenever the CPU utilization for any of the Amazon EC2 instances breaches a certain threshold. Which of the following services would you use for building a solution with the LEAST amount of development effort? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Step Functions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon CloudWatch",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "The correct answers are: Amazon Simple Notification Service (Amazon SNS), Amazon CloudWatch\n\nAmazon CloudWatch monitors EC2 instance metrics like CPU utilization and can trigger alarms when thresholds are breached. CloudWatch alarms can directly publish to Amazon SNS topics, which can then send email notifications. This requires minimal development effort - just configure CloudWatch alarms and SNS topics with email subscriptions. No custom code or Lambda functions are needed for basic monitoring and email notifications.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Step Functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
    "options": [
      {
        "id": 0,
        "text": "Use Auto Scaling group to provide a low latency way to distribute live sports results",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront to provide a low latency way to distribute live sports results",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Global Accelerator to provide a low latency way to distribute live sports results",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Auto Scaling group to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon CloudFront to provide a low latency way to distribute live sports results: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 8,
    "text": "A company has hired you as an AWS Certified Solutions Architect – Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs. Which solution will you recommend to address this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature. Which is the MOST effective way to address this issue so that such incidents do not recur?",
    "options": [
      {
        "id": 0,
        "text": "Remove full database access for all IAM users in the organization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals",
        "correct": true
      },
      {
        "id": 2,
        "text": "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur",
        "correct": false
      },
      {
        "id": 3,
        "text": "Only root user should have full database access in the organization",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Remove full database access for all IAM users in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Only root user should have full database access in the organization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly. How can you figure out what's happening without restricting the rights of the users?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudTrail to analyze API calls",
        "correct": true
      },
      {
        "id": 1,
        "text": "Implement an IAM policy to forbid users to change Amazon S3 bucket settings",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 access logs to analyze user access using Athena",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use AWS CloudTrail to analyze API calls\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Implement an IAM policy to forbid users to change Amazon S3 bucket settings: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 access logs to analyze user access using Athena: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency. As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon Redshift",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon RDS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 4,
        "text": "AWS Lambda",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "The correct answers are: Amazon DynamoDB, AWS Lambda\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon RDS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon ElastiCache: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Route 53 weighted routing to spread traffic across different deployments",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CodeDeploy deployment options to choose the right deployment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Elastic Load Balancing (ELB) to distribute traffic across deployments",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon Route 53 weighted routing to spread traffic across different deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS CodeDeploy deployment options to choose the right deployment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Elastic Load Balancing (ELB) to distribute traffic across deployments: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 13,
    "text": "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects. As a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
    "options": [
      {
        "id": 0,
        "text": "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager",
        "correct": false
      },
      {
        "id": 2,
        "text": "Establish a process to get managerial approval for deleting Amazon S3 objects",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable versioning on the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 4,
        "text": "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "The correct answers are: Enable versioning on the Amazon S3 bucket, Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Establish a process to get managerial approval for deleting Amazon S3 objects: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
    "options": [
      {
        "id": 0,
        "text": "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect – Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort. Which of the following options represents the best solution for this use case?",
    "options": [
      {
        "id": 0,
        "text": "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nRDS Multi-AZ deployments provide high availability and automatic failover. The standby replica in another Availability Zone synchronously replicates data and automatically takes over if the primary fails.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A mobile gaming company is experiencing heavy read traffic to its Amazon Relational Database Service (Amazon RDS) database that retrieves player’s scores and stats. The company is using an Amazon RDS database instance type that is not cost-effective for their budget. The company would like to implement a strategy to deal with the high volume of read traffic, reduce latency, and also downsize the instance size to cut costs. Which of the following solutions do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Move to Amazon Redshift",
        "correct": false
      },
      {
        "id": 1,
        "text": "Switch application code to AWS Lambda for better performance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Setup Amazon ElastiCache in front of Amazon RDS",
        "correct": true
      },
      {
        "id": 3,
        "text": "Setup Amazon RDS Read Replicas",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Setup Amazon ElastiCache in front of Amazon RDS\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Move to Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Switch application code to AWS Lambda for better performance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup Amazon RDS Read Replicas: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity. As a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 18,
    "text": "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory database that supports caching results of SQL queries. As a solutions architect, which of the following AWS services would you recommend for this task?",
    "options": [
      {
        "id": 0,
        "text": "Amazon ElastiCache for Redis/Memcached",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon DocumentDB",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Amazon ElastiCache for Redis/Memcached\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon DynamoDB Accelerator (DAX): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DynamoDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon DocumentDB: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 20,
    "text": "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role",
        "correct": true
      },
      {
        "id": 1,
        "text": "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage. As a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
    "options": [
      {
        "id": 0,
        "text": "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "correct": false
      },
      {
        "id": 1,
        "text": "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "correct": false
      },
      {
        "id": 2,
        "text": "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier",
        "correct": true
      },
      {
        "id": 3,
        "text": "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images",
        "correct": false
      },
      {
        "id": 1,
        "text": "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions",
        "correct": false
      },
      {
        "id": 3,
        "text": "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code",
        "correct": true
      },
      {
        "id": 4,
        "text": "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package",
        "correct": false
      },
      {
        "id": 5,
        "text": "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      3,
      5
    ],
    "explanation": "The correct answers are: By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources, If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code, Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold\n\nWhile Lambda can be used, it requires writing code to process CloudWatch events and send emails, which increases development effort. CloudWatch alarms with SNS provide a simpler, no-code solution for email notifications.\n\nAWS Lambda is a serverless compute service that runs code in response to events. It automatically scales and manages infrastructure, making it ideal for event-driven architectures.\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 24,
    "text": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect – Associate, you have been tasked to make the application more resilient to periodic spikes in request rates. Which of the following solutions would you recommend for the given use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Shield",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use Amazon Aurora Replica",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "The correct answers are: Use Amazon CloudFront distribution in front of the Application Load Balancer, Use Amazon Aurora Replica\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Global Accelerator: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Direct Connect: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Shield: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Kinesis",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon DynamoDB",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "The correct answers are: Amazon S3, Amazon DynamoDB\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Queue Service (Amazon SQS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Simple Notification Service (Amazon SNS): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
    "options": [
      {
        "id": 0,
        "text": "Provision another Amazon Aurora database and link it to the primary database as a read replica",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a read replica and modify the application to use the appropriate endpoint",
        "correct": true
      },
      {
        "id": 2,
        "text": "Activate read-through caching on the Amazon Aurora database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the application to read from the Multi-AZ standby instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Set up a read replica and modify the application to use the appropriate endpoint\n\nRead replicas improve read performance by distributing read traffic across multiple database instances. They can be in the same or different regions and can be promoted to standalone databases if needed.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Provision another Amazon Aurora database and link it to the primary database as a read replica: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Activate read-through caching on the Amazon Aurora database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure the application to read from the Multi-AZ standby instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 27,
    "text": "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer",
        "correct": true
      },
      {
        "id": 1,
        "text": "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted",
        "correct": false
      },
      {
        "id": 2,
        "text": "You need to attach elastic IP address (EIP) to the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Your web-app has a runtime that is not supported by the Application Load Balancer",
        "correct": false
      },
      {
        "id": 4,
        "text": "The route for the health check is misconfigured",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "The correct answers are: The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer, The route for the health check is misconfigured\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- You need to attach elastic IP address (EIP) to the Amazon EC2 instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Your web-app has a runtime that is not supported by the Application Load Balancer: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary to their assumptions. As a solutions architect, could you explain this issue?",
    "options": [
      {
        "id": 0,
        "text": "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated",
        "correct": false
      },
      {
        "id": 1,
        "text": "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume",
        "correct": true
      },
      {
        "id": 2,
        "text": "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume",
        "correct": false
      },
      {
        "id": 3,
        "text": "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of the instance, the default behavior is to also terminate the attached root volume\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss of volume: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Cluster placement group",
        "correct": true
      },
      {
        "id": 2,
        "text": "Optimize the Amazon EC2 kernel using EC2 User Data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Spread placement group",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use a Cluster placement group\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Use Spot Instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Optimize the Amazon EC2 kernel using EC2 User Data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a Spread placement group: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region",
        "correct": true
      },
      {
        "id": 2,
        "text": "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases",
        "correct": false
      },
      {
        "id": 3,
        "text": "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use Amazon Aurora Global Database to enable fast local reads with low latency in each region\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file. Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
    "options": [
      {
        "id": 0,
        "text": "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data",
        "correct": false
      },
      {
        "id": 1,
        "text": "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object",
        "correct": true
      },
      {
        "id": 2,
        "text": "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data",
        "correct": false
      },
      {
        "id": 3,
        "text": "A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Which of the following is correct regarding the pricing for these two services?",
    "options": [
      {
        "id": 0,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used",
        "correct": false
      },
      {
        "id": 1,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
        "correct": false
      },
      {
        "id": 2,
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
    "options": [
      {
        "id": 0,
        "text": "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use IAM authentication to access the database instead of the database user's access credentials",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon RDS to use SSL for data in transit",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Configure Amazon RDS to use SSL for data in transit\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use IAM authentication to access the database instead of the database user's access credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket. Which of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "The correct answers are: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket, Use multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "A financial services company recently launched an initiative to improve the security of its AWS resources and it had enabled AWS Shield Advanced across multiple AWS accounts owned by the company. Upon analysis, the company has found that the costs incurred are much higher than expected. Which of the following would you attribute as the underlying reason for the unexpectedly high costs for AWS Shield Advanced service?",
    "options": [
      {
        "id": 0,
        "text": "Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once",
        "correct": true
      },
      {
        "id": 1,
        "text": "Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Consolidated billing has not been enabled. All the AWS accounts should fall under a single consolidated billing for the monthly fee to be charged only once\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Savings Plans has not been enabled for the AWS Shield Advanced service across all the AWS accounts: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced also covers AWS Shield Standard plan, thereby resulting in increased costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Shield Advanced is being used for custom servers, that are not part of AWS Cloud, thereby resulting in increased costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations. Which of the following services can be used to support this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "correct": false
      },
      {
        "id": 2,
        "text": "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Neptune",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon OpenSearch Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Aurora",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Redshift",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Amazon Neptune\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Amazon OpenSearch Service: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Aurora: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "By default, scripts entered as user data are executed with root user privileges",
        "correct": true
      },
      {
        "id": 1,
        "text": "By default, user data runs only during the boot cycle when you first launch an instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "When an instance is running, you can update user data by using root user credentials",
        "correct": false
      },
      {
        "id": 3,
        "text": "By default, user data is executed every time an Amazon EC2 instance is re-started",
        "correct": false
      },
      {
        "id": 4,
        "text": "By default, scripts entered as user data do not have root user privileges for executing",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "The correct answers are: By default, scripts entered as user data are executed with root user privileges, By default, user data runs only during the boot cycle when you first launch an instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- When an instance is running, you can update user data by using root user credentials: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, user data is executed every time an Amazon EC2 instance is re-started: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- By default, scripts entered as user data do not have root user privileges for executing: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 41,
    "text": "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost. Which of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
    "options": [
      {
        "id": 0,
        "text": "AWS Glue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon EMR",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon FSx for Lustre",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon FSx for Windows File Server",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Amazon FSx for Lustre\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon EMR: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon FSx for Windows File Server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the installation files in Amazon S3 so they can be quickly retrieved",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 user data to customize the dynamic installation parts at boot time",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 user data to install the application at boot time",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Elastic Beanstalk deployment caching feature",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "The correct answers are: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup, Use Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Store the installation files in Amazon S3 so they can be quickly retrieved: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 user data to install the application at boot time: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Elastic Beanstalk deployment caching feature: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets. As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use Identity and Access Management (IAM) policies",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 Bucket Policies",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Security Groups",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Access Control Lists (ACLs)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use Amazon S3 Bucket Policies\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Identity and Access Management (IAM) policies: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Security Groups: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Access Control Lists (ACLs): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use Internet Gateway to access Amazon SQS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use VPN connection to access Amazon SQS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use VPC endpoint to access Amazon SQS",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Network Address Translation (NAT) instance to access Amazon SQS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Use VPC endpoint to access Amazon SQS\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use Internet Gateway to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use VPN connection to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Network Address Translation (NAT) instance to access Amazon SQS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes. Which of the following options offers the LOWEST data transfer egress cost for the company?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching. As a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second",
        "correct": true
      },
      {
        "id": 2,
        "text": "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix",
        "correct": true
      },
      {
        "id": 3,
        "text": "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second",
        "correct": false
      },
      {
        "id": 4,
        "text": "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue",
        "correct": true
      },
      {
        "id": 5,
        "text": "Convert the existing standard queue into a FIFO (First-In-First-Out) queue",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2,
      4
    ],
    "explanation": "The correct answers are: Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second, Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix, Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Convert the existing standard queue into a FIFO (First-In-First-Out) queue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "The business analytics team at a company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the business analytics reporting, the engineering team now wants to continuously replicate this data and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Glue to replicate the data from the databases into Amazon Redshift",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS EMR to replicate the data from the databases into Amazon Redshift",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use AWS Database Migration Service (AWS DMS) to replicate the data from the databases into Amazon Redshift\n\nAmazon SES is for email notifications, not mobile push notifications. For mobile app notifications, SNS is the appropriate service.\n\nThis solution ensures high availability, fault tolerance, and disaster recovery capabilities, which are key aspects of resilient architectures.\n\nWhy other options are incorrect:\n- Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS EMR to replicate the data from the databases into Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
    "options": [
      {
        "id": 0,
        "text": "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days",
        "correct": true
      },
      {
        "id": 2,
        "text": "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days",
        "correct": false
      },
      {
        "id": 3,
        "text": "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service. Which of the following would you identify as data sources supported by Amazon GuardDuty?",
    "options": [
      {
        "id": 0,
        "text": "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs",
        "correct": false
      },
      {
        "id": 1,
        "text": "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "correct": true
      },
      {
        "id": 2,
        "text": "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management. Which of the following solutions would you suggest as the best fit for the given use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS_IAM authorization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Cognito User Pools",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Cognito Identity Pools",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Lambda authorizer for Amazon API Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Use Amazon Cognito User Pools\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Use AWS_IAM authorization: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon Cognito Identity Pools: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Lambda authorizer for Amazon API Gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 dedicated hosts",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 dedicated instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 on-demand instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 reserved instances (RI)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "The correct answer is: Use Amazon EC2 dedicated hosts\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EC2 dedicated instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 on-demand instances: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EC2 reserved instances (RI): This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "NAT instance can be used as a bastion server",
        "correct": true
      },
      {
        "id": 1,
        "text": "NAT gateway can be used as a bastion server",
        "correct": false
      },
      {
        "id": 2,
        "text": "NAT instance supports port forwarding",
        "correct": true
      },
      {
        "id": 3,
        "text": "NAT gateway supports port forwarding",
        "correct": false
      },
      {
        "id": 4,
        "text": "Security Groups can be associated with a NAT instance",
        "correct": true
      },
      {
        "id": 5,
        "text": "Security Groups can be associated with a NAT gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2,
      4
    ],
    "explanation": "The correct answers are: NAT instance can be used as a bastion server, NAT instance supports port forwarding, Security Groups can be associated with a NAT instance\n\nNAT Gateways or NAT Instances allow private subnets to access the internet for outbound traffic while remaining private. They're placed in public subnets and route traffic from private subnets through the Internet Gateway.\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- NAT gateway can be used as a bastion server: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- NAT gateway supports port forwarding: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Security Groups can be associated with a NAT gateway: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account. As a solutions architect, which of the following steps would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Both IAM roles and IAM users can be used interchangeably for cross-account access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment",
        "correct": true
      },
      {
        "id": 2,
        "text": "It is not possible to access cross-account resources",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Both IAM roles and IAM users can be used interchangeably for cross-account access: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- It is not possible to access cross-account resources: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A cyber security company is running a mission critical application using a single Spread placement group of Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance. How many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given use-case?",
    "options": [
      {
        "id": 0,
        "text": "7",
        "correct": false
      },
      {
        "id": 1,
        "text": "3",
        "correct": true
      },
      {
        "id": 2,
        "text": "14",
        "correct": false
      },
      {
        "id": 3,
        "text": "15",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: 3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- 7: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 14: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- 15: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The REST API stores the user data in Amazon DynamoDB and any static content, such as images, are served via Amazon Simple Storage Service (Amazon S3). On analyzing the usage trends, it is found that 90% of the read requests are for commonly accessed data across all users. As a Solutions Architect, which of the following would you suggest as the MOST efficient solution to improve the application performance?",
    "options": [
      {
        "id": 0,
        "text": "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs. As a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
    "options": [
      {
        "id": 0,
        "text": "Establish VPC peering connections between all VPCs",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an internet gateway to interconnect the VPCs",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a VPC endpoint to interconnect the VPCs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS transit gateway to interconnect the VPCs",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use AWS transit gateway to interconnect the VPCs\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Establish VPC peering connections between all VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use an internet gateway to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use a VPC endpoint to interconnect the VPCs: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The development team has looked into the network usage and found that 90% of it is due to distributing static content of the application. As a Solutions Architect, what do you recommend to improve the application's network usage and decrease costs?",
    "options": [
      {
        "id": 0,
        "text": "Distribute the static content through Amazon EFS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Distribute the dynamic content through Amazon EFS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Distribute the static content through Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Distribute the dynamic content through Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Distribute the static content through Amazon S3\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Distribute the static content through Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Distribute the dynamic content through Amazon EFS: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Distribute the dynamic content through Amazon S3: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "AWS Schema Conversion Tool (AWS SCT)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Basic Schema Copy",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Database Migration Service (AWS DMS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Snowball Edge",
        "correct": false
      },
      {
        "id": 4,
        "text": "AWS Glue",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "The correct answers are: AWS Schema Conversion Tool (AWS SCT), AWS Database Migration Service (AWS DMS)\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Basic Schema Copy: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Snowball Edge: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- AWS Glue: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 60,
    "text": "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
    "options": [
      {
        "id": 0,
        "text": "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the instance ID specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS. Which of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Data moving between the volume and the instance is NOT encrypted",
        "correct": false
      },
      {
        "id": 1,
        "text": "Any snapshot created from the volume is encrypted",
        "correct": true
      },
      {
        "id": 2,
        "text": "Any snapshot created from the volume is NOT encrypted",
        "correct": false
      },
      {
        "id": 3,
        "text": "Data moving between the volume and the instance is encrypted",
        "correct": true
      },
      {
        "id": 4,
        "text": "Data at rest inside the volume is NOT encrypted",
        "correct": false
      },
      {
        "id": 5,
        "text": "Data at rest inside the volume is encrypted",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      3,
      5
    ],
    "explanation": "The correct answers are: Any snapshot created from the volume is encrypted, Data moving between the volume and the instance is encrypted, Data at rest inside the volume is encrypted\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Data moving between the volume and the instance is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Any snapshot created from the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Data at rest inside the volume is NOT encrypted: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Software VPN",
        "correct": false
      },
      {
        "id": 1,
        "text": "VPC Peering connection",
        "correct": false
      },
      {
        "id": 2,
        "text": "VPC Endpoint",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS VPN CloudHub",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: AWS VPN CloudHub\n\nThis solution maintains security best practices, proper access controls, and data protection while addressing the functional requirements.\n\nWhy other options are incorrect:\n- Software VPN: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Peering connection: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- VPC Endpoint: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports. The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift",
        "correct": false
      },
      {
        "id": 1,
        "text": "Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "The correct answer is: Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
    "options": [
      {
        "id": 0,
        "text": "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues",
        "correct": false
      },
      {
        "id": 1,
        "text": "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues",
        "correct": false
      },
      {
        "id": 2,
        "text": "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Enhanced Fanout feature of Amazon Kinesis Data Streams",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "The correct answer is: Use Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nThis solution optimizes for performance, scalability, and efficiency, which are key considerations in high-performing architectures.\n\nWhy other options are incorrect:\n- Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 65,
    "text": "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console. Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "The correct answer is: Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nThis solution provides cost optimization while meeting the performance and availability requirements specified in the scenario.\n\nWhy other options are incorrect:\n- Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.\n- Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes: This option does not fully address the requirements, introduces unnecessary complexity, or is not the most appropriate solution for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  }
]