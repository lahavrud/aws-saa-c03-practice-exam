[
  {
    "id": 1,
    "text": "A company hosted a web application in an Auto Scaling group of EC2 instances. The IT manager is concerned about the over-provisioning of the resources that can cause higher operating costs. A Solutions Architect has been instructed to create a cost-effective solution without affecting the performance of the application.Which dynamic scaling policy should be used to satisfy this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use simple scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use scheduled scaling.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use suspend and resume scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use target tracking scaling.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 2,
    "text": "A financial services company plans to migrate its trading application from on-premises Microsoft Windows Server to Amazon Web Services (AWS).The solution must ensure high availability across multiple Availability Zones and offer low-latency access to block storage.Which of the following solutions will fulfill these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for Windows File Server for shared storage.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon Elastic File System (Amazon EFS) to provide shared storage between the instances. Configure Amazon EFS with cross-region replication to sync data across Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the trading application on Amazon EC2 Windows Server instances across two Availability Zones. Use Amazon FSx for NetApp ONTAP to create a Multi-AZ file system and access the data via iSCSI protocol.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the trading application on Amazon EC2 Windows instances across two Availability Zones. Use Amazon Simple Storage Service (Amazon S3) for storage and configure cross-region replication to sync data between S3 buckets in each Availability Zone.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A company is using AWS Fargate to run a batch job whenever an object is uploaded to an Amazon S3 bucket. The minimum ECS task count is initially set to 1 to save on costs and should only be increased based on new objects uploaded to the S3 bucket.Which is the most suitable option to implement with the LEAST amount of effort?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to a Lambda function that will run theStartTaskAPI command.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an alarm in Amazon CloudWatch to monitor S3 object-level operations that are recorded on CloudTrail. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers the ECS cluster when new CloudTrail events are detected.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an alarm in CloudWatch to monitor S3 object-level operations recorded on CloudTrail. Set two alarm actions to update the ECS task count to scale-out/scale-in depending on the S3 event.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances with different instance types and sizes. The application is extensively used during office hours from 9 in the morning to 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.Which of the following is the MOST operationally efficient solution to implement to ensure the application works properly at the beginning of the day?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a Predictive scaling policy for the Auto Scaling group to automatically adjust the number of Amazon EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "A company has a highly available architecture consisting of an Elastic Load Balancer and multiple Amazon EC2 instances configured with Auto Scaling across three Availability Zones. The company needs to monitor EC2 instances based on a specific metric that is not readily available in Amazon CloudWatch.Which of the following is a custom metric in CloudWatch that requires manual setup?",
    "options": [
      {
        "id": 0,
        "text": "Memory Utilizationof an EC2 instance",
        "correct": true
      },
      {
        "id": 1,
        "text": "CPU Utilizationof an EC2 instance",
        "correct": false
      },
      {
        "id": 2,
        "text": "Disk Read activityof an EC2 instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Network packets outof an EC2 instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "A company is using Amazon S3 to store frequently accessed data. When an object is created or deleted, the S3 bucket will send an event notification to the Amazon SQS queue. A solutions architect needs to create a solution that will notify the development and operations team about the created or deleted objects.Which of the following would satisfy this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Set up another SQS queue for the other team. Grant S3 permission to send a notification to the second SQS queue.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new Amazon SNS FIFO topic for the other team. Grant S3 permission to send the notification to the second SNS topic.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an Amazon SNS topic and configure two SQS queues to poll the SNS topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon SNS topic and configure two SQS queues to subscribe to the topic. Grant S3 permission to send notifications to SNS and update the bucket to use the new SNS topic.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon FSx For Lustre and the Provisioned IOPS SSD (io1) volumes of Amazon EBS for hot and cold storage respectively.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon FSx For Lustre and Amazon S3 for hot and cold storage respectively.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic File System and Amazon S3 for hot and cold storage respectively.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon FSx For Windows File Server and Amazon S3 for hot and cold storage respectively.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "A startup is using Amazon RDS to store data from a web application. Most of the time, the application has low user activity but it receives bursts of traffic within seconds whenever there is a new product announcement. The Solutions Architect needs to create a solution that will allow users around the globe to access the data using an API.What should the Solutions Architect do meet the above requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an API using Amazon API Gateway and use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic in seconds.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an API using Amazon API Gateway and use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic in seconds.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an API using Amazon API Gateway and use AWS Lambda to handle the bursts of traffic in seconds.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an API using Amazon API Gateway and use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic in seconds.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using Amazon DynamoDB. The application requires improved database performance by distributing the workload evenly and utilizing the provisioned throughput efficiently.Which of the following should be implemented for the DynamoDB table?",
    "options": [
      {
        "id": 0,
        "text": "Reduce the number of partition keys in the DynamoDB table.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Avoid using a composite primary key, which is composed of a partition key and a sort key.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand Amazon EC2 instances that use Amazon Aurora as its database. Currently, the system stores the file documents that users upload in one of the attached Amazon EBS volumes. The system's performance has been observed to be slow, and the manager has instructed the team to improve the architecture.In this scenario, which solution should be implemented to achieve a scalable, highly available, POSIX-compliant shared file system?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket and use this as the storage for the CMS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EFS to provide a shared file system for concurrent access to data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Upgrade your existing EBS volumes to Provisioned IOPS SSD volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage Amazon ElastiCache to cache frequently accessed data and reduce latency",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 8,
    "text": "An e-commerce company runs a highly scalable web application that depends on an Amazon Aurora database. As the number of users increases, the read replica faces difficulties keeping up with the increasing read traffic, causing performance bottlenecks during peak periods.Which of the following will resolve the issue with the most cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Increase the size of the Aurora DB cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use automatic scaling for the Aurora read replica using Aurora Auto Scaling.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement read scaling with Aurora Global Database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a read replica that can operate across different regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A popular social network is hosted in AWS and is using a Amazon DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email.Which of the following is the most suitable solution to implement to meet the requirement?",
    "options": [
      {
        "id": 0,
        "text": "Using the Amazon Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using Amazon SNS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function that uses DynamoDB Streams Amazon Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an Amazon SNS Topic that will notify the subscribers via email when there is an update made by a particular user.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and an AWS Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using Amazon SNS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to Amazon SNS Topic that will notify the subscribers via email.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A retail company receives raw.csvdata files into its Amazon S3 bucket from multiple sources on an hourly basis, with an average file size of 2 GB.An automated process must be implemented to convert these.csvfiles into the more efficient Apache Parquet format and store the converted files in another S3 bucket. Additionally, the conversion process must be automatically initiated each time a new file is uploaded into the S3 bucket.Which of the following options must be implemented to meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS Lambda function triggered by anS3 PUTevent to convert the.csvfiles to Parquet format. Use the AWS Transfer Family with SFTP service to move the output files to the target S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Utilize an AWS Glue extract, transform, and load (ETL) job to process and convert the.csvfiles to Apache Parquet format and then store the output files into the target S3 bucket. Set up an S3 Event Notification to track everyS3 PUTevent and invoke the ETL job in Glue through Amazon SQS.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an Apache Spark job running in an Amazon EC2 instance and create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitorS3 PUTevents in the S3 bucket. Configure AWS Lambda to invoke the Spark job for every new.csvfile added via a Function URL.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an ETL (Extract, Transform, Load) job and a Data Catalog table in AWS Glue. Configure the Glue crawler to run on a schedule to check for new files in the S3 bucket every hour and convert them to Parquet format.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead.In this scenario, how can you protect the backend systems of the platform from traffic spikes?",
    "options": [
      {
        "id": 0,
        "text": "Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable throttling limits and result caching in API Gateway.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use CloudFront in front of the API Gateway to act as a cache.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the Lambda function in a VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 12,
    "text": "A company is using a combination of API Gateway and AWS Lambda for the web services of an online web portal that is accessed by hundreds of thousands of clients each day. The company will be announcing a new revolutionary product, and it is expected that the web portal will receive a massive number of visitors from all around the globe.How can the back-end systems and applications be protected from traffic spikes?",
    "options": [
      {
        "id": 0,
        "text": "Use throttling limits in API Gateway",
        "correct": true
      },
      {
        "id": 1,
        "text": "API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Manually upgrade the Amazon EC2 instances being used by API Gateway",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy Multi-AZ in API Gateway with Read Replica",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 13,
    "text": "An online learning company hosts its Microsoft .NET e-Learning application on a Windows Server in its on-premises data center. The application uses an Oracle Database Standard Edition as its backend database.The company wants a high-performing solution to migrate this workload to the AWS cloud to take advantage of the cloud’s high availability. The migration process should minimize development changes, and the environment should be easier to manage.Which of the following options should be implemented to meet the company requirements? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Perform a homogeneous migration by moving the Oracle database to Amazon RDS for Oracle in a Multi-AZ deployment using AWS Database Migration Service (AWS DMS).",
        "correct": true
      },
      {
        "id": 1,
        "text": "Refactor the application to .NET Core and run it as a serverless container service using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Application Migration Service (AWS MGN) to migrate the on-premises Oracle database server to a new Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment which runs in multiple Availability Zones.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Provision and replatform the application to Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. Use the Windows Server Amazon Machine Image (AMI) and deploy the .NET application using to the ECS cluster via the ECS Anywhere service.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 3 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 14,
    "text": "A popular social media website uses a Amazon CloudFront web distribution to serve static content to millions of users around the globe. Recently, the website has received a number of complaints about long login times. Additionally, there are instances where users encounter HTTP 504 errors. The manager has instructed the team to significantly reduce login time and further optimize the system.Which of the following options should be used together to set up a cost-effective solution that improves the application's performance? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your AWS Lambda functions to execute the authentication process in AWS locations closer to the users.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Establish multiple Amazon VPCs in different AWS regions and configure a transit VPC to interconnect all of your resources. To handle the requests faster, set up AWS Lambda functions in each region with the AWS Serverless Application Model (SAM) service.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure your origin to add aCache-Control max-agedirective to your objects, and specify the longest practical value formax-ageto increase the cache hit ratio of your CloudFront distribution.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Implement an origin failover by creating an origin group that includes two origins. Assign one as the primary origin and the other as secondary, which enables CloudFront to automatically switch to if the primary origin encounters specific HTTP status code failure responses.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 4 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 15,
    "text": "A healthcare organization wants to build a system that can predict drug prescription abuse. The organization will gather real-time data from multiple sources, which include Personally Identifiable Information (PII). It's crucial that this sensitive information is anonymized prior to landing in a NoSQL database for further processing.Which solution would meet the requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a data lake in Amazon S3 and use it as the primary storage for patient health data. Use an S3 trigger to run an AWS Lambda function that performs anonymization. Send the anonymized data to Amazon DynamoDB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Stream the data in an Amazon DynamoDB table. Enable DynamoDB Streams, and configure an AWS Lambda function withAmazonDynamoDBFullAccesspermissions to perform anonymization on newly written items.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Data Firehose stream to capture and transform the streaming data. Deliver the anonymized data to Amazon Redshift for analysis.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Ingest real-time data using Amazon Kinesis Data Stream. Use an AWS Lambda function to anonymize the PII, then store it in Amazon DynamoDB.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A company has a web application that uses Internet Information Services (IIS) for Windows Server. A file share is used to store the application data on the network-attached storage of the company’s on-premises data center. To achieve a highly available system, the company plans to migrate the application and file share to AWS.Which of the following can be used to fulfill this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the existing file share configuration to AWS Storage Gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the existing file share configuration to Amazon FSx for Windows File Server.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the existing file share configuration to Amazon EFS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the existing file share configuration to Amazon EBS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A company collects atmospheric data such as temperature, air pressure, and humidity from different countries. Each site location is equipped with various weather instruments and a high-speed Internet connection. The average collected data in each location is around 500 GB and will be analyzed by a weather forecasting application hosted in Northern Virginia. The Solutions Architect must determine the fastest way to aggregate all the data.Which of the following options can satisfy the given requirement?",
    "options": [
      {
        "id": 0,
        "text": "Enable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Upload the data to the closest Amazon S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Snowball Edge to transfer large amounts of data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a Site-to-Site VPN connection.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 18,
    "text": "A company wishes to query data that resides in multiple AWS accounts from a central data lake. Each account has its own Amazon S3 bucket that stores data unique to its business function. Access to the data lake must be granted based on user roles.Which solution will minimize overhead and costs while meeting the required access patterns?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lake Formation to consolidate data from multiple accounts into a single account.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Data Firehose to consolidate data from multiple accounts into a single account.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a scheduled AWS Lambda function using Amazon EventBridge for transferring data from multiple accounts to the S3 buckets of the central account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Control Tower to centrally manage each account's S3 buckets.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 19,
    "text": "A global IT company with offices around the world has multiple AWS accounts. To improve efficiency and drive costs down, the Chief Information Officer (CIO) wants to set up a solution that centrally manages their AWS resources. This will allow them to procure AWS resources centrally and share resources such as AWS Transit Gateways, AWS License Manager configurations, or Amazon Route 53 Resolver rules across their various accounts.As the Solutions Architect, which combination of options should you implement in this scenario? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Resource Access Manager (RAM) service to easily and securely share your resources with your AWS accounts.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Control Tower to easily and securely share your resources with your AWS accounts.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Consolidate all of the company's accounts using AWS Organizations.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Consolidate all of the company's accounts using AWS ParallelCluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 3 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 20,
    "text": "A car dealership website hosted in Amazon EC2 stores car listings in an Amazon Aurora database managed by Amazon RDS. Once a vehicle has been sold, its data must be removed from the current listings and forwarded to a distributed processing system.Which of the following options can satisfy the given requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an RDS event subscription and send the notifications to Amazon SQS. Configure the SQS queues to fan out the event notifications to multiple Amazon SNS topics. Process the data using AWS Lambda functions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an RDS event subscription and send the notifications to AWS Lambda. Configure the Lambda function to fanout the event notifications to multiple Amazon SQS queues to update the target groups.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an RDS event subscription and send the notifications to Amazon SNS. Configure the SNS topic to fan out the event notifications to multiple Amazon SQS queues. Process the data using AWS Lambda functions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a native function or a stored procedure that invokes an AWS Lambda function. Configure the Lambda function to send event notifications to an Amazon SQS queue for the processing system to consume.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a DynamoDB VPC endpoint and an S3 VPC endpoint to route all access to these services via private endpoints.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS VPN CloudHub to route all access to S3 and DynamoDB via private endpoints.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable DynamoDB Encryption at Rest with the default AWS-managed key and S3 Server-Side Encryption with the default AWS KMS key to route all traffic to DynamoDB and S3 via private endpoints.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 1,
    "text": "There was an incident in a production environment where user data stored in an Amazon S3 bucket was accidentally deleted by a Junior DevOps Engineer. The issue was escalated to management, and after a few days, an instruction was given to improve the security and protection of AWS resources.What combination of the following options will protect the S3 objects in the bucket from both accidental deletion and overwriting? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Enable Versioning",
        "correct": true
      },
      {
        "id": 1,
        "text": "Provide access to S3 data strictly through pre-signed URL only",
        "correct": false
      },
      {
        "id": 2,
        "text": "Disallow S3 Delete using an IAM bucket policy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable S3 Intelligent-Tiering",
        "correct": false
      },
      {
        "id": 4,
        "text": "Enable Multi-Factor Authentication Delete",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 4 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "An application that records weather data every minute is deployed in a fleet of Amazon EC2 Spot instances and uses a MySQL RDS database instance. Currently, there is only one Amazon RDS instance running in one Availability Zone. The database needs to be improved to ensure high availability by enabling synchronous data replication to another RDS instance.Which of the following performs synchronous data replication in RDS?",
    "options": [
      {
        "id": 0,
        "text": "RDS DB instance running as a Multi-AZ deployment",
        "correct": true
      },
      {
        "id": 1,
        "text": "RDS Read Replica",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon DynamoDB Read Replica",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon CloudFront running as a Multi-AZ deployment",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "An online cryptocurrency exchange platform is hosted in AWS, utilizing an Amazon ECS Cluster and Amazon RDS in a Multi-AZ Deployments configuration. The application heavily uses the RDS instance to process complex read and write database operations. To maintain reliability, availability, and performance, it is necessary to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of CPU bandwidth and total memory consumed by each process.Which of the following is the most suitable solution to monitor the database properly?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch to monitor the CPU Utilization of your database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a script that collects and publishes custom metrics to Amazon CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Enhanced Monitoring in RDS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Check theCPU%andMEM%metrics which are readily available in the RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 4,
    "text": "A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in an on-premises data center and uses an Oracle database. Due to a recent cooling problem in its data center, the company urgently needs to migrate its infrastructure to AWS to improve the performance of its applications. As the Solutions Architect, the responsibility is to ensure that the database is properly migrated and remains available in case of database server failure in the future, following AWS Prescriptive Guidance for database migration and high availability.Which combination of actions would meet the requirement? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Launch an Oracle database instance in Amazon RDS with Recovery Manager (RMAN) enabled.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Convert the database schema using the AWS Schema Conversion Tool.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Oracle database in Amazon RDS with Multi-AZ deployments.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the Oracle database to a non-cluster Amazon Aurora with a single instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Migrate the Oracle database to AWS using the AWS Database Migration Service",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 4 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A company plans to host a web application in an Auto Scaling group of Amazon EC2 instances. The application will be used globally by users to upload and store several types of files. Based on user trends, files that are older than 2 years must be stored in a different storage class. The Solutions Architect of the company needs to create a cost-effective and scalable solution to store the old files yet still provide durability and high availability.Which of the following approach can be used to fulfill this requirement? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Glacier after 2 years.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EFS and create a lifecycle policy that will move the objects to EFS-IA after 2 years.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 and create a lifecycle policy that will move the objects to S3 Standard-IA after 2 years.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon EBS volumes to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use a RAID 0 storage configuration that stripes multiple Amazon EBS volumes together to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 2 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "An e-commerce company utilizes a regional Amazon API Gateway to host its public REST APIs. The API Gateway endpoint is accessed through a custom domain name set up with an Amazon Route 53 alias record. To support continuous improvement, the company intends to launch a new version of its APIs with enhanced features and performance optimizations.How can the company reduce customer disruption and ensure MINIMAL data loss during the update process in the MOST cost-effective way?",
    "options": [
      {
        "id": 0,
        "text": "Create a new API Gateway with the updated version of the APIs in OpenAPI JSON or YAML file format, but keep the same custom domain name for the new API Gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement a canary release deployment strategy for the API Gateway. Deploy the latest version of the APIs to a canary stage and direct a portion of the user traffic to this stage. Verify the new APIs. Gradually increase the traffic percentage, monitor for any issues, and, if successful, promote the canary stage to production.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Modify the existing API Gateway with the updated version of the APIs, but keep the same custom domain name for the new API Gateway by using the import-to-update operation in either overwrite or merge mode.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement a blue-green deployment strategy for the API Gateway, deploying the latest version of the APIs to the green environment. Route some user traffic to it, validate the new APIs, and once thoroughly validated, promote the green environment to production.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 7,
    "text": "A company plans to migrate its on-premises workload to AWS. The current architecture is composed of a Microsoft SharePoint server that uses a Windows shared file storage. The Solutions Architect needs to use a cloud storage solution that is highly available and can be integrated with Active Directory for access control and authentication.Which of the following options can satisfy the given requirement?",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 Windows Server to mount a new Amazon S3 bucket as a file volume.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a file system using Amazon EFS and join it to an Active Directory domain.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Network File System (NFS) file share using AWS Storage Gateway.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a file system using Amazon FSx for Windows File Server and join it to an Active Directory domain in AWS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company has a cloud architecture composed of Linux and Windows Amazon EC2 instances that process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of the systems, the Solutions Architect must create a solution that enables monitoring of memory and disk utilization metrics for all instances.Which of the following is the most suitable monitoring solution to implement?",
    "options": [
      {
        "id": 0,
        "text": "Use the default Amazon CloudWatch configuration to EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all the EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Install the Amazon CloudWatch agent to all the EC2 instances that gather the memory and disk utilization data. View the custom metrics in the CloudWatch console.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable the Enhanced Monitoring option in EC2 and install Amazon CloudWatch agent to all the EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Inspector and install the Inspector agent to all EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "An organization requires a persistent block storage volume to support its mission-critical workloads. The backup data will be stored in an object storage service and, after 30 days, transitioned to an archival storage service for long-term retention.What should be done to meet the above requirement?",
    "options": [
      {
        "id": 0,
        "text": "Attach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an Amazon EBS volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Attach an instance store volume in your existing Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 Glacier Flexible Retrieval.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach an instance store volume in your Amazon EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to S3 One Zone-IA.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A suite of web applications is hosted in an Auto Scaling group of Amazon EC2 instances across three Availability Zones and is configured with default settings. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application.Which EC2 instance will be the first one to be terminated by the Auto Scaling group?",
    "options": [
      {
        "id": 0,
        "text": "The EC2 instance which has the least number of user sessions",
        "correct": false
      },
      {
        "id": 1,
        "text": "The EC2 instance which has been running for the longest time",
        "correct": false
      },
      {
        "id": 2,
        "text": "The EC2 instance launched from the oldest launch template.",
        "correct": true
      },
      {
        "id": 3,
        "text": "The instance will be randomly selected by the Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A logistics company plans to automate its order management application. The company wants to use SFTP file transfer for uploading business-critical documents. Since the files are confidential, encryption at rest is required, and high availability must be ensured. Additionally, each file must be automatically deleted one month after creation.Which of the following options should be implemented to meet the company’s requirements with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket with encryption enabled. Configure AWS Transfer for SFTP to securely upload files to the S3 bucket. Configure the retention policy on the SFTP server to delete files after a month.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) and enable encryption. Configure AWS Transfer for SFTP to securely upload files to the EFS file system. Apply an EFS lifecycle policy to delete files after 30 days.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an Amazon EC2 instance and install the SFTP service. Mount an encrypted Amazon EFS file system on the EC2 instance to store the uploaded files. Add a cron job to delete the files older than a month.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket with encryption enabled. Launch an AWS Transfer for SFTP endpoint to securely upload files to the S3 bucket. Configure an S3 lifecycle rule to delete files after a month.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "An online shopping platform is hosted on an Auto Scaling group of Amazon EC2 Spot instances and utilizes Amazon Aurora PostgreSQL as its database. It is required to optimize database workloads in the cluster by directing the production traffic to high-capacity instances and routing the reporting queries from the internal staff to the low-capacity instances.Which is the most suitable configuration for the application as well as the Aurora database cluster to achieve this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas.",
        "correct": false
      },
      {
        "id": 1,
        "text": "In your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company is experiencing repeated outages in the Availability Zone where its Amazon RDS database instance is deployed, resulting in a complete loss of access to the database during each incident.Which solution should be implemented to prevent losing database access if this occurs again?",
    "options": [
      {
        "id": 0,
        "text": "Make a snapshot of the database",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Multi-AZ failover",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the database instance size",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a read replica",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company has recently migrated its microservices-based application to Amazon Elastic Kubernetes Service (Amazon EKS). As part of the migration, the company must ensure that all sensitive configuration data and credentials, such as database passwords and API keys, are stored securely and encrypted within the Amazon EKS cluster's etcd key-value store.What is the most suitable solution to meet the company's requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable secret encryption with a new AWS KMS key on an existing Amazon EKS cluster to encrypt sensitive data stored in the EKS cluster's etcd key-value store.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Secrets Manager with a new AWS KMS key to securely manage and store sensitive data within the EKS cluster's etcd key-value store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable default Amazon EBS volume encryption for the account with a new AWS KMS key to ensure encryption of sensitive data within the Amazon EKS cluster.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EKS default options and the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on to securely store sensitive data within the Amazon EKS cluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "A telecommunications company is planning to grant AWS Console access to its developers. Company policy requires the use of identity federation and role-based access control. Currently, the roles are already assigned via groups in the corporate Active Directory.In this scenario, what combination of the following services can provide developers access to the AWS console? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "AWS Directory Service AD Connector",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Directory Service Simple AD",
        "correct": false
      },
      {
        "id": 2,
        "text": "IAM Groups",
        "correct": false
      },
      {
        "id": 3,
        "text": "IAM Roles",
        "correct": true
      },
      {
        "id": 4,
        "text": "AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 3 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "An application consists of multiple Amazon EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A Solutions Architect is designing a highly available relational database solution to mitigate the risk of a multi-region failure. The database must meet a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute. The architect needs a disaster recovery plan that enables automatic cross-region replication with minimal data loss and rapid recovery in the event of a failure.Which AWS feature best fulfills this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Amazon DynamoDB Global table",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon RDS for PostgreSQL with cross-region read replicas",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Timestream for Analytics",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Aurora Global Database",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A company needs to deploy at least two Amazon EC2 instances to support the normal workloads of its application and automatically scale up to six EC2 instances to handle the peak load. The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.As a Solutions Architect, what should be done to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Deploy 4 instances in Availability Zone A.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Use 2 Availability Zones and deploy 1 instance for each AZ.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 4. Deploy 2 instances in Availability Zone A and 2 instances in Availability Zone B.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A company has a hybrid cloud architecture that connects its on-premises data center and cloud infrastructure in AWS. It requires a durable storage backup for its corporate documents stored on-premises and a local cache that provides low-latency access to recently accessed data to reduce data egress charges. The documents must be stored on and retrieved from AWS via the Server Message Block (SMB) protocol. These files must be immediately accessible within minutes for six months and archived for another decade to meet data compliance.Which of the following is the best and most cost-effective approach to implement in this scenario?",
    "options": [
      {
        "id": 0,
        "text": "Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an Amazon S3 bucket, and then later to Glacier for archival.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to transfer all files from the on-premises network directly to an Amazon S3 bucket, and set up a lifecycle policy to move the data into Glacier for archival.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A financial application consists of an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer, and a MySQL RDS instance set up in a Multi-AZ Deployment configuration. To protect customers' confidential data, it must be ensured that the Amazon RDS database is only accessible using an authentication token specific to the profile credentials of EC2 instances.Which of the following actions should be taken to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Enable the IAM DB Authentication.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure SSL in your application to encrypt the database connection to RDS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM Role and assign it to your EC2 instances which will grant exclusive access to your RDS instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a combination of IAM and STS to enforce restricted access to your RDS instance using a temporary authentication token.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "An online medical system hosted in AWS stores sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. Both the master keys and the unencrypted data should never be sent to AWS to comply with the strict compliance and regulatory requirements of the company.Which S3 encryption technique should the Architect use?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 client-side encryption with an AWS KMS key.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use S3 client-side encryption with a client-side master key.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use S3 server-side encryption with an AWS KMS key.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use S3 server-side encryption with customer provided key.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A Solutions Architect is hosting a website in an Amazon S3 bucket namedtutorialsdojo. The users load the website using the following URL:http://tutorialsdojo.s3-website-us-east-1.amazonaws.com. A new requirement has been introduced to add JavaScript on the webpages to make authenticated HTTPGETrequests against the same bucket using the S3 API endpoint (tutorialsdojo.s3.amazonaws.com). However, upon testing, the web browser blocks JavaScript from allowing those requests.Which of the following options is the MOST suitable solution to implement for this scenario?",
    "options": [
      {
        "id": 0,
        "text": "Enable cross-account access.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Cross-Zone Load Balancing.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Cross-origin resource sharing (CORS) configuration in the bucket.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable Cross-Region Replication (CRR).",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. To secure session data and ensure that Cloud Engineers must authenticate before executing Redis commands, specificallyMULTI EXECcommands, the system should enforce strong authentication by requiring users to enter a password. Additionally, access should be managed with long-lived credentials while supporting robust security practices.Which of the following actions should be taken to meet the above requirement?",
    "options": [
      {
        "id": 0,
        "text": "Generate an IAM authentication token using AWS credentials and provide this token as a password.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Redis replication group and enable theAtRestEncryptionEnabledparameter.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the--transit-encryption-enabledand--auth-tokenparameters enabled.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable the in-transit encryption for Redis replication groups.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and handle frequent schema changes. The application should not have any downtime or performance issues whenever there is a schema change in the database. It should also provide a low latency response to high-traffic queries.Which is the most suitable database solution to use to achieve this requirement?",
    "options": [
      {
        "id": 0,
        "text": "An Amazon RDS instance in Multi-AZ Deployments configuration",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 2,
        "text": "An Amazon Aurora database with Read Replicas",
        "correct": false
      },
      {
        "id": 3,
        "text": "Redshift",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A software development company is using serverless computing with AWS Lambda to build and run applications without having to set up or manage servers. The company has a Lambda function that connects to a MongoDB Atlas, which is a popular Database as a Service (DBaaS) platform, and also uses a third-party API to fetch certain data for its application. One of the developers was instructed to create the environment variables for the MongoDB database hostname, username, and password, as well as the API credentials that will be used by the Lambda function for DEV, SIT, UAT, and PROD environments.Considering that the Lambda function is storing sensitive database and API credentials, how can this information be secured to prevent other developers on the team, or anyone, from seeing these credentials in plain text? Select the best option that provides maximum security.",
    "options": [
      {
        "id": 0,
        "text": "There is no need to do anything because, by default, Lambda already encrypts the environment variables using the AWS Key Management Service.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Lambda does not provide encryption for the environment variables. Deploy your code to an Amazon EC2 instance instead.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new AWS KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A payment processing company plans to migrate its on-premises application to an Amazon EC2 instance. An IPv6 CIDR block is attached to the company’s Amazon VPC. Strict security policy mandates that the production VPC must only allow outbound communication over IPv6 between the instance and the internet but should prevent the internet from initiating an inbound IPv6 connection. The new architecture should also allow traffic flow inspection and traffic filtering.What should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch the EC2 instance to a public subnet and attach an Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use Traffic Mirroring to set up the required rules for traffic inspection and traffic filtering.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch the EC2 instance to a private subnet and attach AWS PrivateLink interface endpoint to the VPC to control outbound IPv6 communication to the internet. Use Amazon GuardDuty to set up the required rules for traffic inspection and traffic filtering.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch the EC2 instance to a private subnet and attach a NAT Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Firewall Manager to set up the required rules for traffic inspection and traffic filtering.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch the EC2 instance to a private subnet and attach an Egress-Only Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A pharmaceutical company has resources hosted on both its on-premises network and in the AWS cloud. The company requires all Software Architects to access resources in both environments using on-premises credentials, which are stored in Active Directory.In this scenario, which of the following can be used to fulfill this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Set up SAML 2.0-Based Federation by using a Web Identity Federation.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use IAM users",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon VPC",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A Solutions Architect needs to make sure that the On-Demand Amazon EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection.Which configuration below will satisfy this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Security Group Inbound Rule: Protocol – TCP, Port Range – 22, Source 110.238.98.71/32",
        "correct": true
      },
      {
        "id": 1,
        "text": "Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32",
        "correct": false
      },
      {
        "id": 2,
        "text": "Security Group Outbound Rule: Protocol – TCP, Port Range – 22, Destination 110.238.98.71/32",
        "correct": false
      },
      {
        "id": 3,
        "text": "Security Group Outbound Rule: Protocol – UDP, Port Range – 22, Destination 0.0.0.0/0",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for the storage of their personal documents.Which of the following will you need to consider so you can set up a solution that incorporates a single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Map each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an IAM role and an IAM Policy to access the bucket.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Set up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 3 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "A business has recently migrated its applications to AWS. The audit team must be able to assess whether the services the company is using meet common security and regulatory standards. A solutions architect needs to provide the team with a report of all compliance-related documents for their account.Which action should a solutions architect consider?",
    "options": [
      {
        "id": 0,
        "text": "Run an Amazon Inspector assessment job to download all of the AWS compliance-related information.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Artifact to view the security reports as well as other AWS compliance-related information.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Run an Amazon Macie job to view the Service Organization Control (SOC), Payment Card Industry (PCI), and other compliance reports from AWS Certificate Manager (ACM).",
        "correct": false
      },
      {
        "id": 3,
        "text": "View all of the AWS security compliance reports from AWS Security Hub.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company has a web application that uses Amazon CloudFront to distribute its images, videos, and other static content stored in its Amazon S3 bucket to users around the world. The company has recently introduced a new member-only access feature for some of its high-quality media files. There is a requirement to provide access to multiple private media files only to paying subscribers without having to change the current URLs.Which of the following is the most suitable solution to implement to satisfy this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Signed URL with a custom policy which only allows the members to see the private files.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the requiredSet-Cookieheaders to the viewer which will unlock the content only to them.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A Solutions Architect identified a series of DDoS attacks while monitoring the Amazon VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Shield Advanced to detect and mitigate DDoS attacks.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks, and other DDoS attacks.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A travel photo-sharing website is using Amazon S3 to serve high-quality photos to visitors. After a few days, it was discovered that other travel websites are linking to and using these photos. This has resulted in financial losses for the business.What is the MOST effective method to mitigate this issue?",
    "options": [
      {
        "id": 0,
        "text": "Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon CloudFront distributions for your photos.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Block the IP addresses of the offending websites using NACL.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store and privately serve the high-quality photos on Amazon WorkDocs instead.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company uses an Application Load Balancer (ALB) for its public-facing multi-tier web applications. The security team has recently reported that there has been a surge of SQL injection attacks lately, which causes critical data discrepancy issues. The same issue is also encountered by its other web applications in other AWS accounts that are behind an ALB. An immediate solution is required to prevent the remote injection of unauthorized SQL queries and protect their applications hosted across multiple accounts.As a Solutions Architect, what solution would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Network Firewall to filter web vulnerabilities and brute force attacks using stateful rule groups across all Application Load Balancers on all AWS accounts. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS WAF and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer. Integrate AWS WAF with AWS Firewall Manager to reuse the rules across all the AWS accounts.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Macie to scan for vulnerabilities and unintended network exposure. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment. Utilize the AWS Audit Manager to reuse the security assessment across all AWS accounts.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon GuardDuty and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer and utilize the AWS Security Hub service to reuse the managed rules across all the AWS accounts",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A company has 3 DevOps engineers that are handling its software development and infrastructure management processes. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service.What can the DevOps engineers do to prevent this from happening again?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Infrequently Accessed storage to store the data.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a signed URL for all users.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM bucket policy that disables delete operation.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company requires all the data stored in the cloud to be encrypted at rest. To easily integrate this with other AWS services, they must have full control over the encryption of the created keys and also the ability to immediately remove the key material from AWS KMS. The solution should also be able to audit the key usage independently of AWS CloudTrail.Which of the following options will meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Key Management Service to create AWS owned Keys and store the non-extractable key material in AWS CloudHSM.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Key Management Service to create AWS managed keys and store the non-extractable key material in AWS CloudHSM.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Key Management Service to create a KMS key in a custom key store and store the non-extractable key material in AWS CloudHSM.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company hosted an e-commerce website on an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer. The Solutions Architect noticed that the website is receiving a high number of illegitimate external requests from multiple systems with frequently changing IP addresses. To address the performance issues, the Solutions Architect must implement a solution that would block these requests while having minimal impact on legitimate traffic.Which of the following options fulfills this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create a regular rule in AWS WAF and associate the web ACL to an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a custom network ACL and associate it with the subnet of the Application Load Balancer to block the offending requests.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a rate-based rule in AWS WAF and associate the web ACL to an Application Load Balancer.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a custom rule in the security group of the Application Load Balancer to block the offending requests.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A government agency plans to store confidential tax documents on AWS. Due to the sensitive information in the files, the Solutions Architect must restrict the data access requests made to the storage solution to a specific Amazon VPC only. The solution should also prevent the files from being deleted or overwritten to meet the regulatory requirement of having a write-once-read-many (WORM) storage model.Which combination of the following options should the Architect implement? (Select TWO.)",
    "options": [
      {
        "id": 0,
        "text": "Set up a new Amazon S3 bucket to store the tax documents and integrate it with AWS Network Firewall. Configure the Network Firewall to only accept data access requests from a specific VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon S3 Access Point for the S3 bucket to restrict data access to a particular VPC only.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new Amazon S3 bucket with the S3 Object Lock feature enabled. Store the documents in the bucket and set the Legal Hold option for object retention.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the tax documents in the Amazon S3 Glacier Instant Retrieval storage class. Use thePutBucketPolicyAPI to apply a bucket policy that restricts access requests to a specific VPC.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Enable Object Lock but disable Object Versioning on the new Amazon S3 bucket to comply with the write-once-read-many (WORM) storage model requirement.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2
    ],
    "explanation": "**Why option 1 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 2 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A medical records company is planning to store sensitive clinical trial data in an Amazon S3 repository with the object-level versioning feature enabled. The Solutions Architect is tasked with ensuring that no object can be overwritten or deleted by any user in a period of one year only. To meet the strict compliance requirements, the root user of the company’s AWS account must also be restricted from making any changes to an object in the S3 bucket.Which of the following is the most secure way of storing the data in S3?",
    "options": [
      {
        "id": 0,
        "text": "Enable S3 Object Lock in governance mode with a retention period of one year.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable S3 Object Lock in compliance mode with a retention period of one year.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable S3 Object Lock in governance mode with a legal hold of one year.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable S3 Object Lock in compliance mode with a legal hold of one year.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A government entity is conducting a population and housing census in the city. Each household information uploaded on their online portal is stored in encrypted files in Amazon S3. The government assigned its Solutions Architect to set compliance policies that verify data containing personally identifiable information (PII) in a manner that meets their compliance standards. They should also be alerted if there are potential policy violations with the privacy of their S3 buckets.Which of the following should the Architect implement to satisfy this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Set up and configure Amazon Macie to monitor their Amazon S3 data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up and configure Amazon Kendra to monitor malicious activity on their Amazon S3 data",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up and configure Amazon Polly to scan for usage patterns on Amazon S3 data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up and configure Amazon Fraud Detector to send out alert notifications whenever a security violation is detected on their Amazon S3 data.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that are used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.{ \n\"Version\": \"2012-10-17\", \n\"Statement\": [ \n{ \n\"Effect\": \"Allow\", \n\"Action\": [ \n\"s3:Get*\", \n\"s3:List*\" \n], \n\"Resource\": \"*\" \n}, \n{ \n\"Effect\": \"Allow\", \n\"Action\": \"s3:PutObject\", \n\"Resource\": \"arn:aws:s3:::boracay/*\" \n} \n] \n}What does the above IAM policy allow? (Select THREE.)",
    "options": [
      {
        "id": 0,
        "text": "An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.",
        "correct": true
      },
      {
        "id": 1,
        "text": "An IAM user with this IAM policy is allowed to write objects into theboracayS3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "An IAM user with this IAM policy is allowed to change access rights for theboracayS3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "An IAM user with this IAM policy is allowed to read objects in theboracayS3 bucket but not allowed to list the objects in the bucket.",
        "correct": false
      },
      {
        "id": 4,
        "text": "An IAM user with this IAM policy is allowed to read objects from theboracayS3 bucket.",
        "correct": true
      },
      {
        "id": 5,
        "text": "An IAM user with this IAM policy is allowed to read and delete objects from theboracayS3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 1 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 4 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.\n\n**Why option 5 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  }
]