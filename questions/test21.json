[
  {
    "id": 0,
    "text": "A company needs a solution to enforce data encryption at rest on Amazon EC2 instances. The \nsolution must automatically identify noncompliant resources and enforce compliance policies on \nfindings. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use an IAM policy that allows users to create only encrypted Amazon Elastic Block Store",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Key Management Service (AWS KMS) to manage access to encrypted Amazon Elastic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Macie to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon inspector to detect unencrypted Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy creating an IAM policy that allows users to create only encrypted EBS volumes, you proactively prevent the creation of unencrypted volumes. Using AWS Config, you can set up rules to detect noncompliant resources, and AWS Systems Manager Automation can be used for automated remediation. This approach provides a proactive and automated solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A company is migrating its multi-tier on-premises application to AWS. The application consists of \na single-node MySQL database and a multi-node web tier. The company must minimize changes \nto the application during the migration. The company wants to improve application resiliency after \nthe migration. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon EC2 instances in an Auto Scaling group behind a Network Load",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon RDS Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the web tier to an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Migrate the database to an Amazon DynamoDB table.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company wants to migrate its web applications from on premises to AWS. The company is \nlocated close to the eu-central-1 Region. Because of regulations, the company cannot launch \nsome of its applications in eu-central-1. The company wants to achieve single-digit millisecond \nlatency. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the applications in eu-central-1. Extend the company's VPC from eu-central-1 to an edge",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the applications in eu-central-1. Extend the company's VPC from eu-central-1 to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the applications in AWS Wavelength Zones by extending the company's VPC from eu-",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "A company's ecommerce website has unpredictable traffic and uses AWS Lambda functions to \ndirectly access a private Amazon RDS for PostgreSQL DB instance. The company wants to \nmaintain predictable database performance and ensure that the Lambda invocations do not \noverload the database with too many connections. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Point the client driver at an RDS custom endpoint. Deploy the Lambda functions inside a VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions inside a VPC.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Point the client driver at an RDS custom endpoint. Deploy the Lambda functions outside a VPC.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions outside a VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company is creating an application. The company stores data from tests of the application in \nmultiple on-premises locations. \n \nThe company needs to connect the on-premises locations to VPCs in an AWS Region in the \nAWS Cloud. The number of accounts and VPCs will increase during the next year. The network \narchitecture must simplify the administration of new connections and must provide the ability to \nscale. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a peering connection between the VPCs. Create a VPN connection between the VPCs",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch an Amazon EC2 instance. On the instance, include VPN software that uses a VPN",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a transit gateway. Create VPC attachments for the VPC connections. Create VPN",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Direct Connect connection between the on-premises locations and a central",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 5,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n373 \nA company that uses AWS needs a solution to predict the resources needed for manufacturing \nprocesses each month. The solution must use historical values that are currently stored in an \nAmazon S3 bucket. The company has no machine learning (ML) experience and wants to use a \nmanaged service for the training and predictions. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon SageMaker model. Create a SageMaker endpoint for inference.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon SageMaker to train a model by using the historical data in the S3 bucket.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an AWS Lambda function with a function URL that uses Amazon SageMaker endpoints",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Lambda function with a function URL that uses an Amazon Forecast predictor",
        "correct": false
      },
      {
        "id": 4,
        "text": "Train an Amazon Forsecast predictor by using the historical data in the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A company manages AWS accounts in AWS Organizations. AWS IAM Identity Center (AWS \nSingle Sign-On) and AWS Control Tower are configured for the accounts. The company wants to \nmanage multiple user permissions across all the accounts. \n \nThe permissions will be used by multiple IAM users and must be split between the developer and \nadministrator teams. Each team requires different permissions. The company wants a solution \nthat includes new users that are hired on both teams. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create individual users in IAM Identity Center for each account. Create separate developer and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create individual users in IAM Identity Center for each account. Create separate developer and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create individual users in IAM Identity Center. Create new developer and administrator groups in",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create individual users in IAM Identity Center. Create new permission sets that include the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company wants to standardize its Amazon Elastic Block Store (Amazon EBS) volume \nencryption strategy. The company also wants to minimize the cost and configuration effort \nrequired to operate the volume encryption check. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n374 \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Run",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Identity and Access Management (IAM) policy that requires the use of tags on",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to flag the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company regularly uploads GB-sized files to Amazon S3. After the company uploads the files, \nthe company uses a fleet of Amazon EC2 Spot Instances to transcode the file format. The \ncompany needs to scale throughput when the company uploads data from the on-premises data \ncenter to Amazon S3 and when the company downloads data from Amazon S3 to the EC2 \ninstances. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use the S3 bucket access point instead of accessing the S3 bucket directly.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Upload the files into multiple S3 buckets.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 multipart uploads.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Fetch multiple byte-ranges of an object in parallel.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Add a random prefix to each object when uploading the files.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "A company is deploying an application in three AWS Regions using an Application Load \nBalancer. Amazon Route 53 will be used to distribute traffic between these Regions. \n \nWhich Route 53 configuration should a solutions architect use to provide the MOST high-\nperforming experience?",
    "options": [
      {
        "id": 0,
        "text": "Create an A record with a latency policy.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an A record with a geolocation policy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a CNAME record with a failover policy.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a CNAME record with a geoproximity policy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company has a web application that includes an embedded NoSQL database. The application \nruns on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in \nan Amazon EC2 Auto Scaling group in a single Availability Zone. \n \nA recent increase in traffic requires the application to be highly available and for the database to \nbe eventually consistent. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Replace the ALB with a Network Load Balancer. Maintain the embedded NoSQL database with",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the ALB with a Network Load Balancer. Migrate the embedded NoSQL database to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Maintain the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Migrate the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A company is building a shopping application on AWS. The application offers a catalog that \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n376 \nchanges once each month and needs to scale with traffic volume. The company wants the lowest \npossible latency from the application. Data from each user's shopping cart needs to be highly \navailable. User session data must be available even if the user is disconnected and reconnects. \n \nWhat should a solutions architect do to ensure that the shopping cart data is preserved at all \ntimes?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Application Load Balancer to enable the sticky sessions feature (session affinity) for",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon ElastiCache for Redis to cache catalog data from Amazon DynamoDB and",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure Amazon OpenSearch Service to cache catalog data from Amazon DynamoDB and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon EC2 instance with Amazon Elastic Block Store (Amazon EBS) storage for",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A company is building a microservices-based application that will be deployed on Amazon Elastic \nKubernetes Service (Amazon EKS). The microservices will interact with each other. The company \nwants to ensure that the application is observable to identify performance issues in the future. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to use Amazon ElastiCache to reduce the number of requests that are",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon CloudWatch Container Insights to collect metrics from the EKS clusters.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure AWS CloudTrail to review the API calls. Build an Amazon QuickSight dashboard to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Trusted Advisor to understand the performance of the application.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 14,
    "text": "A company needs to provide customers with secure access to its data. The company processes \ncustomer data and stores the results in an Amazon S3 bucket. \n \nAll the data is subject to strong regulations and security requirements. The data must be \nencrypted at rest. Each customer must be able to access only their data from their AWS account. \nCompany employees must not be able to access the data. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an AWS Certificate Manager (ACM) certificate for each customer. Encrypt the data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision an AWS Certificate Manager (ACM) certificate for each customer. Encrypt the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A solutions architect creates a VPC that includes two public subnets and two private subnets. A \ncorporate security mandate requires the solutions architect to launch all Amazon EC2 instances \nin a private subnet. However, when the solutions architect launches an EC2 instance that runs a \nweb server on ports 80 and 443 in a private subnet, no external internet traffic can connect to the \nserver. \n \nWhat should the solutions architect do to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Attach the EC2 instance to an Auto Scaling group in a private subnet. Ensure that the DNS record",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an internet-facing Application Load Balancer (ALB) in a public subnet. Add the EC2",
        "correct": true
      },
      {
        "id": 2,
        "text": "Launch a NAT gateway in a private subnet. Update the route table for the private subnets to add",
        "correct": false
      },
      {
        "id": 3,
        "text": "Ensure that the security group that is attached to the EC2 instance allows HTTP traffic on port 80",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A company is deploying a new application to Amazon Elastic Kubernetes Service (Amazon EKS) \nwith an AWS Fargate cluster. The application needs a storage solution for data persistence. The \nsolution must be highly available and fault tolerant. The solution also must be shared between \nmultiple application containers. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Elastic Block Store (Amazon EBS) volumes in the same Availability Zones where",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Register the file system in a",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Register the volume in a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create Amazon Elastic File System (Amazon EFS) file systems in the same Availability Zones",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A company has an application that uses Docker containers in its local data center. The \napplication runs on a container host that stores persistent data in a volume on the host. The \ncontainer instances use the stored persistent data. \n \nThe company wants to move the application to a fully managed service because the company \ndoes not want to manage any servers or storage infrastructure. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed nodes. Create an",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with an Amazon EC2 launch type. Create",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A gaming company wants to launch a new internet-facing application in multiple AWS Regions. \nThe application will use the TCP and UDP protocols for communication. The company needs to \nprovide high availability and minimum latency for global users. \n \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create internal Network Load Balancers in front of the application in each Region.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create external Application Load Balancers in front of the application in each Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Global Accelerator accelerator to route traffic to the load balancers in each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon Route 53 to use a geolocation routing policy to distribute the traffic.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure Amazon CloudFront to handle the traffic and route requests to the application in each",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.\n\n**Why option 4 is incorrect:**\nis incorrect because Auto Scaling will terminate the unhealthy instance first (triggered by health check failure) and then launch a new instance to maintain desired capacity. The order of operations is important.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A city has deployed a web application running on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The application's users have reported sporadic performance, which \nappears to be related to DDoS attacks originating from random IP addresses. The city needs a \nsolution that requires minimal configuration changes and provides an audit trail for the DDoS \nsources. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable an AWS WAF web ACL on the ALB, and configure rules to block traffic from unknown",
        "correct": false
      },
      {
        "id": 1,
        "text": "Subscribe to Amazon Inspector. Engage the AWS DDoS Response Team (DRT) to integrate",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to AWS Shield Advanced. Engage the AWS DDoS Response Team (DRT) to integrate",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution for the application, and set the ALB as the origin.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A company copies 200 TB of data from a recent ocean survey onto AWS Snowball Edge Storage \nOptimized devices. The company has a high performance computing (HPC) cluster that is hosted \non AWS to look for oil and gas deposits. A solutions architect must provide the cluster with \nconsistent sub-millisecond latency and high-throughput access to the data on the Snowball Edge \nStorage Optimized devices. The company is sending the devices back to AWS. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an AWS Storage",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an Amazon FSx for",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket and an Amazon Elastic File System (Amazon EFS) file system.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Lustre file system. Import the data directly into the FSx for Lustre file",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n380 \nA company has NFS servers in an on-premises data center that need to periodically back up \nsmall amounts of data to Amazon S3. \n \nWhich solution meets these requirements and is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Glue to copy the data from the on-premises servers to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an AWS Direct Connect connection between the on-premises data center and a VPC, and",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 22,
    "text": "An online video game company must maintain ultra-low latency for its game servers. The game \nservers run on Amazon EC2 instances. The company needs a solution that can handle millions of \nUDP internet traffic requests each second. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Application Load Balancer with the required protocol and ports for the internet",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a Gateway Load Balancer for the internet traffic. Specify the EC2 instances as the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a Network Load Balancer with the required protocol and ports for the internet traffic.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Launch an identical set of game servers on EC2 instances in separate AWS Regions. Route",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A company runs a three-tier application in a VPC. The database tier uses an Amazon RDS for \nMySQL DB instance. \n \nThe company plans to migrate the RDS for MySQL DB instance to an Amazon Aurora \nPostgreSQL DB cluster. The company needs a solution that replicates the data changes that \nhappen during the migration to the new database. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Database Migration Service (AWS DMS) Schema Conversion to create an Aurora",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Aurora MySQL read replica for the RDS for MySQL DB instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Promote the Aurora PostgreSQL read replica to a standalone Aurora PostgreSQL DB cluster",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 24,
    "text": "A company hosts a database that runs on an Amazon RDS instance that is deployed to multiple \nAvailability Zones. The company periodically runs a script against the database to report new \nentries that are added to the database. The script that runs against the database negatively \naffects the performance of a critical application. The company needs to improve application \nperformance with minimal costs. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Add functionality to the script to identify the instance that has the fewest active connections.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a read replica of the database. Configure the script to query only the read replica to report",
        "correct": false
      },
      {
        "id": 2,
        "text": "Instruct the development team to manually export the new entries for the day in the database at",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache to cache the common queries that the script runs against the database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company is using an Application Load Balancer (ALB) to present its application to the internet. \nThe company finds abnormal traffic access patterns across the application. A solutions architect \nneeds to improve visibility into the infrastructure to help the company understand these \nabnormalities better. \n \nWhat is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a table in Amazon Athena for AWS CloudTrail logs. Create a query for the relevant",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable ALB access logging to Amazon S3. Create a table in Amazon Athena, and query the logs.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable ALB access logging to Amazon S3. Open each file in a text editor, and search each line",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EMR on a dedicated Amazon EC2 instance to directly query the ALB to acquire",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 \ninstances in private subnets must be able to connect to the public internet through the NAT \ngateways. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create public NAT gateways in the same private subnets as the EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create private NAT gateways in the same private subnets as the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create private NAT gateways in public subnets in the same VPCs as the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company has an organization in AWS Organizations. The company runs Amazon EC2 \ninstances across four AWS accounts in the root organizational unit (OU). There are three \nnonproduction accounts and one production account. The company wants to prohibit users from \nlaunching EC2 instances of a certain size in the nonproduction accounts. The company has \ncreated a service control policy (SCP) to deny access to launch instances that use the prohibited \ntypes. \n \nWhich solutions to deploy the SCP will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Attach the SCP to the root OU for the organization.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Attach the SCP to the three nonproduction Organizations member accounts.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Attach the SCP to the Organizations management account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an OU for the production account. Attach the SCP to the OU. Move the production",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an OU for the required accounts. Attach the SCP to the OU. Move the nonproduction",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company's website hosted on Amazon EC2 instances processes classified data stored in \nAmazon S3. Due to security concerns, the company requires a private and secure connection \nbetween its EC2 resources and Amazon S3. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up S3 bucket policies to allow access from a VPC endpoint.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an IAM policy to grant read-write access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a NAT gateway to access resources outside the private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an access key ID and a secret access key to access the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "An ecommerce company runs its application on AWS. The application uses an Amazon Aurora \nPostgreSQL cluster in Multi-AZ mode for the underlying database. During a recent promotional \ncampaign, the application experienced heavy read load and write load. Users experienced \ntimeout issues when they attempted to access the application. \n \nA solutions architect needs to make the application architecture more scalable and highly \navailable. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n383 \n \nWhich solution will meet these requirements with the LEAST downtime?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon EventBridge rule that has the Aurora cluster as a source. Create an AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the Aurora cluster and activate the zero-downtime restart (ZDR) feature. Use Database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add additional reader instances to the Aurora cluster. Create an Amazon RDS Proxy target group",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon ElastiCache for Redis cache. Replicate data from the Aurora cluster to Redis",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company is designing a web application on AWS. The application will use a VPN connection \nbetween the company's existing data centers and the company's VPCs. \n \nThe company uses Amazon Route 53 as its DNS service. The application must use private DNS \nrecords to communicate with the on-premises services from a VPC. \n \nWhich solution will meet these requirements in the MOST secure manner?",
    "options": [
      {
        "id": 0,
        "text": "Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate the resolver",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Route 53 Resolver inbound endpoint. Create a resolver rule. Associate the resolver rule",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Route 53 private hosted zone. Associate the private hosted zone with the VPC.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Route 53 public hosted zone. Create a record for each service to allow service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company is running a photo hosting service in the us-east-1 Region. The service enables users \nacross multiple countries to upload and view photos. Some photos are heavily viewed for months, \nand others are viewed for less than a week. The application allows uploads of up to 20 MB for \neach photo. The service uses the photo metadata to determine which photos to display to each \nuser. \n \nWhich solution provides the appropriate user access MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the photos in Amazon DynamoDB. Turn on DynamoDB Accelerator (DAX) to cache",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the photos in the Amazon S3 Intelligent-Tiering storage class. Store the photo metadata",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the photos in the Amazon S3 Standard storage class. Set up an S3 Lifecycle policy to move",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the photos in the Amazon S3 Glacier storage class. Set up an S3 Lifecycle policy to move",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 32,
    "text": "A company runs a highly available web application on Amazon EC2 instances behind an \nApplication Load Balancer. The company uses Amazon CloudWatch metrics. \n \nAs the traffic to the web application increases, some EC2 instances become overloaded with \nmany outstanding requests. The CloudWatch metrics show that the number of requests \nprocessed and the time to receive the responses from some EC2 instances are both higher \ncompared to other EC2 instances. The company does not want new requests to be forwarded to \nthe EC2 instances that are already overloaded. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use the round robin routing algorithm based on the RequestCountPerTarget and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the least outstanding requests algorithm based on the RequestCountPerTarget and",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the round robin routing algorithm based on the RequestCount and TargetResponseTime",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the least outstanding requests algorithm based on the RequestCount and",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A company uses Amazon EC2, AWS Fargate, and AWS Lambda to run multiple workloads in the \ncompany's AWS account. The company wants to fully make use of its Compute Savings Plans. \nThe company wants to receive notification when coverage of the Compute Savings Plans drops. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a daily budget for the Savings Plans by using AWS Budgets. Configure the budget with a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Lambda function that runs a coverage report against the Savings Plans. Use Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Budgets report for the Savings Plans budget. Set the frequency to daily.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Savings Plans alert subscription. Enable all notification options. Enter an email address",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "A company runs a real-time data ingestion solution on AWS. The solution consists of the most \nrecent version of Amazon Managed Streaming for Apache Kafka (Amazon MSK). The solution is \ndeployed in a VPC in private subnets across three Availability Zones. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n385 \n \nA solutions architect needs to redesign the data ingestion solution to be publicly available over \nthe internet. The data in transit must also be encrypted. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Configure public subnets in the existing VPC. Deploy an MSK cluster in the public subnets.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new VPC that has public subnets. Deploy an MSK cluster in the public subnets. Update",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Application Load Balancer (ALB) that uses private subnets. Configure an ALB security",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a Network Load Balancer (NLB) that uses private subnets. Configure an NLB listener for",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A company wants to migrate an on-premises legacy application to AWS. The application ingests \ncustomer order files from an on-premises enterprise resource planning (ERP) system. The \napplication then uploads the files to an SFTP server. The application uses a scheduled job that \nchecks for order files every hour. \n \nThe company already has an AWS account that has connectivity to the on-premises network. The \nnew application on AWS must support integration with the existing ERP system. The new \napplication must be secure and resilient and must use the SFTP protocol to process orders from \nthe ERP system immediately. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Transfer Family SFTP internet-facing server in two Availability Zones. Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Transfer Family SFTP internet-facing server in one Availability Zone. Use",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 36,
    "text": "A company's applications use Apache Hadoop and Apache Spark to process data on premises. \nThe existing infrastructure is not scalable and is complex to manage. \n \nA solutions architect must design a scalable solution that reduces operational complexity. The \nsolution must keep the data processing on premises. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n386 \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Site-to-Site VPN to access the on-premises Hadoop Distributed File System (HDFS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS DataSync to connect to the on-premises Hadoop Distributed File System (HDFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Create an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company is migrating a large amount of data from on-premises storage to AWS. Windows, \nMac, and Linux based Amazon EC2 instances in the same AWS Region will access the data by \nusing SMB and NFS storage protocols. The company will access a portion of the data routinely. \nThe company will access the remaining data infrequently. \n \nThe company needs to design a solution to host the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon FSx for ONTAP instance. Create an FSx for ONTAP file system with a root",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket that uses S3 Intelligent-Tiering. Migrate the data to the S3 bucket",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for OpenZFS file system. Migrate the data to the new volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "A manufacturing company runs its report generation application on AWS. The application \ngenerates each report in about 20 minutes. The application is built as a monolith that runs on a \nsingle Amazon EC2 instance. The application requires frequent updates to its tightly coupled \nmodules. The application becomes complex to maintain as the company adds new features. \n \nEach time the company patches a software module, the application experiences downtime. \nReport generation must restart from the beginning after any interruptions. The company wants to \nredesign the application so that the application can be flexible, scalable, and gradually improved. \nThe company wants to minimize application downtime. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Run the application on AWS Lambda as a single function with maximum provisioned",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run the application on Amazon EC2 Spot Instances as microservices with a Spot Fleet default",
        "correct": true
      },
      {
        "id": 2,
        "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run the application on AWS Elastic Beanstalk as a single application environment with an all-at-",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 39,
    "text": "A company wants to rearchitect a large-scale web application to a serverless microservices \narchitecture. The application uses Amazon EC2 instances and is written in Python. \n \nThe company selected one component of the web application to test as a microservice. The \ncomponent supports hundreds of requests each second. The company wants to create and test \nthe microservice on an AWS solution that supports Python. The solution must also scale \nautomatically and require minimal infrastructure and minimal operational support. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a Spot Fleet with auto scaling of EC2 instances that run the most recent Amazon Linux",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Elastic Beanstalk web server environment that has high availability configured.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS). Launch Auto Scaling groups of self-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an AWS Lambda function that runs custom developed code.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company has an AWS Direct Connect connection from its on-premises location to an AWS \naccount. The AWS account has 30 different VPCs in the same AWS Region. The VPCs use \nprivate virtual interfaces (VIFs). Each VPC has a CIDR block that does not overlap with other \nnetworks under the company's control. \n \nThe company wants to centrally manage the networking architecture while still allowing each VPC \nto communicate with all other VPCs and on-premises networks. \n \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a transit gateway, and associate the Direct Connect connection with a new transit VIF.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Direct Connect gateway. Recreate the private VIFs to use the new gateway. Associate",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a transit VPConnect the Direct Connect connection to the transit VPCreate a peering",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create AWS Site-to-Site VPN connections from on premises to each VPC. Ensure that both VPN",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company has applications that run on Amazon EC2 instances. The EC2 instances connect to \nAmazon RDS databases by using an IAM role that has associated policies. The company wants \nto use AWS Systems Manager to patch the EC2 instances without disrupting the running \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n388 \napplications. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a new IAM role. Attach the AmazonSSMManagedInstanceCore policy to the new IAM role.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM user. Attach the AmazonSSMManagedInstanceCore policy to the IAM user.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Default Host Configuration Management in Systems Manager to manage the EC2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Remove the existing policies from the existing IAM role. Add the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon \nEKS) and the Kubernetes Horizontal Pod Autoscaler. The workload is not consistent throughout \nthe day. A solutions architect notices that the number of nodes does not automatically scale out \nwhen the existing nodes have reached maximum capacity in the cluster, which causes \nperformance issues. \n \nWhich solution will resolve this issue with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Scale out the nodes by tracking the memory usage.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an AWS Lambda function to resize the EKS cluster automatically.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon EC2 Auto Scaling group to distribute the workload.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "A company maintains about 300 TB in Amazon S3 Standard storage month after month. The S3 \nobjects are each typically around 50 GB in size and are frequently replaced with multipart uploads \nby their global application. The number and size of S3 objects remain constant, but the \ncompany's S3 storage costs are increasing each month. \n \nHow should a solutions architect reduce costs in this situation?",
    "options": [
      {
        "id": 0,
        "text": "Switch from multipart uploads to Amazon S3 Transfer Acceleration.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure S3 inventory to prevent objects from being archived too quickly.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudFront to reduce the number of objects stored in Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company has deployed a multiplayer game for mobile devices. The game requires live location \ntracking of players based on latitude and longitude. The data store for the game must support \nrapid updates and retrieval of locations. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n389 \nThe game uses an Amazon RDS for PostgreSQL DB instance with read replicas to store the \nlocation data. During peak usage periods, the database is unable to maintain the performance \nthat is needed for reading and writing updates. The game's user base is increasing rapidly. \n \nWhat should a solutions architect do to improve the performance of the data tier?",
    "options": [
      {
        "id": 0,
        "text": "Take a snapshot of the existing DB instance. Restore the snapshot with Multi-AZ enabled.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate from Amazon RDS to Amazon OpenSearch Service with OpenSearch Dashboards.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon DynamoDB Accelerator (DAX) in front of the existing DB instance. Modify the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon ElastiCache for Redis cluster in front of the existing DB instance. Modify the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company stores critical data in Amazon DynamoDB tables in the company's AWS account. An \nIT administrator accidentally deleted a DynamoDB table. The deletion caused a significant loss of \ndata and disrupted the company's operations. The company wants to prevent this type of \ndisruption in the future. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure a trail in AWS CloudTrail. Create an Amazon EventBridge rule for delete actions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a backup and restore plan for the DynamoDB tables. Recover the DynamoDB tables",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure deletion protection on the DynamoDB tables.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable point-in-time recovery on the DynamoDB tables.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 0 is incorrect:**\nis incorrect because partition placement groups are primarily used for large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. While they provide fault tolerance by distributing instances across partitions, they do not offer the low latency and high throughput benefits of cluster placement groups, which are more critical for HPC applications.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 47,
    "text": "A company runs a three-tier web application in a VPC across multiple Availability Zones. Amazon \nEC2 instances run in an Auto Scaling group for the application tier. \n \nThe company needs to make an automated scaling plan that will analyze each resource's daily \nand weekly historical workload trends. The configuration must scale resources appropriately \naccording to both the forecast and live changes in utilization. \n \nWhich scaling strategy should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement dynamic scaling with step scaling based on average CPU utilization from the EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an automated scheduled scaling action based on the traffic patterns of the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a simple scaling policy. Increase the cooldown period based on the EC2 instance startup",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A package delivery company has an application that uses Amazon EC2 instances and an \nAmazon Aurora MySQL DB cluster. As the application becomes more popular, EC2 instance \nusage increases only slightly. DB cluster usage increases at a much faster rate. \n \nThe company adds a read replica, which reduces the DB cluster usage for a short period of time. \nHowever, the load continues to increase. The operations that cause the increase in DB cluster \nusage are all repeated read statements that are related to delivery details. The company needs to \nalleviate the effect of repeated reads on the DB cluster. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Implement an Amazon ElastiCache for Redis cluster between the application and the DB cluster.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Add an additional read replica to the DB cluster.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Aurora Auto Scaling for the Aurora read replicas.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the DB cluster to have multiple writer instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company has an application that uses an Amazon DynamoDB table for storage. A solutions \narchitect discovers that many requests to the table are not returning the latest data. The \ncompany's users have not reported any other issues with database performance. Latency is in an \nacceptable range. \n \nWhich design change should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Add read replicas to the table.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a global secondary index (GSI).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Request strongly consistent reads for the table.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Request eventually consistent reads for the table.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 50,
    "text": "A company has deployed its application on Amazon EC2 instances with an Amazon RDS \ndatabase. The company used the principle of least privilege to configure the database access \ncredentials. The company's security team wants to protect the application and the database from \nSQL injection and other web-based attacks. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use security groups and network ACLs to secure the database and application servers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS WAF to protect the application. Use RDS parameter groups to configure the security",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Network Firewall to protect the application and the database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use different database accounts in the application code for different functions. Avoid granting",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "An ecommerce company runs applications in AWS accounts that are part of an organization in \nAWS Organizations. The applications run on Amazon Aurora PostgreSQL databases across all \nthe accounts. The company needs to prevent malicious activity and must identify abnormal failed \nand incomplete login attempts to the databases. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Attach service control policies (SCPs) to the root of the organization to identity the failed login",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member accounts of",
        "correct": true
      },
      {
        "id": 2,
        "text": "Publish the Aurora general logs to a log group in Amazon CloudWatch Logs. Export the log data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Publish all the Aurora PostgreSQL database events in AWS CloudTrail to a central Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "A company has an AWS Direct Connect connection from its corporate data center to its VPC in \nthe us-east-1 Region. The company recently acquired a corporation that has several VPCs and a \nDirect Connect connection between its on-premises data center and the eu-west-2 Region. The \nCIDR blocks for the VPCs of the company and the corporation do not overlap. The company \nrequires connectivity between two Regions and the data centers. The company needs a solution \nthat is scalable while reducing operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up inter-Region VPC peering between the VPC in us-east-1 and the VPCs in eu-west-2.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create private virtual interfaces from the Direct Connect connection in us-east-1 to the VPCs in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Establish VPN appliances in a fully meshed VPN network hosted by Amazon EC2. Use AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 54,
    "text": "A company has multiple AWS accounts with applications deployed in the us-west-2 Region. \nApplication logs are stored within Amazon S3 buckets in each account. The company wants to \nbuild a centralized log analysis solution that uses a single S3 bucket. Logs must not leave us-\nwest-2, and the company wants to incur minimal operational overhead. \n \nWhich solution meets these requirements and is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Lifecycle policy that copies the objects from one of the application S3 buckets to the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in",
        "correct": true
      },
      {
        "id": 2,
        "text": "Write a script that uses the PutObject API operation every day to copy the entire contents of the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write AWS Lambda functions in these accounts that are triggered every time logs are delivered to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 55,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n393 \nA company has an application that delivers on-demand training videos to students around the \nworld. The application also allows authorized content developers to upload videos. The data is \nstored in an Amazon S3 bucket in the us-east-2 Region. \n \nThe company has created an S3 bucket in the eu-west-2 Region and an S3 bucket in the ap-\nsoutheast-1 Region. The company wants to replicate the data to the new S3 buckets. The \ncompany needs to minimize latency for developers who upload videos and students who stream \nvideos near eu-west-2 and ap-southeast-1. \n \nWhich combination of steps will meet these requirements with the FEWEST changes to the \napplication? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A company has a new mobile app. Anywhere in the world, users can see local news on topics \nthey choose. Users also can post photos and videos from inside the app. \n \nUsers access content often in the first minutes after the content is posted. New content quickly \nreplaces older content, and then the older content disappears. The local nature of the news \nmeans that users consume 90% of the content within the AWS Region where it is uploaded. \n \nWhich solution will optimize the user experience by providing the LOWEST latency for content \nuploads?",
    "options": [
      {
        "id": 0,
        "text": "Upload and store content in Amazon S3. Use Amazon CloudFront for the uploads.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload and store content in Amazon S3 in the Region that is closest to the user. Use multiple",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company is building a new application that uses serverless architecture. The architecture will \nconsist of an Amazon API Gateway REST API and AWS Lambda functions to manage incoming \nrequests. \n \nThe company wants to add a service that can send messages received from the API Gateway \nREST API to multiple target Lambda functions for processing. The service must offer message \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n394 \nfiltering that gives the target Lambda functions the ability to receive only the messages the \nfunctions need. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Send the requests from the API Gateway REST API to an Amazon Simple Notification Service",
        "correct": true
      },
      {
        "id": 1,
        "text": "Send the requests from the API Gateway REST API to Amazon EventBridge. Configure",
        "correct": false
      },
      {
        "id": 2,
        "text": "Send the requests from the API Gateway REST API to Amazon Managed Streaming for Apache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Send the requests from the API Gateway REST API to multiple Amazon Simple Queue Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A company migrated millions of archival files to Amazon S3. A solutions architect needs to \nimplement a solution that will encrypt all the archival data by using a customer-provided key. The \nsolution must encrypt existing unencrypted objects and future objects. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use S3 Storage Lens metrics to identify unencrypted S3 buckets. Configure the S3 default",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "The DNS provider that hosts a company's domain name records is experiencing outages that \ncause service disruption for a website running on AWS. The company needs to migrate to a more \nresilient managed DNS service and wants the service to run on AWS. \n \nWhat should a solutions architect do to rapidly migrate the DNS hosting service?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Route 53 private hosted zone for the domain name. Import the zone file",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Simple AD directory in AWS. Enable zone transfer between the DNS provider and AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Route 53 Resolver inbound endpoint in the VPC. Specify the IP addresses",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "A company is building an application on AWS that connects to an Amazon RDS database. The \ncompany wants to manage the application configuration and to securely store and retrieve \ncredentials for the database and other services. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Lambda to store and manage the application configuration. Use AWS Systems",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an encrypted application configuration file. Store the file in Amazon S3 for the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS AppConfig to store and manage the application configuration. Use Amazon RDS to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "To meet security requirements, a company needs to encrypt all of its application data in transit \nwhile communicating with an Amazon RDS MySQL DB instance. A recent security audit revealed \nthat encryption at rest is enabled using AWS Key Management Service (AWS KMS), but data in \ntransit is not enabled. \n \nWhat should a solutions architect do to satisfy the security requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable IAM database authentication on the database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provide self-signed certificates. Use the certificates in all connections to the RDS instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption",
        "correct": false
      },
      {
        "id": 3,
        "text": "Download AWS-provided root certificates. Provide the certificates in all connections to the RDS",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company is designing a new web service that will run on Amazon EC2 instances behind an \nElastic Load Balancing (ELB) load balancer. However, many of the web service clients can only \nreach IP addresses authorized on their firewalls. \n \nWhat should a solutions architect recommend to meet the clients' needs? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n396",
    "options": [
      {
        "id": 0,
        "text": "A Network Load Balancer with an associated Elastic IP address.",
        "correct": false
      },
      {
        "id": 1,
        "text": "An Application Load Balancer with an associated Elastic IP address.",
        "correct": false
      },
      {
        "id": 2,
        "text": "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.",
        "correct": true
      },
      {
        "id": 3,
        "text": "An EC2 instance with a public IP address running as a proxy in front of the load balancer.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company has established a new AWS account. The account is newly provisioned and no \nchanges have been made to the default settings. The company is concerned about the security of \nthe AWS account root user. \n \nWhat should be done to secure the root user?",
    "options": [
      {
        "id": 0,
        "text": "Create IAM users for daily administrative tasks. Disable the root user.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root",
        "correct": true
      },
      {
        "id": 2,
        "text": "Generate an access key for the root user. Use the access key for daily administration tasks",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provide the root user credentials to the most senior solutions architect. Have the solutions",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company is deploying an application that processes streaming data in near-real time. The \ncompany plans to use Amazon EC2 instances for the workload. The network architecture must be \nconfigurable to provide the lowest possible latency between nodes. \n \nWhich combination of network solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable and configure enhanced networking on each EC2 instance.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Group the EC2 instances in separate accounts.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run the EC2 instances in a cluster placement group.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach multiple elastic network interfaces to each EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Elastic Block Store (Amazon EBS) optimized instance types.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  }
]