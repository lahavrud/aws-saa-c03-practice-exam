[
  {
    "id": 0,
    "text": "A financial services company wants to shut down two data centers and migrate more than 100 TB \nof data to AWS. The data has an intricate directory structure with millions of small files stored in \ndeep hierarchies of subfolders. Most of the data is unstructured, and the company's file storage \nconsists of SMB-based storage types from multiple vendors. The company does not want to \nchange its applications to access the data after migration. \n \nWhat should a solutions architect do to meet these requirements with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Direct Connect to migrate the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS DataSync to migrate the data to Amazon FSx for Lustre.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS DataSync to migrate the data to Amazon FSx for Windows File Server.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Direct Connect to migrate the data on-premises file storage to an AWS Storage",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company uses an organization in AWS Organizations to manage AWS accounts that contain \napplications. The company sets up a dedicated monitoring member account in the organization. \nThe company wants to query and visualize observability data across the accounts by using \nAmazon CloudWatch. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable CloudWatch cross-account observability for the monitoring account. Deploy an AWS",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up service control policies (SCPs) to provide access to CloudWatch in the monitoring account",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a new IAM user in the monitoring account. In each AWS account, configure an IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM user in the monitoring account. Create cross-account IAM policies in each",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company's website is used to sell products to the public. The site runs on Amazon EC2 \ninstances in an Auto Scaling group behind an Application Load Balancer (ALB). There is also an \nAmazon CloudFront distribution, and AWS WAF is being used to protect against SQL injection \nattacks. The ALB is the origin for the CloudFront distribution. A recent review of security logs \nrevealed an external malicious IP that needs to be blocked from accessing the website. \n \nWhat should a solutions architect do to protect the application?",
    "options": [
      {
        "id": 0,
        "text": "Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP",
        "correct": true
      },
      {
        "id": 2,
        "text": "Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company sets up an organization in AWS Organizations that contains 10 AWS accounts. A \nsolutions architect must design a solution to provide access to the accounts for several thousand \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n398 \nemployees. The company has an existing identity provider (IdP). The company wants to use the \nexisting IdP for authentication to AWS. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create IAM users for the employees in the required AWS accounts. Connect IAM users to the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS account root users with user email addresses and passwords that are synchronized",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Resource Access Manager (AWS RAM) to share access to the AWS accounts with the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A solutions architect is designing an AWS Identity and Access Management (IAM) authorization \nmodel for a company's AWS account. The company has designated five specific employees to \nhave full access to AWS services and resources in the AWS account. \n \nThe solutions architect has created an IAM user for each of the five designated employees and \nhas created an IAM user group. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Attach the AdministratorAccess resource-based policy to the IAM user group. Place each of the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Attach the SystemAdministrator identity-based policy to the IAM user group. Place each of the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Attach the AdministratorAccess identity-based policy to the IAM user group. Place each of the five",
        "correct": true
      },
      {
        "id": 3,
        "text": "Attach the SystemAdministrator resource-based policy to the IAM user group. Place each of the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company has a multi-tier payment processing application that is based on virtual machines \n(VMs). The communication between the tiers occurs asynchronously through a third-party \nmiddleware solution that guarantees exactly-once delivery. \n \nThe company needs a solution that requires the least amount of infrastructure management. The \nsolution must guarantee exactly-once delivery for application messaging. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda for the compute layers in the architecture.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 instances for the compute layers in the architecture.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) as the messaging component between",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the messaging component",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use containers that are based on Amazon Elastic Kubernetes Service (Amazon EKS) for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A company has a nightly batch processing routine that analyzes report files that an on-premises \nfile system receives daily through SFTP. The company wants to move the solution to the AWS \nCloud. The solution must be highly available and resilient. The solution also must minimize \noperational effort. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Transfer for SFTP and an Amazon Elastic File System (Amazon EFS) file system for",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy AWS Transfer for SFTP and an Amazon S3 bucket for storage. Modify the application to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 7,
    "text": "A company has users all around the world accessing its HTTP-based application deployed on \nAmazon EC2 instances in multiple AWS Regions. The company wants to improve the availability \nand performance of the application. The company also wants to protect the application against \ncommon web exploits that may affect availability, compromise security, or consume excessive \nresources. Static IP addresses are required. \n \nWhat should a solutions architect recommend to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Deploy AWS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company's data platform uses an Amazon Aurora MySQL database. The database has multiple \nread replicas and multiple DB instances across different Availability Zones. Users have recently \nreported errors from the database that indicate that there are too many connections. The \ncompany wants to reduce the failover time by 20% when a read replica is promoted to primary \nwriter. \n \nWhich solution will meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Switch from Aurora to Amazon RDS with Multi-AZ cluster deployment.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS Proxy in front of the Aurora database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Switch to Amazon DynamoDB with DynamoDB Accelerator (DAX) for read connections.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Switch to Amazon Redshift with relocation capability.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A company stores text files in Amazon S3. The text files include customer chat messages, date \nand time information, and customer personally identifiable information (PII). \n \nThe company needs a solution to provide samples of the conversations to an external service \nprovider for quality control. The external service provider needs to randomly pick sample \nconversations up to the most recent conversation. The company must not share the customer PII \nwith the external service provider. The solution must scale when the number of customer \nconversations increases. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Object Lambda Access Point. Create an AWS Lambda function that redacts the PII",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a batch process on an Amazon EC2 instance that regularly reads all new files, redacts the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a web application on an Amazon EC2 instance that presents a list of the files, redacts the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon DynamoDB table. Create an AWS Lambda function that reads only the data in",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company is running a legacy system on an Amazon EC2 instance. The application code cannot \nbe modified, and the system cannot run on more than one instance. A solutions architect must \ndesign a resilient solution that can improve the recovery time for the system. \n \nWhat should the solutions architect recommend to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n401",
    "options": [
      {
        "id": 0,
        "text": "Enable termination protection for the EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the EC2 instance for Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Launch the EC2 instance with two Amazon Elastic Block Store (Amazon EBS) volumes that use",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company wants to deploy its containerized application workloads to a VPC across three \nAvailability Zones. The company needs a solution that is highly available across Availability \nZones. The solution must require minimal changes to the application. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS Service Auto",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) self-managed nodes. Configure",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 Reserved Instances. Launch three EC2 instances in a spread placement",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Lambda function. Configure the Lambda function to connect to a VPC. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A media company stores movies in Amazon S3. Each movie is stored in a single video file that \nranges from 1 GB to 10 GB in size. \n \nThe company must be able to provide the streaming content of a movie within 5 minutes of a user \npurchase. There is higher demand for movies that are less than 20 years old than for movies that \nare more than 20 years old. The company wants to minimize hosting service costs based on \ndemand. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store all media content in Amazon S3. Use S3 Lifecycle policies to move media data into the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store newer movie video files in S3 Standard. Store older movie video files in S3 Standard-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store newer movie video files in S3 Intelligent-Tiering. Store older movie video files in S3 Glacier",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store newer movie video files in S3 Standard. Store older movie video files in S3 Glacier Flexible",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A solutions architect needs to design the architecture for an application that a vendor provides as \na Docker container image. The container needs 50 GB of storage available for temporary files. \nThe infrastructure must be serverless. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function that uses the Docker container image with an Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function that uses the Docker container image with an Amazon Elastic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the AWS Fargate",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company needs to use its on-premises LDAP directory service to authenticate its users to the \nAWS Management Console. The directory service is not compatible with Security Assertion \nMarkup Language (SAML). \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS IAM Identity Center (AWS Single Sign-On) between AWS and the on-premises",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM policy that uses AWS credentials, and integrate the policy into LDAP.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a process that rotates the IAM credentials whenever LDAP credentials are updated.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Develop an on-premises custom identity broker application or process that uses AWS Security",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 0 is incorrect:**\nis incorrect because storing access keys, even encrypted, on S3 is generally not recommended for the root user. The root user should be used sparingly, and access keys should be avoided if possible. If access keys are absolutely necessary, they should be managed with extreme care and rotated regularly. Storing them on S3, even encrypted, introduces a potential vulnerability if the S3 bucket itself is compromised or if the encryption key is compromised.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company stores multiple Amazon Machine Images (AMIs) in an AWS account to launch its \nAmazon EC2 instances. The AMIs contain critical data and configurations that are necessary for \nthe company's operations. The company wants to implement a solution that will recover \naccidentally deleted AMIs quickly and efficiently. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Elastic Block Store (Amazon EBS) snapshots of the AMIs. Store the snapshots in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy all AMIs to another AWS account periodically.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a retention rule in Recycle Bin.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Upload the AMIs to an Amazon S3 bucket that has Cross-Region Replication.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A company wants to migrate its three-tier application from on premises to AWS. The web tier and \nthe application tier are running on third-party virtual machines (VMs). The database tier is running \non MySQL. \n \nThe company needs to migrate the application by making the fewest possible changes to the \narchitecture. The company also needs a database solution that can restore data to a specific \npoint in time. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the web tier and the application tier to Amazon EC2 instances in private subnets. Migrate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the web tier and the application tier to Amazon EC2 instances in public subnets. Migrate",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A development team is collaborating with another company to create an integrated product. The \nother company needs to access an Amazon Simple Queue Service (Amazon SQS) queue that is \ncontained in the development team's account. The other company wants to poll the queue \nwithout giving up its own account permissions to do so. \n \nHow should a solutions architect provide access to the SQS queue? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n404",
    "options": [
      {
        "id": 0,
        "text": "Create an instance profile that provides the other company access to the SQS queue.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM policy that provides the other company access to the SQS queue.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an SQS access policy that provides the other company access to the SQS queue.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company's developers want a secure way to gain SSH access on the company's Amazon EC2 \ninstances that run the latest version of Amazon Linux. The developers work remotely and in the \ncorporate office. \n \nThe company wants to use AWS services as a part of the solution. The EC2 instances are hosted \nin a VPC private subnet and access the internet through a NAT gateway that is deployed in a \npublic subnet. \n \nWhat should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a bastion host in the same subnet as the EC2 instances. Grant the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Site-to-Site VPN connection between the corporate network and the VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a bastion host in the public subnet of the VPConfigure the security groups and SSH keys",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach the AmazonSSMManagedInstanceCore IAM policy to an IAM role that is associated with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A pharmaceutical company is developing a new drug. The volume of data that the company \ngenerates has grown exponentially over the past few months. The company's researchers \nregularly require a subset of the entire dataset to be immediately available with minimal lag. \nHowever, the entire dataset does not need to be accessed on a daily basis. All the data currently \nresides in on-premises storage arrays, and the company wants to reduce ongoing capital \nexpenses. \n \nWhich storage solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Run AWS DataSync as a scheduled cron job to migrate the data to an Amazon S3 bucket on an",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS Storage Gateway file gateway with an Amazon S3 bucket as the target storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway volume gateway with cached volumes with an Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an AWS Site-to-Site VPN connection from the on-premises environment to AWS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A company has a business-critical application that runs on Amazon EC2 instances. The \napplication stores data in an Amazon DynamoDB table. The company must be able to revert the \ntable to any point within the last 24 hours. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure point-in-time recovery for the table.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Backup for the table.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an AWS Lambda function to make an on-demand backup of the table every hour.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on streams on the table to capture a log of all changes to the table in the last 24 hours.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company hosts an application used to upload files to an Amazon S3 bucket. Once uploaded, \nthe files are processed to extract metadata, which takes less than 5 seconds. The volume and \nfrequency of the uploads varies from a few files each hour to hundreds of concurrent uploads. \nThe company has asked a solutions architect to design a cost-effective architecture that will meet \nthese requirements. \n \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the files.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an object-created event notification within the S3 bucket to invoke an AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Kinesis Data Streams to process and send data to Amazon S3. Invoke an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Notification Service (Amazon SNS) topic to process the files",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A company's application is deployed on Amazon EC2 instances and uses AWS Lambda \nfunctions for an event-driven architecture. The company uses nonproduction development \nenvironments in a different AWS account to test new features before the company deploys the \nfeatures to production. \n \nThe production instances show constant usage because of customers in different time zones. \nThe company uses nonproduction instances only during business hours on weekdays. The \ncompany does not use the nonproduction instances on the weekends. The company wants to \noptimize the costs to run its application on AWS. \n \nWhich solution will meet these requirements MOST cost-effectively? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n406",
    "options": [
      {
        "id": 0,
        "text": "Use On-Demand Instances for the production instances. Use Dedicated Hosts for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances for the production instances and the nonproduction instances. Shut",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Compute Savings Plans for the production instances. Use On-Demand Instances for the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Dedicated Hosts for the production instances. Use EC2 Instance Savings Plans for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A company stores data in an on-premises Oracle relational database. The company needs to \nmake the data available in Amazon Aurora PostgreSQL for analysis. The company uses an AWS \nSite-to-Site VPN connection to connect its on-premises network to AWS. \n \nThe company must capture the changes that occur to the source database during the migration to \nAurora PostgreSQL. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS DataSync to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Import the S3 data to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company built an application with Docker containers and needs to run the application in the \nAWS Cloud. The company wants to use a managed service to host the application. \n \nThe solution must scale in and out appropriately according to demand on the individual container \nservices. The solution also must not result in additional operational overhead or infrastructure to \nmanage. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an Amazon API Gateway API. Connect the API to AWS Lambda to run the containers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "An ecommerce company is running a seasonal online sale. The company hosts its website on \nAmazon EC2 instances spanning multiple Availability Zones. The company wants its website to \nmanage sudden traffic increases during the sale. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group that is large enough to handle peak traffic load. Stop half of the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto Scaling group for the website. Set the minimum size of the Auto Scaling group so",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront and Amazon ElastiCache to cache dynamic content with an Auto",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Auto Scaling group to scale out as traffic increases. Create a launch template to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 27,
    "text": "A solutions architect must provide an automated solution for a company's compliance policy that \nstates security groups cannot include a rule that allows SSH from 0.0.0.0/0. The company needs \nto be notified if there is any breach in the policy. A solution is needed as soon as possible. \n \nWhat should the solutions architect do to meet these requirements with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Write an AWS Lambda script that monitors security groups for SSH being open to 0.0.0.0/0",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable the restricted-ssh AWS Config managed rule and generate an Amazon Simple Notification",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an IAM role with permissions to globally open security groups and network ACLs. Create",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a service control policy (SCP) that prevents non-administrative users from creating or",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes. \n \nA company has deployed an application in an AWS account. The application consists of \nmicroservices that run on AWS Lambda and Amazon Elastic Kubernetes Service (Amazon EKS). \nA separate team supports each microservice. The company has multiple AWS accounts and \nwants to give each team its own account for its microservices. \n \nA solutions architect needs to design a solution that will provide service-to-service communication \nover HTTPS (port 443). The solution also must provide a service registry for service discovery. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n408 \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an inspection VPC. Deploy an AWS Network Firewall firewall to the inspection VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPC Lattice service network. Associate the microservices with the service network.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Network Load Balancer (NLB) with an HTTPS listener and target groups for each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create peering connections between VPCs that contain microservices. Create a prefix list for",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company has a mobile game that reads most of its metadata from an Amazon RDS DB \ninstance. As the game increased in popularity, developers noticed slowdowns related to the \ngame's metadata load times. Performance metrics indicate that simply scaling the database will \nnot help. A solutions architect must explore all options that include capabilities for snapshots, \nreplication, and sub-millisecond response times. \n \nWhat should the solutions architect recommend to solve these issues?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon Aurora with Aurora Replicas.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon DynamoDB with global tables.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon ElastiCache for Redis layer in front of the database.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Add an Amazon ElastiCache for Memcached layer in front of the database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "A company uses AWS Organizations for its multi-account AWS setup. The security organizational \nunit (OU) of the company needs to share approved Amazon Machine Images (AMIs) with the \ndevelopment OU. The AMIs are created by using AWS Key Management Service (AWS KMS) \nencrypted snapshots. \n \nWhich solution will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Add the development team's OU Amazon Resource Name (ARN) to the launch permission list for",
        "correct": true
      },
      {
        "id": 1,
        "text": "Add the Organizations root Amazon Resource Name (ARN) to the launch permission list for the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the key policy to allow the development team's OU to use the AWS KMS keys that are",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add the development team's account Amazon Resource Name (ARN) to the launch permission",
        "correct": false
      },
      {
        "id": 4,
        "text": "Recreate the AWS KMS key. Add a key policy to allow the Organizations root Amazon Resource",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A data analytics company has 80 offices that are distributed globally. Each office hosts 1 PB of \ndata and has between 1 and 2 Gbps of internet bandwidth. \n \nThe company needs to perform a one-time migration of a large amount of data from its offices to \nAmazon S3. The company must complete the migration within 4 weeks. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Establish a new 10 Gbps AWS Direct Connect connection to each office. Transfer the data to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use multiple AWS Snowball Edge storage-optimized devices to store and transfer the data to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an AWS Snowmobile to store and transfer the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an AWS Storage Gateway Volume Gateway to transfer the data to Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 32,
    "text": "A company has an Amazon Elastic File System (Amazon EFS) file system that contains a \nreference dataset. The company has applications on Amazon EC2 instances that need to read \nthe dataset. However, the applications must not be able to change the dataset. The company \nwants to use IAM access control to prevent the applications from being able to modify or delete \nthe dataset. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Mount the EFS file system in read-only mode from within the EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a resource policy for the EFS file system that denies the elasticfilesystem:ClientWrite",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an identity policy for the EFS file system that denies the elasticfilesystem:ClientWrite",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an EFS access point for each application. Use Portable Operating System Interface",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company has hired an external vendor to perform work in the company's AWS account. The \nvendor uses an automated tool that is hosted in an AWS account that the vendor owns. The \nvendor does not have IAM access to the company's AWS account. The company needs to grant \nthe vendor access to the company's AWS account. \n \nWhich solution will meet these requirements MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role in the company's account to delegate access to the vendor's IAM role. Attach",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM user in the company's account with a password that meets the password",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM group in the company's account. Add the automated tool's IAM user from the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user in the company's account that has a permission boundary that allows the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 34,
    "text": "A company wants to run its experimental workloads in the AWS Cloud. The company has a \nbudget for cloud spending. The company's CFO is concerned about cloud spending \naccountability for each department. The CFO wants to receive notification when the spending \nthreshold reaches 60% of the budget. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use cost allocation tags on AWS resources to label owners. Create usage budgets in AWS",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Cost Explorer forecasts to determine resource owners. Use AWS Cost Anomaly",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use cost allocation tags on AWS resources to label owners. Use AWS Support API on AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Cost Explorer forecasts to determine resource owners. Create usage budgets in AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company wants to deploy an internal web application on AWS. The web application must be \naccessible only from the company's office. The company needs to download security patches for \nthe web application from the internet. \n \nThe company has created a VPC and has configured an AWS Site-to-Site VPN connection to the \ncompany's office. A solutions architect must design a secure architecture for the web application. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the web application on Amazon EC2 instances in public subnets behind a public",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the web application on Amazon EC2 instances in private subnets behind an internal",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy the web application on Amazon EC2 instances in public subnets behind an internal",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the web application on Amazon EC2 instances in private subnets behind a public",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company maintains its accounting records in a custom application that runs on Amazon EC2 \ninstances. The company needs to migrate the data to an AWS managed service for development \nand maintenance of the application data. The solution must require minimal operational support \nand provide immutable, cryptographically verifiable logs of data changes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Copy the records from the application into an Amazon Redshift cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy the records from the application into an Amazon Neptune cluster.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the records from the application into an Amazon Timestream database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy the records from the application into an Amazon Quantum Ledger Database (Amazon",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 37,
    "text": "A company's marketing data is uploaded from multiple sources to an Amazon S3 bucket. A series \nof data preparation jobs aggregate the data for reporting. The data preparation jobs need to run \nat regular intervals in parallel. A few jobs need to run in a specific order later. \n \nThe company wants to remove the operational overhead of job error handling, retry logic, and \nstate management. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS Lambda function to process the data as soon as the data is uploaded to the S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Athena to process the data. Use Amazon EventBridge Scheduler to invoke Athena",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Glue DataBrew to process the data. Use an AWS Step Functions state machine to run",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Data Pipeline to process the data. Schedule Data Pipeline to process the data once at",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A solutions architect is designing a payment processing application that runs on AWS Lambda in \nprivate subnets across multiple Availability Zones. The application uses multiple Lambda \nfunctions and processes millions of transactions each day. \n \nThe architecture must ensure that the application does not process duplicate payments. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n412 \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Lambda to retrieve all due payments. Store the due payments in an Amazon DynamoDB",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company runs multiple workloads in its on-premises data center. The company's data center \ncannot scale fast enough to meet the company's expanding business needs. The company wants \nto collect usage and configuration data about the on-premises servers and workloads to plan a \nmigration to AWS. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set the home AWS Region in AWS Migration Hub. Use AWS Systems Manager to collect data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set the home AWS Region in AWS Migration Hub. Use AWS Application Discovery Service to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 40,
    "text": "A company has an organization in AWS Organizations that has all features enabled. The \ncompany requires that all API calls and logins in any existing or new AWS account must be \naudited. The company needs a managed solution to prevent additional work and to minimize \ncosts. The company also needs to know when any AWS account is not compliant with the AWS \nFoundational Security Best Practices (FSBP) standard. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS Control Tower environment in the Organizations management account. Enable",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an AWS Control Tower environment in a dedicated Organizations member account.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company has stored 10 TB of log files in Apache Parquet format in an Amazon S3 bucket. The \ncompany occasionally needs to use SQL to analyze the log files. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Aurora MySQL database. Migrate the data from the S3 bucket into Aurora by",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Redshift cluster. Use Redshift Spectrum to run SQL statements directly on the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Glue crawler to store and retrieve table metadata from the S3 bucket. Use",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon EMR cluster. Use Apache Spark SQL to run SQL statements directly on the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company needs a solution to prevent AWS CloudFormation stacks from deploying AWS \nIdentity and Access Management (IAM) resources that include an inline policy or \"*\" in the \nstatement. The solution must also prohibit deployment of Amazon EC2 instances with public IP \naddresses. The company has AWS Control Tower enabled in its organization in AWS \nOrganizations. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Control Tower proactive controls to block deployment of EC2 instances with public IP",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Control Tower detective controls to block deployment of EC2 instances with public IP",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Config to create rules for EC2 and IAM compliance. Configure the rules to run an AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a service control policy (SCP) to block actions for the EC2 instances and IAM resources if",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company's web application that is hosted in the AWS Cloud recently increased in popularity. \nThe web application currently exists on a single Amazon EC2 instance in a single public subnet. \nThe web application has not been able to meet the demand of the increased web traffic. \n \nThe company needs a solution that will provide high availability and scalability to meet the \nincreased user demand without rewriting the web application. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n414 \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Replace the EC2 instance with a larger compute optimized instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon EC2 Auto Scaling with multiple Availability Zones in private subnets.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a NAT gateway in a public subnet to handle web requests.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replace the EC2 instance with a larger memory optimized instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure an Application Load Balancer in a public subnet to distribute web traffic.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 44,
    "text": "A company has AWS Lambda functions that use environment variables. The company does not \nwant its developers to see environment variables in plaintext. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy code to Amazon EC2 instances instead of using Lambda functions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure SSL encryption on the Lambda functions to use AWS CloudHSM to store and encrypt",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a certificate in AWS Certificate Manager (ACM). Configure the Lambda functions to use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Key Management Service (AWS KMS) key. Enable encryption helpers on the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "An analytics company uses Amazon VPC to run its multi-tier services. The company wants to use \nRESTful APIs to offer a web analytics service to millions of users. Users must be verified by using \nan authentication service to access the APIs. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Cognito user pool for user authentication. Implement Amazon API Gateway",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure an Amazon Cognito identity pool for user authentication. Implement Amazon API",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS Lambda function to handle user authentication. Implement Amazon API",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an IAM user to handle user authentication. Implement Amazon API Gateway HTTP",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company has a mobile app for customers. The app's data is sensitive and must be encrypted at \nrest. The company uses AWS Key Management Service (AWS KMS). \n \nThe company needs a solution that prevents the accidental deletion of KMS keys. The solution \nmust use Amazon Simple Notification Service (Amazon SNS) to send an email notification to \nadministrators when a user attempts to delete a KMS key. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n415 \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon EventBridge rule that reacts when a user tries to delete a KMS key. Configure",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function that has custom logic to prevent KMS key deletion. Create an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge rule that reacts when the KMS DeleteKey operation is performed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS CloudTrail trail. Configure the trail to deliver logs to a new Amazon CloudWatch",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A company wants to analyze and generate reports to track the usage of its mobile app. The app \nis popular and has a global user base. The company uses a custom report building program to \nanalyze application usage. \n \nThe program generates multiple reports during the last week of each month. The program takes \nless than 10 minutes to produce each report. The company rarely uses the program to generate \nreports outside of the last week of each month The company wants to generate reports in the \nleast amount of time when the reports are requested. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Run the program by using Amazon EC2 On-Demand Instances. Create an Amazon EventBridge",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run the program in AWS Lambda. Create an Amazon EventBridge rule to run a Lambda function",
        "correct": true
      },
      {
        "id": 2,
        "text": "Run the program in Amazon Elastic Container Service (Amazon ECS). Schedule Amazon ECS to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run the program by using Amazon EC2 Spot Instances. Create an Amazon EventBndge rule to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 48,
    "text": "A company is designing a tightly coupled high performance computing (HPC) environment in the \nAWS Cloud. The company needs to include features that will optimize the HPC environment for \nnetworking and storage. \n \nWhich combination of solutions will meet these requirements? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n416",
    "options": [
      {
        "id": 0,
        "text": "Create an accelerator in AWS Global Accelerator. Configure custom routing for the accelerator.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon FSx for Lustre file system. Configure the file system with scratch storage.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution. Configure the viewer protocol policy to be HTTP and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch Amazon EC2 instances. Attach an Elastic Fabric Adapter (EFA) to the instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an AWS Elastic Beanstalk deployment to manage the environment.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company needs a solution to prevent photos with unwanted content from being uploaded to the \ncompany's web application. The solution must not involve training a machine learning (ML) \nmodel. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create and deploy a model by using Amazon SageMaker Autopilot. Create a real-time endpoint",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function that uses Amazon Rekognition to detect unwanted content.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront function that uses Amazon Comprehend to detect unwanted",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that uses Amazon Rekognition Video to detect unwanted",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company uses AWS to run its ecommerce platform. The platform is critical to the company's \noperations and has a high volume of traffic and transactions. The company configures a multi-\nfactor authentication (MFA) device to secure its AWS account root user credentials. The company \nwants to ensure that it will not lose access to the root user account if the MFA device is lost. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up a backup administrator account that the company can use to log in if the company loses",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add multiple MFA devices for the root user account to handle the disaster scenario.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new administrator account when the company cannot access the root account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach the administrator policy to another IAM user when the company cannot access the root",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A social media company is creating a rewards program website for its users. The company gives \nusers points when users create and upload videos to the website. Users redeem their points for \ngifts or discounts from the company's affiliated partners. A unique ID identifies users. The \npartners refer to this ID to verify user eligibility for rewards. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n417 \n \nThe partners want to receive notification of user IDs through an HTTP endpoint when the \ncompany gives users points. Hundreds of vendors are interested in becoming affiliated partners \nevery day. The company wants to design an architecture that gives the website the ability to add \npartners rapidly in a scalable way. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Timestream database to keep a list of affiliated partners. Implement an AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an endpoint",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Step Functions state machine. Create a task for every affiliated partner. Invoke",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a data stream in Amazon Kinesis Data Streams. Implement producer and consumer",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company needs to extract the names of ingredients from recipe records that are stored as text \nfiles in an Amazon S3 bucket. A web application will use the ingredient names to query an \nAmazon DynamoDB table and determine a nutrition score. \n \nThe application can handle non-food records and errors. The company does not have any \nemployees who have machine learning knowledge to develop this solution. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an Amazon EventBridge rule to invoke an AWS Lambda function when PutObject requests",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon EventBridge rule to invoke an AWS Lambda function when a PutObject request",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company needs to create an AWS Lambda function that will run in a VPC in the company's \nprimary AWS account. The Lambda function needs to access files that the company stores in an \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n418 \nAmazon Elastic File System (Amazon EFS) file system. The EFS file system is located in a \nsecondary AWS account. As the company adds files to the file system, the solution must scale to \nmeet the demand. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a new EFS file system in the primary account. Use AWS DataSync to copy the contents of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPC peering connection between the VPCs that are in the primary account and the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a second Lambda function in the secondary account that has a mount that is configured",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the contents of the file system to a Lambda layer. Configure the Lambda layer's",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 0 is incorrect:**\nis incorrect because simple scaling policies react to alarms based on thresholds. While you *could* use simple scaling, it requires manual configuration of the scaling adjustment (how many instances to add or remove) and doesn't automatically adjust to maintain a target value. It's less sophisticated and requires more manual intervention than target tracking.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A financial company needs to handle highly sensitive data. The company will store the data in an \nAmazon S3 bucket. The company needs to ensure that the data is encrypted in transit and at \nrest. The company must manage the encryption keys outside the AWS Cloud. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key",
        "correct": false
      },
      {
        "id": 1,
        "text": "Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key",
        "correct": false
      },
      {
        "id": 2,
        "text": "Encrypt the data in the S3 bucket with the default server-side encryption (SSE).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the data at the company's data center before storing the data in the S3 bucket.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company wants to run its payment application on AWS. The application receives payment \nnotifications from mobile devices. Payment notifications require a basic validation before they are \nsent for further processing. \n \nThe backend processing application is long running and requires compute and memory to be \nadjusted. The company does not want to manage the infrastructure. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon API Gateway API. Integrate the API with an AWS Step Functions state",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon API Gateway API. Integrate the API with AWS Lambda to receive payment",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A solutions architect is designing a user authentication solution for a company. The solution must \ninvoke two-factor authentication for users that log in from inconsistent geographical locations, IP \naddresses, or devices. The solution must also be able to scale up to accommodate millions of \nusers. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Cognito user pools for user authentication. Enable the risk-based adaptive",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure Amazon Cognito identity pools for user authentication. Enable multi-factor",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Identity and Access Management (IAM) users for user authentication. Attach an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On) authentication for user authentication.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company has an Amazon S3 data lake. The company needs a solution that transforms the data \nfrom the data lake and loads the data into a data warehouse every day. The data warehouse \nmust have massively parallel processing (MPP) capabilities. \n \nData analysts then need to create and train machine learning (ML) models by using SQL \ncommands on the data. The solution must use serverless AWS services wherever possible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Run a daily Amazon EMR job to transform the data and load the data into Amazon Redshift. Use",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run a daily Amazon EMR job to transform the data and load the data into Amazon Aurora",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run a daily AWS Glue job to transform the data and load the data into Amazon Redshift",
        "correct": true
      },
      {
        "id": 3,
        "text": "Run a daily AWS Glue job to transform the data and load the data into Amazon Athena tables.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SNS is a publish/subscribe messaging service that does not guarantee message order. While Lambda can process the updates, SNS is not the right choice for ordered processing. Also, running a SQL database on an EC2 instance increases management overhead compared to a managed database service like DynamoDB or RDS.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company runs containers in a Kubernetes environment in the company's local data center. The \ncompany wants to use Amazon Elastic Kubernetes Service (Amazon EKS) and other AWS \nmanaged services. Data must remain locally in the company's data center and cannot be stored \nin any remote site or cloud to maintain compliance. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Local Zones in the company's data center.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an AWS Snowmobile in the company's data center.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Install an AWS Outposts rack in the company's data center.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Install an AWS Snowball Edge Storage Optimized node in the data center.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A social media company has workloads that collect and process data. The workloads store the \ndata in on-premises NFS storage. The data store cannot scale fast enough to meet the \ncompany's expanding business needs. The company wants to migrate the current data store to \nAWS. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Set up an AWS Storage Gateway Volume Gateway. Use an Amazon S3 Lifecycle policy to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an AWS Storage Gateway Amazon S3 File Gateway. Use an Amazon S3 Lifecycle policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the Amazon Elastic File System (Amazon EFS) Standard-Infrequent Access (Standard-IA)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the Amazon Elastic File System (Amazon EFS) One Zone-Infrequent Access (One Zone-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 60,
    "text": "A company uses high concurrency AWS Lambda functions to process a constantly increasing \nnumber of messages in a message queue during marketing events. The Lambda functions use \nCPU intensive code to process the messages. The company wants to reduce the compute costs \nand to maintain service latency for its customers. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure reserved concurrency for the Lambda functions. Decrease the memory allocated to the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure reserved concurrency for the Lambda functions. Increase the memory according to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure provisioned concurrency for the Lambda functions. Decrease the memory allocated to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure provisioned concurrency for the Lambda functions. Increase the memory according to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 61,
    "text": "A company runs its workloads on Amazon Elastic Container Service (Amazon ECS). The \ncontainer images that the ECS task definition uses need to be scanned for Common \nVulnerabilities and Exposures (CVEs). New container images that are created also need to be \nscanned. \n \nWhich solution will meet these requirements with the FEWEST changes to the workloads?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository to store the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the container images in an Amazon S3 bucket. Use Amazon Macie to scan the images.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the workloads to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the container images in an Amazon S3 bucket that has versioning enabled. Configure an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 62,
    "text": "A company uses an AWS Batch job to run its end-of-day sales process. The company needs a \nserverless solution that will invoke a third-party reporting application when the AWS Batch job is \nsuccessful. The reporting application has an HTTP API interface that uses username and \npassword authentication. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon EventBridge rule to match incoming AWS Batch job SUCCEEDED events.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon EventBridge Scheduler to match incoming AWS Batch job SUCCEEDED",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company collects and processes data from a vendor. The vendor stores its data in an Amazon \nRDS for MySQL database in the vendor's own AWS account. The company's VPC does not have \nan internet gateway, an AWS Direct Connect connection, or an AWS Site-to-Site VPN \nconnection. The company needs to access the data that is in the vendor database. \n \nWhich solution will meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Instruct the vendor to sign up for the AWS Hosted Connection Direct Connect Program. Use VPC",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a client VPN connection between the company's VPC and the vendor's VPC. Use VPC",
        "correct": false
      },
      {
        "id": 2,
        "text": "Instruct the vendor to create a Network Load Balancer (NLB). Place the NLB in front of the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Transit Gateway to integrate the company's VPC and the vendor's VPC. Use VPC",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company wants to set up Amazon Managed Grafana as its visualization tool. The company \nwants to visualize data from its Amazon RDS database as one data source. The company needs \na secure solution that will not expose the data over the internet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Managed Grafana workspace without a VPC. Create a public endpoint for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Managed Grafana workspace in a VPC. Create a private endpoint for the RDS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon Managed Grafana workspace without a VPCreate an AWS PrivateLink",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Managed Grafana workspace in a VPC. Create a public endpoint for the RDS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.",
    "domain": "Design High-Performing Architectures"
  }
]