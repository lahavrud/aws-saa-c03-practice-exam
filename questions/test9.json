[
  {
    "id": 0,
    "text": "A company has a website hosted on AWS The website is behind an Application Load Balancer \n(ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward \nall requests to the website so that the requests will use HTTPS. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Update the ALB's network ACL to accept only HTTPS traffic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a rule that replaces the HTTP in the URL with HTTPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the ALB with a Network Load Balancer configured to use Server Name Indication",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by configuring the ALB to automatically redirect all incoming HTTP requests to their HTTPS equivalent. This ensures that all users accessing the website are using a secure connection. Creating a listener rule on the ALB that listens for HTTP traffic (port 80) and redirects it to HTTPS (port 443) is the standard and most efficient way to enforce HTTPS redirection.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Network ACLs (NACLs) act as a firewall at the subnet level. While they can block HTTP traffic, they cannot redirect it to HTTPS. Blocking HTTP traffic entirely would prevent users from accessing the website at all, even if they tried to use HTTPS directly. The requirement is to redirect, not block.\n\n**Why option 1 is incorrect:**\nThis is incorrect because ALBs do not have the capability to directly manipulate the URL in the way described. While ALBs can perform URL-based routing, they cannot rewrite the protocol from HTTP to HTTPS within the URL itself. This would require more complex solutions involving custom code or other services, which is unnecessary given the simpler and more direct solution of using listener rules.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A company is developing a two-tier web application on AWS. The company's developers have \ndeployed the application on an Amazon EC2 instance that connects directly to a backend \nAmazon RDS database. The company must not hardcode database credentials in the application. \nThe company must also implement a solution to automatically rotate the database credentials on \na regular basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the database credentials in the instance metadata.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the database credentials as a secret in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by providing a centralized, secure, and managed service for storing secrets. AWS Secrets Manager is specifically designed for managing database credentials, API keys, and other sensitive information. It offers automatic rotation capabilities, which eliminates the need for manual rotation and reduces the risk of using compromised credentials. The application can retrieve the credentials programmatically using the AWS SDK, avoiding hardcoding. Secrets Manager integrates well with RDS and other AWS services, simplifying the implementation and reducing operational overhead compared to other options.\n\n**Why option 0 is incorrect:**\nStoring database credentials in instance metadata is generally not recommended for sensitive information like database passwords. While instance metadata is accessible from within the instance, it's not designed for secure storage of secrets and lacks built-in rotation capabilities. It would require custom scripting and management to implement rotation, increasing operational overhead and complexity. Also, it's not the intended use case for instance metadata.\n\n**Why option 1 is incorrect:**\nStoring credentials in an encrypted S3 bucket is more secure than storing them in instance metadata, but it still requires significant operational overhead for managing encryption keys, access policies, and implementing automatic rotation. The application would need to download the configuration file, decrypt it, and parse the credentials. Implementing automatic rotation would involve creating and managing Lambda functions or other automation tools to update the file and rotate the credentials, increasing complexity. This approach is less streamlined and more complex than using a dedicated secrets management service.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company is deploying a new public web application to AWS. The application will run behind an \nApplication Load Balancer (ALB).  \nThe application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by \nan external certificate authority (CA). \nThe certificate must be rotated each year before the certificate expires. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of using a certificate issued by an external CA. ACM allows importing certificates obtained from external CAs, making them available for use with AWS services like Application Load Balancers. While ACM cannot automatically renew imported certificates, the question only states the certificate must be rotated annually, not that it must be automated. Importing allows for centralized management of the certificate within AWS.\n\n**Why option 0 is incorrect:**\nThis is incorrect because the question explicitly states that the certificate must be issued by an *external* certificate authority. ACM can issue certificates, but they are managed by AWS and not from an external CA as required.\n\n**Why option 1 is incorrect:**\nThis is incorrect for the same reason as option 0. ACM issuing certificates does not fulfill the requirement of using a certificate from an external CA.\n\n**Why option 2 is incorrect:**\nThis is incorrect because ACM Private CA is used for issuing private certificates, which are typically used for internal applications and services, not public-facing web applications. The question specifies a public web application, implying the need for a publicly trusted certificate from an external CA.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its \ndocument management application. The company intends to create a product that converts \nlarge .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to \nstore the original files and the converted files. A solutions architect must design a scalable \nsolution to accommodate demand that will grow rapidly over time. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Save the .pdf files to Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-effective and scalable option for storing large files. Amazon S3 is designed for object storage and offers virtually unlimited storage capacity. It provides various storage classes to optimize costs based on access frequency. S3 is highly durable and available, making it suitable for storing important data like original .pdf files.\n\n**Why option 1 is incorrect:**\nDynamoDB is a NoSQL database, which is not designed for storing large binary files like .pdf documents. While DynamoDB can technically store binary data, it's not cost-effective or efficient for this purpose. DynamoDB is optimized for key-value or document data, and storing large files would lead to high storage costs and performance issues. Also, DynamoDB Streams are for data changes within the database, not for triggering file conversions directly from uploaded files.\n\n**Why option 2 is incorrect:**\nElastic Beanstalk is a platform-as-a-service (PaaS) that simplifies deploying and managing web applications. While you can store files within an Elastic Beanstalk environment, it's not the most cost-effective or scalable solution for storing large files. Elastic Beanstalk typically uses EC2 instances for storage, which are more expensive than S3 for storing large amounts of data. Managing storage within EC2 instances also requires more operational overhead. Furthermore, Elastic Beanstalk is designed for running applications, not primarily for file storage.\n\n**Why option 3 is incorrect:**\nElastic Beanstalk is a platform-as-a-service (PaaS) that simplifies deploying and managing web applications. While you can store files within an Elastic Beanstalk environment, it's not the most cost-effective or scalable solution for storing large files. Elastic Beanstalk typically uses EC2 instances for storage, which are more expensive than S3 for storing large amounts of data. Managing storage within EC2 instances also requires more operational overhead. Furthermore, Elastic Beanstalk is designed for running applications, not primarily for file storage.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company has more than 5 TB of file data on Windows file servers that run on premises. Users \nand applications interact with the data each day. \nThe company is moving its Windows workloads to AWS. As the company continues this process, \nthe company requires access to AWS and on-premises file storage with minimum latency. The \ncompany needs a solution that minimizes operational overhead and requires no significant \nchanges to the existing file access patterns. The company uses an AWS Site-to-Site VPN \nconnection for connectivity to AWS. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy and configure Amazon FSx for Windows File Server on AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy and configure an Amazon S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy and configure an Amazon S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy and configure Amazon FSx for Windows File Server on AWS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by deploying a native Windows file server in AWS. Amazon FSx for Windows File Server provides fully managed, highly available, and scalable Windows file servers. Since the company is moving Windows workloads to AWS, having the file server in AWS minimizes latency for those workloads. The existing Site-to-Site VPN allows on-premises users to access the FSx file share using standard SMB protocols, maintaining existing file access patterns. FSx is a managed service, reducing operational overhead.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because while it deploys FSx for Windows File Server in AWS, it doesn't directly address the low latency requirement for on-premises users. On-premises users would still need to access the file share over the Site-to-Site VPN, which might introduce latency. The question requires access to AWS and on-premises file storage with minimum latency, which this option does not fully satisfy for on-premises users.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because S3 File Gateway caches data in S3. While it provides local access to frequently used files, it doesn't provide a native Windows file server experience. The question states that the company requires no significant changes to the existing file access patterns, which implies the need for SMB protocol support. S3 File Gateway does not directly support SMB. It also introduces additional complexity in managing the gateway and syncing data with S3. Furthermore, the question specifies a need for access to AWS and on-premises file storage with minimum latency, and S3 File Gateway primarily focuses on caching data from S3 on-premises, not providing a low latency solution for AWS workloads accessing the same data.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The \nhospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n5 \nformat. The hospital needs to modify the Lambda code to identify protected health information \n(PHI) in the reports. Which solution will meet these requirements with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Use existing Python libraries to extract the text from the reports and to identify the PHI from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Textract to extract the text from the reports.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Textract to extract the text from the reports.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Rekognition to extract the text from the reports.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by providing a managed service specifically designed for extracting text and data from documents and images. Amazon Textract can handle both PDF and JPEG formats, automatically detecting text, tables, and forms. This eliminates the need to develop and maintain custom text extraction logic using Python libraries, which would involve significant operational overhead in terms of development, maintenance, and potential accuracy issues. Textract also offers features like detecting personally identifiable information (PII), which can be adapted to identify PHI, further reducing the effort required to implement the solution.\n\n**Why option 0 is incorrect:**\nWhile using existing Python libraries might seem appealing due to familiarity, it introduces significant operational overhead. Implementing robust text extraction from PDFs and JPEGs using libraries like PyPDF2, Tesseract OCR, or PIL requires considerable development effort, including handling different document layouts, image quality variations, and potential OCR errors. Furthermore, maintaining and updating these libraries and the associated code adds to the operational burden. Identifying PHI would then require additional custom code and regular updates to comply with evolving regulations.\n\n**Why option 1 is incorrect:**\nAmazon Rekognition is primarily designed for image analysis and facial recognition, not for extracting text from documents. While it can detect text in images, its text extraction capabilities are limited compared to Amazon Textract, especially for complex documents like PDFs. Using Rekognition would likely result in lower accuracy and require more complex pre-processing and post-processing steps, increasing operational overhead.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A company has an application that generates a large number of files, each approximately 5 MB in \nsize. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 \nyears before they can be deleted. Immediate accessibility is always required as the files contain \ncritical business data that is not easy to reproduce. The files are frequently accessed in the first \n30 days of the object creation but are rarely accessed after the first 30 days. \nWhich storage solution is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket lifecycle policy to move Mm from S3 Standard to S3 Glacier 30 days from",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 bucket lifecycle policy to move tiles from S3 Standard to S3 One Zone-infrequent",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket lifecycle policy to move files from S3 Standard-infrequent Access (S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 bucket Lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the most cost-effective solution because it leverages S3 Standard for the initial 30 days when the files are frequently accessed. After 30 days, the files are moved to S3 Standard-Infrequent Access (S3 Standard-IA), which offers lower storage costs for infrequently accessed data while still providing immediate accessibility. This balances the need for immediate access with cost optimization for the long-term storage requirement. The 4-year retention requirement is also met by keeping the files in S3 Standard-IA.\n\n**Why option 0 is incorrect:**\nThis is incorrect because S3 Glacier is designed for archival data and has retrieval times ranging from minutes to hours, which violates the requirement for immediate accessibility. While Glacier is cost-effective for long-term storage, it is not suitable when immediate access is needed.\n\n**Why option 1 is incorrect:**\nThis is incorrect because S3 One Zone-Infrequent Access (S3 One Zone-IA) stores data in a single Availability Zone, making it less resilient than other S3 storage classes. While it is cheaper than S3 Standard-IA, it is not recommended for critical business data that is not easy to reproduce, as data loss is possible if the Availability Zone becomes unavailable. The question states that the data is critical and not easily reproducible, making S3 One Zone-IA an unacceptable risk.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company hosts an application on multiple Amazon EC2 instances. The application processes \nmessages from an Amazon SQS queue writes to an Amazon RDS table and deletes the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n6 \nmessage from the queue Occasional duplicate records are found in the RDS table. The SQS \nqueue does not contain any duplicate messages. \n \nWhat should a solutions architect do to ensure messages are being processed once only?",
    "options": [
      {
        "id": 0,
        "text": "Use the CreateQueue API call to create a new queue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Add Permission API call to add appropriate permissions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the ReceiveMessage API call to set an appropriate wail time",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the ChangeMessageVisibility APi call to increase the visibility timeout",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the problem of duplicate processing by increasing the visibility timeout. The visibility timeout is the amount of time that a message is invisible to other consumers after a consumer receives it from the queue. If the application takes longer than the current visibility timeout to process a message, the message will become visible again and another instance might pick it up for processing, leading to duplicates. Increasing the visibility timeout provides the application with more time to process the message before it becomes visible again, thus reducing the chance of duplicate processing. The ChangeMessageVisibility API call allows you to dynamically adjust the visibility timeout for a specific message, allowing for fine-grained control.\n\n**Why option 0 is incorrect:**\nCreating a new queue does not solve the problem of duplicate processing. The issue is not with the queue itself, but with how messages are being handled by the consumers. A new queue would simply be a fresh queue with the same potential for duplicate processing if the visibility timeout is not properly configured.\n\n**Why option 1 is incorrect:**\nAdding permissions to the queue does not address the issue of duplicate processing. Permissions control who can access and perform actions on the queue, but they do not affect how messages are processed or how long they remain invisible after being received. The problem lies in the message processing logic and the visibility timeout.\n\n**Why option 2 is incorrect:**\nSetting an appropriate wait time using the ReceiveMessage API call is related to long polling and optimizing the retrieval of messages from the queue. While long polling can improve efficiency, it does not directly prevent duplicate processing. The issue of duplicate processing is primarily related to the visibility timeout and the application's ability to process messages within that time.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A solutions architect is designing a new hybrid architecture to extend a company s on-premises \ninfrastructure to AWS. The company requires a highly available connection with consistent low \nlatency to an AWS Region. The company needs to minimize costs and is willing to accept slower \ntraffic if the primary connection fails. \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Provision a VPN tunnel connection to a Region for private connectivity.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution directly addresses the requirements by providing a dedicated, private network connection to AWS. Direct Connect offers consistent low latency and high bandwidth compared to VPN connections over the public internet. While Direct Connect can be more expensive than VPN, it fulfills the primary need for a highly available, low-latency connection. The question states the company is willing to accept slower traffic if the primary connection fails, implying a secondary, less expensive connection (like a VPN) could be used for failover. This aligns with minimizing costs while prioritizing performance under normal circumstances.\n\n**Why option 1 is incorrect:**\nWhile a VPN connection is a valid option for hybrid connectivity and is generally less expensive than Direct Connect, it relies on the public internet. This makes it less reliable and subject to variable latency, failing to meet the requirement for a highly available connection with consistent low latency. VPN connections are suitable for less critical workloads or as a backup to a Direct Connect connection, but not as the primary solution in this scenario.\n\n**Why option 2 is incorrect:**\nThis option is a duplicate of option 0 and therefore also correct, but the question only asks for one solution. The analysis for option 0 applies here as well.\n\n**Why option 3 is incorrect:**\nThis option is a duplicate of option 0 and therefore also correct, but the question only asks for one solution. The analysis for option 0 applies here as well.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A company is running a business-critical web application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances are in an Auto Scaling group. The application \nuses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The \ncompany wants the application to be highly available with minimum downtime and minimum loss \nof data. \n \nWhich solution will meet these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Place the EC2 instances in different AWS Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Auto Scaling group to use multiple Availability Zones.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the Auto Scaling group to use one Availability Zone.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the Auto Scaling group to use multiple AWS Regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the high availability requirement by distributing the EC2 instances across multiple Availability Zones. If one AZ fails, the application can continue to operate using instances in the other AZs. The Application Load Balancer automatically distributes traffic to the healthy instances. This approach minimizes downtime and requires minimal operational effort compared to cross-region deployments.\n\n**Why option 0 is incorrect:**\nPlacing EC2 instances in different AWS Regions would provide disaster recovery capabilities but introduces significant complexity and operational overhead. It would require replicating data across regions, managing routing between regions, and dealing with increased latency. This is not the most efficient solution for high availability within the same geographic area and increases operational effort.\n\n**Why option 2 is incorrect:**\nConfiguring the Auto Scaling group to use only one Availability Zone would negate the benefits of high availability. If that single AZ fails, the entire application would become unavailable. This directly contradicts the requirement for minimal downtime.\n\n**Why option 3 is incorrect:**\nConfiguring the Auto Scaling group to use multiple AWS Regions, similar to option 0, provides disaster recovery but is overkill for high availability within a region. It introduces significant complexity in terms of data replication, routing, and management, increasing operational effort unnecessarily. The question explicitly asks for the solution with the *least* operational effort.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group \nis configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the \nweb service. \n \nThe company notices that the NLB is not detecting HTTP errors for the application. These errors \nrequire a manual restart of the EC2 instances that run the web service. The company needs to \nimprove the application's availability without writing custom scripts or code. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable HTTP health checks on the NLB. supplying the URL of the company's application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a cron job to the EC2 instances to check the local application's logs once each minute.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the NLB with an Application Load Balancer.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by replacing the NLB with an Application Load Balancer (ALB). ALBs support HTTP/HTTPS health checks, allowing them to detect application-level errors (e.g., HTTP 500 errors). When an ALB detects an unhealthy instance, it stops routing traffic to it, improving availability. This approach avoids custom scripting and directly addresses the problem of the NLB's inability to perform HTTP health checks.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Network Load Balancers (NLBs) operate at Layer 4 (TCP/UDP) and do not perform HTTP health checks. They can only check if a TCP connection can be established with the target instance. They cannot interpret HTTP response codes, so they won't detect HTTP errors like 500 Internal Server Error.\n\n**Why option 1 is incorrect:**\nThis is incorrect because it violates the requirement of not writing custom scripts or code. Adding a cron job involves writing a script to check application logs, which the question explicitly prohibits. Furthermore, this approach is less efficient and reliable than using a load balancer with built-in HTTP health checks.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company runs a shopping application that uses Amazon DynamoDB to store customer \ninformation. In case of data corruption, a solutions architect needs to design a solution that meets \na recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. \n \nWhat should the solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure DynamoDB global tables.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure DynamoDB point-in-time recovery.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Export the DynamoDB data to Amazon S3 Glacier on a daily basis.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the RPO and RTO requirements by enabling automated backups of the DynamoDB table. Point-in-time recovery allows restoring the table to any point in time within the last 35 days, providing granular recovery to meet the 15-minute RPO. The recovery process is relatively quick, typically completing within minutes to an hour, satisfying the 1-hour RTO.\n\n**Why option 0 is incorrect:**\nWhile Global Tables provide replication across regions for high availability and disaster recovery, they primarily address availability and not necessarily data corruption scenarios within a single region. If data corruption occurs in one region, it will be replicated to other regions, making it unsuitable for recovering from data corruption within the 15-minute RPO. Global Tables do not provide point-in-time recovery.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nEBS snapshots are not directly applicable to DynamoDB. DynamoDB manages its own storage and backup mechanisms. While you could potentially use AWS Data Pipeline or similar tools to export DynamoDB data to EBS volumes and then snapshot those volumes, this approach is complex, inefficient, and unlikely to meet the stringent 15-minute RPO and 1-hour RTO. Furthermore, restoring from EBS snapshots would involve a more complex process than restoring directly from DynamoDB's built-in features. Exporting to S3 Glacier is also not a suitable solution. Glacier is designed for long-term archival storage with retrieval times ranging from minutes to hours, which violates the 1-hour RTO. Daily backups also fail to meet the 15-minute RPO.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A company runs a photo processing application that needs to frequently upload and download \npictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect \nhas noticed an increased cost in data transfer fees and needs to implement a solution to reduce \nthese costs. \n \nHow can the solutions architect meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application Into a public subnet and allow it to route through an internet gateway to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by creating a direct, private connection between the VPC and S3. Traffic between the application and S3 will then stay within the AWS network, avoiding data transfer charges associated with internet gateways or NAT gateways. The endpoint policy allows you to control which S3 buckets can be accessed through the endpoint, enhancing security.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because deploying API Gateway in a public subnet and routing S3 calls through it will not reduce data transfer costs. API Gateway itself will incur costs, and data transfer between the application and API Gateway, and API Gateway and S3, will still be charged. API Gateway is not designed for direct, high-volume data transfer to S3 in this scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because a NAT gateway is used to allow instances in private subnets to access the internet. Using a NAT gateway for S3 access will incur data transfer costs, as traffic will be routed through the NAT gateway. While an endpoint policy can restrict access, it doesn't eliminate the data transfer charges associated with using a NAT gateway for S3 communication.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because deploying the application into a public subnet and routing traffic through an internet gateway will incur data transfer costs. Data transferred between the application and S3 via the internet gateway will be charged. The goal is to avoid using the internet gateway for S3 communication to minimize costs.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company recently launched Linux-based application instances on Amazon EC2 in a private \nsubnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet \nof a VPC. A solutions architect needs to connect from the on-premises network, through the \ncompany's internet connection, to the bastion host, and to the application servers. The solutions \narchitect must make sure that the security groups of all the EC2 instances will allow that access. \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because the bastion host acts as a secure gateway. It should only allow inbound SSH access from the company's public IP address. This limits access to the bastion host to only authorized connections originating from the on-premises network. Using the company's public IP address ensures that only traffic coming from their internet connection can reach the bastion host.\n\n**Why option 3 is correct:**\nThis is correct because the application instances should only be accessible from the bastion host. Allowing inbound SSH access from only the private IP address of the bastion host ensures that only traffic originating from the bastion host within the VPC can reach the application instances. This prevents direct access to the application instances from the internet or any other unauthorized source.\n\n**Why option 0 is incorrect:**\nThis option is incomplete. While restricting inbound access to the bastion host is necessary, it doesn't specify *what* IP address or range should be allowed. Simply saying 'only allows inbound access' is insufficient for a secure configuration. It needs to specify the source IP.\n\n**Why option 1 is incorrect:**\nThis option is incomplete. While restricting inbound access to the bastion host is necessary, it doesn't specify *what* IP address or range should be allowed. Simply saying 'only allows inbound access' is insufficient for a secure configuration. It needs to specify the source IP.\n\n**Why option 4 is incorrect:**\nThis is incorrect because using the public IP address of the bastion host in the security group of the application instances is not a secure practice. The public IP address of an EC2 instance can change, especially if the instance is stopped and started. Relying on a public IP address for security rules is unreliable and creates a security risk. The application instances should only allow traffic from the *private* IP address of the bastion host for a more secure and stable configuration.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A solutions architect is designing a two-tier web application. The application consists of a public-\nfacing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft \nSQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the \ncompany. \nHow should security groups be configured in this situation? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the security group for the database tier to allow inbound traffic on port 1433 from the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the database tier to allow outbound traffic on ports 443 and",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because the web tier is public-facing and needs to accept incoming HTTPS traffic. Port 443 is the standard port for HTTPS, and allowing inbound traffic from 0.0.0.0/0 makes the web application accessible from any IP address on the internet. This is necessary for a public-facing web application.\n\n**Why option 1 is incorrect:**\nThis is incorrect because outbound traffic on port 443 from 0.0.0.0/0 is not a security best practice. While the web tier might need to make outbound HTTPS requests, restricting the destination to specific services or IP ranges is more secure than allowing it to any IP address. This option doesn't directly address the requirement of making the web application accessible to users.\n\n**Why option 2 is incorrect:**\nThis is correct because the database tier should only accept inbound traffic on port 1433 (the default SQL Server port) from the web tier's security group. This limits access to the database only to the web servers, enhancing security. The question asks for two correct answers, and this is one of them.\n\n**Why option 3 is incorrect:**\nThis is incorrect because the database tier, residing in a private subnet, should not be initiating outbound traffic to arbitrary destinations on ports 443 or 1433. Outbound traffic should be restricted to specific services or IP ranges if needed, and allowing all outbound traffic is a security risk. Also, the database typically responds to requests, it doesn't initiate them.\n\n**Why option 4 is incorrect:**\nThis is incorrect because the database tier should only accept inbound traffic on port 1433 (the default SQL Server port) from the web tier's security group. Allowing inbound traffic on port 443 to the database server is unnecessary and a security risk, as it's not the port SQL Server uses for communication. Furthermore, allowing inbound traffic from anywhere is not a security best practice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company wants to move a multi-tiered application from on premises to the AWS Cloud to \nimprove the application's performance. The application consists of application tiers that \ncommunicate with each other by way of RESTful services. Transactions are dropped when one \ntier becomes overloaded. A solutions architect must design a solution that resolves these issues \nand modernizes the application. \n \nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch metrics to analyze the application performance history to determine",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon API Gateway can act as a front door for the application, routing requests to AWS Lambda functions. API Gateway can handle a large volume of requests and provides features like throttling, caching, and request validation, which can prevent overload. Lambda functions allow for serverless execution of code, scaling automatically based on demand. This combination provides a scalable, resilient, and operationally efficient solution for handling RESTful service calls between tiers, addressing the dropped transaction issue and modernizing the application by leveraging serverless technology.\n\n**Why option 1 is incorrect:**\nWhile analyzing application performance history is important for identifying bottlenecks and areas for improvement, it doesn't directly address the immediate issue of dropped transactions due to overload. CloudWatch metrics provide insights but don't actively prevent or mitigate overload situations. It's a monitoring tool, not a solution for handling request routing or scaling.\n\n**Why option 2 is incorrect:**\nAmazon SNS is a publish/subscribe messaging service primarily used for asynchronous communication. The application uses RESTful services, which are typically synchronous. SNS is not designed for handling synchronous request/response patterns between application tiers. Using SNS would require significant architectural changes to convert the application to an event-driven model, which is not implied by the question and would be less operationally efficient than using API Gateway and Lambda.\n\n**Why option 3 is incorrect:**\nAmazon SQS is a message queuing service used for asynchronous communication. Similar to SNS, SQS is not suitable for handling the synchronous RESTful service calls between the application tiers. Introducing SQS would require significant code changes to decouple the tiers and implement an asynchronous communication pattern. This would be more complex and less operationally efficient than using API Gateway and Lambda for the existing RESTful architecture.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A company receives 10 TB of instrumentation data each day from several machines located at a \nsingle factory. The data consists of JSON files stored on a storage area network (SAN) in an on-\npremises data center located within the factory. The company wants to send this data to Amazon \nS3 where it can be accessed by several additional systems that provide critical near-real-lime \nanalytics.  \nA secure transfer is important because the data is considered sensitive.  \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n11 \nWhich solution offers the MOST reliable data transfer?",
    "options": [
      {
        "id": 0,
        "text": "AWS DataSync over public internet",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS DataSync over AWS Direct Connect",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Database Migration Service (AWS DMS) over public internet",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution provides a reliable and secure method for transferring large amounts of data. AWS DataSync is specifically designed for efficiently and securely moving data between on-premises storage and AWS services like S3. Using AWS Direct Connect ensures a dedicated, private network connection, bypassing the public internet. This enhances security by avoiding potential internet-based threats and improves reliability due to the consistent and predictable network performance of a dedicated connection. The dedicated connection also provides better bandwidth and lower latency compared to transferring data over the public internet, which is crucial for handling 10 TB of data daily.\n\n**Why option 0 is incorrect:**\nWhile AWS DataSync is suitable for transferring data, using the public internet introduces security risks and potential unreliability. Transferring 10 TB of data daily over the public internet can be slow, inconsistent, and vulnerable to network congestion and security threats. This option does not meet the requirement for a secure transfer.\n\n**Why option 2 is incorrect:**\nAWS Database Migration Service (DMS) is primarily designed for migrating databases, not for transferring files from a SAN to S3. While DMS can technically be used to move data from file systems to databases, it is not optimized for this use case and would be an inefficient and complex solution compared to DataSync. Furthermore, using the public internet for the transfer introduces security and reliability concerns.\n\n**Why option 3 is incorrect:**\nAWS Database Migration Service (DMS) is primarily designed for migrating databases, not for transferring files from a SAN to S3. While DMS can technically be used to move data from file systems to databases, it is not optimized for this use case and would be an inefficient and complex solution compared to DataSync.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company needs to configure a real-time data ingestion architecture for its application. The \ncompany needs an API, a process that transforms data as the data is streamed, and a storage \nsolution for the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon API Gateway API to send data to AWS Glue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by using Amazon API Gateway to provide a managed API endpoint for data ingestion. The data is then sent to an Amazon Kinesis data stream, which is designed for real-time data ingestion and processing. Kinesis Data Streams can be configured to transform the data as it is streamed using Kinesis Data Analytics or Kinesis Data Firehose. This combination provides a real-time data ingestion architecture with minimal operational overhead, as API Gateway and Kinesis are managed services.\n\n**Why option 0 is incorrect:**\nUsing an EC2 instance to host an API increases operational overhead because you are responsible for managing the server, including patching, scaling, and security. While Kinesis Data Streams is a good choice for real-time ingestion, managing the API on EC2 adds unnecessary complexity and overhead.\n\n**Why option 1 is incorrect:**\nAWS Glue is primarily designed for batch-oriented ETL processes, not real-time data ingestion. While Glue can be used for some streaming scenarios, it's not the ideal choice for real-time data ingestion and transformation compared to Kinesis Data Streams. Also, hosting the API on an EC2 instance adds operational overhead.\n\n**Why option 3 is incorrect:**\nAWS Glue is not designed for real-time data ingestion. It is primarily used for batch ETL processes. While API Gateway can provide the API endpoint, sending the data directly to Glue doesn't fulfill the real-time requirement efficiently.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A company needs to keep user transaction data in an Amazon DynamoDB table. \nThe company must retain the data for 7 years. \nWhat is the MOST operationally efficient solution that meets these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n12",
    "options": [
      {
        "id": 0,
        "text": "Use DynamoDB point-in-time recovery to back up the table continuously.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Backup to create backup schedules and retention policies for the table.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an on-demand backup of the table by using the DynamoDB console.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirements by providing a managed backup service with scheduling and retention policies. AWS Backup simplifies the process of creating and managing backups, allowing for automated backups at specified intervals and the enforcement of retention policies to meet the 7-year requirement. It centralizes backup management across multiple AWS services, making it more operationally efficient than other options that require custom solutions or manual intervention.\n\n**Why option 0 is incorrect:**\nWhile DynamoDB point-in-time recovery (PITR) allows restoring the table to any point in time within the past 35 days, it does not provide a mechanism to retain backups for 7 years. PITR is primarily for recovery from accidental writes or deletes, not long-term archival. It also doesn't offer the centralized management and policy-driven approach of AWS Backup, making it less operationally efficient for long-term retention.\n\n**Why option 2 is incorrect:**\nCreating on-demand backups using the DynamoDB console requires manual intervention. This is not operationally efficient, especially for a long-term retention requirement. It also doesn't provide an automated way to manage retention policies, increasing the risk of non-compliance with the 7-year requirement.\n\n**Why option 3 is incorrect:**\nCreating an EventBridge rule to invoke a Lambda function to back up the DynamoDB table introduces unnecessary complexity and operational overhead. It requires custom code to handle the backup process and retention management. This approach is less operationally efficient than using a managed backup service like AWS Backup, which provides built-in scheduling and retention capabilities.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is planning to use an Amazon DynamoDB table for data storage. The company is \nconcerned about cost optimization. The table will not be used on most mornings. In the evenings, \nthe read and write traffic will often be unpredictable. When traffic spikes occur, they will happen \nvery quickly. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create a DynamoDB table in on-demand capacity mode.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a DynamoDB table with a global secondary index.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a DynamoDB table with provisioned capacity and auto scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best option because DynamoDB on-demand capacity mode automatically scales up or down based on the application's traffic. It charges only for the reads and writes your application performs, making it ideal for unpredictable workloads and periods of low usage. Since the table is not used on most mornings, on-demand capacity mode will not incur costs during those times. It also handles the unpredictable traffic spikes in the evenings without requiring manual intervention or over-provisioning.\n\n**Why option 1 is incorrect:**\nWhile a global secondary index (GSI) can improve query performance, it doesn't directly address the cost optimization requirement related to variable traffic patterns. GSIs add to the cost of DynamoDB, as writes to the base table are also written to the index. Creating a GSI without a clear need for improved query performance would increase costs unnecessarily.\n\n**Why option 2 is incorrect:**\nProvisioned capacity with auto scaling can be a cost-effective solution for predictable workloads. However, the question states that the traffic spikes are unpredictable and happen very quickly. Auto scaling takes time to adjust capacity, and it might not be fast enough to handle sudden spikes, potentially leading to throttled requests. Furthermore, even with auto scaling, the table will still be provisioned with a minimum capacity, incurring costs even during the low-usage mornings. On-demand capacity is more responsive to rapid changes and avoids the minimum capacity cost.\n\n**Why option 3 is incorrect:**\nConfiguring the table as a global table replicates the data across multiple AWS regions, which is not necessary based on the problem description. This would significantly increase costs and is only relevant if the application requires low-latency access from multiple regions or needs disaster recovery capabilities across regions. The question focuses solely on cost optimization within a single region.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for \nhelp with an application migration initiative. A solutions architect needs to share an Amazon \nMachine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The \nAMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses a customer managed \ncustomer master key (CMK) to encrypt EBS volume snapshots. \nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's \nAWS account?",
    "options": [
      {
        "id": 0,
        "text": "Make the encrypted AMI and snapshots publicly available.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the launchPermission property of the AMI.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Modify the launchPermission property of the AMI.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the most secure way to share the AMI. Modifying the launchPermission property of the AMI allows granting specific AWS accounts (in this case, the MSP Partner's account) the permission to launch instances from the AMI. This approach avoids making the AMI publicly available, which would be a significant security risk. Because the AMI is encrypted with a CMK, the MSP Partner's account will also need permission to use the CMK. This can be achieved by granting the MSP Partner's account access to the CMK through the CMK's key policy. This approach ensures that only the intended recipient can use the AMI and that the encryption is maintained throughout the sharing process. The other options either introduce unnecessary security risks or are not the most efficient or secure way to share an encrypted AMI.\n\n**Why option 0 is incorrect:**\nMaking the encrypted AMI and snapshots publicly available is extremely insecure. It exposes the AMI and its data to anyone, regardless of whether they are authorized to access it. Even though the AMI is encrypted, making it publicly available increases the risk of unauthorized access or misuse if the encryption is compromised or if the key is inadvertently exposed. This option violates the principle of least privilege and is a major security risk.\n\n**Why option 2 is incorrect:**\nExporting the AMI to an S3 bucket in the MSP Partner's AWS account is more complex and potentially less secure than directly sharing the AMI. It involves creating an S3 bucket, granting permissions for the source account to export the AMI, and then granting the MSP Partner's account access to the S3 bucket. This adds unnecessary steps and potential points of failure. Furthermore, if the S3 bucket is not properly secured, it could expose the AMI to unauthorized access. Sharing the AMI directly through launch permissions is a simpler and more secure approach.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect is designing the cloud architecture for a new application being deployed on \nAWS. The process should run in parallel while adding and removing application nodes as needed \nbased on the number of jobs to be processed. The processor application is stateless. The \nsolutions architect must ensure that the application is loosely coupled and the job items are \ndurably stored. \n \nWhich design should the solutions architect use?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon SNS topic to send the jobs that need to be processed.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon SQS queue to hold the jobs that need to be processed.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon SQS queue to hold the jobs that needs to be processed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon SNS topic to send the jobs that need to be processed.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements of durable storage and loose coupling. Amazon SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS queues provide durable storage for messages until they are processed, ensuring that no jobs are lost. The queue allows application nodes to consume jobs independently and in parallel, enabling scalability. The stateless nature of the processor application is also well-suited for this architecture, as each node can process jobs without relying on shared state.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Amazon SNS is a publish/subscribe messaging service, primarily designed for broadcasting messages to multiple subscribers. While it can be used for decoupling, it doesn't inherently provide durable storage for messages in the same way that SQS does. If a subscriber is unavailable when a message is published to an SNS topic, the message may be lost. SNS is better suited for fan-out scenarios where multiple services need to be notified of an event, not for ensuring that every job is processed.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because, while Amazon SQS is the correct service, the question is attempting to trick you by repeating the same option twice. Option 2 is the correct answer.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company hosts its web applications in the AWS Cloud. The company configures Elastic Load \nBalancers to use certificate that are imported into AWS Certificate Manager (ACM). The \ncompany's security team must be notified 30 days before the expiration of each certificate.  \nWhat should a solutions architect recommend to meet the requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Config rule that checks for certificates that will expire within 30 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS trusted Advisor to check for certificates that will expire within to days.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by leveraging AWS Config's ability to evaluate the configuration of AWS resources. AWS Config can be configured with a rule that specifically checks ACM certificates for their expiration date. The rule can be set to trigger when a certificate is within 30 days of expiration. This allows for proactive notification of the security team, meeting the stated requirement.\n\n**Why option 0 is incorrect:**\nWhile ACM can publish messages to SNS, it doesn't have a built-in rule engine for monitoring certificate expiration dates and triggering notifications based on a specific timeframe (30 days before expiration). Creating a custom solution using ACM and SNS would require significantly more effort and complexity compared to using AWS Config.\n\n**Why option 2 is incorrect:**\nAWS Trusted Advisor provides best practice checks and recommendations, including checks for expiring SSL certificates. However, it doesn't offer the granularity of specifying a 30-day notification window. Trusted Advisor is primarily a dashboard for viewing potential issues, not a system for automated, proactive notifications tailored to a specific timeframe. Also, Trusted Advisor checks are not as customizable or programmable as AWS Config rules.\n\n**Why option 3 is incorrect:**\nWhile EventBridge can react to events, ACM doesn't natively emit events specifically for certificate expiration warnings 30 days in advance. You would need to create a custom solution to monitor certificate expiration and generate the necessary EventBridge events, which is more complex and less efficient than using AWS Config.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A company's dynamic website is hosted using on-premises servers in the United States. The \ncompany is launching its product in Europe, and it wants to optimize site loading times for new \nEuropean users. The site's backend must remain in the United States. The product is being \nlaunched in a few days, and an immediate solution is needed. \n \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the website to Amazon S3. Use cross-Region replication between Regions.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using Amazon CloudFront, a content delivery network (CDN), to cache website content closer to European users. CloudFront can be configured with a custom origin pointing to the company's on-premises servers in the United States. When a user in Europe requests content, CloudFront will serve it from the nearest edge location, reducing latency and improving loading times. This approach avoids migrating the backend and provides a relatively quick and easy implementation.\n\n**Why option 0 is incorrect:**\nThis is incorrect because launching an EC2 instance in us-east-1 does not address the latency issue for European users. It simply moves the entire site to another location within the United States, which doesn't improve performance for users in Europe. Furthermore, migrating the entire site would take more time than the few days available.\n\n**Why option 1 is incorrect:**\nThis is incorrect because moving the website to Amazon S3 and using cross-Region replication is not suitable for a dynamic website. S3 is primarily for static content. While S3 can host a static website, the question specifies a dynamic website. Cross-Region replication would also not directly address the latency issue for European users accessing a dynamic website. It's also not a quick solution to implement.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A company wants to reduce the cost of its existing three-tier web architecture. The web, \napplication, and database servers are running on Amazon EC2 instances for the development, \ntest, and production environments. The EC2 instances average 30% CPU utilization during peak \nhours and 10% CPU utilization during non-peak hours. \n \nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run \nfor at least 8 hours each day. The company plans to implement automation to stop the \ndevelopment and test EC2 instances when they are not in use. \n \nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-\neffectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the production EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances for the production EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Spot blocks for the production EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use On-Demand Instances for the production EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the most cost-effective option for production instances because they run 24/7. Reserved Instances provide a significant discount compared to On-Demand pricing in exchange for a commitment to use the instance for a specified period (1 or 3 years). Since the production instances are always running, the commitment is easily met, resulting in substantial cost savings.\n\n**Why option 0 is incorrect:**\nUsing Spot Instances for production workloads is generally not recommended due to their interruptible nature. Spot Instances can be terminated with a two-minute warning if the Spot price exceeds your bid price. This unpredictability is unacceptable for production environments that require continuous availability.\n\n**Why option 2 is incorrect:**\nSpot Blocks offer a guaranteed duration (1 to 6 hours) for Spot Instances, but they are still subject to interruption after the specified block duration. While better than regular Spot Instances, they don't guarantee 24/7 availability for production workloads and are therefore not as suitable as Reserved Instances for a continuously running production environment. Furthermore, Spot Blocks are generally more expensive than Reserved Instances for long-term, consistent usage.\n\n**Why option 3 is incorrect:**\nOn-Demand Instances provide flexibility but are the most expensive EC2 purchasing option. Since the production instances run 24/7, there's no benefit to paying the higher On-Demand price when Reserved Instances offer a significant discount for a predictable workload.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 25,
    "text": "A company has a production web application in which users upload documents through a web \ninterlace or a mobile app. \n According to a new regulatory requirement, new documents cannot be modified or deleted after \nthey are stored. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the uploaded documents in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the regulatory requirement by storing the documents in Amazon S3 with both S3 Versioning and S3 Object Lock enabled. S3 Versioning ensures that every version of an object is preserved, preventing accidental overwrites or deletions. S3 Object Lock, specifically in 'Governance' or 'Compliance' mode, prevents objects from being deleted or overwritten for a specified retention period or until a certain date. This combination guarantees immutability and meets the regulatory requirement.\n\n**Why option 1 is incorrect:**\nStoring the uploaded documents in an Amazon S3 bucket alone does not provide any mechanism for preventing modification or deletion. While S3 offers durability and availability, it doesn't inherently enforce immutability. Without versioning or object lock, objects can be overwritten or deleted, failing to meet the regulatory requirement.\n\n**Why option 2 is incorrect:**\nStoring the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled only protects against accidental overwrites and deletions by creating new versions. It does not prevent a user with sufficient permissions from deleting all versions of an object, effectively removing it. Therefore, versioning alone does not guarantee immutability as required by the regulation.\n\n**Why option 3 is incorrect:**\nAmazon Elastic File System (EFS) is a network file system designed for use with EC2 instances. While EFS provides shared file storage, it does not offer built-in immutability features like S3 Object Lock. Files stored on EFS can be modified or deleted by users with appropriate permissions, failing to meet the regulatory requirement of preventing modification or deletion.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company has several web servers that need to frequently access a common Amazon RDS \nMySQL Multi-AZ DB instance. The company wants a secure method for the web servers to \nconnect to the database while meeting a security requirement to rotate user credentials \nfrequently. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the database user credentials in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the database user credentials in AWS Systems Manager OpsCenter.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the database user credentials in a secure Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the database user credentials in files encrypted with AWS Key Management Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because AWS Secrets Manager is specifically designed for managing secrets like database credentials. It allows you to store, retrieve, and rotate secrets securely. The web servers can retrieve the credentials programmatically, and Secrets Manager can automatically rotate the credentials on a schedule, meeting the security requirement of frequent rotation.\n\n**Why option 1 is incorrect:**\nAWS Systems Manager OpsCenter is designed for operational tasks and incident management, not for storing and rotating sensitive credentials. While Systems Manager Parameter Store (a different feature within Systems Manager) can store secrets, it doesn't offer the same level of automated rotation and auditing capabilities as Secrets Manager, making it less suitable for this scenario.\n\n**Why option 2 is incorrect:**\nStoring database credentials in an S3 bucket, even if it's secure, is not a best practice. It lacks built-in features for secret rotation and fine-grained access control specifically designed for managing credentials. Managing access and rotation would require significant custom development and would be more complex and error-prone than using Secrets Manager.\n\n**Why option 3 is incorrect:**\nWhile encrypting files with KMS adds a layer of security, it doesn't address the requirement of frequent credential rotation. Rotating the credentials would involve manually updating the encrypted files and distributing them to the web servers, which is a cumbersome and error-prone process. Secrets Manager provides automated rotation, making it a more efficient and secure solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company hosts an application on AWS Lambda functions mat are invoked by an Amazon API \nGateway API. The Lambda functions save customer data to an Amazon Aurora MySQL \ndatabase. Whenever the company upgrades the database, the Lambda functions fail to establish \ndatabase connections until the upgrade is complete. The result is that customer data Is not \nrecorded for some of the event. \nA solutions architect needs to design a solution that stores customer data that is created during \ndatabase upgrades. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon RDS proxy to sit between the Lambda functions and the database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the run time of me Lambda functions to the maximum.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Persist the customer data to Lambda local storage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the customer data in an Amazon Simple Queue Service (Amazon SOS) FIFO queue.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by providing a reliable queuing mechanism. Amazon SQS FIFO queues guarantee that messages are processed in the order they are sent and exactly once. During a database upgrade, the Lambda functions can enqueue the customer data into the SQS FIFO queue. Once the database is back online, another process (e.g., another Lambda function or an EC2 instance) can consume the messages from the queue and write the data to the database. This ensures no data is lost during the upgrade process and that the data is processed in the correct order.\n\n**Why option 0 is incorrect:**\nWhile an RDS Proxy can help manage database connections and potentially improve connection pooling, it does not solve the fundamental problem of database unavailability during upgrades. The proxy itself will likely also be unable to connect to the database during the upgrade window, and thus will not prevent data loss. It primarily addresses connection management and scaling, not temporary outages.\n\n**Why option 1 is incorrect:**\nIncreasing the Lambda function's runtime does not address the issue of database unavailability. The Lambda function will still fail to connect to the database during the upgrade, regardless of its configured runtime. It only allows the function to run longer if it is performing other tasks, but it does nothing to mitigate the connection errors during the database upgrade.\n\n**Why option 2 is incorrect:**\nLambda local storage is ephemeral and not guaranteed to persist across invocations, especially during errors or scaling events. If the Lambda function fails during the database upgrade, the data stored in local storage may be lost. Furthermore, Lambda local storage has limited capacity and is not designed for persistent data storage. It's more suited for temporary files or caching within a single invocation.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 28,
    "text": "A survey company has gathered data for several years from areas m\\ the United States. The \ncompany hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company \nhas started to share the data with a European marketing firm that has S3 buckets. The company \nwants to ensure that its data transfer costs remain as low as possible. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Requester Pays feature on the company's S3 bucket",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure cross-account access for the marketing firm so that the marketing firm has access to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the company's S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because configuring the Requester Pays feature on the company's S3 bucket shifts the responsibility for data transfer costs to the marketing firm when they access the data. Since the company wants to minimize its own costs, this is the most direct and cost-effective solution. The marketing firm will pay for the data they download from the bucket.\n\n**Why option 1 is incorrect:**\nThis is incorrect because S3 Cross-Region Replication would replicate the entire 3 TB dataset to a region closer to the marketing firm. While it might reduce latency for the marketing firm, the company would incur significant data transfer costs for the initial replication and ongoing replication of any changes. This directly contradicts the requirement to minimize the company's data transfer costs.\n\n**Why option 2 is incorrect:**\nThis is incorrect because configuring cross-account access simply grants the marketing firm permission to access the S3 bucket. It doesn't inherently reduce data transfer costs. The company would still be responsible for the data transfer costs when the marketing firm downloads the data. This does not address the cost optimization requirement.\n\n**Why option 3 is incorrect:**\nThis is incorrect because S3 Intelligent-Tiering is designed to optimize storage costs based on access patterns. While it can be beneficial for long-term storage, it doesn't directly address the data transfer costs associated with sharing the data with the marketing firm. The company would still incur data transfer costs when the marketing firm accesses the data, regardless of the storage tier. Syncing the S3 bucket is not a valid option.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 29,
    "text": "A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses \nbucket policies to restrict access to audit team IAM user credentials according to the principle of \nleast privilege. Company managers are worried about accidental deletion of documents in the S3 \nbucket and want a more secure solution. \n \nWhat should a solutions architect do to secure the audit documents?",
    "options": [
      {
        "id": 0,
        "text": "Enable the versioning and MFA Delete features on the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling versioning on the S3 bucket allows for the recovery of accidentally deleted objects. When an object is deleted, it's not permanently removed; instead, a delete marker is created. Versioning keeps previous versions of the object, allowing for restoration. Enabling MFA Delete adds an extra layer of security, requiring multi-factor authentication to permanently delete an object version. This prevents unauthorized or accidental permanent deletion, fulfilling the requirement for a more secure solution against accidental deletion.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while MFA on IAM users enhances security, it doesn't directly address the accidental deletion problem. It only prevents unauthorized access and modification but doesn't help recover accidentally deleted objects. The question specifically asks for a solution to prevent accidental deletion, not just unauthorized access.\n\n**Why option 2 is incorrect:**\nThis is incorrect because adding an S3 Lifecycle policy to deny the s3:DeleteObject permission to the audit team's IAM user accounts would prevent them from deleting objects at all. This is too restrictive and would hinder their ability to perform necessary audit tasks. The requirement is to prevent *accidental* deletion, not to completely block deletion capabilities.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while encrypting the S3 bucket with KMS enhances data security at rest, it doesn't directly address the issue of accidental deletion. Encryption protects against unauthorized access to the data, but it doesn't prevent authorized users from accidentally deleting objects. It also doesn't provide a mechanism for recovering deleted objects.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours. The company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue. Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Modify the DB instance to be a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a read replica of the database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Instruct the development team to manually export the entries in the database at the end of each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache to cache the common queries that the script runs against the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreating a read replica allows the script to run its queries against the replica instead of the primary database. This offloads the read workload from the primary database, preventing performance degradation for the development team. Read replicas are relatively easy to set up and manage, minimizing operational overhead. The data in the read replica is kept up-to-date with the primary database through asynchronous replication, ensuring the script has access to the latest movie data.\n\n**Why option 0 is incorrect:**\nModifying the DB instance to be a Multi-AZ deployment improves availability and provides failover capabilities, but it does not address the performance issue caused by the script's queries. The script would still be running against the primary database, regardless of whether it's a Single-AZ or Multi-AZ deployment. Multi-AZ is for high availability, not read scaling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nWhile Amazon ElastiCache can improve performance for frequently accessed data, it doesn't directly address the problem of the script overloading the database. The script still needs to query the database to get the initial data to populate the cache. Furthermore, caching the entire database or a significant portion of it in ElastiCache would likely be more complex to manage and potentially more expensive than using a read replica. The question specifies that the script runs at random intervals each day, so the benefit of caching common queries is limited as the queries are not consistently repeated.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company has applications that run on Amazon EC2 instances in a VPC. One of the applications \nneeds to call the Amazon S3 API to store and read objects. According to the company's security \nregulations, no traffic from the applications is allowed to travel across the internet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 interface endpoint.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 gateway endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an S3 bucket in a private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket in the same Region as the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating a gateway endpoint for S3 within the VPC. A gateway endpoint is a virtual device that allows traffic to S3 to stay within the AWS network, avoiding the public internet. It's a cost-effective and secure way to connect to S3 from EC2 instances in a private subnet.\n\n**Why option 0 is incorrect:**\nWhile interface endpoints provide private connectivity to AWS services, they use AWS PrivateLink, which incurs additional costs and is generally used for services other than S3. Gateway endpoints are specifically designed for S3 and DynamoDB and are more cost-effective for this use case.\n\n**Why option 2 is incorrect:**\nCreating an S3 bucket in a private subnet is not a valid concept. S3 buckets exist independently of subnets. Subnets are associated with EC2 instances and other resources within a VPC. The location of the S3 bucket does not inherently prevent traffic from going over the internet.\n\n**Why option 3 is incorrect:**\nCreating an S3 bucket in the same Region as the EC2 instance is a best practice for performance and cost, but it does not inherently prevent traffic from going over the internet. Without a gateway or interface endpoint, traffic to S3 will still traverse the public internet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company is storing sensitive user information in an Amazon S3 bucket. The company wants to \nprovide secure access to this bucket from the application tier running on Amazon EC2 instances \ninside a VPC. \nWhich combination of steps should a solutions architect take to accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure a VPC gateway endpoint for Amazon S3 within the VPC",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a bucket policy to make the objects to the S3 bucket public",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a bucket policy that limits access to only the application tier running in the VPC",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a NAT instance and have the EC2 instances use the NAT instance to access the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a VPC gateway endpoint for S3 allows EC2 instances within the VPC to access S3 without traversing the public internet. This enhances security by keeping the traffic within the AWS network and avoiding exposure to external threats. It also simplifies network configuration and reduces latency.\n\n**Why option 2 is correct:**\nThis is correct because a bucket policy can be configured to restrict access to the S3 bucket based on the VPC ID or the source IP address range of the EC2 instances within the VPC. This ensures that only the application tier running within the specified VPC has access to the sensitive data, preventing unauthorized access from other sources.\n\n**Why option 1 is incorrect:**\nThis is incorrect because making the objects in the S3 bucket public would expose the sensitive user information to anyone on the internet, violating the security requirements of the scenario. This is a highly insecure practice and should be avoided.\n\n**Why option 3 is incorrect:**\nThis is incorrect because embedding IAM user credentials directly on EC2 instances is a security risk. If the EC2 instance is compromised, the credentials could be exposed, allowing unauthorized access to the S3 bucket. Using IAM roles for EC2 instances is a more secure approach.\n\n**Why option 4 is incorrect:**\nThis is incorrect because while a NAT instance allows EC2 instances in a private subnet to access the internet, it's not the most secure or efficient way to access S3 from within a VPC. A VPC gateway endpoint provides a direct and secure connection to S3 without the need for internet access or NAT.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company runs an on-premises application that is powered by a MySQL database. The \ncompany is migrating the application to AWS to Increase the application's elasticity and \navailability. The current architecture shows heavy read activity on the database during times of \nnormal operation. Every 4 hours the company's development team pulls a full export of the \nproduction database to populate a database in the staging environment. During this period, users \nexperience unacceptable application latency. The development team is unable to use the staging \nenvironment until the procedure completes. \nA solutions architect must recommend replacement architecture that alleviates the application \nlatency issue.  \nThe replacement architecture also must give the development team the ability to continue using \nthe staging environment without delay. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon RDS for MySQL with a Mufti AZ deployment and read replicas for production.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by utilizing Aurora MySQL with Multi-AZ Aurora Replicas. Aurora's architecture is designed for high performance and availability. The Multi-AZ deployment ensures failover capabilities, improving availability. Aurora Replicas provide read scaling, offloading read traffic from the primary instance and reducing application latency during normal operation. Furthermore, the development team can create a read replica from the production Aurora cluster specifically for the staging environment. This allows them to perform the full export from the read replica without impacting the performance of the production database or delaying access to the staging environment.\n\n**Why option 0 is incorrect:**\nWhile using Amazon Aurora MySQL with Multi-AZ Aurora Replicas is a good start, this option is incomplete because it doesn't explicitly address how the staging environment will be populated without impacting production performance. It only mentions using Aurora Replicas for production, but not for the staging environment's database export.\n\n**Why option 2 is incorrect:**\nUsing Amazon RDS for MySQL with a Multi-AZ deployment and read replicas is a valid approach for improving availability and read performance. However, Aurora offers superior performance and scalability compared to standard RDS for MySQL, especially for read-heavy workloads. Also, the question asks for a solution that alleviates the application latency issue *and* gives the development team the ability to continue using the staging environment without delay. While read replicas help with the latency issue, this option doesn't explicitly address how the staging environment will be populated without impacting production performance. The development team would still need to export data from a read replica, potentially causing performance issues, or from the primary instance, which is undesirable.\n\n**Why option 3 is incorrect:**\nUsing Amazon RDS for MySQL with a Multi-AZ deployment and read replicas is a valid approach for improving availability and read performance. However, Aurora offers superior performance and scalability compared to standard RDS for MySQL, especially for read-heavy workloads. Also, the question asks for a solution that alleviates the application latency issue *and* gives the development team the ability to continue using the staging environment without delay. While read replicas help with the latency issue, this option doesn't explicitly address how the staging environment will be populated without impacting production performance. The development team would still need to export data from a read replica, potentially causing performance issues, or from the primary instance, which is undesirable.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A company is preparing to store confidential data in Amazon S3. For compliance reasons the \ndata must be encrypted at rest Encryption key usage must be logged tor auditing purposes. Keys \nmust be rotated every year. \nWhich solution meets these requirements and the MOST operationally efferent?",
    "options": [
      {
        "id": 0,
        "text": "Server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual",
        "correct": false
      },
      {
        "id": 3,
        "text": "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution meets all requirements. Using SSE-KMS with customer master keys (CMKs) allows for encryption at rest. AWS KMS automatically logs key usage to AWS CloudTrail, fulfilling the auditing requirement. KMS also supports automatic key rotation, which can be configured for annual rotation, satisfying the final requirement. This approach is operationally efficient because AWS manages the key rotation and logging aspects.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Server-Side Encryption with Customer-Provided Keys (SSE-C) requires the customer to manage the encryption keys. This includes generating, storing, rotating, and providing the keys with each request. This adds significant operational overhead and complexity, making it less operationally efficient than SSE-KMS. Also, key usage logging is not directly integrated and would require custom implementation.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because Server-Side Encryption with Amazon S3 Managed Keys (SSE-S3) does not provide detailed logging of key usage. While it encrypts data at rest, the lack of detailed logging makes it unsuitable for compliance requirements that mandate auditing of key usage. Also, you cannot control or audit the key rotation schedule.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A bicycle sharing company is developing a multi-tier architecture to track the location of its \nbicycles during peak operating hours. The company wants to use these data points in its existing \nanalytics platform. A solutions architect must determine the most viable multi-tier option to \nsupport this architecture. The data points must be accessible from the REST API.  \nWhich action meets these requirements for storing and retrieving location data?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Athena with Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon API Gateway with AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon QuickSight with Amazon Redshift.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway with Amazon Kinesis Data Analytics",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of providing a REST API endpoint to access the location data. Amazon API Gateway allows you to create and manage REST APIs, and AWS Lambda provides a serverless compute service that can be used to process API requests and interact with a data store (e.g., DynamoDB) to store and retrieve the bicycle location data. This creates a multi-tier architecture where the API Gateway acts as the front-end, Lambda as the application logic, and the data store as the back-end. This is a viable option for storing and retrieving location data accessible from a REST API.\n\n**Why option 0 is incorrect:**\nWhile Amazon Athena can query data in Amazon S3, it's primarily an interactive query service for analyzing data at rest. It doesn't inherently provide a REST API for real-time data access. Building a REST API on top of Athena would require additional components and is not the most direct or viable solution for the stated requirements.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nAmazon Kinesis Data Analytics is used for processing streaming data in real-time. While it can process location data, it doesn't directly provide a REST API for retrieving the stored location data. It's more focused on real-time analytics and transformations of the data stream. API Gateway is needed to expose the data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company has an automobile sales website that stores its listings in a database on Amazon \nRDS. When an automobile is sold the listing needs to be removed from the website and the data \nmust be sent to multiple target systems. \nWhich design should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using RDS event notifications to trigger a message sent to Amazon SNS. SNS is a publish/subscribe service that allows a single message to be distributed to multiple subscribers. This aligns with the requirement to send data to multiple target systems when an automobile is sold and removed from the database. The RDS event notification provides the trigger for the process.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because directly triggering a Lambda function on a database update is generally not recommended for RDS. It's better to use RDS event notifications for asynchronous processing. Also, a single Lambda function would need to handle the fan-out logic to multiple target systems, which adds complexity and potential for failure within the Lambda function itself.\n\n**Why option 1 is incorrect:**\nThis option is incorrect for the same reasons as option 0. Directly triggering a Lambda function on a database update is not a best practice for RDS. RDS event notifications provide a more decoupled and scalable approach. The lack of a fan-out mechanism also makes this option unsuitable for sending data to multiple target systems.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because SQS is a queueing service, designed for point-to-point integration, not publish/subscribe. While a Lambda function could poll the SQS queue and then send the data to multiple targets, it adds unnecessary complexity and latency. SNS is a more direct and efficient solution for fan-out scenarios.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A company needs to store data in Amazon S3 and must prevent the data from being changed. \nThe company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a \nnonspecific amount of time until the company decides to modify the objects. Only specific users in \nthe company's AWS account can have the ability 10 delete the objects. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Glacier vault.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 bucket with S3 Object Lock enabled.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket with S3 Object Lock enabled.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution directly addresses the requirements by enabling S3 Object Lock on the bucket. S3 Object Lock provides Write Once Read Many (WORM) protection, ensuring that objects cannot be deleted or overwritten for a specified retention period or indefinitely. By enabling Object Lock, the company can prevent data changes. Furthermore, IAM policies can be configured to restrict deletion permissions to specific users, fulfilling the requirement of controlled deletion.\n\n**Why option 0 is incorrect:**\nS3 Glacier is an archive storage service designed for long-term, infrequent access data. While it provides cost-effective storage, it's not suitable for scenarios where the company intends to modify the objects at some point in the future. Restoring data from Glacier can take time, making it impractical for this use case. Also, Glacier vaults do not inherently provide the granular permission control for deletion specified in the question.\n\n**Why option 1 is incorrect:**\nCreating an S3 bucket alone does not provide any immutability features. While access control can be managed through IAM policies, it doesn't prevent users with sufficient permissions from modifying or deleting objects. The core requirement of preventing data changes is not addressed by simply creating an S3 bucket.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A social media company allows users to upload images to its website. The website runs on \nAmazon EC2 instances.  \nDuring upload requests, the website resizes the images to a standard size and stores the resized \nimages in Amazon S3.  \nUsers are experiencing slow upload requests to the website. \n \nThe company needs to reduce coupling within the application and improve website performance.  \nA solutions architect must design the most operationally efficient process for image uploads. \n \nWhich combination of actions should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to upload images to S3 Glacier.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the web server to upload the original images to Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the application to upload images directly from each user's browser to Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nThis is a correct action because it offloads the initial image storage to S3, freeing up the web servers to handle other requests. By having the web server directly upload the original images to S3, the EC2 instance is no longer burdened with storing the image during the resizing process, improving its responsiveness and reducing the load. This also reduces coupling as the web server's responsibility is limited to uploading the image.\n\n**Why option 3 is correct:**\nThis is a correct action because it allows for asynchronous image resizing. By configuring S3 Event Notifications to trigger a Lambda function upon image upload, the resizing process can be decoupled from the initial upload request. The Lambda function can then handle the resizing and store the processed image back in S3. This approach improves website performance by allowing users to upload images without waiting for the resizing to complete. It also enhances operational efficiency by leveraging a serverless architecture for image processing.\n\n**Why option 0 is incorrect:**\nS3 Glacier is designed for long-term archival storage and is not suitable for frequently accessed images. Using Glacier would introduce significant latency and cost overhead for image retrieval, making it an inappropriate choice for this scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nWhile EventBridge can trigger events, it's not the most efficient or direct way to respond to S3 uploads in this scenario. S3 Event Notifications are specifically designed for this purpose and provide a more streamlined and cost-effective solution. Using EventBridge would add unnecessary complexity and potential latency to the image processing pipeline.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A company recently migrated a message processing system to AWS. The system receives \nmessages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are \nprocessed by a consumer application running on Amazon EC2. The consumer application \nprocesses the messages and writes results to a MySQL database running on Amazon EC2. The \ncompany wants this application to be highly available with low operational complexity. \nWhich architecture offers the HIGHEST availability?",
    "options": [
      {
        "id": 0,
        "text": "Add a second ActiveMQ server to another Availably Zone.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon MO with active/standby brokers configured across two Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon MO with active/standby blotters configured across two Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon MQ with active/standby brokers configured across two Availability Zones.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement for high availability and low operational complexity by using Amazon MQ with active/standby brokers configured across two Availability Zones. Amazon MQ is a managed message broker service, which reduces the operational burden of managing ActiveMQ instances. The active/standby configuration ensures that if the active broker fails, the standby broker in another Availability Zone will automatically take over, minimizing downtime. This provides a highly available message queue without requiring manual intervention for failover.\n\n**Why option 0 is incorrect:**\nAdding a second ActiveMQ server to another Availability Zone improves availability compared to a single instance. However, it requires manual configuration for failover and monitoring. This increases operational complexity, as you would need to implement mechanisms for detecting failures and switching traffic to the standby broker. It doesn't leverage a managed service, which is key to reducing operational overhead.\n\n**Why option 1 is incorrect:**\nThe question mentions MySQL database running on EC2. Amazon MO is not a valid AWS service. It is likely a typo and should be Amazon MQ. However, even if it were Amazon MQ, the term 'blotters' is not relevant to message broker configurations. The correct term is 'brokers'. Therefore, this option is incorrect due to the incorrect terminology and potential misunderstanding of AWS services.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company hosts a containerized web application on a fleet of on-premises servers that process \nincoming requests. The number of requests is growing quickly. The on-premises servers cannot \nhandle the increased number of requests. The company wants to move the application to AWS \nwith minimum code changes and minimum development effort. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use two Amazon EC2 instances to host the containerized web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda with a new code that uses one of the supported languages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a high performance computing (HPC) solution such as AWS ParallelClusterto establish an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution directly addresses the requirements by allowing the company to run their existing containerized application on AWS without significant code changes. AWS Fargate is a serverless compute engine for containers, which eliminates the need to manage underlying EC2 instances, thus minimizing operational overhead. ECS provides the orchestration needed to manage the containers at scale. This option provides scalability and requires minimal development effort since the application is already containerized.\n\n**Why option 1 is incorrect:**\nWhile using EC2 instances would allow running the containerized application, it introduces significant operational overhead. The company would be responsible for managing the EC2 instances, including patching, scaling, and ensuring high availability. This contradicts the requirement of minimizing operational overhead.\n\n**Why option 2 is incorrect:**\nUsing AWS Lambda would require significant code changes to adapt the existing application to a serverless function architecture. This violates the requirement of minimizing code changes and development effort. Lambda is also not designed for long-running web applications, which are typically better suited for containerized environments.\n\n**Why option 3 is incorrect:**\nHigh Performance Computing (HPC) solutions like AWS ParallelCluster are designed for computationally intensive tasks, such as scientific simulations or data analysis. They are not suitable for hosting a general-purpose web application and would introduce unnecessary complexity and operational overhead. This option is not aligned with the requirements of the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company uses 50 TB of data for reporting. The company wants to move this data from on \npremises to AWS A custom application in the company's data center runs a weekly data \ntransformation job. The company plans to pause the application until the data transfer is complete \nand needs to begin the transfer process as soon as possible. \nThe data center does not have any available network bandwidth for additional workloads.  \nA solutions architect must transfer the data and must configure the transformation job to continue \nto run in the AWS Cloud. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to move the data.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order an AWS Snowcone device to move the data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Order an AWS Snowball Edge Storage Optimized device.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Order an AWS D. Snowball Edge Storage Optimized device that includes Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements because Snowball Edge Storage Optimized devices are designed for transferring large amounts of data when network bandwidth is limited. They provide a secure and efficient way to move data to AWS. The 'Storage Optimized' version is appropriate for large datasets like the 50 TB mentioned in the question. While it doesn't include EC2 instances directly, the data can be easily transferred to S3 after the Snowball Edge device is shipped to AWS, and the transformation job can then be executed on EC2 instances using the data in S3. This approach minimizes operational overhead compared to managing EC2 instances directly on the Snowball Edge device.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because the data center has no available network bandwidth for additional workloads. AWS DataSync relies on network connectivity to transfer data, making it unsuitable for this scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because AWS Snowcone has a smaller storage capacity than Snowball Edge Storage Optimized. Snowcone is designed for edge computing and smaller data transfers, and it would likely require multiple devices and transfers to move 50 TB of data, increasing operational overhead. It is not suitable for transferring large amounts of data when bandwidth is limited.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because while it would allow the transformation job to run directly on the Snowball Edge device, it increases operational overhead. Managing EC2 instances on the Snowball Edge device adds complexity compared to simply transferring the data to S3 and running the transformation job on EC2 instances in AWS. The question specifically asks for the solution with the LEAST operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company has created an image analysis application in which users can upload photos and add \nphoto frames to their images. The users upload images and metadata to indicate which photo \nframes they want to add to their images. The application uses a single Amazon EC2 instance and \nAmazon DynamoDB to store the metadata. \n \nThe application is becoming more popular, and the number of users is increasing. The company \nexpects the number of concurrent users to vary significantly depending on the time of day and \nday of week. The company must ensure that the application can scale to meet the needs of the \ngrowing user base. \n \nWhich solution meats these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda to process the photos.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda to process the photos.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Increase the number of EC2 instances to three.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the scalability requirement by leveraging AWS Lambda. Lambda allows the photo processing to be executed in a serverless environment, automatically scaling based on the number of incoming requests. This eliminates the bottleneck of a single EC2 instance and ensures the application can handle varying workloads efficiently. Lambda functions can be triggered by events such as new image uploads, making it a suitable solution for processing photos asynchronously and independently.\n\n**Why option 0 is incorrect:**\nWhile AWS Lambda is a good choice for processing photos, this option is incomplete. It doesn't specify how Lambda is triggered or how the overall architecture is structured to handle the image uploads and metadata. Simply using Lambda without a proper event trigger and data flow mechanism is insufficient.\n\n**Why option 1 is incorrect:**\nAmazon Kinesis Data Firehose is primarily used for streaming data to data lakes or analytics services. It's not designed for processing images and applying photo frames. While Firehose can ingest the photos, it doesn't provide the compute capabilities to perform the image processing tasks. Furthermore, storing photos directly in Firehose is not its intended use case and would be inefficient and costly.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A medical records company is hosting an application on Amazon EC2 instances. The application \nprocesses customer data files that are stored on Amazon S3. The EC2 instances are hosted in \npublic subnets. The EC2 instances access Amazon S3 over the internet, but they do not require \nany other network access. \nA new requirement mandates that the network traffic for file transfers take a private route and not \nbe sent over the internet. \nWhich change to the network architecture should a solutions architect recommend to meet this \nrequirement?",
    "options": [
      {
        "id": 0,
        "text": "Create a NAT gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the EC2 instances to private subnets.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Remove the internet gateway from the VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by placing the EC2 instances in private subnets and configuring a VPC endpoint for S3. VPC endpoints allow private connectivity to S3 without traversing the internet. By moving the EC2 instances to private subnets, they no longer have direct access to the internet. The VPC endpoint provides a private path to S3, fulfilling the requirement for private file transfers.\n\n**Why option 0 is incorrect:**\nA NAT gateway allows instances in private subnets to initiate outbound traffic to the internet, but it doesn't provide a private route to S3. It would still require the traffic to traverse the internet to reach S3, which violates the requirement.\n\n**Why option 1 is incorrect:**\nWhile restricting outbound traffic on the security group is a good security practice, it doesn't solve the problem of routing traffic privately to S3. It only controls what traffic is allowed, not how it's routed. The traffic would still attempt to go over the internet unless a private route is established.\n\n**Why option 3 is incorrect:**\nRemoving the internet gateway would prevent the EC2 instances in the public subnets from accessing S3 at all, even over the internet. This would break the application functionality and not meet the requirement of transferring files privately.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company uses a popular content management system (CMS) for its corporate website. \nHowever, the required patching and maintenance are burdensome. The company is redesigning \nits website and wants anew solution. The website will be updated four times a year and does not \nneed to have any dynamic content available. The solution must provide high scalability and \nenhanced security. \n \nWhich combination of changes will meet these requirements with the LEAST operational \noverhead? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create and deploy an AWS Lambda function to manage and serve the website content",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create the new website and an Amazon S3 bucket Deploy the website on the S3 bucket with",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create the new website. Deploy the website on an Amazon S3 bucket with static website hosting enabled. Use Amazon CloudFront to distribute the website content and require HTTPS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS WAF (Web Application Firewall) can be deployed in front of the website to provide HTTPS functionality. While CloudFront is the primary service for HTTPS in this scenario, WAF can also handle HTTPS and provides additional security features like protection against common web exploits, bot management, and custom rules, which contributes to enhanced security as required by the scenario. It also has minimal operational overhead as it's a managed service.\n\n**Why option 3 is correct:**\nThis is correct because creating the website and deploying it to an Amazon S3 bucket with static website hosting enabled allows for serving the content directly from S3. Using Amazon CloudFront to distribute the website content provides high scalability through its global edge locations and caching capabilities. Requiring HTTPS ensures secure communication between users and the website. This combination minimizes operational overhead because S3 and CloudFront are managed services, reducing the need for server management and patching.\n\n**Why option 1 is incorrect:**\nThis is incorrect because using AWS Lambda to manage and serve website content, while possible, introduces unnecessary complexity and operational overhead for a static website. Lambda is better suited for dynamic content or serverless applications. For a static website, S3 and CloudFront are more efficient and cost-effective.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while using S3 for hosting is a good start, it doesn't address the scalability and security requirements as effectively as using CloudFront in conjunction with S3. Without CloudFront, the website would be served directly from the S3 bucket, which lacks the global distribution and caching capabilities needed for high scalability. Also, the option doesn't explicitly mention HTTPS, which is crucial for enhanced security.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company stores its application logs in an Amazon CloudWatch Logs log group.  \nA new policy requires the company to store all application logs in Amazon OpenSearch Service \n(Amazon Elasticsearch Service) in near-real time. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Install and configure Amazon Kinesis Agent on each application server to deliver the logs to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because CloudWatch Logs subscriptions provide a direct, managed, and near-real-time mechanism to stream logs to Amazon OpenSearch Service. It requires minimal configuration and maintenance compared to other options, directly addressing the 'least operational overhead' requirement. It avoids the need for custom code or additional infrastructure management.\n\n**Why option 1 is incorrect:**\nThis is incorrect because using a Lambda function would require writing, deploying, and managing custom code to poll CloudWatch Logs, process the logs, and send them to OpenSearch Service. This adds significant operational overhead compared to a CloudWatch Logs subscription.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while Kinesis Data Firehose can deliver logs to OpenSearch Service, it requires configuring a source (which would likely be a Lambda function polling CloudWatch Logs or a custom application pushing logs to Firehose). This adds complexity and operational overhead compared to using a CloudWatch Logs subscription directly.\n\n**Why option 3 is incorrect:**\nThis is incorrect because installing and configuring the Kinesis Agent on each application server introduces significant operational overhead. It requires managing the agent's configuration, deployment, and maintenance across all servers. Furthermore, it doesn't directly address the existing logs in CloudWatch Logs; it only captures new logs from the application servers.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "A company is building a web-based application running on Amazon EC2 instances in multiple \nAvailability Zones. The web application will provide access to a repository of text documents \ntotaling about 900 TB in size. The company anticipates that the web application will experience \nperiods of high demand. A solutions architect must ensure that the storage component for the text \ndocuments can scale to meet the demand of the application at all times. The company is \nconcerned about the overall cost of the solution. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elasticsearch Service (Amazon ES)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the most cost-effective and scalable solution for storing 900 TB of text documents. Amazon S3 provides virtually unlimited storage capacity and automatically scales to handle varying levels of demand. S3's pay-as-you-go pricing model makes it cost-effective for large datasets, especially when compared to other storage options like EBS or EFS. Furthermore, S3's object storage model is well-suited for storing and retrieving text documents, and it integrates seamlessly with EC2 instances.\n\n**Why option 0 is incorrect:**\nThis is not the most cost-effective solution. While EBS can be attached to EC2 instances, it is block storage and would require provisioning a large volume (or multiple volumes) to store 900 TB of data. EBS volumes are priced based on provisioned capacity, regardless of actual usage, making it more expensive than S3 for this use case. Additionally, managing and scaling EBS volumes to handle fluctuating demand would be more complex than using S3.\n\n**Why option 1 is incorrect:**\nThis is not the most cost-effective solution for storing 900 TB of text documents. While EFS provides a shared file system that can be accessed by multiple EC2 instances, it is significantly more expensive than S3, especially for large datasets. EFS is designed for use cases that require shared file access and POSIX compliance, which is not explicitly required in this scenario. The cost of storing 900 TB in EFS would be considerably higher than using S3.\n\n**Why option 2 is incorrect:**\nThis is not the correct solution. While Amazon Elasticsearch Service is suitable for indexing and searching large volumes of text data, it is not designed for primary storage of the documents themselves. Elasticsearch is optimized for search and analytics, and storing 900 TB of text documents directly in Elasticsearch would be very expensive and inefficient. It's more appropriate to store the documents in a cost-effective storage solution like S3 and then index them in Elasticsearch for search purposes.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in \nthe us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution \nto protect these API Gateway managed REST APIs across multiple accounts from SQL injection \nand cross-site scripting attacks. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS WAF in both Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Firewall Manager in both Regions.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Shield in bath Regions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up AWS Shield in one of the Regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing centralized management of AWS WAF rules across multiple AWS accounts and regions. AWS Firewall Manager allows you to define a single set of rules and automatically apply them to your API Gateways in both us-east-1 and ap-southeast-2, minimizing administrative effort. It simplifies the deployment and maintenance of WAF rules across your organization, ensuring consistent protection against web application attacks like SQL injection and XSS.\n\n**Why option 0 is incorrect:**\nWhile setting up AWS WAF in both regions would provide protection, it would require manual configuration and management of WAF rules in each region separately. This increases administrative effort compared to using AWS Firewall Manager, which centralizes management.\n\n**Why option 2 is incorrect:**\nAWS Shield provides protection against Distributed Denial of Service (DDoS) attacks. While Shield Advanced offers more comprehensive DDoS protection and integration with WAF, it primarily focuses on network and transport layer attacks, not application-layer attacks like SQL injection and XSS. It does not directly address the requirement of protecting against SQL injection and XSS attacks.\n\n**Why option 3 is incorrect:**\nAWS Shield provides protection against Distributed Denial of Service (DDoS) attacks. While Shield Advanced offers more comprehensive DDoS protection and integration with WAF, it primarily focuses on network and transport layer attacks, not application-layer attacks like SQL injection and XSS. Furthermore, only setting it up in one region would leave the other region vulnerable.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company has implemented a self-managed DNS solution on three Amazon EC2 instances \nbehind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are \nlocated in the United States and Europe. The company wants to improve the performance and \navailability of the solution. The company launches and configures three EC2 instances in the eu-\nwest-1 Region and adds the EC2 instances as targets for a new NLB. \n \nWhich solution can the company use to route traffic to all the EC2 instances?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Route 53 geolocation routing policy to route requests to one of the two",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a standard accelerator in AWS Global Accelerator.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Attach Elastic IP addresses to the six EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replace the two NLBs with two Application Load Balancers (ALBs).",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing a static entry point (two static IPs) for the application. AWS Global Accelerator intelligently routes traffic to the closest healthy endpoint based on network conditions and user location, improving performance by reducing latency. It also enhances availability by automatically failing over to the other region if one region becomes unavailable. Standard accelerator is the correct choice as it supports NLBs as endpoints.\n\n**Why option 0 is incorrect:**\nWhile geolocation routing can direct traffic based on user location, it doesn't inherently provide the same level of performance and availability as Global Accelerator. Geolocation routing relies on DNS resolution, which can be cached and may not always reflect the most optimal path. It also doesn't provide automatic failover in the same way as Global Accelerator. Route 53 alone cannot monitor the health of the EC2 instances behind the NLBs directly, so it cannot react as quickly to regional failures.\n\n**Why option 2 is incorrect:**\nAttaching Elastic IP addresses to the EC2 instances would not provide a global routing solution. Users would need to know the specific IP address of an instance in a particular region, and there would be no automatic failover or intelligent routing based on location or network conditions. This approach also doesn't address the need for a single point of entry for the DNS service.\n\n**Why option 3 is incorrect:**\nReplacing NLBs with ALBs would not directly solve the problem of routing traffic across multiple regions. While ALBs offer more advanced features like content-based routing, they are regional resources. To achieve cross-region routing and failover, you would still need a global service like Global Accelerator or a complex DNS configuration with health checks, making this option less efficient and more complex than using Global Accelerator directly.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company is running an online transaction processing (OLTP) workload on AWS. This workload \nuses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database \nsnapshots are taken from this instance. \n \nWhat should a solutions architect do to ensure the database and snapshots are always encrypted \nmoving forward?",
    "options": [
      {
        "id": 0,
        "text": "Encrypt a copy of the latest DB snapshot.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because you can encrypt a copy of the latest unencrypted snapshot. Once the encrypted snapshot is created, you can restore a new, encrypted RDS instance from it. Then, you can configure the new RDS instance to encrypt future snapshots. This ensures that both the database and all future snapshots are encrypted moving forward. This approach minimizes downtime and avoids complex migrations.\n\n**Why option 1 is incorrect:**\nThis is incorrect because creating a new EBS volume and copying data to it doesn't directly address the encryption of the RDS instance or its snapshots. RDS manages its own storage, and directly manipulating EBS volumes is not the correct approach for encrypting an RDS database. This option also doesn't address the requirement of encrypting future snapshots.\n\n**Why option 2 is incorrect:**\nThis is incorrect because copying snapshots alone and enabling encryption using KMS does not encrypt the original RDS instance. While the copied snapshots would be encrypted, the running database would still be unencrypted, and new snapshots taken from the unencrypted database would also be unencrypted. The question requires the database itself to be encrypted moving forward.\n\n**Why option 3 is incorrect:**\nThis is incorrect because copying snapshots to an S3 bucket, even with server-side encryption, does not encrypt the RDS database or future snapshots. S3 encryption protects the snapshots while they are stored in S3, but it does not address the core requirement of encrypting the RDS instance and all future snapshots taken from it. The database itself remains unencrypted.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company wants to build a scalable key management Infrastructure to support developers who need to encrypt data in their applications. What should a solutions architect do to reduce the operational burden?",
    "options": [
      {
        "id": 0,
        "text": "Use multifactor authentication (MFA) to protect the encryption keys.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Key Management Service (AWS KMS) to protect the encryption keys",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an IAM policy to limit the scope of users who have access permissions to protect the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because AWS Key Management Service (KMS) is a managed service that simplifies the creation, storage, and control of encryption keys used to encrypt data. Using KMS reduces the operational burden by offloading the complexities of key management, such as key generation, rotation, storage, and access control, to AWS. KMS also integrates with other AWS services, making it easier for developers to incorporate encryption into their applications without having to manage the underlying key infrastructure.\n\n**Why option 0 is incorrect:**\nWhile multifactor authentication (MFA) enhances security by requiring multiple forms of authentication, it doesn't directly address the operational burden of managing encryption keys. MFA protects access to the AWS account and resources, but it doesn't simplify the key management process itself.\n\n**Why option 2 is incorrect:**\nAWS Certificate Manager (ACM) is primarily used for managing SSL/TLS certificates for securing network communications. It is not designed for general-purpose encryption key management for applications. Using ACM for this purpose would be inappropriate and not reduce the operational burden.\n\n**Why option 3 is incorrect:**\nIAM policies are crucial for access control and security, but they don't directly manage the encryption keys themselves. While IAM policies can restrict access to encryption keys, they don't simplify the key management lifecycle (creation, rotation, storage, etc.) and therefore don't significantly reduce the operational burden of key management.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company has a dynamic web application hosted on two Amazon EC2 instances. The company \nhas its own SSL certificate, which is on each instance to perform SSL termination. \n \nThere has been an increase in traffic recently, and the operations team determined that SSL \nencryption and decryption is causing the compute capacity of the web servers to reach their \nmaximum limit. \n \nWhat should a solutions architect do to increase the application's performance?",
    "options": [
      {
        "id": 0,
        "text": "Create a new SSL certificate using AWS Certificate Manager (ACM) install the ACM certificate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create another EC2 instance as a proxy server Migrate the SSL certificate to the new instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Import the SSL certificate into AWS Certificate Manager (ACM).",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct solution because importing the SSL certificate into AWS Certificate Manager (ACM) allows you to then integrate ACM with a load balancer like Application Load Balancer (ALB) or Network Load Balancer (NLB). The load balancer can then handle the SSL termination, offloading the CPU-intensive encryption/decryption tasks from the EC2 instances. This frees up the EC2 instances to focus on serving application requests, thereby increasing performance.\n\n**Why option 0 is incorrect:**\nCreating a new SSL certificate using ACM is not the correct approach because the company already has its own SSL certificate that they want to use. Creating a new certificate would not solve the problem of offloading the SSL processing using their existing certificate.\n\n**Why option 1 is incorrect:**\nMigrating the SSL certificate to an S3 bucket does not address the problem of SSL termination overhead. S3 is an object storage service and does not provide SSL termination capabilities. Storing the certificate in S3 would not offload the SSL processing from the EC2 instances. This option also doesn't provide a mechanism to actually use the certificate for SSL termination.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job. What should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Implement EC2 Spot Instances",
        "correct": true
      },
      {
        "id": 1,
        "text": "Purchase EC2 Reserved Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement EC2 On-Demand Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement the processing on AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-effective option because Spot Instances offer significant discounts compared to On-Demand instances. The job's stateless nature and ability to be interrupted make it suitable for Spot Instances, as the job can be restarted if a Spot Instance is terminated. The long duration of the job (60+ minutes) makes the potential cost savings of Spot Instances even more significant.\n\n**Why option 1 is incorrect:**\nReserved Instances are a good choice for predictable, long-term workloads. However, the question specifies a 'highly dynamic' batch processing job, implying that the resource needs may fluctuate. Reserved Instances commit you to paying for a specific instance type and size for a long period (1 or 3 years), which may not be cost-effective if the job's resource requirements change frequently. Also, the question emphasizes cost optimization, and Spot Instances generally offer better cost savings for interruptible workloads.\n\n**Why option 2 is incorrect:**\nOn-Demand Instances provide flexibility but are the most expensive option. While they are suitable for short-term, unpredictable workloads, the question explicitly asks for a cost-effective solution. Given the job's tolerance for interruption, On-Demand Instances are not the best choice.\n\n**Why option 3 is incorrect:**\nAWS Lambda is designed for short-running, event-driven functions. The question states that the job typically takes upwards of 60 minutes to complete. Lambda functions have a maximum execution time of 15 minutes, making Lambda unsuitable for this workload.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company runs its two-tier ecommerce website on AWS. The web tier consists of a load \nbalancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS \nDB instance. The EC2 instances and the RDS DB instance should not be exposed to the public \ninternet. The EC2 instances require internet access to complete payment processing of orders \nthrough a third-party web service. The application must be highly available. \n \nWhich combination of configuration options will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use an Auto Scaling group to launch the EC2 instances in private subnets.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a VPC with two private subnets and two NAT gateways across two Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure a VPC with two public subnets, two private subnets, and two NAT gateways across",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because launching EC2 instances in private subnets ensures they are not directly accessible from the public internet, fulfilling the requirement that they should not be exposed to the public internet. Using an Auto Scaling group ensures high availability by automatically replacing any failed instances.\n\n**Why option 4 is correct:**\nThis is correct because configuring a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones provides the necessary infrastructure. The EC2 instances are placed in the private subnets, fulfilling the requirement that they should not be exposed to the public internet. The NAT gateways, deployed in the public subnets, allow the EC2 instances in the private subnets to initiate outbound connections to the internet for payment processing. Having two of each across two Availability Zones ensures high availability and fault tolerance.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while it provides private subnets and NAT Gateways, it doesn't address the high availability aspect for the EC2 instances themselves. Without an Auto Scaling group, the application is vulnerable to instance failures.\n\n**Why option 2 is incorrect:**\nThis is incorrect because launching EC2 instances in public subnets directly exposes them to the internet, violating the requirement that they should not be publicly accessible. While Auto Scaling provides high availability, it doesn't address the security requirement.\n\n**Why option 3 is incorrect:**\nThis is incorrect because having only one private and one public subnet does not provide high availability across multiple Availability Zones. Also, the question requires the EC2 instances to be in private subnets, but this option doesn't explicitly state that.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by transitioning data to S3 Glacier Deep Archive after the initial two-year period of high availability. S3 Glacier Deep Archive is the lowest-cost storage class suitable for long-term archiving and meets the 25-year retention requirement. The initial two years remain in S3 Standard, providing the necessary high availability and immediate retrieval.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because transitioning data to S3 Glacier Deep Archive immediately would violate the requirement for high availability and immediate retrieval for the most recent 2 years of data. The data needs to remain in a highly available tier like S3 Standard for the first two years.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because while S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns, it does not guarantee archiving to S3 Glacier Deep Archive and might not be the most cost-effective solution for data that is known to be infrequently accessed after 2 years. Also, the archiving option in Intelligent-Tiering does not directly map to Glacier Deep Archive and might not be the cheapest option for long-term storage.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because S3 One Zone-Infrequent Access (S3 One Zone-IA) offers lower availability than S3 Standard and is not suitable for data requiring high availability for the first 2 years. While it is cheaper than S3 Standard, it doesn't address the long-term archival requirement as effectively as Glacier Deep Archive.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company runs its ecommerce application on AWS. Every new order is published as a message \nin a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These \nmessages are processed by a different application that runs on a separate EC2 instance. This \napplication stores the details in a PostgreSQL database on another EC2 instance. All the EC2 \ninstances are in the same Availability Zone. \nThe company needs to redesign its architecture to provide the highest availability with the least \noperational overhead. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement for high availability with minimal operational overhead by migrating the RabbitMQ queue to Amazon MQ. Amazon MQ provides a managed RabbitMQ service with built-in redundancy and automatic failover capabilities. By using a redundant pair (active/standby) configuration, Amazon MQ ensures that the queue remains available even if the primary instance fails. This eliminates the need for manual management of failover and reduces the operational burden on the company.\n\n**Why option 0 is incorrect:**\nThis option is identical to the correct option and therefore cannot be incorrect. The question only has one correct answer.\n\n**Why option 2 is incorrect:**\nWhile creating a Multi-AZ Auto Scaling group for the EC2 instances hosting RabbitMQ would improve availability compared to the current single-instance setup, it would also significantly increase operational overhead. Setting up and managing Auto Scaling groups, configuring RabbitMQ clustering, and handling failover scenarios manually would require considerable effort. Amazon MQ provides a managed service that handles these tasks automatically, making it a more suitable solution for minimizing operational overhead.\n\n**Why option 3 is incorrect:**\nWhile creating a Multi-AZ Auto Scaling group for the EC2 instances hosting RabbitMQ would improve availability compared to the current single-instance setup, it would also significantly increase operational overhead. Setting up and managing Auto Scaling groups, configuring RabbitMQ clustering, and handling failover scenarios manually would require considerable effort. Amazon MQ provides a managed service that handles these tasks automatically, making it a more suitable solution for minimizing operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically to the analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines. What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements efficiently. S3 replication handles the file copying automatically with minimal operational overhead. Configuring event notifications from the analysis S3 bucket to EventBridge allows for flexible routing and filtering of events. EventBridge can then target both the Lambda function and the SageMaker Pipeline, triggering them whenever a new object is created in the analysis bucket. This approach avoids custom coding for file copying and provides a centralized event management system.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it creates an S3 event notification for the *analysis* S3 bucket. The initial S3 bucket is where the files are being created, so the event notification needs to be configured there. Additionally, directly configuring Lambda and SageMaker Pipelines as destinations of an S3 event notification is less flexible and scalable than using EventBridge for routing.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it requires a Lambda function to copy the files. While functional, using a Lambda function for simple file copying introduces unnecessary operational overhead compared to using S3 replication, which is a managed service specifically designed for this purpose. S3 replication is more efficient and requires less maintenance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A solutions architect needs to help a company optimize the cost of running an application on \nAWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for \ncompute within the architecture. \n \nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic \nand unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The \napplication front end will run on Fargate, and Lambda will serve the API layer. The front-end \nutilization and API layer utilization will be predictable over the course of the next year. \n \nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting \nthis application? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the data ingestion layer",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use On-Demand Instances for the data ingestion layer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a 1-year Compute Savings Plan for the front end and API layer.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase 1-year All Upfront Reserved instances for the data ingestion layer.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Spot Instances offer significant cost savings compared to On-Demand instances, especially for workloads that are fault-tolerant and can handle interruptions. The data ingestion layer's sporadic and unpredictable usage, combined with its ability to be interrupted, makes it an ideal candidate for Spot Instances. Using Spot Instances allows the company to bid for unused EC2 capacity, potentially saving a substantial amount of money.\n\n**Why option 2 is correct:**\nThis is correct because Compute Savings Plans offer significant cost savings compared to On-Demand pricing for compute resources. Since the front end (Fargate) and API layer (Lambda) have predictable utilization over the next year, committing to a certain amount of compute usage with a Compute Savings Plan will result in lower costs than using On-Demand pricing. Compute Savings Plans are flexible and apply to EC2, Lambda, and Fargate, making them suitable for this scenario.\n\n**Why option 1 is incorrect:**\nThis is incorrect because On-Demand Instances are the most expensive option for EC2 instances. While they offer flexibility, they are not cost-effective for workloads that can be interrupted. Given that the data ingestion layer can tolerate interruptions, using On-Demand instances would be a waste of resources and budget.\n\n**Why option 3 is incorrect:**\nThis is incorrect because All Upfront Reserved Instances require a significant upfront payment and are best suited for consistent, predictable workloads. The data ingestion layer has sporadic and unpredictable usage, making Reserved Instances a poor choice. If the instances are not fully utilized, the company will still be paying for them, leading to wasted resources.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 58,
    "text": "A company runs a web-based portal that provides users with global breaking news, local alerts, \nand weather updates. The portal delivers each user a personalized view by using mixture of static \nand dynamic content. Content is served over HTTPS through an API server running on an \nAmazon EC2 instance behind an Application Load Balancer (ALB). The company wants the \nportal to provide this content to its users across the world as quickly as possible. \n \nHow should a solutions architect design the application to ensure the LEAST amount of latency \nfor all users?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by using Amazon CloudFront, a CDN, to cache both static and dynamic content. By specifying the ALB as the origin, CloudFront can distribute the content to edge locations globally, reducing latency for users regardless of their location. CloudFront automatically handles caching and invalidation, ensuring users receive the most up-to-date content with minimal delay.\n\n**Why option 1 is incorrect:**\nWhile deploying the application stack in multiple AWS Regions and using Route 53 latency-based routing can improve latency, it doesn't leverage caching as effectively as CloudFront. Each region would need to serve the content directly, potentially leading to higher latency for users far from those regions. Also, managing data synchronization and consistency across multiple regions adds complexity.\n\n**Why option 2 is incorrect:**\nServing static content via CloudFront is a good practice to reduce latency. However, serving dynamic content directly from the ALB without caching will result in higher latency, especially for users geographically distant from the AWS Region. This option doesn't fully leverage the benefits of a CDN for all content types.\n\n**Why option 3 is incorrect:**\nWhile deploying the application stack in multiple AWS Regions and using Route 53 geolocation routing can improve latency, it doesn't leverage caching as effectively as CloudFront. Geolocation routing directs users to the closest region based on their geographic location, but it doesn't cache content at edge locations closer to the users. Also, managing data synchronization and consistency across multiple regions adds complexity. Geolocation routing is also less dynamic than latency based routing.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A gaming company is designing a highly available architecture. The application runs on a \nmodified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier \nto provide the best possible user experience. That tier must have low latency, route traffic to the \nnearest edge location, and provide static IP addresses for entry into the application endpoints. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Route 53 to forward requests to an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon CloudFront to forward requests to a Network Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Global Accelerator to forward requests to a Network Load Balancer.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon API Gateway to forward requests to an Application Load Balancer.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by using AWS Global Accelerator, which provides static IP addresses that serve as entry points for the application. Global Accelerator routes traffic to the nearest healthy endpoint based on network proximity, minimizing latency. A Network Load Balancer (NLB) is used as the endpoint because it supports UDP traffic, which is a requirement for the gaming application. The NLB can then distribute traffic to the backend servers.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Application Load Balancers (ALBs) do not support UDP traffic. Also, while Route 53 can route traffic based on latency, it doesn't provide static IP addresses for the application endpoints. It also doesn't inherently route to the 'nearest edge location' in the same way Global Accelerator does, which optimizes for network performance.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because while CloudFront can distribute content globally with low latency, it's primarily designed for caching static and dynamic content. It does not support UDP traffic directly to the origin. CloudFront is typically used with HTTP/HTTPS traffic. While it can work with dynamic content, it's not the ideal solution for a real-time UDP-based gaming application. Also, CloudFront doesn't provide static IP addresses for the application endpoints; it uses dynamic IP addresses.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "A company wants to migrate its existing on-premises monolithic application to AWS. The \ncompany wants to keep as much of the front-end code and the backend code as possible. \nHowever, the company wants to break the application into smaller applications. A different team \nwill manage each application. The company needs a highly scalable solution that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host the application on AWS Lambda Integrate the application with Amazon API Gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application on Amazon Elastic Container Service (Amazon ECS) Set up an Application",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution is correct because Amazon ECS allows you to containerize the application components, enabling independent deployment and management by different teams. Containerization minimizes code changes required during migration. ECS provides high scalability through features like auto-scaling and load balancing. Using ECS also reduces operational overhead compared to managing EC2 instances directly, as ECS handles container orchestration and management.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because while AWS Lambda offers scalability and low operational overhead, it is not suitable for migrating large portions of existing monolithic applications without significant refactoring. Lambda functions are designed for event-driven, stateless workloads, and migrating a monolithic application to Lambda would likely require a complete rewrite, violating the requirement to keep as much of the existing code as possible. Also, managing a complex monolithic application as a set of Lambda functions can become operationally complex.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because AWS Amplify is primarily designed for building front-end web and mobile applications. While it can integrate with backend services through API Gateway, it doesn't provide a suitable platform for hosting and managing the backend components of a monolithic application that needs to be broken down into smaller, independently managed applications. Amplify is not designed for the type of backend decomposition and management required in this scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company recently started using Amazon Aurora as the data store for its global ecommerce \napplication.  \nWhen large reports are run developers report that the ecommerce application is performing \npoorly After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the \nReadlOPS and CPUUtilization metrics are spiking when monthly reports run. \nWhat is the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the monthly reporting to Amazon Redshift.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the monthly reporting to an Aurora Replica",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the Aurora database to a larger instance class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase the Provisioned IOPS on the Aurora instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the performance issue by offloading the read-heavy reporting workload to an Aurora Replica. Aurora Replicas share the same underlying storage as the primary instance, providing near real-time data consistency for reporting. This reduces the load on the primary instance, preventing performance degradation for the ecommerce application. Using an Aurora Replica is generally more cost-effective than migrating to a different database service like Redshift or scaling up the primary instance, as it leverages the existing Aurora infrastructure and provides a dedicated resource for read operations.\n\n**Why option 0 is incorrect:**\nMigrating to Amazon Redshift is not the most cost-effective solution. Redshift is designed for large-scale data warehousing and analytics, which is overkill for monthly reporting. It would require significant data migration, ETL processes, and ongoing management overhead, leading to higher costs and complexity compared to using an Aurora Replica. The question specifically asks for the *most* cost-effective solution.\n\n**Why option 2 is incorrect:**\nIncreasing the instance size of the primary Aurora database would address the CPU utilization issue, but it's not the most cost-effective solution. While it would provide more resources for both the application and the reporting, it would also increase the cost of the primary database instance even when the reporting is not running. Offloading the reporting to a replica is a more targeted and cost-efficient approach.\n\n**Why option 3 is incorrect:**\nIncreasing the Provisioned IOPS on the Aurora instance might improve read performance, but it's not the most cost-effective solution. Provisioned IOPS are billed regardless of whether they are used, so increasing them significantly would increase costs even when the reporting is not running. Furthermore, the problem is not solely IOPS, but also CPU utilization, which this option does not address directly. Offloading the reporting to a replica is a more targeted and cost-efficient approach.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. \nThe analytics software is written in PHP and uses a MySQL database. The analytics software, the \nweb server that provides PHP, and the database server are all hosted on the EC2 instance. The \napplication is showing signs of performance degradation during busy times and is presenting 5xx \nerrors.  \nThe company needs to make the application scale seamlessly. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template. Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by migrating the database to Amazon Aurora MySQL, which provides better performance and scalability than a single MySQL instance on EC2. Creating an AMI of the web application allows for easy replication. Using a launch template with the AMI ensures consistent configuration for new instances. An Auto Scaling group automatically adjusts the number of EC2 instances based on demand, providing seamless scaling. Configuring the launch template to use a Spot Fleet leverages potentially lower-cost Spot Instances, enhancing cost-effectiveness. Finally, attaching an Application Load Balancer distributes traffic across the instances in the Auto Scaling group, ensuring high availability and load balancing. This combination provides scalability, cost optimization, and seamless operation.\n\n**Why option 0 is incorrect:**\nWhile this option addresses the scaling of the web application by using an Application Load Balancer and a second EC2 instance, it doesn't leverage Auto Scaling. The solution requires manual intervention to launch new instances when demand increases. Also, it uses On-Demand instances, which are more expensive than Spot Instances. Therefore, it is not the MOST cost-effective solution.\n\n**Why option 1 is incorrect:**\nThis option addresses the scaling of the web application by using a second EC2 instance, but it uses Amazon Route 53 weighted routing instead of an Application Load Balancer. Route 53 weighted routing is suitable for global traffic distribution and failover, but it's not ideal for load balancing within a region. An Application Load Balancer provides more granular control and health checks for distributing traffic across instances. Also, it doesn't leverage Auto Scaling or Spot Instances, making it less cost-effective and less scalable than option 3.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 63,
    "text": "A company runs a stateless web application in production on a group of Amazon EC2 On-\nDemand Instances behind an Application Load Balancer. The application experiences heavy \nusage during an 8-hour period each business day. Application usage is moderate and steady \novernight Application usage is low during weekends. \nThe company wants to minimize its EC2 costs without affecting the availability of the application. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the entire workload.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved instances for the baseline level of usage.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use On-Demand Instances for the baseline level of usage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Dedicated Instances for the baseline level of usage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by utilizing Reserved Instances (RIs) for the baseline level of usage (moderate overnight and low weekends). RIs provide a significant discount compared to On-Demand Instances in exchange for a commitment to a specific instance type and availability zone for a 1- or 3-year term. By covering the consistent, moderate usage with RIs, the company can significantly reduce its overall EC2 costs. The remaining heavy usage during business hours can be handled with On-Demand or Spot instances, depending on the tolerance for interruption.\n\n**Why option 0 is incorrect:**\nUsing Spot Instances for the entire workload is risky and could affect availability. Spot Instances can be terminated with short notice (2-minute warning) if the Spot price exceeds the bid price. While Spot Instances offer significant cost savings, relying solely on them for a production application, especially during peak hours, is not recommended due to the potential for interruptions. The question specifically states that availability should not be affected.\n\n**Why option 2 is incorrect:**\nUsing On-Demand Instances for the baseline level of usage is the most expensive option. While On-Demand Instances provide flexibility, they do not offer any discounts for consistent usage. Given the predictable usage patterns, utilizing Reserved Instances for the baseline is a more cost-effective approach.\n\n**Why option 3 is incorrect:**\nDedicated Instances are the most expensive EC2 instance type and are generally used for compliance or licensing reasons. They do not provide any cost benefits for the described usage pattern. Using Dedicated Instances for the baseline level of usage would significantly increase costs without providing any additional value in this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A company needs to retain application logs files for a critical application for 10 years. The \napplication team regularly accesses logs from the past month for troubleshooting, but logs older \nthan 1 month are rarely accessed. The application generates more than 10 TB of logs per month. \nWhich storage option meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by initially storing the logs in Amazon S3, which provides cost-effective storage and good performance for recent logs. S3 Lifecycle policies are then used to automatically transition logs older than 1 month to S3 Glacier Deep Archive. This storage class offers the lowest cost for long-term archival storage, making it ideal for logs that are rarely accessed but need to be retained for 10 years. S3 Lifecycle policies are a native feature of S3 and are designed for this type of data lifecycle management, making it a cost-effective and efficient solution.\n\n**Why option 0 is incorrect:**\nWhile this option uses S3 and Glacier Deep Archive, using AWS Backup to move logs is not the most cost-effective approach. AWS Backup is designed for backing up and restoring entire systems or datasets, not for archiving individual log files based on age. S3 Lifecycle policies are specifically designed for this purpose and are a more efficient and cheaper solution for managing the lifecycle of objects within S3.\n\n**Why option 2 is incorrect:**\nStoring logs directly in CloudWatch Logs is generally more expensive than storing them in S3, especially for large volumes of data. CloudWatch Logs is better suited for real-time monitoring and analysis, not for long-term archival storage. Also, AWS Backup is not designed for archiving individual log files from CloudWatch Logs to Glacier Deep Archive in a cost-effective manner. Exporting logs from CloudWatch Logs to S3 and then using S3 lifecycle policies is a better approach.\n\n**Why option 3 is incorrect:**\nStoring logs directly in CloudWatch Logs is generally more expensive than storing them in S3, especially for large volumes of data. CloudWatch Logs is better suited for real-time monitoring and analysis, not for long-term archival storage. While S3 Lifecycle policies can be used to transition data to Glacier Deep Archive, they cannot be directly applied to CloudWatch Logs. Logs would need to be exported to S3 first, making this a less efficient and more costly solution.",
    "domain": "Design Cost-Optimized Architectures"
  }
]