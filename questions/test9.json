[
  {
    "id": 0,
    "text": "A company has a website hosted on AWS The website is behind an Application Load Balancer \n(ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward \nall requests to the website so that the requests will use HTTPS. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Update the ALB's network ACL to accept only HTTPS traffic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a rule that replaces the HTTP in the URL with HTTPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the ALB with a Network Load Balancer configured to use Server Name Indication",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nTo redirect HTTP traffic to HTTPS, a solutions architect should create a listener rule on the ALB to redirect HTTP traffic to HTTPS. Application Load Balancers support listener rules that can redirect HTTP requests to HTTPS automatically. This is the standard AWS-recommended approach for enforcing HTTPS on web applications. The ALB listener rule can be configured to redirect all HTTP traffic (port 80) to HTTPS (port 443), ensuring all requests use encrypted connections. This solution requires minimal configuration and provides seamless redirection without requiring changes to the application code.\n\n**Why option 0 is incorrect:**\nThe option that says update the ALB's network ACL to accept only HTTPS traffic is incorrect because network ACLs operate at the subnet level and control traffic flow based on IP addresses and ports, but they do not have the ability to redirect traffic from one protocol to another. Network ACLs can only allow or deny traffic, not perform protocol-level redirection. Additionally, blocking HTTP traffic entirely would prevent users from accessing the site, rather than redirecting them to HTTPS.\n\n**Why option 1 is incorrect:**\nThe option that says create a rule that replaces the HTTP in the URL with HTTPS is incorrect because simply replacing text in URLs does not actually redirect traffic or enforce HTTPS connections. This approach would not work at the network level and would require application-level changes. The ALB listener rule for redirection is the proper way to handle this at the load balancer level.\n\n**Why option 3 is incorrect:**\nThe option that says replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI) is incorrect because Network Load Balancers operate at Layer 4 (TCP/UDP) and do not have the ability to handle HTTP/HTTPS redirection at Layer 7. While NLB supports SNI for SSL/TLS termination, it cannot perform HTTP to HTTPS redirection like an Application Load Balancer can. NLB is designed for high-performance, low-latency traffic routing, not for application-level features like HTTP redirection.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company is developing a two-tier web application on AWS. The company's developers have \ndeployed the application on an Amazon EC2 instance that connects directly to a backend \nAmazon RDS database. The company must not hardcode database credentials in the application. \nThe company must also implement a solution to automatically rotate the database credentials on \na regular basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the database credentials in the instance metadata.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the database credentials as a secret in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Secrets Manager is a service that enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Secrets Manager supports automatic rotation of secrets, which is a key differentiator from AWS Systems Manager Parameter Store. By storing the database credentials as a secret in Secrets Manager, you can ensure that they are not hardcoded in the application and that they are automatically rotated on a regular basis. To grant the EC2 instance access to the secret, you can attach the required IAM permissions to the EC2 instance role. This will allow the application to retrieve the secret from Secrets Manager as needed. Secrets Manager integrates with RDS databases and can automatically rotate credentials without application downtime, providing the least operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says store the database credentials in the instance metadata and use Amazon EventBridge rules to run a scheduled AWS Lambda function that updates the RDS credentials and instance metadata at the same time is incorrect because instance metadata is not designed for storing sensitive credentials securely. Additionally, this approach requires significant operational overhead to manage Lambda functions, EventBridge rules, and coordinate credential updates between RDS and instance metadata. Secrets Manager provides automatic rotation without requiring custom Lambda functions or manual coordination.\n\n**Why option 1 is incorrect:**\nThe option that says store the database credentials in a configuration file in an encrypted Amazon S3 bucket and use EventBridge rules to run a scheduled Lambda function that updates the RDS credentials and the credentials in the configuration file is incorrect because this approach requires managing S3 bucket encryption, versioning, Lambda functions, and EventBridge rules, which adds significant operational overhead. Additionally, the application would need to be modified to retrieve credentials from S3, and there's no built-in automatic rotation capability like Secrets Manager provides.\n\n**Why option 3 is incorrect:**\nThe option that says store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store and turn on automatic rotation is incorrect because Parameter Store does not support automatic rotation of secrets. While Parameter Store can store encrypted parameters using AWS KMS, it lacks the built-in rotation capabilities that Secrets Manager provides. Parameter Store is better suited for configuration data and non-sensitive parameters, while Secrets Manager is specifically designed for secrets that require rotation.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "A company is deploying a new public web application to AWS. The application will run behind an \nApplication Load Balancer (ALB).  \nThe application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by \nan external certificate authority (CA). \nThe certificate must be rotated each year before the certificate expires. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Certificate Manager (ACM) allows you to import SSL/TLS certificates that are issued by external certificate authorities (CAs). When you import a certificate into ACM, you provide the certificate, private key, and certificate chain. However, ACM does not manage the renewal process for imported certificates. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. To meet the requirement of rotating the certificate each year, you can use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration, and then manually rotate the certificate by importing the new certificate from your external CA. This is the correct approach when you need to use certificates from an external CA.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate, apply it to the ALB, and use the managed renewal feature to automatically rotate the certificate is incorrect because ACM can only issue certificates through AWS Certificate Manager's public CA or private CA, not from external certificate authorities. If the requirement specifies that the certificate must be issued by an external CA, ACM-issued certificates would not meet this requirement.\n\n**Why option 1 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate, import the key material from the certificate, apply it to the ALB, and use the managed renewal feature is incorrect because this is contradictory - you cannot both issue a certificate from ACM and import key material from an external certificate. Additionally, ACM-issued certificates are automatically renewed by AWS, but the scenario requires certificates from an external CA.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA, apply it to the ALB, and use the managed renewal feature is incorrect because ACM Private CA issues certificates from AWS's private certificate authority, not from an external certificate authority. The scenario explicitly requires certificates issued by an external CA, so Private CA certificates would not meet this requirement.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its \ndocument management application. The company intends to create a product that converts \nlarge .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to \nstore the original files and the converted files. A solutions architect must design a scalable \nsolution to accommodate demand that will grow rapidly over time. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Save the .pdf files to Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSaving PDF files to Amazon S3 and configuring an S3 PUT event to invoke an AWS Lambda function to convert the files to JPG format and store them back in Amazon S3 is the most cost-effective and scalable solution. S3 provides virtually unlimited storage capacity and can handle millions of objects. Lambda functions are serverless and only charge for compute time used, making them highly cost-effective for file conversion tasks. The S3 event-driven architecture automatically triggers the conversion process when files are uploaded, eliminating the need to manage servers or infrastructure. This solution can scale automatically to handle the growing demand from 700,000 users without requiring manual scaling or infrastructure management.\n\n**Why option 1 is incorrect:**\nThe option that says save the PDF files to Amazon DynamoDB and use DynamoDB Streams to invoke a Lambda function to convert files and store them back in DynamoDB is incorrect because DynamoDB has a 400KB item size limit, which is insufficient for PDF files averaging 5MB in size. Additionally, DynamoDB is designed for structured data storage, not for storing large binary files like PDFs and images. Using DynamoDB for file storage would be both technically infeasible and cost-ineffective compared to S3.\n\n**Why option 2 is incorrect:**\nThe option that says upload PDF files to an AWS Elastic Beanstalk application with EC2 instances, EBS storage, and Auto Scaling groups, and use a program in EC2 instances to convert files is incorrect because Elastic Beanstalk requires managing EC2 instances, which incurs ongoing costs even when not processing files. EBS storage is more expensive than S3 for large-scale file storage, and this approach requires managing servers, scaling groups, and application deployment, resulting in higher operational overhead and costs compared to the serverless S3 + Lambda approach.\n\n**Why option 3 is incorrect:**\nThe option that says upload PDF files to an AWS Elastic Beanstalk application with EC2 instances, EFS storage, and Auto Scaling groups is incorrect for similar reasons as option 2. While EFS provides shared file storage, it is more expensive than S3 for large-scale storage, and the EC2-based approach requires ongoing server management and costs. The serverless S3 + Lambda solution is more cost-effective and requires less operational overhead.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company has more than 5 TB of file data on Windows file servers that run on premises. Users \nand applications interact with the data each day. \nThe company is moving its Windows workloads to AWS. As the company continues this process, \nthe company requires access to AWS and on-premises file storage with minimum latency. The \ncompany needs a solution that minimizes operational overhead and requires no significant \nchanges to the existing file access patterns. The company uses an AWS Site-to-Site VPN \nconnection for connectivity to AWS. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy and configure Amazon FSx for Windows File Server on AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy and configure an Amazon S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy and configure an Amazon S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy and configure Amazon FSx for Windows File Server on AWS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx File Gateway (FSx File Gateway) is a File Gateway type that provides low latency and efficient access to in-cloud FSx for Windows File Server file shares from your on-premises facility. This solution allows the company to deploy FSx for Windows File Server on AWS while maintaining access from both AWS and on-premises environments. The FSx File Gateway deployed on-premises provides seamless access to the cloud-based FSx file shares, maintaining the existing SMB protocol and file access patterns. This minimizes operational overhead since FSx is fully managed, and the gateway handles the hybrid connectivity automatically. The solution provides minimum latency for on-premises users while allowing cloud workloads to access the same file shares directly.\n\n**Why option 0 is incorrect:**\nThe option that says deploy and configure Amazon FSx for Windows File Server on AWS, move the on-premises file data to FSx, and reconfigure workloads to use FSx on AWS is incorrect because this approach only provides access from AWS, not from on-premises. The scenario requires access to both AWS and on-premises file storage with minimum latency. Simply moving data to AWS and reconfiguring workloads would not provide the hybrid access pattern required.\n\n**Why option 1 is incorrect:**\nThe option that says deploy and configure an Amazon S3 File Gateway on-premises, move the on-premises file data to the S3 File Gateway, and reconfigure workloads to use the S3 File Gateway is incorrect because S3 File Gateway provides access to S3 buckets, not Windows file shares. The scenario involves Windows file servers and requires maintaining Windows file access patterns (SMB protocol). S3 File Gateway would not support the existing Windows file access patterns and would require significant changes to the application.\n\n**Why option 2 is incorrect:**\nThe option that says deploy and configure an Amazon S3 File Gateway on-premises, move the on-premises file data to Amazon S3, and reconfigure workloads to use either S3 directly or the S3 File Gateway is incorrect because S3 is object storage, not file storage. Windows applications expect file system access with SMB protocol, not object storage APIs. This approach would require rewriting applications to use S3 APIs instead of file system calls, which violates the requirement of \"no significant changes to the existing file access patterns.\"",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The \nhospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n5 \nformat. The hospital needs to modify the Lambda code to identify protected health information \n(PHI) in the reports. Which solution will meet these requirements with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Use existing Python libraries to extract the text from the reports and to identify the PHI from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Textract to extract the text from the reports.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Textract to extract the text from the reports.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Rekognition to extract the text from the reports.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing Amazon Textract to extract the text from the reports and Amazon Comprehend Medical to identify the PHI from the extracted text is the most efficient solution with the least operational overhead. Amazon Textract is a fully managed machine learning service specifically designed for extracting text and data from documents, including PDFs and images. Amazon Comprehend Medical is a specialized natural language processing service that can accurately identify protected health information (PHI) in medical text, such as patient names, medical conditions, medications, and other sensitive health data. Both services are fully managed, requiring no infrastructure provisioning or model training. This solution integrates seamlessly with Lambda functions, requires minimal code changes, and automatically scales to handle varying workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use existing Python libraries to extract text from reports and identify PHI from the extracted text is incorrect because this approach requires significant operational overhead. You would need to maintain and update Python libraries, handle different document formats, implement PHI detection algorithms, and manage the infrastructure to run these libraries. Additionally, custom PHI detection would be less accurate than Comprehend Medical, which is specifically trained on medical text and HIPAA-compliant.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Textract to extract text and Amazon SageMaker to identify PHI is incorrect because SageMaker is a machine learning platform that requires building, training, and deploying custom models. This approach would involve significant operational overhead including model training, hyperparameter tuning, model deployment, and ongoing maintenance. Comprehend Medical is a pre-trained, fully managed service that already understands medical terminology and PHI patterns, eliminating the need for custom model development.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Rekognition to extract text from reports and Amazon Comprehend Medical to identify PHI is incorrect because Amazon Rekognition is designed for image and video analysis, not for text extraction from documents. While Rekognition can detect text in images, it is not optimized for document text extraction like Textract. Textract is specifically designed for extracting structured and unstructured text from documents with higher accuracy for document-based workflows.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A company has an application that generates a large number of files, each approximately 5 MB in \nsize. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 \nyears before they can be deleted. Immediate accessibility is always required as the files contain \ncritical business data that is not easy to reproduce. The files are frequently accessed in the first \n30 days of the object creation but are rarely accessed after the first 30 days. \nWhich storage solution is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket lifecycle policy to move Mm from S3 Standard to S3 Glacier 30 days from",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 bucket lifecycle policy to move tiles from S3 Standard to S3 One Zone-infrequent",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket lifecycle policy to move files from S3 Standard-infrequent Access (S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 bucket Lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCreating an S3 bucket lifecycle policy to move files from S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation is the most cost-effective solution that meets all requirements. S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It offers the same high durability, high throughput, and low latency as S3 Standard, but at a lower storage price. Since the files are frequently accessed in the first 30 days but rarely accessed afterward, transitioning to Standard-IA after 30 days provides immediate accessibility (unlike Glacier, which has retrieval delays) while reducing storage costs. The files can remain in Standard-IA for the 4-year retention period, and then be deleted via lifecycle policy.\n\n**Why option 0 is incorrect:**\nThe option that says create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation is incorrect because Glacier storage classes (except Glacier Instant Retrieval) have retrieval delays ranging from minutes to hours, which does not meet the requirement for immediate accessibility. The scenario explicitly states that immediate accessibility is always required for critical business data. Unless Glacier Instant Retrieval is specifically mentioned, standard Glacier retrieval times would violate the immediate access requirement.\n\n**Why option 1 is incorrect:**\nThe option that says create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation is incorrect because S3 One Zone-IA stores data in a single Availability Zone, which provides less durability than Standard-IA (which stores data across multiple AZs). While One Zone-IA is cheaper, the scenario mentions that files contain critical business data that is not easy to reproduce, making durability a concern. Standard-IA provides better durability while still being cost-effective.\n\n**Why option 3 is incorrect:**\nThe option that says create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-IA 30 days from object creation, then move files to S3 Glacier 4 years after object creation is incorrect because moving files to Glacier after 4 years would violate the immediate accessibility requirement. The scenario states that immediate accessibility is always required, and Glacier (unless Instant Retrieval) does not provide immediate access. Additionally, the policy requires files to be stored for 4 years before deletion, not moved to Glacier.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company hosts an application on multiple Amazon EC2 instances. The application processes \nmessages from an Amazon SQS queue writes to an Amazon RDS table and deletes the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n6 \nmessage from the queue Occasional duplicate records are found in the RDS table. The SQS \nqueue does not contain any duplicate messages. \n \nWhat should a solutions architect do to ensure messages are being processed once only?",
    "options": [
      {
        "id": 0,
        "text": "Use the CreateQueue API call to create a new queue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Add Permission API call to add appropriate permissions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the ReceiveMessage API call to set an appropriate wail time",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the ChangeMessageVisibility APi call to increase the visibility timeout",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe visibility timeout begins when Amazon SQS returns a message to a consumer. During this time, the consumer processes the message and should delete it. However, if the consumer fails before deleting the message and your system doesn't call the DeleteMessage action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again, leading to duplicate processing. To ensure messages are processed only once, you should use the ChangeMessageVisibility API call to increase the visibility timeout if the message processing takes longer than the default visibility timeout. This gives the consumer enough time to complete processing and delete the message before it becomes visible to other consumers. For applications that write to databases, increasing the visibility timeout ensures that database operations complete before the message can be reprocessed.\n\n**Why option 0 is incorrect:**\nThe option that says use the CreateQueue API call to create a new queue is incorrect because creating a new queue does not solve the duplicate message processing problem. The issue is that messages are being processed multiple times from the existing queue, not that a new queue is needed. Creating a new queue would not prevent duplicate processing of messages already in the original queue.\n\n**Why option 1 is incorrect:**\nThe option that says use the Add Permission API call to add appropriate permissions is incorrect because permissions control who can access the queue, not how messages are processed. Adding permissions would not prevent duplicate message processing or ensure messages are processed only once. The duplicate processing issue is related to message visibility timeout, not permissions.\n\n**Why option 2 is incorrect:**\nThe option that says use the ReceiveMessage API call to set an appropriate wait time is incorrect because the wait time (long polling) controls how long the ReceiveMessage call waits for messages to arrive, not how long messages remain invisible to other consumers. The wait time does not affect message visibility timeout or prevent duplicate processing. The visibility timeout is what controls how long a message remains hidden from other consumers after being received.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A solutions architect is designing a new hybrid architecture to extend a company s on-premises \ninfrastructure to AWS. The company requires a highly available connection with consistent low \nlatency to an AWS Region. The company needs to minimize costs and is willing to accept slower \ntraffic if the primary connection fails. \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Provision a VPN tunnel connection to a Region for private connectivity.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nProvisioning an AWS Direct Connect connection to a Region and provisioning a VPN connection as a backup provides a highly available, cost-effective hybrid connectivity solution. AWS Direct Connect provides dedicated network connections from on-premises to AWS with consistent low latency and high bandwidth. However, Direct Connect connections can fail, so having a Site-to-Site VPN as a backup ensures continuity. VPN connections are much more cost-effective than a second Direct Connect connection, and while VPN may have slower performance than Direct Connect, it meets the requirement of accepting slower traffic if the primary connection fails. This solution minimizes costs by using VPN as backup instead of a second Direct Connect, while still providing the high availability and low latency required.\n\n**Why option 1 is incorrect:**\nThe option that says provision a VPN tunnel connection to a Region and provision a second VPN tunnel as backup is incorrect because VPN connections, while cost-effective, do not provide the same consistent low latency as Direct Connect. The scenario requires \"consistent low latency,\" which is a key characteristic of Direct Connect. VPN connections have variable latency depending on internet conditions, which may not meet the consistent low latency requirement.\n\n**Why option 2 is incorrect:**\nThe option that says provision an AWS Direct Connect connection and provision a second Direct Connect connection to the same Region as backup is incorrect because this approach is expensive. While it would provide high availability and low latency, it does not minimize costs as required. A second Direct Connect connection requires additional port fees and data transfer costs, making it significantly more expensive than using a VPN as backup.\n\n**Why option 3 is incorrect:**\nThe option that says provision an AWS Direct Connect connection and use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection is incorrect because there is no such \"failover attribute\" in the AWS CLI that automatically creates backup connections. Direct Connect does not have built-in automatic failover to create new connections. Failover must be configured manually using routing protocols (BGP) and requires pre-provisioned backup connections.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A company is running a business-critical web application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances are in an Auto Scaling group. The application \nuses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The \ncompany wants the application to be highly available with minimum downtime and minimum loss \nof data. \n \nWhich solution will meet these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Place the EC2 instances in different AWS Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Auto Scaling group to use multiple Availability Zones.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the Auto Scaling group to use one Availability Zone.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the Auto Scaling group to use multiple AWS Regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy configuring the Auto Scaling group to use multiple Availability Zones, the application will be able to continue running even if one Availability Zone goes down, providing high availability for the EC2 instances. Configuring the Aurora PostgreSQL database as Multi-AZ ensures that the database remains available in the event of a failure in one Availability Zone, with automatic failover to a standby replica in another AZ. Using Amazon RDS Proxy for the database allows the application to automatically route traffic to healthy database instances, manage connection pooling, and handle failover transparently, further increasing the availability of the application. This solution meets the requirements for high availability with minimum downtime and minimum data loss (Multi-AZ provides synchronous replication) with minimal operational effort, as all components are managed services.\n\n**Why option 0 is incorrect:**\nThe option that says place EC2 instances in different AWS Regions, use Route 53 health checks to redirect traffic, and use Aurora PostgreSQL Cross-Region Replication is incorrect because cross-region replication for Aurora is asynchronous, which could result in data loss during failover. Additionally, placing instances in different regions introduces significant latency between the application and database, and Route 53 health check-based failover takes time, resulting in more downtime than Multi-AZ failover. This approach also requires more operational effort to manage cross-region replication and Route 53 configurations.\n\n**Why option 2 is incorrect:**\nThe option that says configure the Auto Scaling group to use one Availability Zone, generate hourly snapshots, and recover from snapshots in the event of failure is incorrect because using a single Availability Zone creates a single point of failure. If that AZ goes down, both the application and database would be unavailable. Hourly snapshots mean potential data loss of up to an hour, which does not meet the \"minimum loss of data\" requirement. Additionally, recovering from snapshots requires manual intervention and takes significant time, resulting in extended downtime.\n\n**Why option 3 is incorrect:**\nThe option that says configure the Auto Scaling group to use multiple AWS Regions, write data to S3, and use S3 Event Notifications to launch Lambda functions to write to the database is incorrect because this architecture introduces significant complexity and operational overhead. Writing to S3 and then using Lambda to write to the database adds latency and potential points of failure. This approach does not provide the same level of high availability as Multi-AZ deployments and requires significant operational effort to manage the S3-Lambda-database pipeline.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group \nis configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the \nweb service. \n \nThe company notices that the NLB is not detecting HTTP errors for the application. These errors \nrequire a manual restart of the EC2 instances that run the web service. The company needs to \nimprove the application's availability without writing custom scripts or code. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable HTTP health checks on the NLB. supplying the URL of the company's application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a cron job to the EC2 instances to check the local application's logs once each minute.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the NLB with an Application Load Balancer.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nReplacing the NLB with an Application Load Balancer (ALB) is the correct solution because ALB operates at Layer 7 (application layer) and can detect HTTP errors, while Network Load Balancer (NLB) operates at Layer 4 (transport layer) and only handles TCP/UDP traffic. ALB can perform HTTP health checks that examine the HTTP response codes and content, allowing it to detect application-level errors and automatically remove unhealthy instances from the target group. This provides automatic failover without requiring custom scripts or code. ALB's health checks can be configured to check specific URLs, expected HTTP response codes, and response body content, making it ideal for detecting HTTP application errors.\n\n**Why option 0 is incorrect:**\nThe option that says enable HTTP health checks on the NLB by supplying the URL of the company's application is incorrect because Network Load Balancers do not support HTTP health checks. NLB operates at Layer 4 (TCP/UDP) and can only perform TCP health checks, which verify that a TCP connection can be established but cannot detect HTTP-level errors or application issues. NLB cannot examine HTTP response codes or content to determine if the application is functioning correctly.\n\n**Why option 1 is incorrect:**\nThe option that says add a cron job to the EC2 instances to check the local application's logs once each minute is incorrect because this approach requires writing and maintaining custom scripts, which violates the requirement of improving availability \"without writing custom scripts or code.\" Additionally, checking logs does not automatically restart unhealthy instances or remove them from the load balancer target group. This would require additional automation to integrate with the load balancer.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon CloudWatch alarm that monitors the UnhealthyHostCount metric for the NLB is incorrect because while CloudWatch can monitor metrics, it cannot automatically remove unhealthy instances from the target group or restart them. CloudWatch alarms can trigger notifications or actions, but you would still need to write custom automation (Lambda functions, Systems Manager automation, etc.) to actually handle the unhealthy instances, which violates the \"without writing custom scripts or code\" requirement. Additionally, NLB's health checks are TCP-based and may not detect HTTP errors effectively.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company runs a shopping application that uses Amazon DynamoDB to store customer \ninformation. In case of data corruption, a solutions architect needs to design a solution that meets \na recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. \n \nWhat should the solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure DynamoDB global tables.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure DynamoDB point-in-time recovery.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Export the DynamoDB data to Amazon S3 Glacier on a daily basis.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nDynamoDB Point-in-Time Recovery (PITR) provides continuous backups of your DynamoDB table data. With PITR enabled, you can restore your table to any point in time within the last 35 days. PITR provides a recovery point objective (RPO) of less than 1 second in most cases, easily meeting the 15-minute RPO requirement. The recovery time objective (RTO) of 1 hour can be met by restoring the table to a point in time before the corruption occurred. PITR is a fully managed service that requires no manual intervention for backups and provides fast recovery capabilities. This solution is operationally efficient as it's automatically enabled and managed by AWS.\n\n**Why option 0 is incorrect:**\nThe option that says configure DynamoDB global tables and point the application to a different AWS Region for RPO recovery is incorrect because global tables provide multi-region replication for high availability, but they do not protect against data corruption. If data is corrupted in one region, that corruption will be replicated to all regions. Global tables are designed for disaster recovery and low-latency access, not for point-in-time recovery from data corruption.\n\n**Why option 2 is incorrect:**\nThe option that says export DynamoDB data to Amazon S3 Glacier on a daily basis and import from Glacier for RPO recovery is incorrect because daily exports do not meet the 15-minute RPO requirement. Daily backups mean you could lose up to 24 hours of data, far exceeding the 15-minute RPO. Additionally, importing data from Glacier requires retrieval time (which can take hours), and the import process itself takes time, likely exceeding the 1-hour RTO requirement.\n\n**Why option 3 is incorrect:**\nThe option that says schedule Amazon EBS snapshots for the DynamoDB table every 15 minutes is incorrect because DynamoDB is a managed NoSQL database service that does not use EBS volumes. You cannot take EBS snapshots of DynamoDB tables as DynamoDB's storage is abstracted and managed by AWS. EBS snapshots are only applicable to EC2 instances with attached EBS volumes, not DynamoDB tables.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A company runs a photo processing application that needs to frequently upload and download \npictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect \nhas noticed an increased cost in data transfer fees and needs to implement a solution to reduce \nthese costs. \n \nHow can the solutions architect meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application Into a public subnet and allow it to route through an internet gateway to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nBy deploying an S3 VPC gateway endpoint into the VPC and attaching an endpoint policy that allows access to the S3 buckets, the application can access the S3 buckets over a private network connection within the VPC, eliminating the need for data transfer over the internet. VPC endpoints provide private connectivity between your VPC and AWS services without requiring internet gateways, NAT devices, VPN connections, or AWS Direct Connect connections. This eliminates data transfer charges for traffic between your VPC and S3 within the same region, significantly reducing costs. The endpoint policy can be used to specify which S3 buckets the application has access to, providing fine-grained access control. This solution improves both cost efficiency and performance by keeping traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nThe option that says deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it is incorrect because API Gateway is not designed to route S3 calls and would not reduce data transfer costs. API Gateway is used for creating RESTful APIs, not for routing S3 traffic. Additionally, routing through API Gateway would add unnecessary complexity and costs without addressing the data transfer fee issue.\n\n**Why option 1 is incorrect:**\nThe option that says deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to S3 buckets is incorrect because NAT gateways are used to allow resources in private subnets to access the internet, but they still route traffic through the internet gateway, which incurs data transfer charges. NAT gateways do not provide private connectivity to S3 like VPC endpoints do. Additionally, NAT gateways themselves incur hourly charges and data processing charges.\n\n**Why option 2 is incorrect:**\nThe option that says deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets is incorrect because routing through an internet gateway means traffic goes over the public internet, which incurs data transfer charges. This approach does not reduce costs and actually increases them by requiring internet gateway routing. VPC endpoints provide private connectivity that avoids internet routing entirely.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company recently launched Linux-based application instances on Amazon EC2 in a private \nsubnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet \nof a VPC. A solutions architect needs to connect from the on-premises network, through the \ncompany's internet connection, to the bastion host, and to the application servers. The solutions \narchitect must make sure that the security groups of all the EC2 instances will allow that access. \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nThe bastion host security group should allow inbound access from the external IP range for the company because connections from on-premises networks go through the internet, so the bastion host will see the public IP addresses of the on-premises resources. The bastion host is in a public subnet and accessible via the internet gateway, so it needs to accept connections from the company's external/public IP addresses.\n\n**Why option 3 is correct:**\nThe application instances security group should allow inbound SSH access from only the private IP address of the bastion host because the bastion host and application instances are in the same VPC. When the bastion host connects to the application instances, it uses its private IP address (not public IP) since both are within the same VPC. This provides secure access by only allowing connections from the bastion host's private IP, following the principle of least privilege.\n\n**Why option 0 is incorrect:**\nThe option that says replace the bastion host security group with one that only allows inbound access from the application instances is incorrect because this would prevent the on-premises network from connecting to the bastion host. The bastion host needs to accept connections from on-premises (via internet) first, before it can connect to the application instances.\n\n**Why option 1 is incorrect:**\nThe option that says replace the bastion host security group with one that only allows inbound access from the internal IP range for the company is incorrect because when connecting from on-premises through the internet, the bastion host will see the public/external IP addresses of the on-premises resources, not their internal IP addresses. Internal IP ranges are not routable over the internet.\n\n**Why option 4 is incorrect:**\nThe option that says replace the application instances security group with one that allows inbound SSH access from only the public IP address of the bastion host is incorrect because within the same VPC, instances communicate using private IP addresses, not public IP addresses. The bastion host's public IP is only used for internet routing to reach the bastion, but VPC-internal communication uses private IPs.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A solutions architect is designing a two-tier web application. The application consists of a public-\nfacing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft \nSQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the \ncompany. \nHow should security groups be configured in this situation? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the security group for the database tier to allow inbound traffic on port 1433 from the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the database tier to allow outbound traffic on ports 443 and",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nConfigure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0 is correct because the web tier is public-facing and needs to accept HTTPS connections from the internet. Port 443 is the standard port for HTTPS traffic. Allowing traffic from 0.0.0.0/0 means accepting connections from any IP address on the internet, which is necessary for a public-facing web application. Security groups are stateful, meaning that return traffic is automatically allowed, so you don't need to explicitly create outbound rules for responses.\n\n**Why option 2 is correct:**\nConfigure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier is correct because port 1433 is the default port for Microsoft SQL Server. The database should only accept connections from the web tier, not from the internet. By referencing the web tier's security group instead of IP addresses, you create a more secure and flexible configuration that automatically adapts if the web tier instances change. This follows the principle of least privilege by only allowing necessary database access from the web tier.\n\n**Why option 1 is incorrect:**\nThe option that says configure the web tier security group to allow outbound traffic on port 443 from 0.0.0.0/0 is incorrect because outbound rules control traffic leaving the instance, not incoming traffic. For a public-facing web application, you need inbound rules to accept incoming HTTPS requests. Additionally, security groups are stateful, so return traffic for established connections is automatically allowed without needing explicit outbound rules.\n\n**Why option 3 is incorrect:**\nThe option that says configure the database tier security group to allow outbound traffic on ports 443 and 1433 to the web tier is incorrect because outbound rules are not needed for the database to respond to queries from the web tier. Security groups are stateful, so return traffic for established connections is automatically allowed. Additionally, port 443 (HTTPS) is not used for SQL Server communication - port 1433 is the correct port for SQL Server.\n\n**Why option 4 is incorrect:**\nThe option that says configure the database tier security group to allow inbound traffic on ports 443 and 1433 from the web tier is incorrect because port 443 (HTTPS) is not used for SQL Server database connections. SQL Server uses port 1433 for database communication. Allowing port 443 on the database tier would be unnecessary and could create a security risk if not properly secured.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company wants to move a multi-tiered application from on premises to the AWS Cloud to \nimprove the application's performance. The application consists of application tiers that \ncommunicate with each other by way of RESTful services. Transactions are dropped when one \ntier becomes overloaded. A solutions architect must design a solution that resolves these issues \nand modernizes the application. \n \nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch metrics to analyze the application performance history to determine",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing Amazon API Gateway and directing transactions to AWS Lambda functions as the application layer, with Amazon SQS as the communication layer between application services, is the most operationally efficient solution. API Gateway provides a fully managed API service that handles request routing, authentication, and throttling. Lambda functions are serverless and automatically scale to handle varying loads, eliminating the need to manage servers. SQS decouples the application tiers, preventing transaction drops when one tier becomes overloaded by buffering messages in a queue. This serverless architecture modernizes the application by eliminating server management, provides automatic scaling, and ensures high availability. The solution requires minimal operational overhead as all services are fully managed.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon CloudWatch metrics to analyze application performance history and increase EC2 instance sizes is incorrect because this approach requires manual analysis and intervention, which is not operationally efficient. Additionally, simply increasing instance sizes does not prevent transaction drops when tiers become overloaded - it only provides more capacity. This approach also doesn't modernize the application architecture and requires ongoing capacity planning and management.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon SNS to handle messaging between EC2 servers in an Auto Scaling group and use CloudWatch to monitor SNS queue length is incorrect because SNS is a pub/sub messaging service, not a queuing service. SNS does not buffer messages like SQS does, so it cannot prevent transaction drops when a tier becomes overloaded. SNS delivers messages immediately to all subscribers, and if a subscriber is overloaded, messages may be lost. Additionally, SNS doesn't have a \"queue length\" metric like SQS does.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon SQS to handle messaging between EC2 servers in an Auto Scaling group and use CloudWatch to monitor SQS queue length and scale up when communication failures are detected is partially correct but less operationally efficient than the serverless approach. While SQS can help prevent transaction drops, using EC2 instances requires managing servers, Auto Scaling groups, and scaling policies. The serverless Lambda + API Gateway approach eliminates server management entirely and provides better operational efficiency.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A company receives 10 TB of instrumentation data each day from several machines located at a \nsingle factory. The data consists of JSON files stored on a storage area network (SAN) in an on-\npremises data center located within the factory. The company wants to send this data to Amazon \nS3 where it can be accessed by several additional systems that provide critical near-real-lime \nanalytics.  \nA secure transfer is important because the data is considered sensitive.  \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n11 \nWhich solution offers the MOST reliable data transfer?",
    "options": [
      {
        "id": 0,
        "text": "AWS DataSync over public internet",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS DataSync over AWS Direct Connect",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Database Migration Service (AWS DMS) over public internet",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS DataSync over AWS Direct Connect provides the most reliable data transfer solution for moving large volumes of data from on-premises to S3. DataSync is specifically designed for data migration and can rapidly move active datasets over the network into Amazon S3, Amazon EFS, or FSx for Windows File Server. DataSync includes automatic encryption and data integrity validation to ensure data arrives securely, intact, and ready to use. Using Direct Connect provides a dedicated network connection with consistent bandwidth and low latency, which is essential for transferring 10 TB of data daily. Direct Connect also provides more reliable connectivity than public internet, reducing the risk of transfer failures. The combination of DataSync's optimized transfer protocol and Direct Connect's dedicated connection ensures the most reliable transfer for critical near-real-time analytics requirements.\n\n**Why option 0 is incorrect:**\nThe option that says AWS DataSync over public internet is incorrect because while DataSync provides encryption and integrity validation, using the public internet introduces variability in bandwidth, latency, and reliability. For 10 TB of daily transfers of sensitive data requiring critical near-real-time analytics, the public internet may experience congestion, packet loss, or outages that could interrupt transfers. Direct Connect provides a dedicated connection that is more reliable and predictable for large-scale data transfers.\n\n**Why option 2 is incorrect:**\nThe option that says AWS Database Migration Service (AWS DMS) over public internet is incorrect because DMS is designed for migrating databases, not for transferring files from a SAN. The scenario involves JSON files stored on a SAN, not a database. DMS would not be the appropriate service for this use case. Additionally, using public internet has the same reliability concerns as option 0.\n\n**Why option 3 is incorrect:**\nThe option that says AWS Database Migration Service (AWS DMS) over AWS Direct Connect is incorrect because DMS is designed for database migration, not for file transfer from SAN storage. While Direct Connect provides reliable connectivity, DMS is not the right tool for transferring JSON files from a SAN to S3. DataSync is specifically designed for file-based data migration scenarios like this one.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company needs to configure a real-time data ingestion architecture for its application. The \ncompany needs an API, a process that transforms data as the data is streamed, and a storage \nsolution for the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon API Gateway API to send data to AWS Glue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nConfiguring an Amazon API Gateway API to send data to an Amazon Kinesis data stream, creating a Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source, using AWS Lambda functions to transform the data, and using the Kinesis Data Firehose delivery stream to send the data to Amazon S3 provides a fully serverless, managed solution with minimal operational overhead. API Gateway provides the API layer without managing servers. Kinesis Data Streams provides real-time data streaming capabilities. Lambda functions can transform data as it streams through, and Kinesis Data Firehose automatically delivers the transformed data to S3. All components are fully managed services that scale automatically and require no infrastructure management.\n\n**Why option 0 is incorrect:**\nThe option that says deploy an Amazon EC2 instance to host an API that sends data to a Kinesis data stream, then use Firehose and Lambda is incorrect because deploying EC2 instances requires managing servers, operating systems, scaling, patching, and monitoring, which adds significant operational overhead. The serverless API Gateway approach eliminates all server management requirements.\n\n**Why option 1 is incorrect:**\nThe option that says deploy an Amazon EC2 instance to host an API that sends data to AWS Glue is incorrect because AWS Glue is an ETL (Extract, Transform, Load) service designed for batch processing and data cataloging, not for real-time streaming data ingestion. Glue is not suitable for real-time streaming scenarios. Additionally, using EC2 instances adds operational overhead for server management.\n\n**Why option 3 is incorrect:**\nThe option that says configure an Amazon API Gateway API to send data to AWS Glue, use Lambda functions to transform data, and use Glue to send data to S3 is incorrect because AWS Glue is designed for batch ETL jobs, not for real-time streaming data ingestion. Glue cannot handle real-time data streams like Kinesis can. The architecture described would not support real-time data ingestion requirements.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A company needs to keep user transaction data in an Amazon DynamoDB table. \nThe company must retain the data for 7 years. \nWhat is the MOST operationally efficient solution that meets these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n12",
    "options": [
      {
        "id": 0,
        "text": "Use DynamoDB point-in-time recovery to back up the table continuously.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Backup to create backup schedules and retention policies for the table.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an on-demand backup of the table by using the DynamoDB console.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing AWS Backup to create backup schedules and retention policies for the DynamoDB table is the most operationally efficient solution for long-term data retention. AWS Backup is a fully managed, centralized backup service that can automatically back up DynamoDB tables according to a schedule you define. It supports retention policies that can automatically delete backups after a specified retention period (up to the required 7 years), eliminating the need for manual backup management. AWS Backup provides a single interface for managing backups across multiple AWS services, simplifies compliance reporting, and handles all backup lifecycle management automatically. This solution requires minimal operational effort compared to manual backup processes.\n\n**Why option 0 is incorrect:**\nThe option that says use DynamoDB point-in-time recovery (PITR) to back up the table continuously is incorrect because PITR only retains backups for 35 days, which does not meet the 7-year retention requirement. PITR is designed for short-term recovery scenarios, not for long-term data retention. While PITR is useful for recovering from accidental deletions or corruption, it cannot be used to meet regulatory or compliance requirements for 7-year data retention.\n\n**Why option 2 is incorrect:**\nThe option that says create an on-demand backup using the DynamoDB console, store it in S3, and set an S3 Lifecycle configuration is incorrect because this approach requires manual intervention to create backups, which is not operationally efficient. On-demand backups must be created manually each time, and managing backup schedules, retention, and cleanup would require custom automation or manual processes. While this could work, it requires significantly more operational overhead than AWS Backup's automated approach.\n\n**Why option 3 is incorrect:**\nThe option that says create an EventBridge rule to invoke a Lambda function that backs up the table and stores it in S3, then set an S3 Lifecycle configuration is incorrect because this approach requires writing and maintaining custom Lambda code, managing EventBridge rules, and handling error scenarios manually. This adds significant operational overhead compared to AWS Backup, which handles all of this automatically. Additionally, you would need to implement your own retention logic in the Lambda function, whereas AWS Backup provides built-in retention policies.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is planning to use an Amazon DynamoDB table for data storage. The company is \nconcerned about cost optimization. The table will not be used on most mornings. In the evenings, \nthe read and write traffic will often be unpredictable. When traffic spikes occur, they will happen \nvery quickly. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create a DynamoDB table in on-demand capacity mode.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a DynamoDB table with a global secondary index.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a DynamoDB table with provisioned capacity and auto scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCreating a DynamoDB table in on-demand capacity mode is the best recommendation for this scenario. On-demand mode automatically scales to accommodate your workload's traffic without requiring capacity planning or manual scaling configuration. It's ideal when you have unpredictable application traffic, as described in the scenario where traffic spikes happen very quickly. On-demand mode charges you only for the read and write requests you make, making it cost-effective for workloads with variable or unpredictable traffic patterns. Since the table is not used most mornings and has unpredictable evening traffic, on-demand mode eliminates the cost of paying for provisioned capacity that sits idle, while automatically handling rapid traffic spikes without any configuration changes.\n\n**Why option 1 is incorrect:**\nThe option that says create a DynamoDB table with a global secondary index (GSI) is incorrect because GSIs are used for querying data by different attributes, not for handling unpredictable traffic or cost optimization. GSIs don't address the capacity mode question or help with traffic spikes. The question is about choosing the right capacity mode for cost optimization and unpredictable traffic, not about indexing strategies.\n\n**Why option 2 is incorrect:**\nThe option that says create a DynamoDB table with provisioned capacity and auto scaling is incorrect because while auto scaling can adjust capacity based on traffic, it requires time to scale up (typically minutes), which may not be fast enough for traffic spikes that happen \"very quickly\" as described. Additionally, provisioned capacity with auto scaling requires capacity planning and may result in paying for minimum capacity even during idle periods (mornings). On-demand mode provides instant scaling without any delay.\n\n**Why option 3 is incorrect:**\nThe option that says create a DynamoDB table in provisioned capacity mode and configure it as a global table is incorrect because global tables are used for multi-region replication and high availability, not for handling unpredictable traffic or cost optimization. Provisioned capacity mode requires capacity planning and doesn't automatically scale instantly like on-demand mode. Global tables add complexity and cost without addressing the core requirements of unpredictable traffic and cost optimization.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for \nhelp with an application migration initiative. A solutions architect needs to share an Amazon \nMachine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The \nAMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses a customer managed \ncustomer master key (CMK) to encrypt EBS volume snapshots. \nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's \nAWS account?",
    "options": [
      {
        "id": 0,
        "text": "Make the encrypted AMI and snapshots publicly available.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the launchPermission property of the AMI.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Modify the launchPermission property of the AMI.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nModifying the launchPermission property of the AMI to share it with the MSP Partner's AWS account only, and modifying the CMK's key policy to allow the MSP Partner's AWS account to use the key, is the most secure way to share an encrypted AMI. When an AMI is backed by EBS volumes encrypted with a customer-managed CMK, both the AMI and the underlying snapshots must be shared, and the CMK must be accessible to the target account. By modifying the AMI's launch permissions to share only with the specific MSP Partner account (not making it public), you maintain security through least-privilege access. Sharing the existing CMK with the MSP Partner account allows them to decrypt and use the AMI, as the snapshots are already encrypted with this key. This approach is more secure than creating new keys or making resources publicly available.\n\n**Why option 0 is incorrect:**\nThe option that says make the encrypted AMI and snapshots publicly available and modify the CMK's key policy to allow the MSP Partner's account to use the key is incorrect because making AMIs and snapshots publicly available exposes them to anyone on the internet, which is a significant security risk. Even though the data is encrypted, making resources public violates security best practices and the principle of least privilege. The AMI should only be shared with the specific MSP Partner account, not made publicly available.\n\n**Why option 2 is incorrect:**\nThe option that says modify the launchPermission property of the AMI to share it with the MSP Partner's account and modify the CMK's key policy to trust a new CMK owned by the MSP Partner is incorrect because you cannot simply \"trust a new CMK\" to decrypt data encrypted with a different CMK. The AMI snapshots are encrypted with the existing CMK, so the MSP Partner needs access to that same CMK to decrypt and use the AMI. Creating a new CMK would require re-encrypting the snapshots, which is not necessary and adds complexity.\n\n**Why option 3 is incorrect:**\nThe option that says export the AMI from the source account to an S3 bucket in the MSP Partner's account and encrypt the S3 bucket with a CMK owned by the MSP Partner is incorrect because exporting AMIs to S3 is a complex, time-consuming process that requires significant storage and transfer costs. Additionally, the exported AMI would still need to be imported and launched in the target account, and the original encryption key issue would still need to be resolved. The direct AMI sharing approach is simpler, faster, and more secure.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect is designing the cloud architecture for a new application being deployed on \nAWS. The process should run in parallel while adding and removing application nodes as needed \nbased on the number of jobs to be processed. The processor application is stateless. The \nsolutions architect must ensure that the application is loosely coupled and the job items are \ndurably stored. \n \nWhich design should the solutions architect use?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon SNS topic to send the jobs that need to be processed.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon SQS queue to hold the jobs that need to be processed.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon SQS queue to hold the jobs that needs to be processed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon SNS topic to send the jobs that need to be processed.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCreating an Amazon SQS queue to hold the jobs that need to be processed, creating an AMI with the processor application, creating a launch template using the AMI, creating an Auto Scaling group using the launch template, and setting the scaling policy to add and remove nodes based on the number of items in the SQS queue is the correct design. SQS provides durable storage for job items, ensuring they are not lost even if processing nodes fail. SQS decouples the job producers from the processors, creating a loosely coupled architecture. The Auto Scaling group can scale based on the ApproximateNumberOfMessagesVisible metric from SQS, automatically adding instances when the queue has many messages and removing them when the queue is empty. This ensures parallel processing capability while dynamically adjusting capacity based on workload. Launch templates are preferred over launch configurations as they support versioning and more features.\n\n**Why option 0 is incorrect:**\nThe option that says create an Amazon SNS topic to send jobs, create an AMI, launch configuration, Auto Scaling group, and scale based on CPU usage is incorrect because SNS is a pub/sub messaging service that delivers messages immediately to all subscribers. SNS does not provide durable storage or queuing - if a subscriber is not available, the message may be lost. Additionally, scaling based on CPU usage may not accurately reflect the number of jobs waiting to be processed, especially if jobs are CPU-intensive but arrive sporadically. SQS provides better durability and job queuing for this use case.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon SQS queue, AMI, launch configuration, Auto Scaling group, and scale based on network usage is incorrect because while SQS is correct for durable job storage, scaling based on network usage is not an appropriate metric for determining when to add or remove processing nodes. Network usage doesn't directly correlate with the number of jobs in the queue. The scaling policy should be based on the SQS queue depth (ApproximateNumberOfMessagesVisible) to accurately reflect workload and ensure nodes are added when jobs are waiting.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon SNS topic to send jobs, create an AMI, launch template, Auto Scaling group, and scale based on the number of messages published to the SNS topic is incorrect because SNS does not provide durable storage or queuing capabilities. SNS delivers messages immediately and doesn't maintain a queue of unprocessed jobs. Additionally, the number of messages published to SNS doesn't indicate how many jobs are waiting to be processed, as SNS messages are delivered immediately and don't accumulate like SQS messages do.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company hosts its web applications in the AWS Cloud. The company configures Elastic Load \nBalancers to use certificate that are imported into AWS Certificate Manager (ACM). The \ncompany's security team must be notified 30 days before the expiration of each certificate.  \nWhat should a solutions architect recommend to meet the requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Config rule that checks for certificates that will expire within 30 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS trusted Advisor to check for certificates that will expire within to days.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreating an AWS Config rule that checks for certificates that will expire within 30 days, and configuring Amazon EventBridge (CloudWatch Events) to invoke a custom alert via Amazon SNS when AWS Config reports a noncompliant resource, is the recommended approach for monitoring ACM certificate expiration. AWS Config can continuously monitor ACM certificates and evaluate them against rules you define. When a certificate is found to be expiring within 30 days, Config marks it as noncompliant. EventBridge can detect Config compliance changes and trigger SNS notifications to alert the security team. This solution provides automated, continuous monitoring without requiring manual checks or custom Lambda functions for certificate evaluation.\n\n**Why option 0 is incorrect:**\nThe option that says add a rule in ACM to publish a custom message to an SNS topic every day beginning 30 days before any certificate will expire is incorrect because ACM does not have a built-in feature to publish messages to SNS based on certificate expiration dates. ACM manages certificate provisioning and renewal for AWS-issued certificates, but it does not provide notification capabilities for imported certificates. You cannot configure ACM rules to send notifications.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Trusted Advisor to check for certificates expiring within 30 days and create a CloudWatch alarm based on Trusted Advisor metrics is incorrect because Trusted Advisor is a best practices checking service that provides recommendations, but it does not provide real-time monitoring or CloudWatch metrics for certificate expiration. Trusted Advisor checks are performed periodically and may not meet the requirement for timely notifications. Additionally, Trusted Advisor does not expose metrics that can be used in CloudWatch alarms for this purpose.\n\n**Why option 3 is incorrect:**\nThe option that says create an EventBridge rule to detect certificates expiring within 30 days, configure it to invoke a Lambda function, and configure the Lambda function to send an SNS alert is incorrect because EventBridge does not have built-in events for ACM certificate expiration. EventBridge rules are triggered by AWS service events, but ACM does not emit events for certificate expiration dates. You would need to write a Lambda function that periodically checks certificate expiration dates, which adds operational overhead compared to using AWS Config's built-in certificate monitoring capabilities.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A company's dynamic website is hosted using on-premises servers in the United States. The \ncompany is launching its product in Europe, and it wants to optimize site loading times for new \nEuropean users. The site's backend must remain in the United States. The product is being \nlaunched in a few days, and an immediate solution is needed. \n \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the website to Amazon S3. Use cross-Region replication between Regions.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing Amazon CloudFront with a custom origin pointing to the on-premises servers is the best solution for optimizing site loading times for European users while keeping the backend in the United States. CloudFront is a content delivery network (CDN) that caches content at edge locations worldwide, including locations in Europe. When European users request content, CloudFront serves it from the nearest edge location, significantly reducing latency. For dynamic content, CloudFront can still route requests to the on-premises origin in the United States, but it can cache static assets at edge locations. CloudFront supports custom origins, allowing you to point to on-premises servers via the internet. This solution can be implemented quickly (within days) without requiring infrastructure migration, making it ideal for the immediate solution requirement.\n\n**Why option 0 is incorrect:**\nThe option that says launch an Amazon EC2 instance in us-east-1 and migrate the site to it is incorrect because this would still keep the site in the United States, not optimizing it for European users. Additionally, migrating a website to AWS would take significantly more time than \"a few days\" and would require application changes, data migration, and testing. This approach doesn't address the latency optimization requirement for European users.\n\n**Why option 1 is incorrect:**\nThe option that says move the website to Amazon S3 and use cross-Region replication between Regions is incorrect because S3 is designed for static website hosting, not dynamic websites. The scenario mentions a \"dynamic website,\" which typically requires server-side processing, databases, and application logic that S3 cannot provide. Additionally, cross-Region replication copies objects between S3 buckets in different regions but doesn't automatically route users to the nearest region - you would still need CloudFront or Route 53 for that.\n\n**Why option 3 is incorrect:**\nThe option that says use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers is incorrect because Route 53 DNS routing can direct users to different endpoints based on location, but it cannot reduce latency for users accessing on-premises servers in the United States from Europe. The physical distance and network latency would remain the same. Route 53 routing policies are useful when you have multiple endpoints in different locations, but here there's only one on-premises location in the United States. CloudFront's edge caching is what actually reduces latency by serving content from locations closer to users.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A company wants to reduce the cost of its existing three-tier web architecture. The web, \napplication, and database servers are running on Amazon EC2 instances for the development, \ntest, and production environments. The EC2 instances average 30% CPU utilization during peak \nhours and 10% CPU utilization during non-peak hours. \n \nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run \nfor at least 8 hours each day. The company plans to implement automation to stop the \ndevelopment and test EC2 instances when they are not in use. \n \nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-\neffectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the production EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances for the production EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Spot blocks for the production EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use On-Demand Instances for the production EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Reserved Instances for the production EC2 instances and On-Demand Instances for the development and test EC2 instances is the most cost-effective solution. Reserved Instances provide significant cost savings (up to 72% compared to On-Demand) for predictable workloads that run 24/7, which matches the production environment. Since production instances run continuously, committing to Reserved Instances makes financial sense. For development and test instances that run only 8 hours per day and can be stopped when not in use, On-Demand Instances are appropriate because you only pay when they're running. The automation to stop dev/test instances when not in use further optimizes costs. This combination maximizes savings on production while minimizing costs for non-production environments.\n\n**Why option 0 is incorrect:**\nThe option that says use Spot Instances for production EC2 instances and Reserved Instances for dev/test instances is incorrect because Spot Instances can be interrupted by AWS with only a 2-minute notice, making them unsuitable for production workloads that require 24/7 availability. Production environments need guaranteed capacity and cannot tolerate interruptions. Additionally, Spot Instances are not recommended for critical production workloads due to their unpredictable availability.\n\n**Why option 2 is incorrect:**\nThe option that says use Spot blocks for production EC2 instances and Reserved Instances for dev/test instances is incorrect because Spot blocks are no longer available (AWS discontinued Spot blocks). Even when they were available, Spot instances (including Spot blocks) are not suitable for production workloads that require continuous availability, as they can still be interrupted.\n\n**Why option 3 is incorrect:**\nThe option that says use On-Demand Instances for production and Spot blocks for dev/test instances is incorrect because On-Demand Instances are the most expensive option and don't provide cost savings for predictable 24/7 workloads. Reserved Instances would provide significant savings for production. Additionally, Spot blocks are no longer available, and Spot instances are not ideal for dev/test environments that need to run during specific hours, as availability is not guaranteed.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 25,
    "text": "A company has a production web application in which users upload documents through a web \ninterlace or a mobile app. \n According to a new regulatory requirement, new documents cannot be modified or deleted after \nthey are stored. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the uploaded documents in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nStoring the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled is the correct solution to meet the regulatory requirement that documents cannot be modified or deleted after they are stored. S3 Object Lock provides a write-once-read-many (WORM) model that prevents objects from being deleted or overwritten for a fixed amount of time or indefinitely. Object Lock supports two modes: Governance mode (allows some users with special permissions to delete objects) and Compliance mode (prevents all users from deleting objects, even root users). S3 Versioning is required and automatically activated when Object Lock is enabled, ensuring that all versions of objects are preserved. This combination provides the strongest protection against modification or deletion, meeting strict regulatory requirements.\n\n**Why option 1 is incorrect:**\nThe option that says store the uploaded documents in an Amazon S3 bucket and configure an S3 Lifecycle policy to archive the documents periodically is incorrect because lifecycle policies can transition objects to different storage classes or delete them, but they do not prevent modification or deletion of objects. Lifecycle policies are used for cost optimization through storage class transitions, not for enforcing immutability. Objects can still be deleted or overwritten before the lifecycle policy acts on them.\n\n**Why option 2 is incorrect:**\nThe option that says store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled and configure an ACL to restrict all access to read-only is incorrect because ACLs control who can access objects, but they cannot prevent authorized users (including the bucket owner) from deleting objects. Versioning preserves previous versions when objects are overwritten, but it doesn't prevent deletion of current or previous versions. Object Lock is specifically designed to prevent deletion and overwriting, which ACLs cannot do.\n\n**Why option 3 is incorrect:**\nThe option that says store the uploaded documents on an Amazon Elastic File System (EFS) volume and access the data by mounting the volume in read-only mode is incorrect because mounting a volume in read-only mode only prevents modification from that specific mount point, but the volume itself can still be modified or deleted by users with appropriate permissions. EFS does not provide the same level of immutability protection as S3 Object Lock. Additionally, EFS is designed for shared file storage with multiple concurrent access, not for document storage with regulatory compliance requirements.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company has several web servers that need to frequently access a common Amazon RDS \nMySQL Multi-AZ DB instance. The company wants a secure method for the web servers to \nconnect to the database while meeting a security requirement to rotate user credentials \nfrequently. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the database user credentials in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the database user credentials in AWS Systems Manager OpsCenter.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the database user credentials in a secure Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the database user credentials in files encrypted with AWS Key Management Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nStoring the database user credentials in AWS Secrets Manager and granting the necessary IAM permissions to allow the web servers to access Secrets Manager is the best solution. Secrets Manager enables you to replace hardcoded credentials in your code with an API call to retrieve secrets programmatically, ensuring secrets aren't exposed in code. Most importantly, Secrets Manager supports automatic rotation of secrets according to a schedule you define, which meets the security requirement to rotate credentials frequently. Secrets Manager can automatically rotate RDS database credentials by creating new credentials, updating the database, and updating the secret. The web servers can retrieve the latest credentials from Secrets Manager whenever needed. This solution provides both secure credential storage and automated rotation with minimal operational overhead.\n\n**Why option 1 is incorrect:**\nThe option that says store the database user credentials in AWS Systems Manager OpsCenter and grant IAM permissions to access OpsCenter is incorrect because OpsCenter is part of AWS Systems Manager and is designed for operational insights and troubleshooting, not for secrets management. OpsCenter does not provide automatic credential rotation capabilities like Secrets Manager does. While Systems Manager Parameter Store can store encrypted parameters, it doesn't support automatic rotation for database credentials.\n\n**Why option 2 is incorrect:**\nThe option that says store the database user credentials in a secure Amazon S3 bucket and grant IAM permissions to retrieve credentials is incorrect because while S3 can store encrypted objects, it does not provide automatic credential rotation capabilities. You would need to manually rotate credentials and update the S3 object, which doesn't meet the requirement for frequent rotation. Additionally, managing credentials in S3 requires custom application logic to retrieve and use them, whereas Secrets Manager provides a dedicated API and automatic rotation.\n\n**Why option 3 is incorrect:**\nThe option that says store the database user credentials in files encrypted with AWS KMS on the web server file system is incorrect because storing credentials on the file system (even if encrypted) creates security risks if the server is compromised. Additionally, this approach requires manual credential rotation and updating files on each web server, which is operationally complex and doesn't scale well. Secrets Manager provides centralized secret management and automatic rotation, eliminating the need to manage credentials on individual servers.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company hosts an application on AWS Lambda functions mat are invoked by an Amazon API \nGateway API. The Lambda functions save customer data to an Amazon Aurora MySQL \ndatabase. Whenever the company upgrades the database, the Lambda functions fail to establish \ndatabase connections until the upgrade is complete. The result is that customer data Is not \nrecorded for some of the event. \nA solutions architect needs to design a solution that stores customer data that is created during \ndatabase upgrades. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon RDS proxy to sit between the Lambda functions and the database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the run time of me Lambda functions to the maximum.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Persist the customer data to Lambda local storage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the customer data in an Amazon Simple Queue Service (Amazon SOS) FIFO queue.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nStoring the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue and creating a new Lambda function that polls the queue and stores the customer data in the database is the correct solution. SQS provides durable message storage, ensuring that customer data is not lost even if the Lambda function fails or if the database is unavailable during upgrades. When the database is upgraded and connections fail, the Lambda functions can still successfully write customer data to the SQS queue. Once the database upgrade is complete and connections are restored, the separate Lambda function can process the queued messages and store them in the database. This decouples the data ingestion from database availability, ensuring no data loss during maintenance windows. FIFO queues ensure message ordering, which may be important for customer data.\n\n**Why option 0 is incorrect:**\nThe option that says provision an Amazon RDS Proxy to sit between the Lambda functions and the database and configure Lambda functions to connect to the RDS proxy is incorrect because RDS Proxy is designed for connection pooling and management, not for storing data during database unavailability. While RDS Proxy can help manage database connections and provide some resilience, it cannot store customer data when the database itself is unavailable during upgrades. If the database is down for maintenance, RDS Proxy cannot help store the data.\n\n**Why option 1 is incorrect:**\nThe option that says increase the run time of the Lambda functions to the maximum and create a retry mechanism in the code to store customer data in the database is incorrect because retry mechanisms will fail if the database is completely unavailable during upgrades. Increasing Lambda timeout doesn't solve the fundamental problem that the database cannot accept connections during maintenance. Retries will continue to fail until the database upgrade is complete, potentially causing Lambda functions to timeout and lose customer data.\n\n**Why option 2 is incorrect:**\nThe option that says persist the customer data to Lambda local storage and configure new Lambda functions to scan the local storage to save the data to the database is incorrect because Lambda local storage (/tmp) is ephemeral and is not shared between Lambda invocations or different Lambda functions. Each Lambda invocation gets a fresh /tmp directory, so data stored there would be lost when the function completes. Additionally, Lambda functions are stateless and cannot reliably share local storage between different function executions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 28,
    "text": "A survey company has gathered data for several years from areas m\\ the United States. The \ncompany hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company \nhas started to share the data with a European marketing firm that has S3 buckets. The company \nwants to ensure that its data transfer costs remain as low as possible. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Requester Pays feature on the company's S3 bucket",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure cross-account access for the marketing firm so that the marketing firm has access to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the company's S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nConfiguring the Requester Pays feature on the company's S3 bucket is the most cost-effective solution for sharing data with the European marketing firm. When Requester Pays is enabled, the requester (the marketing firm) pays for the data transfer costs and requests instead of the bucket owner. This is ideal for sharing large datasets (3 TB and growing) where the data owner wants to share data without incurring the costs of others accessing it. The marketing firm will pay for the data transfer costs when they download data from the US-based S3 bucket to their European S3 buckets, keeping the company's data transfer costs at zero. This is commonly used for sharing large datasets, reference data, or public data where the data consumer should bear the transfer costs.\n\n**Why option 1 is incorrect:**\nThe option that says configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets is incorrect because Cross-Region Replication incurs data transfer costs that are charged to the bucket owner (the company), not the requester. Additionally, Cross-Region Replication automatically replicates all objects, which may not be desired if the company only wants to share specific data. The company would incur ongoing replication costs for the 3 TB+ dataset, which doesn't meet the requirement to keep data transfer costs as low as possible.\n\n**Why option 2 is incorrect:**\nThe option that says configure cross-account access for the marketing firm so they have access to the company's S3 bucket is incorrect because while this allows the marketing firm to access the data, the company (bucket owner) would still incur all data transfer costs when the marketing firm downloads data. Cross-account access controls who can access the bucket, but doesn't change who pays for data transfer. The company would still pay for all data transfer costs.\n\n**Why option 3 is incorrect:**\nThe option that says configure the company's S3 bucket to use S3 Intelligent-Tiering and sync the S3 bucket to one of the marketing firm's S3 buckets is incorrect because S3 Intelligent-Tiering optimizes storage costs by automatically moving objects between access tiers, but it doesn't reduce data transfer costs. Additionally, \"syncing\" buckets would require data transfer, and the company would incur those costs. Intelligent-Tiering doesn't address the data transfer cost issue when sharing data with external parties.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 29,
    "text": "A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses \nbucket policies to restrict access to audit team IAM user credentials according to the principle of \nleast privilege. Company managers are worried about accidental deletion of documents in the S3 \nbucket and want a more secure solution. \n \nWhat should a solutions architect do to secure the audit documents?",
    "options": [
      {
        "id": 0,
        "text": "Enable the versioning and MFA Delete features on the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nEnabling versioning and MFA Delete features on the S3 bucket provides comprehensive protection against accidental deletion of audit documents. S3 Versioning preserves all versions of objects, so if a document is deleted or overwritten, previous versions are retained and can be restored. MFA Delete adds an additional security layer by requiring multi-factor authentication (MFA) before any delete operation can be performed on versioned objects. This means that even if someone with delete permissions accidentally attempts to delete a document, they must provide an MFA code, significantly reducing the risk of accidental deletion. This combination provides both recovery capability (versioning) and prevention (MFA Delete), making it the most secure solution for protecting confidential audit documents.\n\n**Why option 1 is incorrect:**\nThe option that says enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account is incorrect because IAM MFA protects IAM user access to AWS services, but it doesn't prevent deletion of S3 objects once the user is authenticated. IAM MFA ensures that only authorized users can access AWS, but it doesn't add an extra layer of protection specifically for S3 delete operations like MFA Delete does. MFA Delete is a bucket-level feature that requires MFA for delete operations, which is more specific to the requirement.\n\n**Why option 2 is incorrect:**\nThe option that says add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates is incorrect because S3 Lifecycle policies are used for managing object transitions between storage classes and expiration, not for access control. You cannot use lifecycle policies to deny IAM actions. Access control for S3 operations is managed through bucket policies, IAM policies, or ACLs, not lifecycle policies. Additionally, restricting deletion only during audit dates doesn't provide comprehensive protection.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key is incorrect because encryption protects data confidentiality, not data deletion. If audit team members have s3:DeleteObject permissions, they can still delete encrypted objects even if they don't have KMS key access (the objects would just be deleted in encrypted form). Encryption doesn't prevent deletion - it only protects data at rest. To prevent deletion, you need versioning and MFA Delete, not just encryption.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours. The company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue. Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Modify the DB instance to be a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a read replica of the database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Instruct the development team to manually export the entries in the database at the end of each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache to cache the common queries that the script runs against the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreating a read replica of the database and configuring the script to query only the read replica is the best solution with the least operational overhead. Read replicas are asynchronously replicated copies of the primary database that can handle read traffic, offloading read queries from the primary instance. By directing the script's queries to the read replica, the development team's queries on the primary database won't be impacted by the script's workload. Read replicas are fully managed by RDS, require minimal configuration, and automatically replicate data from the primary instance. This solution isolates the reporting script's workload from the development workload without requiring code changes to the development application.\n\n**Why option 0 is incorrect:**\nThe option that says modify the DB instance to be a Multi-AZ deployment is incorrect because Multi-AZ deployments provide high availability and failover capabilities, but they don't improve performance or separate read workloads. In a Multi-AZ deployment, there's still only one primary instance handling all read and write traffic. The standby replica in Multi-AZ is only used for failover, not for serving read traffic. This wouldn't solve the performance issue caused by the script's queries competing with development queries.\n\n**Why option 2 is incorrect:**\nThe option that says instruct the development team to manually export the entries in the database at the end of each day is incorrect because this approach requires manual work, doesn't solve the performance issue during the day when the script runs, and doesn't meet the requirement that the script must report a final total during business hours. Manual exports add operational overhead and don't address the root cause of performance degradation.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon ElastiCache to cache the common queries that the script runs against the database is incorrect because ElastiCache is designed for caching frequently accessed, relatively static data. The script is looking for new movies added to the database, which means it's querying for data that changes frequently. Caching wouldn't be effective for this use case since the script needs current data, not cached results. Additionally, ElastiCache requires application code changes to implement caching logic, adding operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company has applications that run on Amazon EC2 instances in a VPC. One of the applications \nneeds to call the Amazon S3 API to store and read objects. According to the company's security \nregulations, no traffic from the applications is allowed to travel across the internet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 interface endpoint.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 gateway endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an S3 bucket in a private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket in the same Region as the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nConfiguring an S3 gateway endpoint within the VPC is the correct solution for allowing EC2 instances to access S3 without internet connectivity. Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect. Traffic between your VPC and S3 stays within the AWS network and never traverses the public internet. Gateway endpoints are free to use (you only pay for data transfer) and are automatically scaled and highly available. They route traffic to S3 using prefix lists in your route tables, ensuring all S3 traffic stays on the AWS backbone network.\n\n**Why option 0 is incorrect:**\nThe option that says configure an S3 interface endpoint is incorrect because S3 does not support interface endpoints (PrivateLink). Interface endpoints are available for many AWS services, but S3 and DynamoDB use gateway endpoints instead. Interface endpoints use ENIs (Elastic Network Interfaces) in your subnets, while gateway endpoints are virtual devices that are added to your route tables.\n\n**Why option 2 is incorrect:**\nThe option that says create an S3 bucket in a private subnet is incorrect because S3 is a regional service that doesn't run in subnets. S3 buckets are not deployed in VPCs or subnets - they exist at the AWS account level in a specific region. You cannot place an S3 bucket in a subnet. S3 access is controlled through VPC endpoints, IAM policies, and bucket policies, not through subnet placement.\n\n**Why option 3 is incorrect:**\nThe option that says create an S3 bucket in the same Region as the EC2 instance is incorrect because while being in the same region reduces latency and data transfer costs, it doesn't prevent traffic from traversing the internet. Without a VPC endpoint, EC2 instances in private subnets would need a NAT gateway to reach S3, which routes traffic through the internet. The requirement is that no traffic should travel across the internet, which requires a VPC endpoint regardless of region.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company is storing sensitive user information in an Amazon S3 bucket. The company wants to \nprovide secure access to this bucket from the application tier running on Amazon EC2 instances \ninside a VPC. \nWhich combination of steps should a solutions architect take to accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure a VPC gateway endpoint for Amazon S3 within the VPC",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a bucket policy to make the objects to the S3 bucket public",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a bucket policy that limits access to only the application tier running in the VPC",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a NAT instance and have the EC2 instances use the NAT instance to access the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nConfiguring a VPC gateway endpoint for Amazon S3 within the VPC provides private connectivity between EC2 instances and S3 without requiring internet gateway or NAT devices. This ensures that traffic between the VPC and S3 stays within the AWS network and never traverses the public internet, meeting security requirements for sensitive data access.\n\n**Why option 2 is correct:**\nCreating a bucket policy that limits access to only the application tier running in the VPC provides fine-grained access control. The bucket policy can use conditions like `aws:SourceVpc` or `aws:SourceIp` to restrict access to requests originating from the VPC or specific VPC endpoints. This ensures that only the EC2 instances in the VPC can access the sensitive user information, providing an additional layer of security beyond IAM permissions.\n\n**Why option 1 is incorrect:**\nThe option that says create a bucket policy to make the objects in the S3 bucket public is incorrect because making objects public would expose sensitive user information to anyone on the internet, which violates security best practices and likely compliance requirements. Public access should never be used for sensitive data.\n\n**Why option 3 is incorrect:**\nThe option that says create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance is incorrect because storing IAM credentials on EC2 instances is a security anti-pattern. Credentials stored on instances can be compromised, and managing credentials across multiple instances is operationally complex. Instead, EC2 instances should use IAM roles, which provide temporary credentials automatically and don't require credential management.\n\n**Why option 4 is incorrect:**\nThe option that says create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket is incorrect because NAT instances route traffic through the internet, which violates the security requirement. Additionally, NAT instances add unnecessary complexity and cost. VPC gateway endpoints provide private connectivity without requiring NAT devices.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company runs an on-premises application that is powered by a MySQL database. The \ncompany is migrating the application to AWS to Increase the application's elasticity and \navailability. The current architecture shows heavy read activity on the database during times of \nnormal operation. Every 4 hours the company's development team pulls a full export of the \nproduction database to populate a database in the staging environment. During this period, users \nexperience unacceptable application latency. The development team is unable to use the staging \nenvironment until the procedure completes. \nA solutions architect must recommend replacement architecture that alleviates the application \nlatency issue.  \nThe replacement architecture also must give the development team the ability to continue using \nthe staging environment without delay. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon RDS for MySQL with a Mufti AZ deployment and read replicas for production.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production and using database cloning to create the staging database on-demand is the best solution. Aurora Replicas can handle read traffic, offloading reads from the primary instance and reducing latency during normal operation. Most importantly, Aurora's database cloning feature creates a new database cluster that shares the same storage volume as the source database, allowing near-instantaneous creation of staging databases without copying data. This eliminates the 4-hour export process that causes application latency. The development team can create a fresh staging database clone on-demand without waiting, and the clone doesn't impact production performance since it uses copy-on-write technology. This solution provides both elasticity/availability for production and instant staging database creation.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production and populate the staging database using mysqldump utility is incorrect because mysqldump still requires exporting the entire database, which takes 4 hours and causes the same application latency issues. While Aurora Replicas help with read traffic, the mysqldump process still impacts the primary database during the export, and the development team still has to wait for the export to complete before using staging.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon RDS for MySQL with Multi-AZ deployment and read replicas, and use the standby instance for the staging database is incorrect because in a Multi-AZ deployment, the standby instance is only used for failover and cannot be used for read traffic or staging purposes. The standby instance is not accessible for queries - it's only activated during failover events. This approach doesn't solve the latency issue or provide staging database access.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon RDS for MySQL with Multi-AZ deployment and read replicas, and populate staging using mysqldump utility is incorrect because this still requires the 4-hour export process that causes application latency. While read replicas can help with read traffic, mysqldump exports still impact the primary database and don't solve the staging environment delay issue. RDS doesn't have the database cloning feature that Aurora provides.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A company is preparing to store confidential data in Amazon S3. For compliance reasons the \ndata must be encrypted at rest Encryption key usage must be logged tor auditing purposes. Keys \nmust be rotated every year. \nWhich solution meets these requirements and the MOST operationally efferent?",
    "options": [
      {
        "id": 0,
        "text": "Server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual",
        "correct": false
      },
      {
        "id": 3,
        "text": "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUsing server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automatic rotation is the most operationally efficient solution that meets all requirements. SSE-KMS provides encryption at rest for S3 objects using keys managed by AWS KMS. KMS automatically logs all key usage to CloudTrail, providing the auditing capability required for compliance. Most importantly, KMS supports automatic key rotation for customer-managed CMKs, which rotates the cryptographic material every 365 days automatically without any manual intervention. This meets the requirement for annual key rotation with minimal operational overhead. KMS handles all key management, rotation, and auditing automatically.\n\n**Why option 0 is incorrect:**\nThe option that says use server-side encryption with customer-provided keys (SSE-C) is incorrect because SSE-C requires you to manage and provide encryption keys yourself. You would need to manually rotate keys every year, which adds operational overhead. Additionally, SSE-C doesn't provide automatic key usage logging to CloudTrail - you would need to implement your own logging solution. This approach requires significant operational effort for key management and rotation.\n\n**Why option 1 is incorrect:**\nThe option that says use server-side encryption with Amazon S3 managed keys (SSE-S3) is incorrect because SSE-S3 uses keys that are fully managed by S3, and you cannot access or control these keys. S3 managed keys don't provide key usage logging to CloudTrail for auditing purposes, which is a compliance requirement. Additionally, you cannot rotate S3-managed keys - AWS handles this internally without visibility or control. This doesn't meet the auditing and key rotation requirements.\n\n**Why option 2 is incorrect:**\nThe option that says use server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual rotation is incorrect because while SSE-KMS provides encryption and CloudTrail logging, manual key rotation requires operational overhead. You would need to create new keys, update applications to use new keys, and manage the rotation process manually every year. Automatic rotation eliminates this operational burden while still meeting all requirements.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A bicycle sharing company is developing a multi-tier architecture to track the location of its \nbicycles during peak operating hours. The company wants to use these data points in its existing \nanalytics platform. A solutions architect must determine the most viable multi-tier option to \nsupport this architecture. The data points must be accessible from the REST API.  \nWhich action meets these requirements for storing and retrieving location data?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Athena with Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon API Gateway with AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon QuickSight with Amazon Redshift.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway with Amazon Kinesis Data Analytics",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon API Gateway with AWS Lambda provides a REST API for storing and retrieving location data points. API Gateway provides the REST API interface that can accept location data from bicycles during peak operating hours. Lambda functions can process the incoming data, store it in a durable storage solution like Amazon DynamoDB or Amazon S3, and retrieve it when requested through the API. Lambda can also integrate with analytics platforms to process the data. This serverless architecture scales automatically to handle peak traffic, provides low latency for real-time location tracking, and integrates seamlessly with existing analytics platforms. The REST API makes the data accessible as required.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Athena with Amazon S3 is incorrect because Athena is a serverless interactive query service for analyzing data in S3, not for storing and retrieving real-time location data through a REST API. Athena is designed for ad-hoc SQL queries on data already stored in S3, not for real-time data ingestion or API-based data access. It doesn't provide a REST API for storing location data points.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon QuickSight with Amazon Redshift is incorrect because QuickSight is a business intelligence and visualization tool, not a data storage or API service. Redshift is a data warehouse for analytics, but it's not optimized for real-time location tracking or REST API access. This combination doesn't provide the REST API interface required for storing and retrieving location data points.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon API Gateway with Amazon Kinesis Data Analytics is incorrect because Kinesis Data Analytics is designed for real-time stream processing and analytics, not for providing REST API access to stored data. While API Gateway can send data to Kinesis, Kinesis Data Analytics processes streaming data and outputs to Kinesis Data Streams, Firehose, or Lambda - it doesn't provide a REST API for retrieving stored location data. The data would need to be stored elsewhere (like S3 or DynamoDB) and accessed through a different mechanism.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company has an automobile sales website that stores its listings in a database on Amazon \nRDS. When an automobile is sold the listing needs to be removed from the website and the data \nmust be sent to multiple target systems. \nWhich design should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nSubscribing to an RDS event notification and sending to an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues, then using AWS Lambda functions to update the targets, is the correct design. RDS event notifications use SNS to publish events when database changes occur. When an automobile is sold and the listing is removed from the RDS database, RDS can publish an event to an SNS topic. SNS can then fan out the message to multiple SQS queues (one for each target system), ensuring each target system receives the data. Lambda functions can then process messages from each SQS queue and update the respective target systems. This design decouples the database from target systems and provides reliable message delivery.\n\n**Why option 0 is incorrect:**\nThe option that says create an AWS Lambda function triggered when the RDS database is updated to send information to an Amazon SQS queue for targets to consume is incorrect because RDS doesn't natively trigger Lambda functions when database records are updated. RDS doesn't have built-in change data capture (CDC) capabilities like DynamoDB Streams. You would need to implement custom application logic or use Database Migration Service (DMS) for CDC, which adds complexity. RDS event notifications are the proper way to react to RDS events.\n\n**Why option 1 is incorrect:**\nThe option that says create an AWS Lambda function triggered when the RDS database is updated to send information to an Amazon SQS FIFO queue for targets to consume is incorrect for the same reason as option 0 - RDS doesn't trigger Lambda functions on data changes. Additionally, using a single FIFO queue for multiple target systems would require all targets to consume from the same queue, which doesn't provide proper fan-out to multiple systems. Each target system should have its own queue.\n\n**Why option 2 is incorrect:**\nThe option that says subscribe to an RDS event notification and send to an SQS queue fanned out to multiple SNS topics, then use Lambda functions to update targets is incorrect because the fan-out pattern is reversed. SNS should be used to fan out to multiple SQS queues (one per target), not the other way around. SQS queues are designed for point-to-point messaging, while SNS topics are designed for pub/sub fan-out to multiple subscribers. The correct pattern is SNS topic  multiple SQS queues  Lambda functions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A company needs to store data in Amazon S3 and must prevent the data from being changed. \nThe company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a \nnonspecific amount of time until the company decides to modify the objects. Only specific users in \nthe company's AWS account can have the ability 10 delete the objects. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Glacier vault.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 bucket with S3 Object Lock enabled.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket with S3 Object Lock enabled.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nCreating an S3 bucket with S3 Object Lock enabled, enabling versioning, adding a legal hold to the objects, and adding the s3:PutObjectLegalHold permission to IAM policies of users who need to delete objects is the correct solution. S3 Object Lock with legal hold prevents objects from being overwritten or deleted, and unlike retention periods, legal holds don't have a specific time duration - they remain in effect until explicitly removed. This meets the requirement for objects to remain unchangeable for a non-specific amount of time. Versioning is required for Object Lock and preserves all object versions. By granting s3:PutObjectLegalHold permission only to specific users, only those authorized users can remove the legal hold and delete objects, meeting the requirement that only specific users can delete objects.\n\n**Why option 0 is incorrect:**\nThe option that says create an S3 Glacier vault and apply a write-once, read-many (WORM) vault lock policy is incorrect because S3 Glacier is an archival storage service, not suitable for active data storage that needs to be accessible through S3 APIs. Glacier vaults are used for long-term archival, and accessing data from Glacier requires retrieval requests that can take minutes to hours. The scenario requires storing data in S3 (not Glacier) with Object Lock capabilities.\n\n**Why option 1 is incorrect:**\nThe option that says create an S3 bucket with Object Lock enabled, enable versioning, set a retention period of 100 years, and use governance mode is incorrect because setting a fixed retention period of 100 years doesn't meet the requirement for a \"non-specific amount of time until the company decides to modify the objects.\" Retention periods have fixed durations, whereas legal holds can be removed at any time by authorized users. Additionally, governance mode allows users with special permissions to bypass retention, but doesn't provide the same level of control as legal holds for non-specific time periods.\n\n**Why option 2 is incorrect:**\nThe option that says create an S3 bucket, use CloudTrail to track S3 API events, and restore modified objects from backups upon notification is incorrect because this is a reactive approach that doesn't prevent modification or deletion - it only detects and attempts to recover after the fact. CloudTrail logs events but doesn't prevent them. This approach doesn't meet the requirement to prevent data from being changed. Object Lock provides proactive protection that prevents changes, rather than reactive recovery.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A social media company allows users to upload images to its website. The website runs on \nAmazon EC2 instances.  \nDuring upload requests, the website resizes the images to a standard size and stores the resized \nimages in Amazon S3.  \nUsers are experiencing slow upload requests to the website. \n \nThe company needs to reduce coupling within the application and improve website performance.  \nA solutions architect must design the most operationally efficient process for image uploads. \n \nWhich combination of actions should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to upload images to S3 Glacier.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the web server to upload the original images to Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the application to upload images directly from each user's browser to Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nConfiguring the web server to upload the original images to Amazon S3 decouples the image upload from the image processing. Instead of the web server handling both upload and resizing (which causes slow upload requests), the web server can quickly upload the original image to S3 and return a response to the user immediately. This improves website performance by reducing the time users wait for uploads. The web server no longer needs to process/resize images, reducing coupling between upload and processing functions.\n\n**Why option 3 is correct:**\nConfiguring S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded, and using the function to resize the image, provides an operationally efficient, serverless solution for image processing. When an image is uploaded to S3, S3 automatically triggers a Lambda function that resizes the image and stores it back in S3. This decouples image resizing from the upload process, allowing uploads to complete quickly while resizing happens asynchronously. Lambda automatically scales to handle varying workloads and requires no server management, providing the most operationally efficient solution.\n\n**Why option 0 is incorrect:**\nThe option that says configure the application to upload images to S3 Glacier is incorrect because Glacier is an archival storage service with retrieval delays (minutes to hours), making it completely unsuitable for web application image uploads. Users would experience extremely slow uploads and wouldn't be able to access images immediately. Glacier is designed for long-term archival, not active web application storage.\n\n**Why option 2 is incorrect:**\nThe option that says configure the application to upload images directly from each user's browser to Amazon S3 through presigned URLs is incorrect because while this can reduce server load, it doesn't address the image resizing requirement. Users would upload original images directly to S3, but there would be no mechanism to resize them. Additionally, managing presigned URLs for each user adds complexity. The web server still needs to generate presigned URLs, and image resizing would still need to be handled separately.\n\n**Why option 4 is incorrect:**\nThe option that says create an Amazon EventBridge rule that invokes a Lambda function on a schedule to resize uploaded images is incorrect because scheduled processing (polling) is less efficient than event-driven processing. S3 Event Notifications provide immediate, event-driven triggers when images are uploaded, whereas scheduled rules would require periodic polling, causing delays in image processing. Event-driven architecture is more operationally efficient and provides better user experience.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A company recently migrated a message processing system to AWS. The system receives \nmessages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are \nprocessed by a consumer application running on Amazon EC2. The consumer application \nprocesses the messages and writes results to a MySQL database running on Amazon EC2. The \ncompany wants this application to be highly available with low operational complexity. \nWhich architecture offers the HIGHEST availability?",
    "options": [
      {
        "id": 0,
        "text": "Add a second ActiveMQ server to another Availably Zone.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon MO with active/standby brokers configured across two Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon MO with active/standby blotters configured across two Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon MQ with active/standby brokers configured across two Availability Zones.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUsing Amazon MQ with active/standby brokers configured across two Availability Zones provides the highest availability with low operational complexity. Amazon MQ is a managed message broker service that supports ActiveMQ and RabbitMQ. The active/standby broker configuration provides automatic failover - if the active broker fails, the standby broker automatically takes over, ensuring message queue availability. Deploying brokers across two Availability Zones protects against AZ-level failures. Amazon MQ is fully managed, eliminating the operational complexity of managing ActiveMQ on EC2 instances. Combined with Auto Scaling groups for consumer EC2 instances across multiple AZs and RDS Multi-AZ for the database, this provides comprehensive high availability with minimal operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says add a second ActiveMQ server to another Availability Zone is incorrect because simply adding a second ActiveMQ server doesn't provide automatic failover or high availability. You would need to configure ActiveMQ for high availability (like network of brokers or master/slave configuration), which requires significant operational complexity and manual configuration. Managing ActiveMQ on EC2 instances requires patching, monitoring, and maintenance, which increases operational overhead compared to the managed Amazon MQ service.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon MQ with active/standby brokers configured across two Availability Zones appears to be a duplicate or typo of the correct answer. However, if this refers to a different configuration, it may not include all the necessary components (consumer Auto Scaling, RDS Multi-AZ) for complete high availability.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon MQ with active/standby brokers configured across two Availability Zones appears to be another duplicate or typo (\"blotters\" instead of \"brokers\"). The correct answer is option 3, which specifies the proper Amazon MQ configuration.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company hosts a containerized web application on a fleet of on-premises servers that process \nincoming requests. The number of requests is growing quickly. The on-premises servers cannot \nhandle the increased number of requests. The company wants to move the application to AWS \nwith minimum code changes and minimum development effort. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use two Amazon EC2 instances to host the containerized web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda with a new code that uses one of the supported languages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a high performance computing (HPC) solution such as AWS ParallelClusterto establish an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application provides the least operational overhead while meeting all requirements. Fargate is a serverless compute engine for containers that eliminates the need to manage EC2 instances, servers, or clusters. You simply define your container image and resource requirements, and Fargate handles the infrastructure. ECS can automatically scale the number of Fargate tasks based on demand, handling the growing number of requests. Since the application is already containerized, it can be moved to ECS/Fargate with minimal code changes - you just need to adapt the container configuration. An Application Load Balancer (ALB) can be added to distribute traffic across multiple Fargate tasks. This solution requires no server management, automatic scaling, and minimal code changes.\n\n**Why option 1 is incorrect:**\nThe option that says use two Amazon EC2 instances to host the containerized web application is incorrect because using a fixed number of EC2 instances doesn't scale to handle growing requests. You would need to manually add more instances as traffic grows, which requires operational overhead. Additionally, managing EC2 instances (patching, monitoring, scaling) adds significant operational complexity compared to Fargate's serverless approach. Two instances also don't provide the scalability needed for quickly growing request volumes.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Lambda with new code that uses one of the supported languages is incorrect because this would require rewriting the application code to fit Lambda's programming model, which violates the \"minimum code changes and minimum development effort\" requirement. Containerized applications are designed to run as long-running processes, while Lambda functions are event-driven and stateless with execution time limits. Converting a containerized web application to Lambda would require significant code refactoring.\n\n**Why option 3 is incorrect:**\nThe option that says use a high performance computing (HPC) solution such as AWS ParallelCluster is incorrect because ParallelCluster is designed for HPC workloads like scientific computing, simulations, and batch processing, not for web applications. HPC solutions are overkill for a web application and would require significant configuration and operational overhead. The containerized web application should run on container orchestration services like ECS, not HPC clusters.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company uses 50 TB of data for reporting. The company wants to move this data from on \npremises to AWS A custom application in the company's data center runs a weekly data \ntransformation job. The company plans to pause the application until the data transfer is complete \nand needs to begin the transfer process as soon as possible. \nThe data center does not have any available network bandwidth for additional workloads.  \nA solutions architect must transfer the data and must configure the transformation job to continue \nto run in the AWS Cloud. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to move the data.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order an AWS Snowcone device to move the data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Order an AWS Snowball Edge Storage Optimized device.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Order an AWS D. Snowball Edge Storage Optimized device that includes Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOrdering an AWS Snowball Edge Storage Optimized device to move the 50 TB of data is the correct solution when network bandwidth is unavailable. Snowball Edge Storage Optimized devices can store up to 80 TB of data and are designed for large-scale data transfers when network transfer is impractical or too slow. The device is shipped to the data center, data is copied to it locally (which doesn't require network bandwidth), and then the device is shipped back to AWS where the data is imported into S3. After the data is in AWS, AWS Glue can be used to create and run the transformation job, replacing the on-premises custom application. This solution provides the fastest transfer method (physical shipment) when network bandwidth is unavailable and allows the transformation job to run in AWS with minimal operational overhead using the managed Glue service.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DataSync to move the data and create a custom transformation job using AWS Glue is incorrect because DataSync requires network connectivity to transfer data. The scenario explicitly states that the data center has no available network bandwidth for additional workloads. DataSync transfers data over the network, which would be slow or impossible given the bandwidth constraints. Additionally, transferring 50 TB over the network would take significantly longer than using a Snowball device.\n\n**Why option 1 is incorrect:**\nThe option that says order an AWS Snowcone device to move the data and deploy the transformation application to the device is incorrect because Snowcone devices have a storage capacity of only 8 TB, which is insufficient for the 50 TB dataset. You would need multiple Snowcone devices, which adds complexity and cost. Additionally, while Snowcone can run EC2 instances, it's not designed for running complex transformation applications - AWS Glue is the proper managed service for ETL jobs.\n\n**Why option 3 is incorrect:**\nThe option that says order an AWS Snowball Edge Storage Optimized device with EC2 compute, copy data to the device, and create a new EC2 instance on AWS to run the transformation application is incorrect because while the Snowball device can handle the data transfer, running the transformation application on a separate EC2 instance adds operational overhead. You would need to manage the EC2 instance, install dependencies, and maintain the application. Using AWS Glue (a managed ETL service) is more operationally efficient than managing EC2 instances for transformation jobs.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company has created an image analysis application in which users can upload photos and add \nphoto frames to their images. The users upload images and metadata to indicate which photo \nframes they want to add to their images. The application uses a single Amazon EC2 instance and \nAmazon DynamoDB to store the metadata. \n \nThe application is becoming more popular, and the number of users is increasing. The company \nexpects the number of concurrent users to vary significantly depending on the time of day and \nday of week. The company must ensure that the application can scale to meet the needs of the \ngrowing user base. \n \nWhich solution meats these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda to process the photos.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda to process the photos.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Increase the number of EC2 instances to three.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing AWS Lambda to process the photos, storing the photos in Amazon S3, and retaining DynamoDB to store the metadata is the best solution for scaling. Lambda functions automatically scale to handle varying concurrent user loads without any configuration, meeting the requirement for significant variation in concurrent users. S3 provides virtually unlimited storage for photos and can handle any number of uploads concurrently. DynamoDB is ideal for storing metadata (which is small and structured) and can scale automatically to handle varying read/write loads. This serverless architecture eliminates the need to manage EC2 instances and provides automatic scaling, making it operationally efficient and cost-effective for variable workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Lambda to process photos and store both photos and metadata in DynamoDB is incorrect because storing photos (large binary files) in DynamoDB is not scalable or cost-effective. DynamoDB has a 400 KB item size limit, which would prevent storing full-size photos. Even if photos were small enough, DynamoDB charges for storage and read/write capacity, making it expensive for large binary objects. S3 is designed for object storage and is much more cost-effective for photos.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Kinesis Data Firehose to process photos and store photos and metadata is incorrect because Kinesis Data Firehose is designed for streaming data ingestion and delivery, not for processing images or handling user uploads. Firehose doesn't have image processing capabilities and is not suitable for an interactive application where users upload photos and expect processed results. Additionally, Firehose would require a separate system to handle the uploads and trigger the processing.\n\n**Why option 3 is incorrect:**\nThe option that says increase the number of EC2 instances to three and use Provisioned IOPS SSD (io2) EBS volumes to store photos and metadata is incorrect because a fixed number of instances (three) doesn't provide the automatic scaling needed for significantly varying concurrent user loads. You would need to manually scale instances up and down, which requires operational overhead. Additionally, storing photos on EBS volumes attached to EC2 instances doesn't scale well and creates a single point of failure. EBS volumes are also more expensive than S3 for large-scale object storage.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A medical records company is hosting an application on Amazon EC2 instances. The application \nprocesses customer data files that are stored on Amazon S3. The EC2 instances are hosted in \npublic subnets. The EC2 instances access Amazon S3 over the internet, but they do not require \nany other network access. \nA new requirement mandates that the network traffic for file transfers take a private route and not \nbe sent over the internet. \nWhich change to the network architecture should a solutions architect recommend to meet this \nrequirement?",
    "options": [
      {
        "id": 0,
        "text": "Create a NAT gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the EC2 instances to private subnets.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Remove the internet gateway from the VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nMoving the EC2 instances to private subnets and creating a VPC gateway endpoint for Amazon S3, linking the endpoint to the route table for the private subnets, is the correct solution. VPC gateway endpoints provide private connectivity to S3 without requiring internet gateway, NAT device, or VPN connections. Traffic between the VPC and S3 stays within the AWS network and never traverses the public internet. By moving instances to private subnets and configuring the VPC endpoint in the route table, all S3 traffic will be routed through the private AWS network. This meets the requirement for private routing while maintaining the ability to access S3.\n\n**Why option 0 is incorrect:**\nThe option that says create a NAT gateway and configure the route table for public subnets to send traffic to S3 through the NAT gateway is incorrect because NAT gateways route traffic through the internet gateway, which means traffic still traverses the public internet. NAT gateways are used to allow resources in private subnets to access the internet, but they don't provide private connectivity to AWS services. Additionally, NAT gateways incur hourly charges and data processing fees, whereas VPC endpoints are free (you only pay for data transfer).\n\n**Why option 1 is incorrect:**\nThe option that says configure the security group for EC2 instances to restrict outbound traffic so that only traffic to the S3 prefix list is permitted is incorrect because security groups control access but don't change the routing path. Even with security group restrictions, if instances are in public subnets without a VPC endpoint, traffic to S3 would still go through the internet gateway and traverse the public internet. Security groups don't create private routes - they only filter traffic.\n\n**Why option 3 is incorrect:**\nThe option that says remove the internet gateway from the VPC and set up AWS Direct Connect to route traffic to S3 is incorrect because Direct Connect is designed for hybrid connectivity between on-premises and AWS, not for VPC-to-AWS-service connectivity. Additionally, removing the internet gateway would prevent any internet access if needed in the future, and Direct Connect requires physical connections and significant setup time and cost. VPC endpoints provide immediate, free private connectivity without requiring Direct Connect.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company uses a popular content management system (CMS) for its corporate website. \nHowever, the required patching and maintenance are burdensome. The company is redesigning \nits website and wants anew solution. The website will be updated four times a year and does not \nneed to have any dynamic content available. The solution must provide high scalability and \nenhanced security. \n \nWhich combination of changes will meet these requirements with the LEAST operational \noverhead? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create and deploy an AWS Lambda function to manage and serve the website content",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create the new website and an Amazon S3 bucket Deploy the website on the S3 bucket with",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create the new website. Deploy the website on an Amazon S3 bucket with static website hosting enabled. Use Amazon CloudFront to distribute the website content and require HTTPS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nDeploying an AWS WAF web ACL in front of the website provides enhanced security by protecting against common web exploits like SQL injection and cross-site scripting (XSS) attacks. While WAF doesn't directly provide HTTPS functionality (that's handled by CloudFront with ACM certificates), WAF can be integrated with CloudFront to provide security at the edge. WAF rules can be configured to block malicious requests before they reach the origin, providing an additional security layer. This meets the enhanced security requirement with minimal operational overhead since WAF is a managed service.\n\n**Why option 3 is correct:**\nCreating the new website, deploying it on an Amazon S3 bucket with static website hosting enabled, and using Amazon CloudFront to distribute the website content provides high scalability and minimal operational overhead. Since the website has no dynamic content and is only updated four times a year, static website hosting on S3 is ideal. S3 provides virtually unlimited scalability and requires no server management, patching, or maintenance. CloudFront CDN distributes content globally, improving performance and scalability by caching content at edge locations. This solution eliminates the operational burden of managing CMS servers, patching, and maintenance.\n\n**Why option 1 is incorrect:**\nThe option that says create and deploy an AWS Lambda function to manage and serve the website content is incorrect because Lambda functions are designed for event-driven processing, not for serving static website content. While Lambda@Edge can be used with CloudFront for dynamic content, for a static website that doesn't need dynamic content, S3 static website hosting is simpler and more cost-effective. Lambda functions have execution time limits and are charged per invocation, making them less suitable for serving static content compared to S3.\n\n**Why option 2 is incorrect:**\nThe option that says create the new website and an Amazon S3 bucket and deploy the website on the S3 bucket with static website hosting enabled is partially correct but incomplete. While S3 static website hosting provides scalability, it doesn't include CloudFront for global distribution and enhanced security features. The complete solution should include CloudFront for CDN capabilities, HTTPS enforcement, and integration with WAF for enhanced security. Additionally, S3 static website hosting alone doesn't provide the same level of security features as CloudFront + WAF.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company stores its application logs in an Amazon CloudWatch Logs log group.  \nA new policy requires the company to store all application logs in Amazon OpenSearch Service \n(Amazon Elasticsearch Service) in near-real time. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Install and configure Amazon Kinesis Agent on each application server to deliver the logs to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nConfiguring a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service is the solution with the least operational overhead. CloudWatch Logs has a native feature that automatically creates and manages a Lambda function with pre-populated code to stream logs to OpenSearch. When you enable this subscription, AWS handles all the infrastructure, code, and configuration automatically. The Lambda function is created, configured, and managed by AWS, requiring no manual coding or infrastructure management. This provides near-real-time log streaming with minimal operational effort, as you only need to configure the subscription in the CloudWatch Logs console.\n\n**Why option 1 is incorrect:**\nThe option that says create an AWS Lambda function and use the log group to invoke the function to write logs to OpenSearch is incorrect because while this approach can work, it requires you to write, deploy, and maintain the Lambda function code yourself. You would need to handle error handling, retries, batching, and OpenSearch API integration, which adds significant operational overhead compared to the managed CloudWatch Logs subscription feature. The native subscription feature eliminates all this manual work.\n\n**Why option 2 is incorrect:**\nThe option that says create an Amazon Kinesis Data Firehose delivery stream, configure the log group as the source, and configure OpenSearch as the destination is incorrect because CloudWatch Logs cannot be directly configured as a Firehose source. Firehose can receive data from Kinesis Data Streams, but CloudWatch Logs would need to send data to Kinesis Data Streams first, or you would need a Lambda function to read from CloudWatch Logs and write to Firehose. This adds complexity and operational overhead compared to the native CloudWatch Logs subscription.\n\n**Why option 3 is incorrect:**\nThe option that says install and configure Amazon Kinesis Agent on each application server to deliver logs to Kinesis Data Streams, then configure Kinesis Data Streams to deliver to OpenSearch is incorrect because this requires installing and managing agents on every application server, which adds significant operational overhead. You would need to install, configure, and maintain agents across all servers, handle agent failures, and manage the Kinesis Data Streams infrastructure. Additionally, Kinesis Data Streams doesn't directly deliver to OpenSearch - you would need additional components like Lambda or Kinesis Data Firehose, adding more complexity.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "A company is building a web-based application running on Amazon EC2 instances in multiple \nAvailability Zones. The web application will provide access to a repository of text documents \ntotaling about 900 TB in size. The company anticipates that the web application will experience \nperiods of high demand. A solutions architect must ensure that the storage component for the text \ndocuments can scale to meet the demand of the application at all times. The company is \nconcerned about the overall cost of the solution. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elasticsearch Service (Amazon ES)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon S3 is the most cost-effective storage solution for storing 900 TB of text documents that need to scale to meet high demand. S3 provides virtually unlimited scalability, automatically handling any amount of storage and concurrent access requests without requiring capacity planning or manual scaling. S3 offers multiple storage classes (Standard, Standard-IA, One Zone-IA, Intelligent-Tiering, Glacier) that allow cost optimization based on access patterns. For a web application with periods of high demand, S3 Standard provides low-latency access, and you can use lifecycle policies to transition less frequently accessed documents to cheaper storage classes. S3's pay-as-you-go pricing model means you only pay for what you store and access, making it highly cost-effective for large-scale document storage compared to block or file storage solutions.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Elastic Block Store (Amazon EBS) is incorrect because EBS volumes have size limits (up to 64 TiB per volume) and are attached to EC2 instances, making them unsuitable for 900 TB of storage. You would need multiple EBS volumes and instances, which adds complexity and cost. EBS is designed for block storage attached to EC2 instances, not for large-scale object storage accessible by web applications. EBS also doesn't scale automatically and requires manual provisioning.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Elastic File System (Amazon EFS) is incorrect because while EFS can scale to petabyte scale, it is significantly more expensive than S3 for large-scale storage. EFS charges for storage and data transfer, and its pricing is higher than S3 for most use cases. EFS is designed for shared file storage with low-latency access, but for a repository of text documents accessed via a web application, S3 provides better cost-effectiveness and scalability. EFS also requires EC2 instances to mount the file system, adding infrastructure complexity.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon Elasticsearch Service (Amazon ES) is incorrect because Elasticsearch is a search and analytics engine, not a general-purpose storage solution. While Elasticsearch can store documents, it's designed for full-text search and analytics workloads, not for simple document storage and retrieval. Elasticsearch is significantly more expensive than S3 and requires cluster management, which adds operational overhead. For storing 900 TB of text documents that need to scale cost-effectively, S3 is the appropriate choice.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in \nthe us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution \nto protect these API Gateway managed REST APIs across multiple accounts from SQL injection \nand cross-site scripting attacks. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS WAF in both Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Firewall Manager in both Regions.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Shield in bath Regions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up AWS Shield in one of the Regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nSetting up AWS Firewall Manager in both Regions and centrally configuring AWS WAF rules provides the least administrative effort for protecting API Gateway REST APIs across multiple accounts and regions. Firewall Manager is a security management service that allows you to centrally configure and manage WAF rules across multiple AWS accounts and resources. Instead of manually setting up WAF in each region and associating web ACLs with each API stage, Firewall Manager allows you to define security policies once and automatically apply them to API Gateway APIs across all accounts and regions. Firewall Manager integrates with AWS Organizations to manage WAF rules centrally, significantly reducing administrative overhead. WAF rules can protect against SQL injection and cross-site scripting (XSS) attacks by inspecting web requests and blocking malicious patterns.\n\n**Why option 0 is incorrect:**\nThe option that says set up AWS WAF in both Regions and associate Regional web ACLs with an API stage is incorrect because this approach requires manual setup and configuration in each region for each API Gateway stage. With APIs in two regions and potentially multiple accounts, you would need to configure WAF separately in each region and associate web ACLs with each API stage manually. This requires significant administrative effort compared to Firewall Manager's centralized approach.\n\n**Why option 2 is incorrect:**\nThe option that says set up AWS Shield in both Regions and associate Regional web ACLs with an API stage is incorrect because AWS Shield is a DDoS protection service, not a web application firewall. Shield protects against distributed denial-of-service attacks but doesn't provide protection against SQL injection or XSS attacks. While Shield can work alongside WAF, Shield alone doesn't meet the requirement for protecting against SQL injection and XSS. Additionally, this approach still requires manual configuration in each region.\n\n**Why option 3 is incorrect:**\nThe option that says set up AWS Shield in one of the Regions and associate Regional web ACLs with an API stage is incorrect for the same reasons as option 2 - Shield doesn't protect against SQL injection or XSS attacks. Additionally, setting up protection in only one region leaves the APIs in the other region unprotected, which doesn't meet the requirement to protect APIs across both regions.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company has implemented a self-managed DNS solution on three Amazon EC2 instances \nbehind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are \nlocated in the United States and Europe. The company wants to improve the performance and \navailability of the solution. The company launches and configures three EC2 instances in the eu-\nwest-1 Region and adds the EC2 instances as targets for a new NLB. \n \nWhich solution can the company use to route traffic to all the EC2 instances?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Route 53 geolocation routing policy to route requests to one of the two",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a standard accelerator in AWS Global Accelerator.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Attach Elastic IP addresses to the six EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replace the two NLBs with two Application Load Balancers (ALBs).",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreating a standard accelerator in AWS Global Accelerator, creating endpoint groups in us-west-2 and eu-west-1, and adding the two NLBs as endpoints for the endpoint groups is the best solution for routing traffic to all EC2 instances across regions. Global Accelerator provides static IP addresses that serve as fixed entry points to your applications, eliminating the need to manage region-specific IPs. It automatically routes user traffic to the optimal endpoint based on performance metrics, user location, and application health. Global Accelerator uses AWS's global network infrastructure to route traffic efficiently, improving performance for users in both the United States and Europe. It provides automatic failover and health checks, ensuring high availability. This solution works seamlessly with NLBs and doesn't require replacing the existing load balancers.\n\n**Why option 0 is incorrect:**\nThe option that says create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs and create a CloudFront distribution using the Route 53 record as the origin is incorrect because CloudFront is designed for caching and distributing static content, not for routing to backend services like DNS servers. Additionally, using Route 53 geolocation routing with CloudFront adds unnecessary complexity. Global Accelerator is specifically designed for this use case of routing traffic to application endpoints across multiple regions.\n\n**Why option 2 is incorrect:**\nThe option that says attach Elastic IP addresses to the six EC2 instances, create a Route 53 geolocation routing policy to route to one of the six EC2 instances, and create a CloudFront distribution is incorrect because this approach bypasses the NLBs, eliminating their load balancing and health checking benefits. Directly routing to individual EC2 instances doesn't provide the same level of availability and performance as using load balancers. Additionally, CloudFront is not suitable for routing to backend application servers - it's designed for content delivery.\n\n**Why option 3 is incorrect:**\nThe option that says replace the two NLBs with two Application Load Balancers (ALBs), create a Route 53 latency routing policy, and create a CloudFront distribution is incorrect because replacing NLBs with ALBs may not be necessary and adds operational overhead. NLBs are appropriate for DNS workloads. Additionally, using CloudFront with Route 53 for routing to backend services is not the optimal architecture. Global Accelerator is specifically designed for this multi-region routing scenario and works with NLBs without requiring replacements.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company is running an online transaction processing (OLTP) workload on AWS. This workload \nuses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database \nsnapshots are taken from this instance. \n \nWhat should a solutions architect do to ensure the database and snapshots are always encrypted \nmoving forward?",
    "options": [
      {
        "id": 0,
        "text": "Encrypt a copy of the latest DB snapshot.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nEncrypting a copy of the latest DB snapshot and replacing the existing DB instance by restoring the encrypted snapshot is the correct approach to enable encryption on an existing RDS database. RDS encryption cannot be enabled on an existing database instance - it can only be enabled when creating a new instance. To encrypt an existing database, you must: 1) Create a snapshot of the unencrypted database, 2) Copy the snapshot and enable encryption during the copy process (specifying a KMS key), 3) Restore a new DB instance from the encrypted snapshot, and 4) Replace the old instance with the new encrypted instance. Once the new encrypted instance is running, all future snapshots will be encrypted automatically. This ensures both the database and all future snapshots are encrypted.\n\n**Why option 1 is incorrect:**\nThe option that says create a new encrypted EBS volume and copy the snapshots to it, then enable encryption on the DB instance is incorrect because RDS manages its own storage and you cannot directly attach EBS volumes to RDS instances or enable encryption on an existing RDS instance. RDS encryption must be enabled at instance creation time, not after. You cannot \"enable encryption\" on an existing RDS instance - you must create a new encrypted instance from an encrypted snapshot.\n\n**Why option 2 is incorrect:**\nThe option that says copy the snapshots and enable encryption using AWS KMS, then restore the encrypted snapshot to an existing DB instance is incorrect because you cannot restore a snapshot to an existing DB instance - you must create a new DB instance from the snapshot. The process requires creating a new encrypted instance from the encrypted snapshot, not restoring to an existing instance. Additionally, the existing unencrypted instance would need to be replaced, not updated.\n\n**Why option 3 is incorrect:**\nThe option that says copy the snapshots to an Amazon S3 bucket encrypted with SSE-KMS is incorrect because RDS snapshots are managed by AWS and stored in S3 automatically, but you cannot manually copy RDS snapshots to S3 buckets. RDS snapshots are stored in AWS-managed S3 buckets, not in customer buckets. To encrypt snapshots, you must copy the snapshot within RDS and enable encryption during the copy, then restore a new encrypted instance from that encrypted snapshot.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company wants to build a scalable key management Infrastructure to support developers who need to encrypt data in their applications. What should a solutions architect do to reduce the operational burden?",
    "options": [
      {
        "id": 0,
        "text": "Use multifactor authentication (MFA) to protect the encryption keys.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Key Management Service (AWS KMS) to protect the encryption keys",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an IAM policy to limit the scope of users who have access permissions to protect the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing AWS Key Management Service (AWS KMS) to protect encryption keys provides a scalable, fully managed key management infrastructure that reduces operational burden. KMS is a managed service that handles key creation, storage, rotation, and access control automatically. Developers can use the AWS Encryption SDK with KMS to easily generate, use, and protect symmetric encryption keys in their applications without managing key infrastructure. KMS integrates seamlessly with AWS services and provides APIs for application-level encryption. KMS automatically handles key rotation, backup, and compliance requirements, eliminating the need for developers to manage these aspects. This centralized approach scales to support any number of developers and applications.\n\n**Why option 0 is incorrect:**\nThe option that says use multifactor authentication (MFA) to protect encryption keys is incorrect because MFA is an authentication mechanism, not a key management service. MFA adds an extra layer of security for accessing systems but doesn't provide key management infrastructure. Developers would still need a system to create, store, rotate, and manage encryption keys, which MFA doesn't provide. MFA protects access to keys but doesn't reduce the operational burden of key management itself.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Certificate Manager (ACM) to create, store, and assign encryption keys is incorrect because ACM is designed for managing SSL/TLS certificates for use with AWS services like CloudFront, Elastic Load Balancing, and API Gateway. ACM doesn't provide general-purpose encryption key management for application data encryption. ACM certificates are used for securing network communications, not for encrypting application data. For application-level encryption, developers need a key management service like KMS.\n\n**Why option 3 is incorrect:**\nThe option that says use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys is incorrect because IAM policies control access to AWS resources and services, but they don't provide key management infrastructure. IAM can control who can access KMS keys, but you still need KMS (or another key management service) to actually create, store, and manage the keys. IAM policies alone don't provide the key management functionality that developers need to encrypt data in their applications.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company has a dynamic web application hosted on two Amazon EC2 instances. The company \nhas its own SSL certificate, which is on each instance to perform SSL termination. \n \nThere has been an increase in traffic recently, and the operations team determined that SSL \nencryption and decryption is causing the compute capacity of the web servers to reach their \nmaximum limit. \n \nWhat should a solutions architect do to increase the application's performance?",
    "options": [
      {
        "id": 0,
        "text": "Create a new SSL certificate using AWS Certificate Manager (ACM) install the ACM certificate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create another EC2 instance as a proxy server Migrate the SSL certificate to the new instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Import the SSL certificate into AWS Certificate Manager (ACM).",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nImporting the SSL certificate into AWS Certificate Manager (ACM) and creating an Application Load Balancer (ALB) with an HTTPS listener that uses the SSL certificate from ACM is the correct solution. By moving SSL termination from the EC2 instances to the ALB, the load balancer handles all SSL encryption and decryption, offloading this CPU-intensive work from the web servers. This allows the EC2 instances to focus on processing application logic instead of SSL/TLS processing. The ALB can handle SSL termination at scale and distribute the load across multiple EC2 instances. This architecture improves performance by removing SSL processing overhead from the application servers and allows the application to scale more effectively.\n\n**Why option 0 is incorrect:**\nThe option that says create a new SSL certificate using ACM and install the ACM certificate on each instance is incorrect because this still requires SSL termination on the EC2 instances, which doesn't solve the performance problem. Installing certificates on instances means the instances still need to perform SSL encryption and decryption, consuming CPU resources. The issue is the location of SSL termination, not the certificate source.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon S3 bucket, migrate the SSL certificate to the S3 bucket, and configure EC2 instances to reference the bucket for SSL termination is incorrect because S3 is object storage and cannot perform SSL termination. SSL termination requires a service that can handle TLS handshakes and encrypt/decrypt traffic, which S3 doesn't provide. Additionally, storing certificates in S3 doesn't change where SSL termination occurs - it would still need to happen on the EC2 instances.\n\n**Why option 2 is incorrect:**\nThe option that says create another EC2 instance as a proxy server, migrate the SSL certificate to the new instance, and configure it to direct connections to existing EC2 instances is incorrect because this adds another EC2 instance that would also need to handle SSL termination, potentially creating a bottleneck. While this could offload SSL from the web servers, it doesn't provide the scalability, high availability, and managed service benefits of using an Application Load Balancer. An ALB is a managed service that automatically scales and provides better performance than a single proxy EC2 instance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job. What should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Implement EC2 Spot Instances",
        "correct": true
      },
      {
        "id": 1,
        "text": "Purchase EC2 Reserved Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement EC2 On-Demand Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement the processing on AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nImplementing EC2 Spot Instances is the most cost-effective solution for this highly dynamic, stateless batch processing job. Spot Instances provide up to 90% savings compared to On-Demand Instances, making them ideal for cost optimization. Since the job is stateless and can be started and stopped at any time without negative impact, it can tolerate Spot Instance interruptions. The job takes 60 minutes to complete, which is well-suited for Spot Instances. Spot Instances automatically scale based on availability and can handle the highly dynamic nature of the workload. When Spot capacity is available, the job runs at a fraction of the cost of On-Demand Instances. If interrupted, the job can be restarted on other Spot Instances or On-Demand Instances as a fallback.\n\n**Why option 1 is incorrect:**\nThe option that says purchase EC2 Reserved Instances is incorrect because Reserved Instances require a 1-3 year commitment and are best for steady-state, predictable workloads. The scenario describes a \"highly dynamic\" batch processing job, which means the number of instances needed varies significantly. Reserved Instances don't provide the flexibility needed for dynamic workloads and may result in paying for capacity that isn't always used. Additionally, Reserved Instances don't automatically scale - you still need to manage instance provisioning.\n\n**Why option 2 is incorrect:**\nThe option that says implement EC2 On-Demand Instances is incorrect because while On-Demand Instances provide flexibility and scalability, they are significantly more expensive than Spot Instances (up to 90% more). For a stateless, interruptible batch job that can tolerate interruptions, Spot Instances provide the same functionality at a much lower cost. On-Demand Instances should be reserved for workloads that cannot tolerate interruptions.\n\n**Why option 3 is incorrect:**\nThe option that says implement the processing on AWS Lambda is incorrect because Lambda has a 15-minute execution time limit per invocation, and the batch job takes upwards of 60 minutes to complete. Lambda cannot handle long-running batch jobs that exceed 15 minutes. While you could potentially chain Lambda functions, this would add significant complexity and may not be cost-effective for a 60-minute batch processing job that requires many instances.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company runs its two-tier ecommerce website on AWS. The web tier consists of a load \nbalancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS \nDB instance. The EC2 instances and the RDS DB instance should not be exposed to the public \ninternet. The EC2 instances require internet access to complete payment processing of orders \nthrough a third-party web service. The application must be highly available. \n \nWhich combination of configuration options will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use an Auto Scaling group to launch the EC2 instances in private subnets.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a VPC with two private subnets and two NAT gateways across two Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure a VPC with two public subnets, two private subnets, and two NAT gateways across",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nUsing an Auto Scaling group to launch the EC2 instances in private subnets and deploying an RDS Multi-AZ DB instance in private subnets ensures that neither the EC2 instances nor the RDS database are exposed to the public internet. Private subnets don't have direct internet gateway access, providing security isolation. The Auto Scaling group ensures high availability by automatically replacing failed instances and distributing them across Availability Zones. RDS Multi-AZ provides automatic failover and high availability for the database.\n\n**Why option 4 is correct:**\nConfiguring a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones, and deploying an Application Load Balancer in the public subnets provides the complete architecture. The public subnets host the ALB (which needs internet connectivity to receive traffic) and NAT gateways (for outbound internet access from private subnets). The private subnets host the EC2 instances and RDS database. Two NAT gateways (one per AZ) provide high availability for outbound internet access, allowing EC2 instances in private subnets to access the third-party payment processing service. The ALB in public subnets can route traffic to EC2 instances in private subnets. This architecture provides high availability across multiple AZs.\n\n**Why option 1 is incorrect:**\nThe option that says configure a VPC with two private subnets and two NAT gateways across two Availability Zones, and deploy an ALB in private subnets is incorrect because Application Load Balancers must be deployed in public subnets to receive traffic from the internet. ALBs need internet gateway access to accept incoming connections from users. You cannot deploy an ALB in private subnets and expect it to receive public internet traffic.\n\n**Why option 2 is incorrect:**\nThe option that says use an Auto Scaling group to launch EC2 instances in public subnets across two Availability Zones and deploy RDS Multi-AZ in private subnets is incorrect because placing EC2 instances in public subnets exposes them directly to the public internet, which violates the requirement that EC2 instances should not be exposed to the public internet. EC2 instances should be in private subnets for security.\n\n**Why option 3 is incorrect:**\nThe option that says configure a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones, and deploy an ALB in the public subnet is incorrect because you need at least one public subnet and one private subnet in each Availability Zone for high availability. Having only one public subnet and one private subnet total doesn't provide redundancy across AZs. If one AZ fails, the entire application would be unavailable.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nSetting up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years is the most cost-effective solution that meets all requirements. Data from the most recent 2 years remains in S3 Standard, providing high availability and immediate retrievability. After 2 years, objects automatically transition to Glacier Deep Archive, which is the lowest-cost storage class in S3 (designed for long-term archival with retrieval times of 12 hours). This approach optimizes costs by keeping frequently accessed recent data in Standard storage while archiving older data to the cheapest storage class. The data remains accessible (with retrieval time) and is stored for the required 25+ years at minimal cost.\n\n**Why option 0 is incorrect:**\nThe option that says set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately is incorrect because this would move all data (including recent data) to Glacier Deep Archive immediately, which doesn't meet the requirement that data from the most recent 2 years must be highly available and immediately retrievable. Glacier Deep Archive has a 12-hour retrieval time, which is not \"immediately retrievable.\" Recent data should stay in S3 Standard for immediate access.\n\n**Why option 2 is incorrect:**\nThe option that says use S3 Intelligent-Tiering and activate the archiving option to ensure data is archived in S3 Glacier Deep Archive is incorrect because while Intelligent-Tiering can automatically move objects between access tiers, it doesn't provide the same level of control as lifecycle policies for ensuring data stays in Standard for exactly 2 years before archiving. Additionally, Intelligent-Tiering monitors access patterns and may archive data sooner if it's not accessed, which could violate the requirement for 2 years of immediate availability. Lifecycle policies provide more predictable, rule-based transitions.\n\n**Why option 3 is incorrect:**\nThe option that says set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access immediately and to S3 Glacier Deep Archive after 2 years is incorrect because transitioning to One Zone-IA immediately would move all data (including recent data) to Infrequent Access storage, which may not provide the same level of availability as Standard. Additionally, One Zone-IA stores data in a single Availability Zone, providing less durability than Standard (which stores across multiple AZs). The requirement specifies that recent data must be \"highly available,\" which Standard provides better than One Zone-IA.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company runs its ecommerce application on AWS. Every new order is published as a message \nin a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These \nmessages are processed by a different application that runs on a separate EC2 instance. This \napplication stores the details in a PostgreSQL database on another EC2 instance. All the EC2 \ninstances are in the same Availability Zone. \nThe company needs to redesign its architecture to provide the highest availability with the least \noperational overhead. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nMigrating the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ, creating a Multi-AZ Auto Scaling group for EC2 instances that host the application, and migrating the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL provides the highest availability with the least operational overhead. Amazon MQ is a managed message broker service that provides active/standby broker deployment across multiple Availability Zones, eliminating the need to manage RabbitMQ on EC2 instances. RDS Multi-AZ provides automatic failover, backups, and patching for PostgreSQL, significantly reducing operational overhead compared to managing PostgreSQL on EC2. The Multi-AZ Auto Scaling group ensures the application layer is highly available. This solution leverages managed services to minimize operational complexity while maximizing availability.\n\n**Why option 0 is incorrect:**\nThe option that says migrate the queue to Amazon MQ, create Multi-AZ Auto Scaling groups for the application, and create another Multi-AZ Auto Scaling group for EC2 instances hosting PostgreSQL is incorrect because managing PostgreSQL on EC2 instances requires significant operational overhead including database installation, configuration, backups, patching, replication setup, and monitoring. RDS provides all of this as a managed service, reducing operational burden. Creating an Auto Scaling group for database EC2 instances doesn't provide the same level of database-specific high availability features (like automatic failover, synchronous replication) that RDS Multi-AZ provides.\n\n**Why option 2 is incorrect:**\nThe option that says create a Multi-AZ Auto Scaling group for EC2 instances hosting RabbitMQ queue, create another for the application, and migrate the database to RDS Multi-AZ is incorrect because managing RabbitMQ on EC2 instances in an Auto Scaling group requires significant operational overhead including broker installation, configuration, clustering setup, monitoring, and failover management. Amazon MQ provides all of this as a managed service with active/standby deployment, eliminating the need to manage RabbitMQ infrastructure. Auto Scaling groups don't provide message broker-specific high availability features.\n\n**Why option 3 is incorrect:**\nThe option that says create Multi-AZ Auto Scaling groups for RabbitMQ queue EC2 instances, application EC2 instances, and PostgreSQL database EC2 instances is incorrect because this approach requires managing all three components (RabbitMQ, application, and PostgreSQL) on EC2 instances, which maximizes operational overhead. None of the components benefit from managed services. This solution requires the most operational effort for database management, message broker management, and application infrastructure management compared to using managed services like Amazon MQ and RDS.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically to the analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines. What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nConfiguring S3 replication between the S3 buckets, configuring the analysis S3 bucket to send event notifications to Amazon EventBridge (CloudWatch Events), configuring an ObjectCreated rule in EventBridge, and configuring Lambda and SageMaker Pipelines as targets for the rule provides the least operational overhead. S3 replication automatically copies files from the source bucket to the destination bucket without requiring Lambda functions to manage the copying process, which is more efficient for large files. EventBridge is more advanced than S3 event notifications and supports multiple targets including Lambda and SageMaker Pipelines. EventBridge also provides filtering and pattern matching capabilities, allowing you to route events based on file patterns or metadata. This solution eliminates the need to write and maintain Lambda code for file copying and provides a more scalable, event-driven architecture.\n\n**Why option 0 is incorrect:**\nThe option that says create a Lambda function to copy files to the analysis bucket, create an S3 event notification for the analysis bucket, and configure Lambda and SageMaker as destinations is incorrect because this approach requires a Lambda function to handle file copying, which adds operational overhead for managing Lambda code, error handling, and retries. Additionally, S3 event notifications have limitations - they may not directly support SageMaker Pipelines as a destination, and they don't provide the same filtering capabilities as EventBridge. For large files, Lambda functions have timeout and memory limits that may not be suitable.\n\n**Why option 1 is incorrect:**\nThe option that says create a Lambda function to copy files to the analysis bucket and configure EventBridge with Lambda and SageMaker as targets is incorrect because it still requires a Lambda function to handle file copying, which adds operational overhead. While EventBridge is better than S3 event notifications, using Lambda for file copying is less efficient than S3 replication, especially for large files. S3 replication is a native feature that handles copying automatically without requiring code.\n\n**Why option 2 is incorrect:**\nThe option that says configure S3 replication and create an S3 event notification for the analysis bucket with Lambda and SageMaker as destinations is incorrect because S3 event notifications have limitations - they may not directly support SageMaker Pipelines as a destination, and they don't provide the advanced filtering and routing capabilities that EventBridge offers. EventBridge provides better integration with multiple AWS services and more flexible event routing.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A solutions architect needs to help a company optimize the cost of running an application on \nAWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for \ncompute within the architecture. \n \nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic \nand unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The \napplication front end will run on Fargate, and Lambda will serve the API layer. The front-end \nutilization and API layer utilization will be predictable over the course of the next year. \n \nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting \nthis application? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the data ingestion layer",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use On-Demand Instances for the data ingestion layer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a 1-year Compute Savings Plan for the front end and API layer.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase 1-year All Upfront Reserved instances for the data ingestion layer.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nUsing Spot Instances for the data ingestion layer is the most cost-effective choice because EC2 usage is sporadic, unpredictable, and workloads can be interrupted at any time. Spot Instances provide up to 90% savings compared to On-Demand Instances and are ideal for interruptible workloads. Since the data ingestion layer can tolerate interruptions, Spot Instances maximize cost savings while still providing the compute capacity needed when available.\n\n**Why option 2 is correct:**\nPurchasing a 1-year Compute Savings Plan for the front end (Fargate) and API layer (Lambda) is the most cost-effective option because Compute Savings Plans apply to Fargate and Lambda usage, providing up to 66% savings compared to On-Demand pricing. Since front-end and API layer utilization is predictable over the next year, committing to a Savings Plan makes financial sense. Compute Savings Plans provide flexibility by automatically applying to EC2, Fargate, and Lambda usage regardless of instance family, size, AZ, region, OS, or tenancy, making them ideal for mixed compute architectures.\n\n**Why option 1 is incorrect:**\nThe option that says use On-Demand Instances for the data ingestion layer is incorrect because On-Demand Instances are significantly more expensive than Spot Instances (up to 90% more). Since the workload can be interrupted and is sporadic/unpredictable, Spot Instances provide the same functionality at a much lower cost. On-Demand Instances should be reserved for workloads that cannot tolerate interruptions.\n\n**Why option 3 is incorrect:**\nThe option that says purchase 1-year All Upfront Reserved Instances for the data ingestion layer is incorrect because Reserved Instances require a commitment for predictable workloads, but the scenario states EC2 usage is sporadic and unpredictable. Reserved Instances don't provide the flexibility needed for dynamic workloads and may result in paying for capacity that isn't always used. Additionally, Reserved Instances don't apply to Fargate or Lambda, so they wouldn't help with the front-end and API layer costs.\n\n**Why option 4 is incorrect:**\nThe option that says purchase a 1-year EC2 instance Savings Plan for the front end and API layer is incorrect because EC2 instance Savings Plans only apply to EC2 usage, not to Fargate or Lambda. The front end runs on Fargate and the API layer runs on Lambda, so an EC2 instance Savings Plan would not provide any savings for these components. Compute Savings Plans are needed to cover Fargate and Lambda usage.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 58,
    "text": "A company runs a web-based portal that provides users with global breaking news, local alerts, \nand weather updates. The portal delivers each user a personalized view by using mixture of static \nand dynamic content. Content is served over HTTPS through an API server running on an \nAmazon EC2 instance behind an Application Load Balancer (ALB). The company wants the \nportal to provide this content to its users across the world as quickly as possible. \n \nHow should a solutions architect design the application to ensure the LEAST amount of latency \nfor all users?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nDeploying the application stack in a single AWS Region and using Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin provides the least latency for global users. CloudFront is a content delivery network (CDN) that caches content at edge locations worldwide, bringing content closer to users regardless of their geographic location. CloudFront can cache static content at edge locations and can also accelerate dynamic content by routing requests over AWS's optimized network backbone. By using CloudFront with the ALB as the origin, users worldwide receive content from the nearest edge location, significantly reducing latency compared to accessing the ALB directly from a single region. This approach provides global low latency without requiring multi-region deployment.\n\n**Why option 1 is incorrect:**\nThe option that says deploy the application stack in two AWS Regions and use Route 53 latency routing to serve content from the closest ALB is incorrect because while Route 53 can route users to the closest region, users still access the ALB directly, which means they don't benefit from CloudFront's edge caching and optimized routing. Multi-region deployment adds complexity and cost without providing the same level of latency reduction as CloudFront's global edge network. Route 53 routing is based on DNS resolution, which doesn't provide the same performance benefits as CloudFront's edge caching.\n\n**Why option 2 is incorrect:**\nThe option that says deploy in a single region, use CloudFront for static content, and serve dynamic content directly from the ALB is incorrect because serving dynamic content directly from the ALB means users worldwide must connect to a single region, resulting in higher latency for users far from that region. CloudFront can accelerate dynamic content by using AWS's optimized network and connection pooling, providing better performance than direct ALB access. Using CloudFront for both static and dynamic content provides better overall latency reduction.\n\n**Why option 3 is incorrect:**\nThe option that says deploy in two regions and use Route 53 geolocation routing to serve content from the closest ALB is incorrect because geolocation routing is based on user location, not actual network latency. Users may be geographically close to a region but experience higher latency due to network conditions. Additionally, this approach doesn't leverage CloudFront's edge caching and optimized routing, which provides better performance than direct ALB access. Multi-region deployment also adds operational complexity and cost.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A gaming company is designing a highly available architecture. The application runs on a \nmodified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier \nto provide the best possible user experience. That tier must have low latency, route traffic to the \nnearest edge location, and provide static IP addresses for entry into the application endpoints. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Route 53 to forward requests to an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon CloudFront to forward requests to a Network Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Global Accelerator to forward requests to a Network Load Balancer.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon API Gateway to forward requests to an Application Load Balancer.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nConfiguring AWS Global Accelerator to forward requests to a Network Load Balancer is the correct solution for UDP-based gaming applications requiring low latency, routing to nearest edge locations, and static IP addresses. Global Accelerator uses AWS's global network infrastructure and edge locations to route traffic to the optimal endpoint based on performance metrics, user location, and application health. It provides static IP addresses that serve as fixed entry points, which is essential for gaming applications that may have firewall rules or client configurations tied to specific IPs. Global Accelerator supports both TCP and UDP protocols, making it suitable for gaming workloads. It automatically routes traffic to the nearest healthy endpoint, providing low latency and high availability. Network Load Balancers are appropriate for UDP traffic and gaming workloads.\n\n**Why option 0 is incorrect:**\nThe option that says configure Amazon Route 53 to forward requests to an Application Load Balancer is incorrect because Route 53 is a DNS service that routes based on DNS queries, not a traffic routing service for low-latency UDP connections. Additionally, Application Load Balancers operate at Layer 7 (HTTP/HTTPS) and are not designed for UDP traffic. Gaming applications using UDP require Layer 4 load balancing provided by Network Load Balancers, not ALBs.\n\n**Why option 1 is incorrect:**\nThe option that says configure Amazon CloudFront to forward requests to a Network Load Balancer is incorrect because CloudFront is designed for HTTP/HTTPS content delivery and caching, not for UDP-based gaming traffic. CloudFront operates at the application layer (HTTP) and doesn't support UDP protocols. While CloudFront can accelerate web content, it's not suitable for real-time gaming applications that require UDP support and low-latency packet routing.\n\n**Why option 3 is incorrect:**\nThe option that says configure Amazon API Gateway to forward requests to an Application Load Balancer is incorrect because API Gateway is designed for RESTful APIs over HTTP/HTTPS, not for UDP-based gaming traffic. API Gateway doesn't support UDP protocols and operates at the application layer. Additionally, Application Load Balancers don't support UDP traffic - they only handle HTTP/HTTPS. Gaming applications require UDP support and Layer 4 load balancing.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "A company wants to migrate its existing on-premises monolithic application to AWS. The \ncompany wants to keep as much of the front-end code and the backend code as possible. \nHowever, the company wants to break the application into smaller applications. A different team \nwill manage each application. The company needs a highly scalable solution that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host the application on AWS Lambda Integrate the application with Amazon API Gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application on Amazon Elastic Container Service (Amazon ECS) Set up an Application",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nHosting the application on Amazon Elastic Container Service (Amazon ECS) and setting up an Application Load Balancer with ECS as the target is the best solution for migrating a monolithic application to microservices while keeping existing code. ECS allows you to containerize the existing application code with minimal changes, breaking it into smaller containerized services that can be managed by different teams. Each microservice can run as a separate ECS service, allowing independent scaling, deployment, and management. ECS provides automatic scaling, load balancing, service discovery, and integration with other AWS services, minimizing operational overhead. The Application Load Balancer can route traffic to different ECS services based on path or host, enabling the microservices architecture. This approach allows the company to gradually break down the monolith while reusing existing code.\n\n**Why option 0 is incorrect:**\nThe option that says host the application on AWS Lambda and integrate with API Gateway is incorrect because Lambda functions require significant code refactoring to fit the serverless, event-driven model. The existing monolithic application code would need to be rewritten to work as Lambda functions, which violates the requirement to \"keep as much of the front-end code and the backend code as possible.\" Lambda functions are stateless and have execution time limits, which may not be suitable for all parts of a monolithic application without significant architectural changes.\n\n**Why option 1 is incorrect:**\nThe option that says host the application with AWS Amplify and connect to an API Gateway API integrated with Lambda is incorrect because Amplify is primarily designed for front-end web and mobile applications, not for hosting backend monolithic applications. Additionally, this approach still requires rewriting backend code to work with Lambda functions, which doesn't meet the requirement to keep existing code. Amplify is better suited for building new applications from scratch rather than migrating existing monolithic applications.\n\n**Why option 2 is incorrect:**\nThe option that says host the application on Amazon EC2 instances and set up an Application Load Balancer with EC2 instances in an Auto Scaling group is incorrect because while this allows keeping existing code, it doesn't provide the same level of operational efficiency and microservices support as ECS. Managing applications directly on EC2 requires more operational overhead for container orchestration, service discovery, and deployment automation. ECS provides better tooling and integration for breaking applications into microservices and managing them independently.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company recently started using Amazon Aurora as the data store for its global ecommerce \napplication.  \nWhen large reports are run developers report that the ecommerce application is performing \npoorly After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the \nReadlOPS and CPUUtilization metrics are spiking when monthly reports run. \nWhat is the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the monthly reporting to Amazon Redshift.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the monthly reporting to an Aurora Replica",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the Aurora database to a larger instance class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase the Provisioned IOPS on the Aurora instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nMigrating the monthly reporting to an Aurora Replica is the most cost-effective solution. Aurora Replicas are read-only copies of the primary database that can handle read traffic, offloading reporting queries from the primary instance. By directing reporting queries to a replica, the primary database's ReadIOPS and CPU utilization are not impacted, allowing the ecommerce application to perform normally. Aurora Replicas are automatically replicated from the primary instance and can be created with a single API call. This solution doesn't require migrating data to a different service or increasing instance size, making it the most cost-effective approach. The replica shares the same storage volume as the primary (using copy-on-write), so there's no additional storage cost.\n\n**Why option 0 is incorrect:**\nThe option that says migrate the monthly reporting to Amazon Redshift is incorrect because migrating reporting to Redshift requires extracting, transforming, and loading (ETL) data from Aurora to Redshift, which adds significant complexity and operational overhead. Redshift is a data warehouse designed for analytics workloads, but setting up ETL pipelines, maintaining data synchronization, and managing a separate data warehouse adds cost and complexity. This is overkill for monthly reporting that can be handled by an Aurora Replica.\n\n**Why option 2 is incorrect:**\nThe option that says migrate the Aurora database to a larger instance class is incorrect because this increases costs significantly (larger instances cost more) and doesn't address the root cause - reporting queries competing with production traffic. A larger instance would handle both workloads, but reporting queries would still impact production performance during report execution. This solution is more expensive than using a replica and doesn't provide the same isolation.\n\n**Why option 3 is incorrect:**\nThe option that says increase the Provisioned IOPS on the Aurora instance is incorrect because Aurora doesn't use Provisioned IOPS - it uses a distributed storage system that automatically scales IOPS based on database activity. You cannot manually provision IOPS for Aurora. Additionally, increasing IOPS capacity (if it were possible) would increase costs and still wouldn't isolate reporting queries from production traffic like a replica would.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. \nThe analytics software is written in PHP and uses a MySQL database. The analytics software, the \nweb server that provides PHP, and the database server are all hosted on the EC2 instance. The \napplication is showing signs of performance degradation during busy times and is presenting 5xx \nerrors.  \nThe company needs to make the application scale seamlessly. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template. Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nMigrating the database to an Amazon Aurora MySQL DB instance, creating an AMI of the web application, applying it to a launch template, creating an Auto Scaling group with the launch template configured to use a Spot Fleet, and attaching an Application Load Balancer provides the most cost-effective scalable solution. Aurora provides managed database services with automatic backups and scaling, reducing operational overhead. Spot Fleet allows you to use a combination of Spot and On-Demand Instances, providing significant cost savings (up to 90% with Spot) while ensuring capacity availability. Auto Scaling automatically adjusts the number of instances based on demand, providing seamless scaling. The Application Load Balancer distributes traffic across instances, ensuring high availability. This solution addresses performance degradation by scaling horizontally and provides cost optimization through Spot Fleet.\n\n**Why option 0 is incorrect:**\nThe option that says migrate the database to RDS MySQL, create an AMI, launch a second EC2 On-Demand Instance, and use an ALB is incorrect because this creates a fixed two-instance setup that doesn't scale automatically. While it provides some redundancy, it doesn't address the \"scale seamlessly\" requirement. The application would still experience performance issues if two instances aren't enough, and you would need to manually add more instances. Additionally, using only On-Demand Instances is more expensive than using Spot Fleet.\n\n**Why option 1 is incorrect:**\nThe option that says migrate the database to RDS MySQL, create an AMI, launch a second EC2 On-Demand Instance, and use Route 53 weighted routing is incorrect because Route 53 weighted routing is a DNS-level load balancing solution that doesn't provide the same level of health checking, connection draining, and automatic failover as an Application Load Balancer. DNS-based routing has longer TTLs and doesn't react as quickly to instance failures. Additionally, this still creates a fixed two-instance setup without automatic scaling.\n\n**Why option 2 is incorrect:**\nThe option that says migrate the database to Aurora, create a Lambda function to stop the EC2 instance and change the instance type, and use a CloudWatch alarm is incorrect because stopping an instance to change its type causes downtime, which doesn't meet the \"scale seamlessly\" requirement. This approach also doesn't provide horizontal scaling - it only changes the instance size vertically. Additionally, stopping and changing instance types takes time and causes service interruption, which is not seamless scaling.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 63,
    "text": "A company runs a stateless web application in production on a group of Amazon EC2 On-\nDemand Instances behind an Application Load Balancer. The application experiences heavy \nusage during an 8-hour period each business day. Application usage is moderate and steady \novernight Application usage is low during weekends. \nThe company wants to minimize its EC2 costs without affecting the availability of the application. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the entire workload.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved instances for the baseline level of usage.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use On-Demand Instances for the baseline level of usage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Dedicated Instances for the baseline level of usage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Reserved Instances for the baseline level of usage and Spot Instances for any additional capacity provides the most cost-effective solution while maintaining availability. Reserved Instances provide significant cost savings (up to 72% compared to On-Demand) for predictable baseline workloads, which covers the moderate overnight usage and low weekend usage. Spot Instances can handle the additional capacity needed during the 8-hour heavy usage period, providing up to 90% savings compared to On-Demand. Since the application is stateless and behind a load balancer, if Spot Instances are interrupted, the Reserved Instances continue serving traffic, maintaining availability. This combination optimizes costs for both predictable and variable workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use Spot Instances for the entire workload is incorrect because Spot Instances can be interrupted with only 2 minutes notice, which could impact availability if all instances are Spot. While the application is stateless and behind a load balancer, having all instances as Spot creates a risk of simultaneous interruptions that could affect availability. For production applications, it's best practice to have a baseline of guaranteed capacity (Reserved or On-Demand) with Spot for additional capacity.\n\n**Why option 2 is incorrect:**\nThe option that says use On-Demand Instances for the baseline and Spot Instances for additional capacity is incorrect because while this provides availability, On-Demand Instances are significantly more expensive than Reserved Instances for predictable baseline workloads. Since overnight and weekend usage is moderate and steady (predictable), committing to Reserved Instances for the baseline provides substantial cost savings (up to 72%) compared to On-Demand, without affecting availability.\n\n**Why option 3 is incorrect:**\nThe option that says use Dedicated Instances for the baseline and On-Demand Instances for additional capacity is incorrect because Dedicated Instances are the most expensive option, providing physical isolation but at a premium cost. Dedicated Instances are typically 10-20% more expensive than On-Demand Instances and don't provide the same cost savings as Reserved Instances. This approach maximizes costs rather than minimizing them, which doesn't meet the requirement.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A company needs to retain application logs files for a critical application for 10 years. The \napplication team regularly accesses logs from the past month for troubleshooting, but logs older \nthan 1 month are rarely accessed. The application generates more than 10 TB of logs per month. \nWhich storage option meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nStoring the logs in Amazon S3 and using S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive is the most cost-effective solution. S3 provides immediate access to recent logs (past month) for troubleshooting, meeting the requirement for regular access. S3 Lifecycle policies automatically transition objects to Glacier Deep Archive after 30 days, which is the lowest-cost storage class in S3 (designed for long-term archival with 12-hour retrieval times). For logs older than 1 month that are rarely accessed, Glacier Deep Archive provides the lowest storage cost while still maintaining accessibility. This approach optimizes costs by keeping frequently accessed recent logs in Standard storage and archiving older logs to the cheapest storage class. S3 can store virtually unlimited amounts of data and supports 10-year retention requirements.\n\n**Why option 0 is incorrect:**\nThe option that says store logs in S3 and use AWS Backup to move logs more than 1 month old to Glacier Deep Archive is incorrect because AWS Backup is designed for backing up AWS resources (like EBS volumes, RDS databases, DynamoDB tables), not for managing S3 object lifecycle transitions. S3 Lifecycle policies are the native, automated way to transition objects between storage classes. Using AWS Backup for this purpose adds unnecessary complexity and operational overhead compared to S3 Lifecycle policies, which are simpler and more cost-effective.\n\n**Why option 2 is incorrect:**\nThe option that says store logs in CloudWatch Logs and use AWS Backup to move logs more than 1 month old to Glacier Deep Archive is incorrect because CloudWatch Logs has limitations - it doesn't support direct lifecycle policies to archive to Glacier Deep Archive, and CloudWatch Logs is more expensive than S3 for large-scale log storage (10+ TB per month). CloudWatch Logs charges for ingestion, storage, and data transfer, making it cost-prohibitive for 10 TB+ per month over 10 years. Additionally, CloudWatch Logs doesn't provide the same level of lifecycle management as S3.\n\n**Why option 3 is incorrect:**\nThe option that says store logs in CloudWatch Logs and use S3 Lifecycle policies to move logs to Glacier Deep Archive is incorrect because you cannot apply S3 Lifecycle policies to CloudWatch Logs. CloudWatch Logs and S3 are separate services, and lifecycle policies are specific to S3 buckets. CloudWatch Logs would need to export logs to S3 first before lifecycle policies could be applied. Additionally, CloudWatch Logs is significantly more expensive than S3 for large-scale log storage, making it not cost-effective for 10 TB+ per month.",
    "domain": "Design Cost-Optimized Architectures"
  }
]