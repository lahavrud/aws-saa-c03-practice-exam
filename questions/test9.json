[
  {
    "id": 0,
    "text": "A company has a website hosted on AWS The website is behind an Application Load Balancer \n(ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward \nall requests to the website so that the requests will use HTTPS. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Update the ALB's network ACL to accept only HTTPS traffic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a rule that replaces the HTTP in the URL with HTTPS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the ALB with a Network Load Balancer configured to use Server Name Indication",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nTo redirect HTTP traffic to HTTPS, a solutions architect should create a listener rule on the ALB to redirect HTTP traffic to HTTPS. Application Load Balancers support listener rules that can redirect HTTP requests to HTTPS automatically. This is the standard AWS-recommended approach for enforcing HTTPS on web applications. The ALB listener rule can be configured to redirect all HTTP traffic (port 80) to HTTPS (port 443), ensuring all requests use encrypted connections. This solution requires minimal configuration and provides seamless redirection without requiring changes to the application code.\n\n**Why option 0 is incorrect:**\nThe option that says update the ALB's network ACL to accept only HTTPS traffic is incorrect because network ACLs operate at the subnet level and control traffic flow based on IP addresses and ports, but they do not have the ability to redirect traffic from one protocol to another. Network ACLs can only allow or deny traffic, not perform protocol-level redirection. Additionally, blocking HTTP traffic entirely would prevent users from accessing the site, rather than redirecting them to HTTPS.\n\n**Why option 1 is incorrect:**\nThe option that says create a rule that replaces the HTTP in the URL with HTTPS is incorrect because simply replacing text in URLs does not actually redirect traffic or enforce HTTPS connections. This approach would not work at the network level and would require application-level changes. The ALB listener rule for redirection is the proper way to handle this at the load balancer level.\n\n**Why option 3 is incorrect:**\nThe option that says replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI) is incorrect because Network Load Balancers operate at Layer 4 (TCP/UDP) and do not have the ability to handle HTTP/HTTPS redirection at Layer 7. While NLB supports SNI for SSL/TLS termination, it cannot perform HTTP to HTTPS redirection like an Application Load Balancer can. NLB is designed for high-performance, low-latency traffic routing, not for application-level features like HTTP redirection.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company is developing a two-tier web application on AWS. The company's developers have \ndeployed the application on an Amazon EC2 instance that connects directly to a backend \nAmazon RDS database. The company must not hardcode database credentials in the application. \nThe company must also implement a solution to automatically rotate the database credentials on \na regular basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the database credentials in the instance metadata.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the database credentials as a secret in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "A company is deploying a new public web application to AWS. The application will run behind an \nApplication Load Balancer (ALB).  \nThe application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by \nan external certificate authority (CA). \nThe certificate must be rotated each year before the certificate expires. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its \ndocument management application. The company intends to create a product that converts \nlarge .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to \nstore the original files and the converted files. A solutions architect must design a scalable \nsolution to accommodate demand that will grow rapidly over time. \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Save the .pdf files to Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company has more than 5 TB of file data on Windows file servers that run on premises. Users \nand applications interact with the data each day. \nThe company is moving its Windows workloads to AWS. As the company continues this process, \nthe company requires access to AWS and on-premises file storage with minimum latency. The \ncompany needs a solution that minimizes operational overhead and requires no significant \nchanges to the existing file access patterns. The company uses an AWS Site-to-Site VPN \nconnection for connectivity to AWS. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy and configure Amazon FSx for Windows File Server on AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy and configure an Amazon S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy and configure an Amazon S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy and configure Amazon FSx for Windows File Server on AWS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The \nhospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n5 \nformat. The hospital needs to modify the Lambda code to identify protected health information \n(PHI) in the reports. Which solution will meet these requirements with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Use existing Python libraries to extract the text from the reports and to identify the PHI from the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Textract to extract the text from the reports.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Textract to extract the text from the reports.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Rekognition to extract the text from the reports.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A company has an application that generates a large number of files, each approximately 5 MB in \nsize. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 \nyears before they can be deleted. Immediate accessibility is always required as the files contain \ncritical business data that is not easy to reproduce. The files are frequently accessed in the first \n30 days of the object creation but are rarely accessed after the first 30 days. \nWhich storage solution is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket lifecycle policy to move Mm from S3 Standard to S3 Glacier 30 days from",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 bucket lifecycle policy to move tiles from S3 Standard to S3 One Zone-infrequent",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket lifecycle policy to move files from S3 Standard-infrequent Access (S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an S3 bucket Lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company hosts an application on multiple Amazon EC2 instances. The application processes \nmessages from an Amazon SQS queue writes to an Amazon RDS table and deletes the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n6 \nmessage from the queue Occasional duplicate records are found in the RDS table. The SQS \nqueue does not contain any duplicate messages. \n \nWhat should a solutions architect do to ensure messages are being processed once only?",
    "options": [
      {
        "id": 0,
        "text": "Use the CreateQueue API call to create a new queue",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Add Permission API call to add appropriate permissions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the ReceiveMessage API call to set an appropriate wail time",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the ChangeMessageVisibility APi call to increase the visibility timeout",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A solutions architect is designing a new hybrid architecture to extend a company s on-premises \ninfrastructure to AWS. The company requires a highly available connection with consistent low \nlatency to an AWS Region. The company needs to minimize costs and is willing to accept slower \ntraffic if the primary connection fails. \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Provision a VPN tunnel connection to a Region for private connectivity.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an AWS Direct Connect connection to a Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A company is running a business-critical web application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances are in an Auto Scaling group. The application \nuses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The \ncompany wants the application to be highly available with minimum downtime and minimum loss \nof data. \n \nWhich solution will meet these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Place the EC2 instances in different AWS Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Auto Scaling group to use multiple Availability Zones.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the Auto Scaling group to use one Availability Zone.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the Auto Scaling group to use multiple AWS Regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group \nis configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the \nweb service. \n \nThe company notices that the NLB is not detecting HTTP errors for the application. These errors \nrequire a manual restart of the EC2 instances that run the web service. The company needs to \nimprove the application's availability without writing custom scripts or code. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable HTTP health checks on the NLB. supplying the URL of the company's application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a cron job to the EC2 instances to check the local application's logs once each minute.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the NLB with an Application Load Balancer.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company runs a shopping application that uses Amazon DynamoDB to store customer \ninformation. In case of data corruption, a solutions architect needs to design a solution that meets \na recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. \n \nWhat should the solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure DynamoDB global tables.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure DynamoDB point-in-time recovery.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Export the DynamoDB data to Amazon S3 Glacier on a daily basis.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A company runs a photo processing application that needs to frequently upload and download \npictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect \nhas noticed an increased cost in data transfer fees and needs to implement a solution to reduce \nthese costs. \n \nHow can the solutions architect meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a NAT gateway into a public subnet and attach an end point policy that allows access to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application Into a public subnet and allow it to route through an internet gateway to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company recently launched Linux-based application instances on Amazon EC2 in a private \nsubnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet \nof a VPC. A solutions architect needs to connect from the on-premises network, through the \ncompany's internet connection, to the bastion host, and to the application servers. The solutions \narchitect must make sure that the security groups of all the EC2 instances will allow that access. \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the current security group of the bastion host with one that only allows inbound access",
        "correct": true
      },
      {
        "id": 3,
        "text": "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 3 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A solutions architect is designing a two-tier web application. The application consists of a public-\nfacing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft \nSQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the \ncompany. \nHow should security groups be configured in this situation? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the security group for the database tier to allow inbound traffic on port 1433 from the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the database tier to allow outbound traffic on ports 443 and",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.\n\n**Why option 4 is incorrect:**\nis incorrect because creating and sharing root user access keys is a dangerous practice. Root user access keys grant full administrative access to the AWS account. Sharing these keys, even with the business owner, significantly increases the risk of unauthorized access and potential security breaches. It is strongly recommended to avoid creating root user access keys whenever possible. Instead, use IAM users and roles with least privilege access.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company wants to move a multi-tiered application from on premises to the AWS Cloud to \nimprove the application's performance. The application consists of application tiers that \ncommunicate with each other by way of RESTful services. Transactions are dropped when one \ntier becomes overloaded. A solutions architect must design a solution that resolves these issues \nand modernizes the application. \n \nWhich solution meets these requirements and is the MOST operationally efficient?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch metrics to analyze the application performance history to determine",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A company receives 10 TB of instrumentation data each day from several machines located at a \nsingle factory. The data consists of JSON files stored on a storage area network (SAN) in an on-\npremises data center located within the factory. The company wants to send this data to Amazon \nS3 where it can be accessed by several additional systems that provide critical near-real-lime \nanalytics.  \nA secure transfer is important because the data is considered sensitive.  \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n11 \nWhich solution offers the MOST reliable data transfer?",
    "options": [
      {
        "id": 0,
        "text": "AWS DataSync over public internet",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS DataSync over AWS Direct Connect",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Database Migration Service (AWS DMS) over public internet",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company needs to configure a real-time data ingestion architecture for its application. The \ncompany needs an API, a process that transforms data as the data is streamed, and a storage \nsolution for the data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon API Gateway API to send data to AWS Glue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 18,
    "text": "A company needs to keep user transaction data in an Amazon DynamoDB table. \nThe company must retain the data for 7 years. \nWhat is the MOST operationally efficient solution that meets these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n12",
    "options": [
      {
        "id": 0,
        "text": "Use DynamoDB point-in-time recovery to back up the table continuously.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Backup to create backup schedules and retention policies for the table.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an on-demand backup of the table by using the DynamoDB console.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is planning to use an Amazon DynamoDB table for data storage. The company is \nconcerned about cost optimization. The table will not be used on most mornings. In the evenings, \nthe read and write traffic will often be unpredictable. When traffic spikes occur, they will happen \nvery quickly. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create a DynamoDB table in on-demand capacity mode.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a DynamoDB table with a global secondary index.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a DynamoDB table with provisioned capacity and auto scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for \nhelp with an application migration initiative. A solutions architect needs to share an Amazon \nMachine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The \nAMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses a customer managed \ncustomer master key (CMK) to encrypt EBS volume snapshots. \nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's \nAWS account?",
    "options": [
      {
        "id": 0,
        "text": "Make the encrypted AMI and snapshots publicly available.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the launchPermission property of the AMI.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Modify the launchPermission property of the AMI.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect is designing the cloud architecture for a new application being deployed on \nAWS. The process should run in parallel while adding and removing application nodes as needed \nbased on the number of jobs to be processed. The processor application is stateless. The \nsolutions architect must ensure that the application is loosely coupled and the job items are \ndurably stored. \n \nWhich design should the solutions architect use?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon SNS topic to send the jobs that need to be processed.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon SQS queue to hold the jobs that need to be processed.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon SQS queue to hold the jobs that needs to be processed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon SNS topic to send the jobs that need to be processed.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company hosts its web applications in the AWS Cloud. The company configures Elastic Load \nBalancers to use certificate that are imported into AWS Certificate Manager (ACM). The \ncompany's security team must be notified 30 days before the expiration of each certificate.  \nWhat should a solutions architect recommend to meet the requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Config rule that checks for certificates that will expire within 30 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS trusted Advisor to check for certificates that will expire within to days.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 0 is incorrect:**\nis incorrect because introducing an EC2 instance as an intermediary adds unnecessary complexity and latency. While the EC2 instance might be in the same region as the S3 bucket, the initial transfer from the on-premises data center to the EC2 instance would still be a bottleneck. Furthermore, transferring the file from EC2 to S3 adds another step, increasing the overall upload time. FTP is also generally slower and less secure than using the AWS SDK or CLI directly with S3.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A company's dynamic website is hosted using on-premises servers in the United States. The \ncompany is launching its product in Europe, and it wants to optimize site loading times for new \nEuropean users. The site's backend must remain in the United States. The product is being \nlaunched in a few days, and an immediate solution is needed. \n \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the website to Amazon S3. Use cross-Region replication between Regions.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A company wants to reduce the cost of its existing three-tier web architecture. The web, \napplication, and database servers are running on Amazon EC2 instances for the development, \ntest, and production environments. The EC2 instances average 30% CPU utilization during peak \nhours and 10% CPU utilization during non-peak hours. \n \nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run \nfor at least 8 hours each day. The company plans to implement automation to stop the \ndevelopment and test EC2 instances when they are not in use. \n \nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-\neffectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the production EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances for the production EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Spot blocks for the production EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use On-Demand Instances for the production EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 25,
    "text": "A company has a production web application in which users upload documents through a web \ninterlace or a mobile app. \n According to a new regulatory requirement, new documents cannot be modified or deleted after \nthey are stored. \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the uploaded documents in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company has several web servers that need to frequently access a common Amazon RDS \nMySQL Multi-AZ DB instance. The company wants a secure method for the web servers to \nconnect to the database while meeting a security requirement to rotate user credentials \nfrequently. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the database user credentials in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the database user credentials in AWS Systems Manager OpsCenter.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the database user credentials in a secure Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the database user credentials in files encrypted with AWS Key Management Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company hosts an application on AWS Lambda functions mat are invoked by an Amazon API \nGateway API. The Lambda functions save customer data to an Amazon Aurora MySQL \ndatabase. Whenever the company upgrades the database, the Lambda functions fail to establish \ndatabase connections until the upgrade is complete. The result is that customer data Is not \nrecorded for some of the event. \nA solutions architect needs to design a solution that stores customer data that is created during \ndatabase upgrades. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon RDS proxy to sit between the Lambda functions and the database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the run time of me Lambda functions to the maximum.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Persist the customer data to Lambda local storage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the customer data in an Amazon Simple Queue Service (Amazon SOS) FIFO queue.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 28,
    "text": "A survey company has gathered data for several years from areas m\\ the United States. The \ncompany hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company \nhas started to share the data with a European marketing firm that has S3 buckets. The company \nwants to ensure that its data transfer costs remain as low as possible. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Requester Pays feature on the company's S3 bucket",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure cross-account access for the marketing firm so that the marketing firm has access to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the company's S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 29,
    "text": "A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses \nbucket policies to restrict access to audit team IAM user credentials according to the principle of \nleast privilege. Company managers are worried about accidental deletion of documents in the S3 \nbucket and want a more secure solution. \n \nWhat should a solutions architect do to secure the audit documents?",
    "options": [
      {
        "id": 0,
        "text": "Enable the versioning and MFA Delete features on the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours. The company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue. Which solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Modify the DB instance to be a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a read replica of the database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Instruct the development team to manually export the entries in the database at the end of each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache to cache the common queries that the script runs against the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company has applications that run on Amazon EC2 instances in a VPC. One of the applications \nneeds to call the Amazon S3 API to store and read objects. According to the company's security \nregulations, no traffic from the applications is allowed to travel across the internet. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 interface endpoint.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 gateway endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an S3 bucket in a private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket in the same Region as the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A company is storing sensitive user information in an Amazon S3 bucket. The company wants to \nprovide secure access to this bucket from the application tier running on Amazon EC2 instances \ninside a VPC. \nWhich combination of steps should a solutions architect take to accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure a VPC gateway endpoint for Amazon S3 within the VPC",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a bucket policy to make the objects to the S3 bucket public",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a bucket policy that limits access to only the application tier running in the VPC",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a NAT instance and have the EC2 instances use the NAT instance to access the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 2 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company runs an on-premises application that is powered by a MySQL database. The \ncompany is migrating the application to AWS to Increase the application's elasticity and \navailability. The current architecture shows heavy read activity on the database during times of \nnormal operation. Every 4 hours the company's development team pulls a full export of the \nproduction database to populate a database in the staging environment. During this period, users \nexperience unacceptable application latency. The development team is unable to use the staging \nenvironment until the procedure completes. \nA solutions architect must recommend replacement architecture that alleviates the application \nlatency issue.  \nThe replacement architecture also must give the development team the ability to continue using \nthe staging environment without delay. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon RDS for MySQL with a Mufti AZ deployment and read replicas for production.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A company is preparing to store confidential data in Amazon S3. For compliance reasons the \ndata must be encrypted at rest Encryption key usage must be logged tor auditing purposes. Keys \nmust be rotated every year. \nWhich solution meets these requirements and the MOST operationally efferent?",
    "options": [
      {
        "id": 0,
        "text": "Server-side encryption with customer-provided keys (SSE-C)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual",
        "correct": false
      },
      {
        "id": 3,
        "text": "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A bicycle sharing company is developing a multi-tier architecture to track the location of its \nbicycles during peak operating hours. The company wants to use these data points in its existing \nanalytics platform. A solutions architect must determine the most viable multi-tier option to \nsupport this architecture. The data points must be accessible from the REST API.  \nWhich action meets these requirements for storing and retrieving location data?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Athena with Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon API Gateway with AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon QuickSight with Amazon Redshift.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway with Amazon Kinesis Data Analytics",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company has an automobile sales website that stores its listings in a database on Amazon \nRDS. When an automobile is sold the listing needs to be removed from the website and the data \nmust be sent to multiple target systems. \nWhich design should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A company needs to store data in Amazon S3 and must prevent the data from being changed. \nThe company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a \nnonspecific amount of time until the company decides to modify the objects. Only specific users in \nthe company's AWS account can have the ability 10 delete the objects. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Glacier vault.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 bucket with S3 Object Lock enabled.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket with S3 Object Lock enabled.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A social media company allows users to upload images to its website. The website runs on \nAmazon EC2 instances.  \nDuring upload requests, the website resizes the images to a standard size and stores the resized \nimages in Amazon S3.  \nUsers are experiencing slow upload requests to the website. \n \nThe company needs to reduce coupling within the application and improve website performance.  \nA solutions architect must design the most operationally efficient process for image uploads. \n \nWhich combination of actions should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to upload images to S3 Glacier.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the web server to upload the original images to Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the application to upload images directly from each user's browser to Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 3 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A company recently migrated a message processing system to AWS. The system receives \nmessages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are \nprocessed by a consumer application running on Amazon EC2. The consumer application \nprocesses the messages and writes results to a MySQL database running on Amazon EC2. The \ncompany wants this application to be highly available with low operational complexity. \nWhich architecture offers the HIGHEST availability?",
    "options": [
      {
        "id": 0,
        "text": "Add a second ActiveMQ server to another Availably Zone.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon MO with active/standby brokers configured across two Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon MO with active/standby blotters configured across two Availability Zones.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon MQ with active/standby brokers configured across two Availability Zones.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company hosts a containerized web application on a fleet of on-premises servers that process \nincoming requests. The number of requests is growing quickly. The on-premises servers cannot \nhandle the increased number of requests. The company wants to move the application to AWS \nwith minimum code changes and minimum development effort. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use two Amazon EC2 instances to host the containerized web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda with a new code that uses one of the supported languages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a high performance computing (HPC) solution such as AWS ParallelClusterto establish an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company uses 50 TB of data for reporting. The company wants to move this data from on \npremises to AWS A custom application in the company's data center runs a weekly data \ntransformation job. The company plans to pause the application until the data transfer is complete \nand needs to begin the transfer process as soon as possible. \nThe data center does not have any available network bandwidth for additional workloads.  \nA solutions architect must transfer the data and must configure the transformation job to continue \nto run in the AWS Cloud. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to move the data.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order an AWS Snowcone device to move the data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Order an AWS Snowball Edge Storage Optimized device.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Order an AWS D. Snowball Edge Storage Optimized device that includes Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company has created an image analysis application in which users can upload photos and add \nphoto frames to their images. The users upload images and metadata to indicate which photo \nframes they want to add to their images. The application uses a single Amazon EC2 instance and \nAmazon DynamoDB to store the metadata. \n \nThe application is becoming more popular, and the number of users is increasing. The company \nexpects the number of concurrent users to vary significantly depending on the time of day and \nday of week. The company must ensure that the application can scale to meet the needs of the \ngrowing user base. \n \nWhich solution meats these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda to process the photos.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda to process the photos.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Increase the number of EC2 instances to three.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A medical records company is hosting an application on Amazon EC2 instances. The application \nprocesses customer data files that are stored on Amazon S3. The EC2 instances are hosted in \npublic subnets. The EC2 instances access Amazon S3 over the internet, but they do not require \nany other network access. \nA new requirement mandates that the network traffic for file transfers take a private route and not \nbe sent over the internet. \nWhich change to the network architecture should a solutions architect recommend to meet this \nrequirement?",
    "options": [
      {
        "id": 0,
        "text": "Create a NAT gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the EC2 instances to private subnets.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Remove the internet gateway from the VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company uses a popular content management system (CMS) for its corporate website. \nHowever, the required patching and maintenance are burdensome. The company is redesigning \nits website and wants anew solution. The website will be updated four times a year and does not \nneed to have any dynamic content available. The solution must provide high scalability and \nenhanced security. \n \nWhich combination of changes will meet these requirements with the LEAST operational \noverhead? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create and deploy an AWS Lambda function to manage and serve the website content",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create the new website and an Amazon S3 bucket Deploy the website on the S3 bucket with",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create the new website. Deploy the website on an Amazon S3 bucket with static website hosting enabled. Use Amazon CloudFront to distribute the website content and require HTTPS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      3
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 3 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company stores its application logs in an Amazon CloudWatch Logs log group.  \nA new policy requires the company to store all application logs in Amazon OpenSearch Service \n(Amazon Elasticsearch Service) in near-real time. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Kinesis Data Firehose delivery stream.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Install and configure Amazon Kinesis Agent on each application server to deliver the logs to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "A company is building a web-based application running on Amazon EC2 instances in multiple \nAvailability Zones. The web application will provide access to a repository of text documents \ntotaling about 900 TB in size. The company anticipates that the web application will experience \nperiods of high demand. A solutions architect must ensure that the storage component for the text \ndocuments can scale to meet the demand of the application at all times. The company is \nconcerned about the overall cost of the solution. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elasticsearch Service (Amazon ES)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in \nthe us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution \nto protect these API Gateway managed REST APIs across multiple accounts from SQL injection \nand cross-site scripting attacks. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS WAF in both Regions.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Firewall Manager in both Regions.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up AWS Shield in bath Regions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up AWS Shield in one of the Regions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company has implemented a self-managed DNS solution on three Amazon EC2 instances \nbehind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are \nlocated in the United States and Europe. The company wants to improve the performance and \navailability of the solution. The company launches and configures three EC2 instances in the eu-\nwest-1 Region and adds the EC2 instances as targets for a new NLB. \n \nWhich solution can the company use to route traffic to all the EC2 instances?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Route 53 geolocation routing policy to route requests to one of the two",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a standard accelerator in AWS Global Accelerator.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Attach Elastic IP addresses to the six EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replace the two NLBs with two Application Load Balancers (ALBs).",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company is running an online transaction processing (OLTP) workload on AWS. This workload \nuses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database \nsnapshots are taken from this instance. \n \nWhat should a solutions architect do to ensure the database and snapshots are always encrypted \nmoving forward?",
    "options": [
      {
        "id": 0,
        "text": "Encrypt a copy of the latest DB snapshot.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company wants to build a scalable key management Infrastructure to support developers who need to encrypt data in their applications. What should a solutions architect do to reduce the operational burden?",
    "options": [
      {
        "id": 0,
        "text": "Use multifactor authentication (MFA) to protect the encryption keys.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Key Management Service (AWS KMS) to protect the encryption keys",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an IAM policy to limit the scope of users who have access permissions to protect the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company has a dynamic web application hosted on two Amazon EC2 instances. The company \nhas its own SSL certificate, which is on each instance to perform SSL termination. \n \nThere has been an increase in traffic recently, and the operations team determined that SSL \nencryption and decryption is causing the compute capacity of the web servers to reach their \nmaximum limit. \n \nWhat should a solutions architect do to increase the application's performance?",
    "options": [
      {
        "id": 0,
        "text": "Create a new SSL certificate using AWS Certificate Manager (ACM) install the ACM certificate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create another EC2 instance as a proxy server Migrate the SSL certificate to the new instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Import the SSL certificate into AWS Certificate Manager (ACM).",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 1 is incorrect:**\nstoring the data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect for similar reasons as S3 Standard-IA. While it offers even lower storage costs, it also has higher retrieval costs and a 30-day minimum storage duration charge. More importantly, S3 One Zone-IA stores data in a single Availability Zone, which makes it less durable than S3 Standard. While durability isn't explicitly mentioned as a primary concern, it's generally a good practice to use S3 Standard unless there's a very specific reason to use a lower-durability option. The frequent access pattern and the 30-day minimum storage duration make this option less cost-effective. Also, the loss of an AZ could impact the analytics pipeline.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job. What should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Implement EC2 Spot Instances",
        "correct": true
      },
      {
        "id": 1,
        "text": "Purchase EC2 Reserved Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement EC2 On-Demand Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement the processing on AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company runs its two-tier ecommerce website on AWS. The web tier consists of a load \nbalancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS \nDB instance. The EC2 instances and the RDS DB instance should not be exposed to the public \ninternet. The EC2 instances require internet access to complete payment processing of orders \nthrough a third-party web service. The application must be highly available. \n \nWhich combination of configuration options will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use an Auto Scaling group to launch the EC2 instances in private subnets.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a VPC with two private subnets and two NAT gateways across two Availability Zones.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure a VPC with two public subnets, two private subnets, and two NAT gateways across",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 4 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable. Which solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company runs its ecommerce application on AWS. Every new order is published as a message \nin a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These \nmessages are processed by a different application that runs on a separate EC2 instance. This \napplication stores the details in a PostgreSQL database on another EC2 instance. All the EC2 \ninstances are in the same Availability Zone. \nThe company needs to redesign its architecture to provide the highest availability with the least \noperational overhead. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically to the analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines. What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A solutions architect needs to help a company optimize the cost of running an application on \nAWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for \ncompute within the architecture. \n \nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic \nand unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The \napplication front end will run on Fargate, and Lambda will serve the API layer. The front-end \nutilization and API layer utilization will be predictable over the course of the next year. \n \nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting \nthis application? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the data ingestion layer",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use On-Demand Instances for the data ingestion layer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a 1-year Compute Savings Plan for the front end and API layer.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase 1-year All Upfront Reserved instances for the data ingestion layer.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 2 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 58,
    "text": "A company runs a web-based portal that provides users with global breaking news, local alerts, \nand weather updates. The portal delivers each user a personalized view by using mixture of static \nand dynamic content. Content is served over HTTPS through an API server running on an \nAmazon EC2 instance behind an Application Load Balancer (ALB). The company wants the \nportal to provide this content to its users across the world as quickly as possible. \n \nHow should a solutions architect design the application to ensure the LEAST amount of latency \nfor all users?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A gaming company is designing a highly available architecture. The application runs on a \nmodified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier \nto provide the best possible user experience. That tier must have low latency, route traffic to the \nnearest edge location, and provide static IP addresses for entry into the application endpoints. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon Route 53 to forward requests to an Application Load Balancer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon CloudFront to forward requests to a Network Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Global Accelerator to forward requests to a Network Load Balancer.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon API Gateway to forward requests to an Application Load Balancer.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "A company wants to migrate its existing on-premises monolithic application to AWS. The \ncompany wants to keep as much of the front-end code and the backend code as possible. \nHowever, the company wants to break the application into smaller applications. A different team \nwill manage each application. The company needs a highly scalable solution that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host the application on AWS Lambda Integrate the application with Amazon API Gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application on Amazon Elastic Container Service (Amazon ECS) Set up an Application",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company recently started using Amazon Aurora as the data store for its global ecommerce \napplication.  \nWhen large reports are run developers report that the ecommerce application is performing \npoorly After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the \nReadlOPS and CPUUtilization metrics are spiking when monthly reports run. \nWhat is the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the monthly reporting to Amazon Redshift.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the monthly reporting to an Aurora Replica",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the Aurora database to a larger instance class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase the Provisioned IOPS on the Aurora instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. \nThe analytics software is written in PHP and uses a MySQL database. The analytics software, the \nweb server that provides PHP, and the database server are all hosted on the EC2 instance. The \napplication is showing signs of performance degradation during busy times and is presenting 5xx \nerrors.  \nThe company needs to make the application scale seamlessly. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template. Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 63,
    "text": "A company runs a stateless web application in production on a group of Amazon EC2 On-\nDemand Instances behind an Application Load Balancer. The application experiences heavy \nusage during an 8-hour period each business day. Application usage is moderate and steady \novernight Application usage is low during weekends. \nThe company wants to minimize its EC2 costs without affecting the availability of the application. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances for the entire workload.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved instances for the baseline level of usage.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use On-Demand Instances for the baseline level of usage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Dedicated Instances for the baseline level of usage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A company needs to retain application logs files for a critical application for 10 years. The \napplication team regularly accesses logs from the past month for troubleshooting, but logs older \nthan 1 month are rarely accessed. The application generates more than 10 TB of logs per month. \nWhich storage option meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.",
    "domain": "Design Cost-Optimized Architectures"
  }
]