[
  {
    "id": 1,
    "text": "A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database. Which of the following would you recommend to securely share the database with the auditor?",
    "options": [
      {
        "id": 0,
        "text": "Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it provides a secure and controlled way to share the database. Creating an encrypted snapshot ensures that the data is protected at rest. Sharing the snapshot with the auditor allows them to create their own RDS instance from the snapshot in their AWS account. Granting the auditor access to the AWS KMS encryption key is essential because they need the key to decrypt the snapshot and restore the database. This approach maintains data security and provides the auditor with a separate, independent copy of the database, fulfilling the requirements of the scenario.\n\n**Why option 0 is incorrect:**\nis incorrect because setting up a read replica and granting the auditor access via IAM standard database authentication does not provide the auditor with their own copy of the database. The auditor would be accessing the original database instance, which is not ideal for security and isolation. Also, sharing database credentials directly increases the risk of unauthorized access or misuse.\n\n**Why option 1 is incorrect:**\nis incorrect because exporting the database contents to text files and storing them in S3 is not a secure method. Text files are unencrypted and vulnerable to unauthorized access. Creating a new IAM user for the auditor with access to the S3 bucket is also less secure than using encrypted snapshots and KMS. This method also involves significant data transformation and potential data loss during the export/import process.\n\n**Why option 2 is incorrect:**\nis incorrect because storing the snapshot directly in S3 without encryption is not secure. While IAM roles can control access to the S3 bucket, the data itself is not protected at rest. An encrypted snapshot is essential to protect sensitive data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it utilizes an Amazon SQS FIFO queue with a Group ID attribute. SQS FIFO queues guarantee that messages are delivered in the order they were sent, but only within a specific message group. By setting the Group ID to the Desktop ID, all messages from the same desktop system will be processed in order. Furthermore, different Desktop IDs can be processed concurrently by different consumers, allowing for the desired scalability. This approach effectively partitions the data based on the Desktop ID, ensuring ordered processing within each partition and parallel processing across partitions.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee message ordering. While they offer high throughput, they are not suitable for scenarios where message order is critical. Therefore, using a standard queue would not meet the requirement of processing telemetry data in order for each desktop system.\n\n**Why option 2 is incorrect:**\nis incorrect because while SQS FIFO queues guarantee ordering, they do so only if a single consumer is processing the queue. Without a Group ID, all messages are treated as part of a single sequence, and only one consumer can process messages from the queue at a time. This would prevent the desired scalability, as only one consumer could process all telemetry data, regardless of the number of desktop systems.\n\n**Why option 3 is incorrect:**\nis incorrect because while Kinesis Data Streams can provide ordered data processing within a shard, the number of shards is fixed at creation and cannot dynamically scale to the number of desktop systems. Furthermore, while you can use the Partition Key to ensure data from the same desktop goes to the same shard, the number of consumers is limited by the number of shards. Scaling consumers to match the number of desktops would require a very large number of shards, which can be complex to manage and potentially inefficient if some desktops generate significantly less data than others. SQS FIFO with Group ID is a more suitable solution for this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "\"An enterprise organization is expanding its cloud footprint and needs to centralize its security event data from various AWS accounts and services. The goal is to evaluate security posture across all environments and improve threat detection and response — without requiring significant custom code or manual integration. Which solution will fulfill these needs with the least development effort?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Security Lake to create a centralized data lake that automatically collects security-related logs and events from AWS services and third-party sources. Store the data in an Amazon S3 bucket managed by Security Lake",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Athena with predefined SQL queries to scan security logs stored in multiple S3 buckets. Visualize the findings by exporting results to an Amazon QuickSight dashboard",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a custom Lambda function to aggregate security logs from multiple AWS accounts. Format the data into CSV files and upload them to a central S3 bucket for analysis",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a data lake using AWS Lake Formation to collect and organize security event logs. Use AWS Glue to perform ETL operations and standardize the log formats for centralized analysis",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because Amazon Security Lake is specifically designed to centralize security data from AWS services and third-party sources into a data lake. It automatically collects and manages security logs and events, reducing the need for custom code or manual integration. Security Lake stores the data in an Amazon S3 bucket managed by the service, providing a scalable and secure storage solution. This directly addresses the requirements of centralizing security data, minimizing development effort, and improving threat detection and response.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon Athena can query data in S3 buckets and QuickSight can visualize the results, this approach requires significant manual configuration and maintenance. It involves setting up S3 buckets, defining SQL queries for different log formats, and manually exporting data to QuickSight. This does not fulfill the requirement of minimizing development effort and automated integration. It also lacks the built-in security features and centralized management provided by Security Lake.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying a custom Lambda function to aggregate security logs requires significant development effort to handle different log formats, authentication, and error handling. Formatting the data into CSV files is also not an optimal approach for large-scale data analysis. This solution does not meet the requirement of minimizing development effort and automated integration. It introduces complexity and potential maintenance overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because setting up a data lake using AWS Lake Formation and AWS Glue requires significant configuration and development effort. While Lake Formation can help manage data access and governance, and Glue can perform ETL operations, this approach involves defining data catalogs, creating ETL jobs, and managing the data pipeline. This does not fulfill the requirement of minimizing development effort and automated integration. Security Lake provides a more streamlined and purpose-built solution for centralizing security data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A manufacturing analytics company has a large collection of automated scripts that perform data cleanup, validation, and system integration tasks. These scripts are currently run by a local Linux cron scheduler and have an execution time of up to 30 minutes. The company wants to migrate these scripts to AWS without significant changes, and would prefer a containerized, serverless architecture that automatically scales and can respond to event-based triggers in the future. The solution must minimize infrastructure management. Which solution will best meet these requirements with minimal refactoring and operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions",
        "correct": false
      },
      {
        "id": 1,
        "text": "Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functions’ RunTask integration with ECS Fargate",
        "correct": false
      },
      {
        "id": 3,
        "text": "Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the best solution. Packaging the scripts into a container image allows for minimal changes to the existing scripts. Using Amazon EventBridge Scheduler provides a serverless and managed way to define cron-based schedules, replacing the local Linux cron scheduler. Configuring EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS provides a serverless container execution environment that automatically scales based on demand. Fargate eliminates the need to manage EC2 instances, thus minimizing infrastructure management. This solution directly addresses all the requirements: minimal refactoring (containerization), serverless architecture (EventBridge Scheduler and Fargate), automatic scaling (Fargate), and minimal operational overhead (managed services).\n\n**Why option 0 is incorrect:**\nis incorrect because while AWS Batch can run containerized workloads and supports cron-based scheduling, it requires managing a compute environment, which contradicts the requirement to minimize infrastructure management. Even with a managed compute environment, the operational overhead is higher than using Fargate directly.\n\n**Why option 2 is incorrect:**\nis incorrect because while Step Functions can orchestrate tasks and use ECS Fargate, it introduces unnecessary complexity for a simple cron-based scheduling scenario. Using a Wait state in Step Functions for scheduling is not the intended use case and adds operational overhead. Creating a container image for each script might be overkill, depending on the scripts' dependencies and could increase management complexity.\n\n**Why option 3 is incorrect:**\nis incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The scripts can run up to 30 minutes, making Lambda an unsuitable choice. Also, converting each script to a Lambda function requires significant refactoring, which violates the requirement of minimal changes. While EventBridge Scheduler is a good choice for scheduling, Lambda is not the right compute service in this scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance. As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive",
        "correct": false
      },
      {
        "id": 1,
        "text": "The health check grace period for the instance has not expired",
        "correct": true
      },
      {
        "id": 2,
        "text": "The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG)",
        "correct": false
      },
      {
        "id": 3,
        "text": "The instance has failed the Elastic Load Balancing (ELB) health check status",
        "correct": true
      },
      {
        "id": 4,
        "text": "A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks",
        "correct": false
      },
      {
        "id": 5,
        "text": "The instance maybe in Impaired status",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      3,
      5
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they represent common reasons why an ASG might not terminate an unhealthy instance. \n\n*   **Option 1 (The health check grace period for the instance has not expired):** When an instance is launched, there's a grace period during which the ASG doesn't consider health check results. This allows the instance to initialize and become healthy before health checks are enforced. If the instance becomes unhealthy within this grace period, the ASG will not terminate it immediately.\n*   **Option 3 (The instance has failed the Elastic Load Balancing (ELB) health check status):** If the ASG is configured to use ELB health checks, and the instance fails the ELB health check, the ELB will mark the instance as unhealthy. The ASG should then terminate the instance. If it's not terminating, it's a valid troubleshooting step to check the ELB health check status and ensure the ASG is configured to use ELB health checks.\n*   **Option 5 (The instance maybe in Impaired status):** An instance in 'Impaired' status indicates a problem with the underlying hardware. The ASG should detect this and terminate the instance. If the ASG is not terminating instances in impaired status, it's a valid troubleshooting step to check the instance status and ensure the ASG is configured to respond to impaired status.\n\n**Why option 3 is correct:**\nThese are correct because they represent common reasons why an ASG might not terminate an unhealthy instance. \n\n*   **Option 1 (The health check grace period for the instance has not expired):** When an instance is launched, there's a grace period during which the ASG doesn't consider health check results. This allows the instance to initialize and become healthy before health checks are enforced. If the instance becomes unhealthy within this grace period, the ASG will not terminate it immediately.\n*   **Option 3 (The instance has failed the Elastic Load Balancing (ELB) health check status):** If the ASG is configured to use ELB health checks, and the instance fails the ELB health check, the ELB will mark the instance as unhealthy. The ASG should then terminate the instance. If it's not terminating, it's a valid troubleshooting step to check the ELB health check status and ensure the ASG is configured to use ELB health checks.\n*   **Option 5 (The instance maybe in Impaired status):** An instance in 'Impaired' status indicates a problem with the underlying hardware. The ASG should detect this and terminate the instance. If the ASG is not terminating instances in impaired status, it's a valid troubleshooting step to check the instance status and ensure the ASG is configured to respond to impaired status.\n\n**Why option 5 is correct:**\nThese are correct because they represent common reasons why an ASG might not terminate an unhealthy instance. \n\n*   **Option 1 (The health check grace period for the instance has not expired):** When an instance is launched, there's a grace period during which the ASG doesn't consider health check results. This allows the instance to initialize and become healthy before health checks are enforced. If the instance becomes unhealthy within this grace period, the ASG will not terminate it immediately.\n*   **Option 3 (The instance has failed the Elastic Load Balancing (ELB) health check status):** If the ASG is configured to use ELB health checks, and the instance fails the ELB health check, the ELB will mark the instance as unhealthy. The ASG should then terminate the instance. If it's not terminating, it's a valid troubleshooting step to check the ELB health check status and ensure the ASG is configured to use ELB health checks.\n*   **Option 5 (The instance maybe in Impaired status):** An instance in 'Impaired' status indicates a problem with the underlying hardware. The ASG should detect this and terminate the instance. If the ASG is not terminating instances in impaired status, it's a valid troubleshooting step to check the instance status and ensure the ASG is configured to respond to impaired status.\n\n**Why option 0 is incorrect:**\nis incorrect because while increasing the minimum number of instances *could* prevent scaling down, it wouldn't prevent the ASG from *replacing* an unhealthy instance. The ASG should still terminate the unhealthy instance and launch a new one to maintain the desired minimum capacity. The question specifically asks why an unhealthy instance is *not* being terminated.\n\n**Why option 2 is incorrect:**\nis incorrect because Auto Scaling groups *can* terminate Spot Instances. The ASG will attempt to replace the Spot Instance with another instance (either On-Demand or Spot, depending on the ASG configuration) to maintain the desired capacity. The fact that it's a Spot Instance doesn't prevent the ASG from terminating it if it's unhealthy.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located inus-east-1region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time. As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon Aurora read replicas in the eu-west-1 region",
        "correct": true
      },
      {
        "id": 3,
        "text": "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53",
        "correct": false
      },
      {
        "id": 4,
        "text": "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because creating Aurora read replicas in eu-west-1 will allow the application to serve read requests from a database closer to the European users, reducing latency and improving performance. Aurora read replicas are designed for scaling read capacity and improving read performance in different regions.\n\nOption 4 is correct because setting up another fleet of EC2 instances for the web tier in eu-west-1 and using latency-based routing in Route 53 will direct European users to the web servers in the eu-west-1 region. Latency routing ensures that users are routed to the region with the lowest latency, thereby improving application load time.\n\n**Why option 4 is correct:**\nThis is correct because creating Aurora read replicas in eu-west-1 will allow the application to serve read requests from a database closer to the European users, reducing latency and improving performance. Aurora read replicas are designed for scaling read capacity and improving read performance in different regions.\n\nOption 4 is correct because setting up another fleet of EC2 instances for the web tier in eu-west-1 and using latency-based routing in Route 53 will direct European users to the web servers in the eu-west-1 region. Latency routing ensures that users are routed to the region with the lowest latency, thereby improving application load time.\n\n**Why option 0 is incorrect:**\nis incorrect because geolocation routing, while useful for directing users based on their geographic location, doesn't necessarily guarantee the lowest latency. Latency routing is more suitable for this scenario where the primary concern is minimizing application load time.\n\n**Why option 1 is incorrect:**\nis incorrect because creating an Aurora Multi-AZ standby instance in eu-west-1 is primarily for disaster recovery and high availability, not for improving read performance for users in that region. A Multi-AZ standby instance is a hot standby that is only activated in case of a failure in the primary instance. Read replicas are the correct solution for improving read performance in a different region.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 7,
    "text": "Which of the following IAM policies provides read-only access to the Amazon S3 bucketmybucketand its content?",
    "options": [
      {
        "id": 0,
        "text": "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }",
        "correct": true
      },
      {
        "id": 1,
        "text": "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" }, { \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }",
        "correct": false
      },
      {
        "id": 2,
        "text": "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket/*\" } ] }",
        "correct": false
      },
      {
        "id": 3,
        "text": "{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::mybucket\" } ] }",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it provides the necessary permissions for read-only access. The first statement allows the 's3:ListBucket' action on the bucket itself (arn:aws:s3:::mybucket), which is required to see the contents of the bucket. The second statement allows the 's3:GetObject' action on all objects within the bucket (arn:aws:s3:::mybucket/*), which is required to download or read the objects. The '/*' at the end of the bucket ARN in the second statement is crucial for applying the 'GetObject' permission to all objects within the bucket.\n\n**Why option 1 is incorrect:**\nis incorrect because it reverses the resource ARNs for the 'ListBucket' and 'GetObject' actions. 's3:ListBucket' needs to be applied to the bucket itself (arn:aws:s3:::mybucket), not to all objects within the bucket (arn:aws:s3:::mybucket/*). Similarly, 's3:GetObject' needs to be applied to the objects within the bucket (arn:aws:s3:::mybucket/*), not just the bucket itself (arn:aws:s3:::mybucket).\n\n**Why option 2 is incorrect:**\nis incorrect because it attempts to grant both 's3:ListBucket' and 's3:GetObject' actions on all objects within the bucket (arn:aws:s3:::mybucket/*). While it grants 'GetObject' correctly, 'ListBucket' needs to be applied to the bucket itself, not to all objects within the bucket. Applying 'ListBucket' to the objects will not allow listing the bucket contents.\n\n**Why option 3 is incorrect:**\nis incorrect because it attempts to grant both 's3:ListBucket' and 's3:GetObject' actions on the bucket itself (arn:aws:s3:::mybucket). While it grants 'ListBucket' correctly, 'GetObject' needs to be applied to the objects within the bucket (arn:aws:s3:::mybucket/*) to allow reading the objects. Applying 'GetObject' to the bucket itself will not allow reading the objects.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence. As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Route 53 weighted routing to spread traffic across different deployments",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CodeDeploy deployment options to choose the right deployment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Elastic Load Balancing (ELB) to distribute traffic across deployments",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Global Accelerator, is the correct choice. Global Accelerator provides static IP addresses that act as a fixed entry point to the application. This bypasses DNS caching issues on mobile devices because users connect directly to these static IPs. Global Accelerator can then route traffic to the blue or green environment based on configured weights. This allows for a controlled and rapid shift of traffic to the new deployment, enabling testing on a significant user base within the limited timeframe. The static IPs minimize the impact of DNS caching, ensuring that users are directed to the intended environment quickly.\n\n**Why option 0 is incorrect:**\nusing Amazon Route 53 weighted routing, is incorrect because it relies on DNS. DNS caching on mobile devices will delay the propagation of the new DNS records, hindering the rapid and widespread testing required within the 48-hour timeframe. While weighted routing is a valid technique for blue-green deployments, it is not suitable when DNS caching is a major concern.\n\n**Why option 1 is incorrect:**\nusing AWS CodeDeploy deployment options, is incorrect because CodeDeploy primarily focuses on the deployment process itself (e.g., in-place or blue/green deployments on EC2 instances or Lambda functions). While CodeDeploy can be used in conjunction with a blue-green deployment strategy, it does not directly address the DNS caching issue. It manages the deployment of the application code but doesn't handle traffic routing or DNS propagation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures. Which is the MOST cost-optimal solution for this workload?",
    "options": [
      {
        "id": 0,
        "text": "Run the workload on Reserved Instances (RI)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Run the workload on a Spot Fleet",
        "correct": true
      },
      {
        "id": 2,
        "text": "Run the workload on Spot Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run the workload on Dedicated Hosts",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\n'Run the workload on a Spot Fleet,' is the most cost-optimal solution. Spot Fleets allow you to request multiple Spot Instances across different instance types and Availability Zones. This provides flexibility in terms of instance sizes and helps to minimize the risk of interruption. If a Spot Instance is terminated due to price fluctuations, the Spot Fleet will automatically attempt to replace it with another instance that meets your criteria. This ensures the workload can continue to run even if some instances are interrupted. Spot Instances offer significant cost savings compared to On-Demand or Reserved Instances, especially for workloads that can tolerate interruptions and are fault-tolerant. Spot Fleets also allow you to define a target capacity and let AWS manage the bidding and provisioning of Spot Instances to meet that capacity.\n\n**Why option 0 is incorrect:**\n'Run the workload on Reserved Instances (RI),' is incorrect because Reserved Instances are best suited for long-running, predictable workloads. Since the workload only runs for 2 hours a month, the cost savings from Reserved Instances would not outweigh the upfront commitment and unused capacity for the rest of the month. RI's are not cost-effective for such a short and infrequent workload.\n\n**Why option 2 is incorrect:**\n'Run the workload on Spot Instances,' is partially correct as it leverages Spot Instances for cost savings. However, using individual Spot Instances without a Spot Fleet lacks the fault tolerance and automatic replacement capabilities. If a single Spot Instance is terminated, the workload running on it will be interrupted, and there's no automatic mechanism to replace it. This increases the risk of the workload failing to complete within the desired timeframe. While cheaper than a Spot Fleet, the lack of resilience makes it less suitable.\n\n**Why option 3 is incorrect:**\n'Run the workload on Dedicated Hosts,' is incorrect because Dedicated Hosts are the most expensive option. They are suitable for workloads with specific licensing requirements or compliance needs that require dedicated hardware. For a big data workload that can be distributed across multiple servers and tolerate interruptions, Dedicated Hosts are not cost-effective. They provide no cost advantage for this scenario and are overkill.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos. Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos",
        "correct": true
      },
      {
        "id": 1,
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos",
        "correct": false
      },
      {
        "id": 2,
        "text": "Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos",
        "correct": true
      },
      {
        "id": 3,
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos",
        "correct": false
      },
      {
        "id": 4,
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they address the data consistency issue by migrating the video storage to a shared storage solution. \n\nOption 0 suggests migrating the videos to Amazon S3. S3 provides highly durable and scalable object storage. By storing the videos in S3, all EC2 instances can access the same data, resolving the data consistency problem. S3 standard is appropriate for frequently accessed data like videos.\n\nOption 2 suggests migrating the videos to Amazon EFS. EFS provides a shared file system that can be mounted on multiple EC2 instances simultaneously. This allows all instances to access the same set of videos, resolving the data consistency issue. EFS is suitable for workloads that require shared file storage.\n\n**Why option 2 is correct:**\nThese are correct because they address the data consistency issue by migrating the video storage to a shared storage solution. \n\nOption 0 suggests migrating the videos to Amazon S3. S3 provides highly durable and scalable object storage. By storing the videos in S3, all EC2 instances can access the same data, resolving the data consistency problem. S3 standard is appropriate for frequently accessed data like videos.\n\nOption 2 suggests migrating the videos to Amazon EFS. EFS provides a shared file system that can be mounted on multiple EC2 instances simultaneously. This allows all instances to access the same set of videos, resolving the data consistency issue. EFS is suitable for workloads that require shared file storage.\n\n**Why option 1 is incorrect:**\nsuggests using Amazon RDS. RDS is a relational database service, which is not suitable for storing video files. While you *could* store the video files as BLOBs in the database, it's not an optimal solution for video storage due to performance and scalability limitations. RDS is better suited for structured data.\n\n**Why option 3 is incorrect:**\nsuggests using Amazon DynamoDB. DynamoDB is a NoSQL database, which is not an ideal solution for storing large video files. While DynamoDB can store binary data, it's not designed for storing and serving large objects like videos. S3 or EFS are better choices for this use case.\n\n**Why option 4 is incorrect:**\nsuggests using Amazon S3 Glacier Deep Archive. S3 Glacier Deep Archive is designed for long-term archival storage with infrequent access. It's not suitable for a video backup service where users need to access their videos relatively frequently. The retrieval times for Glacier Deep Archive are too long for this use case.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A Machine Learning research group uses a proprietary computer vision application hosted on an Amazon EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time. As a solutions architect, which of the following solutions would you recommend for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 Meta-Data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 User-Data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 Instance Hibernate",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon EC2 Instance Hibernate, is the correct solution. EC2 Hibernate allows you to stop an instance and later resume it from the exact point it was stopped. This means the application's state, including the loaded auxiliary software, is preserved in memory and written to the EBS root volume. When the instance is started again, it resumes from this saved state, bypassing the 3-minute bootstrap process. This significantly reduces the application's startup time, meeting the research group's requirement. Hibernate is suitable for applications that take a long time to initialize or require specific memory states.\n\n**Why option 0 is incorrect:**\nusing Amazon EC2 Meta-Data, is incorrect. EC2 Meta-Data provides information *about* the instance itself, such as its instance ID, public IP address, and AMI ID. It does not help in reducing the application bootstrap time. Meta-Data is used for configuration and management, not for preserving application state across stop/start cycles.\n\n**Why option 1 is incorrect:**\ncreating an Amazon Machine Image (AMI) and launching your Amazon EC2 instances from that, is incorrect. While creating an AMI allows you to capture the state of the instance at a specific point in time, it doesn't eliminate the startup time. When you launch a new instance from an AMI, the application still needs to go through its initialization process, including executing the auxiliary software. The AMI provides a pre-configured environment, but it doesn't preserve the running state of the application like Hibernate does.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 12,
    "text": "An enterprise is building a secure business intelligence API using Amazon API Gateway to serve internal users with confidential analytics data. The API must be accessible only from a set of trusted IP addresses that are part of the organization's internal network ranges. No external IP traffic should be able to invoke the API. A solutions architect must design this access control mechanism with the least operational complexity. What should the architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a resource policy for the API Gateway API that explicitly denies access to all IP addresses except those listed in an allow list",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the API Gateway resource to an on-premises server using AWS Outposts. Apply host-based firewall rules to filter allowed IPs",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the security group that is attached to API Gateway to allow only traffic from specific IP addresses",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the API Gateway as a regional API in a public subnet and associate the subnet with a security group that permits inbound traffic only from trusted IP ranges",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because API Gateway resource policies provide a fine-grained access control mechanism. By creating a resource policy that explicitly denies access to all IP addresses except those listed in an allow list, the architect can effectively restrict access to the API only to the trusted IP ranges. This approach is straightforward to implement and manage, minimizing operational complexity. Resource policies are attached directly to the API Gateway resource, making the access control rules clear and centralized. This approach avoids the need for more complex infrastructure deployments or network configurations.\n\n**Why option 1 is incorrect:**\nis incorrect because deploying API Gateway to an on-premises server using AWS Outposts adds significant operational complexity. Managing an Outpost involves additional infrastructure management overhead compared to using native AWS services. While host-based firewalls can filter IPs, this approach is more complex than using API Gateway's built-in resource policies. This also negates the benefits of using a managed service like API Gateway.\n\n**Why option 2 is incorrect:**\nis incorrect because security groups in AWS are stateful and operate at the instance level, not the API Gateway level. While you can use security groups to control traffic to backend resources that API Gateway integrates with, you cannot directly attach a security group to an API Gateway API to restrict access based on source IP addresses. API Gateway is a managed service and does not expose the underlying EC2 instances that would allow for security group attachment. Furthermore, API Gateway does not have a security group associated with it in the same way an EC2 instance does.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying API Gateway as a regional API in a public subnet and using security groups is not the most secure or operationally efficient solution. While security groups can restrict inbound traffic, placing the API Gateway in a public subnet exposes it to the internet, even if the security group restricts access. This increases the attack surface and requires careful management of the security group rules. A resource policy provides a more direct and centralized way to control access based on IP addresses without the need for a public subnet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms: Which is the MOST operationally efficient solution?",
    "options": [
      {
        "id": 0,
        "text": "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation",
        "correct": true
      },
      {
        "id": 1,
        "text": "Server-side encryption (SSE-S3) with automatic key rotation",
        "correct": false
      },
      {
        "id": 2,
        "text": "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nServer-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation, is the most operationally efficient solution. SSE-KMS provides a good balance between security and ease of management. AWS KMS handles the key management aspects, including encryption, decryption, and key rotation. Automatic key rotation further reduces the operational burden by automatically rotating the keys without requiring manual intervention. This ensures that the keys are regularly updated, enhancing security and meeting compliance requirements. Using KMS also provides audit trails through CloudTrail, which is beneficial for compliance.\n\n**Why option 1 is incorrect:**\nServer-side encryption (SSE-S3) with automatic key rotation, is incorrect because SSE-S3 uses keys managed by AWS internally. While it's easy to implement, it offers less control and visibility compared to SSE-KMS. The question emphasizes data security and compliance norms, which are better addressed with KMS due to its enhanced control and auditability. Although the question mentions automatic key rotation, SSE-S3 does not offer automatic key rotation in the same way as KMS. The keys are rotated internally by AWS, but the user has no control or visibility over this process.\n\n**Why option 2 is incorrect:**\nServer-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation, is incorrect because manual key rotation introduces operational overhead. The question specifically asks for the *most* operationally efficient solution. Manual key rotation requires administrators to actively manage the key rotation process, which can be time-consuming and error-prone. Automatic key rotation is preferred for operational efficiency.\n\n**Why option 3 is incorrect:**\nServer-side encryption with customer-provided keys (SSE-C) with automatic key rotation, is incorrect for several reasons. First, SSE-C requires the customer to manage the encryption keys themselves, which adds significant operational overhead. The customer is responsible for generating, storing, and rotating the keys. Second, the question does not specify a need for customer-managed keys. Third, while you can rotate the keys you provide, the 'automatic' part is misleading as it would require significant custom scripting and infrastructure to implement and manage, making it far from operationally efficient. The customer would need to handle the key rotation process entirely, which is not ideal for operational efficiency.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available. As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it distributes the application across three Availability Zones with two instances in each AZ. This configuration ensures that even if one AZ fails, the remaining two AZs will still have a total of four instances available, meeting the minimum requirement for acceptable end-user experience. Distributing across three AZs provides better fault tolerance than two AZs, and using only two instances per AZ minimizes the cost compared to launching more instances. This solution provides the best balance between high availability and cost efficiency.\n\n**Why option 0 is incorrect:**\nis incorrect because it launches four instances in each of the two Availability Zones, resulting in a total of eight instances. While this provides high availability, it is more expensive than necessary to meet the requirement of maintaining at least four instances. The question specifically asks for the solution with MINIMUM cost.\n\n**Why option 1 is incorrect:**\nis incorrect because it launches only two instances in each of the two Availability Zones. If one AZ fails, only two instances will remain, which is below the required minimum of four instances. This does not meet the high availability requirement.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 15,
    "text": "Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ)us-east-1aas it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ)us-east-1alike so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour. Which of the following instances would be terminated per the default termination policy?",
    "options": [
      {
        "id": 0,
        "text": "Instance C",
        "correct": false
      },
      {
        "id": 1,
        "text": "Instance A",
        "correct": false
      },
      {
        "id": 2,
        "text": "Instance B",
        "correct": true
      },
      {
        "id": 3,
        "text": "Instance D",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nInstance B) is correct. The default termination policy follows these steps:\n1. Find the Availability Zone with the most instances and terminate an instance from that AZ.\n2. If multiple instances are in the AZ, choose the instance that uses the oldest launch configuration or launch template.\n\nIn this case, Instance A uses the oldest launch template, and Instance B uses the oldest launch configuration. The default policy prioritizes launch configurations over launch templates. Therefore, Instance B will be terminated.\n\n**Why option 0 is incorrect:**\nInstance C) is incorrect. The default termination policy prioritizes older launch configurations/templates. Instance C has the newest launch configuration, making it less likely to be terminated according to the default policy.\n\n**Why option 1 is incorrect:**\nInstance A) is incorrect. While Instance A has the oldest launch template, the default termination policy prioritizes instances using the oldest *launch configuration* before considering launch templates. Since Instance B has the oldest launch configuration, it will be terminated before Instance A.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform. Which of the following solutions would have the LEAST amount of downtime?",
    "options": [
      {
        "id": 0,
        "text": "Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it provides a robust and automated failover solution. Setting up a Route 53 failover record allows for automatic redirection of traffic to AWS in case the on-premises data center fails. Running application servers on EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group ensures that the application is highly available and can handle traffic spikes. The Auto Scaling group automatically adjusts the number of EC2 instances based on demand, providing scalability and resilience. Using AWS Storage Gateway with stored volumes allows for asynchronous data replication from on-premises to S3, ensuring data consistency. The stored volume configuration means the most frequently accessed data is stored locally on the on-premises server for low latency access, while the entire dataset is backed up to S3. This combination minimizes downtime by automatically switching traffic and scaling resources in AWS while ensuring data consistency.\n\n**Why option 0 is incorrect:**\nis incorrect because it relies on a script to provision EC2 instances. This approach is slower and less automated than using an Auto Scaling group. The time it takes to execute the script and provision the instances will result in more downtime during failover. While Storage Gateway provides data backup, the manual provisioning of EC2 instances is a significant drawback.\n\n**Why option 2 is incorrect:**\nis incorrect because it uses a Lambda function to execute a CloudFormation template to create an Application Load Balancer. This adds unnecessary complexity and latency to the failover process. Creating the ALB should be part of the pre-configured failover environment, not something that is dynamically created during the failover event. While Direct Connect provides a dedicated network connection, it doesn't directly contribute to minimizing downtime during the failover process itself. The Lambda function execution adds latency.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses a Lambda function to launch only two EC2 instances, which is insufficient for a production environment and lacks scalability. It also relies on a script to provision EC2 instances, which is slower and less automated than using an Auto Scaling group. The time it takes to execute the script and provision the instances will result in more downtime during failover. While Storage Gateway provides data backup and Direct Connect provides a dedicated network connection, the manual provisioning of EC2 instances and the limited number of instances are significant drawbacks.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?",
    "options": [
      {
        "id": 0,
        "text": "Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a VPC peering connection between all virtual private cloud (VPCs)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Private Link between all the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\ncreating a VPC in one account and sharing subnets using Resource Access Manager (RAM), is the most cost-effective solution. RAM allows you to share resources like subnets across accounts within your AWS Organization. You only pay for the VPC and the EC2 instances running within it. Sharing the subnets doesn't incur additional costs. This avoids the complexities and costs associated with peering, Private Link, or Transit Gateway, especially when the primary goal is simple connectivity.\n\n**Why option 1 is incorrect:**\nCreating VPC peering connections between all VPCs would be complex and expensive. With 'n' VPCs, you'd need n*(n-1)/2 peering connections. This becomes unmanageable and costly as the number of accounts and VPCs grows. VPC peering also has limitations regarding overlapping CIDR blocks, which could further complicate the setup.\n\n**Why option 2 is incorrect:**\nCreating Private Link between all EC2 instances is not the intended use case for Private Link. Private Link is designed for providing private access to services, not for general EC2 instance-to-instance communication. It would be significantly more complex and expensive than necessary. Private Link is more suitable for exposing services to other VPCs or accounts, not for general network connectivity.\n\n**Why option 3 is incorrect:**\nCreating an AWS Transit Gateway (TGW) is a viable solution for connecting multiple VPCs, but it's generally more expensive than using RAM for simple connectivity. TGW involves costs for the TGW itself, attachments to each VPC, and data processing. While TGW offers more advanced features like centralized routing and security policies, it's overkill and not the cheapest option for the stated requirement of basic private communication.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner. What could be the reason for this denial of permission for the bucket owner?",
    "options": [
      {
        "id": 0,
        "text": "By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster",
        "correct": true
      },
      {
        "id": 1,
        "text": "When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures",
        "correct": false
      },
      {
        "id": 2,
        "text": "The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress",
        "correct": false
      },
      {
        "id": 3,
        "text": "When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis correct because, by default, the AWS account that uploads an object to S3 owns that object, regardless of which bucket it's placed in. In this scenario, the Redshift cluster's AWS account is uploading the objects. Therefore, the Redshift account owns the objects, and the S3 bucket owner does not automatically have access. To grant the S3 bucket owner access, the Redshift account needs to explicitly grant permissions using ACLs or bucket policies in conjunction with object ACLs.\n\n**Why option 1 is incorrect:**\nis incorrect because while bucket policies are crucial for cross-account access, the core issue here is object ownership. Even with correct bucket policies allowing Redshift to write, the bucket owner still won't implicitly own or have access to the objects. The problem isn't necessarily an *erroneous* policy, but rather the *lack* of explicit permissions granted to the bucket owner on the objects themselves.\n\n**Why option 2 is incorrect:**\nis incorrect because while the S3 bucket owner typically has access to objects within their bucket, this is not the case when the object is uploaded by a different AWS account. The statement about permissions being set after copying is generally true, but irrelevant here. The issue isn't about a write operation in progress, but rather the fundamental concept of object ownership. S3 operations are generally atomic and quick; the problem is not a delay in permission application.\n\n**Why option 3 is incorrect:**\nis incorrect because the S3 bucket owner does *not* get implicit permissions when objects are uploaded from a different AWS account. This is the opposite of what actually happens. The problem is not an upload error, but a design issue regarding object ownership and permissions. Manual access from the AWS console is a *solution*, not the cause of the problem.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS. How should you configure the security groups? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443",
        "correct": true
      },
      {
        "id": 1,
        "text": "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80",
        "correct": false
      },
      {
        "id": 2,
        "text": "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80",
        "correct": false
      },
      {
        "id": 3,
        "text": "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80",
        "correct": true
      },
      {
        "id": 4,
        "text": "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432",
        "correct": true
      },
      {
        "id": 5,
        "text": "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      3,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because the ALB needs to accept HTTPS traffic from the internet (or a specific CIDR block, but 'anywhere' is acceptable in this context). Option 3 is correct because the EC2 instances in the Auto Scaling Group need to receive HTTP traffic from the ALB. Option 4 is correct because the RDS database needs to accept PostgreSQL connections from the EC2 instances in the Auto Scaling Group on port 5432.\n\n**Why option 3 is correct:**\nThis is correct because the ALB needs to accept HTTPS traffic from the internet (or a specific CIDR block, but 'anywhere' is acceptable in this context). Option 3 is correct because the EC2 instances in the Auto Scaling Group need to receive HTTP traffic from the ALB. Option 4 is correct because the RDS database needs to accept PostgreSQL connections from the EC2 instances in the Auto Scaling Group on port 5432.\n\n**Why option 4 is correct:**\nThis is correct because the ALB needs to accept HTTPS traffic from the internet (or a specific CIDR block, but 'anywhere' is acceptable in this context). Option 3 is correct because the EC2 instances in the Auto Scaling Group need to receive HTTP traffic from the ALB. Option 4 is correct because the RDS database needs to accept PostgreSQL connections from the EC2 instances in the Auto Scaling Group on port 5432.\n\n**Why option 1 is incorrect:**\nis incorrect because while the ALB *could* accept traffic on port 80, the question states that the ALB provides HTTPS termination. Therefore, the primary entry point should be port 443. Allowing port 80 would expose the application to unencrypted traffic, which is generally undesirable.\n\n**Why option 2 is incorrect:**\nis incorrect because the RDS database should not receive traffic directly from the EC2 instances on port 80. Port 80 is typically used for HTTP traffic, and the database communicates using the PostgreSQL protocol on port 5432.\n\n**Why option 5 is incorrect:**\nis incorrect because the RDS database should not initiate connections to the EC2 instances. The EC2 instances initiate connections to the RDS database to query and update data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved. As a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?",
    "options": [
      {
        "id": 0,
        "text": "Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations",
        "correct": false
      },
      {
        "id": 1,
        "text": "Remove any overlapping namespaces for the private and public hosted zones",
        "correct": false
      },
      {
        "id": 2,
        "text": "Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable DNS hostnames and DNS resolution for private hosted zones",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because enabling 'DNS hostnames' and 'DNS resolution' in the VPC is essential for the VPC to use the Route 53 private hosted zone. 'DNS resolution' allows instances within the VPC to resolve DNS queries using the Amazon DNS server. 'DNS hostnames' assigns a hostname to instances, which can then be resolved. Without these enabled, the VPC will not be able to resolve queries against the private hosted zone, leading to the observed resolution failure. This is a fundamental requirement for private hosted zones to function correctly within a VPC.\n\n**Why option 0 is incorrect:**\nis incorrect because while NS and SOA records are important for DNS delegation and zone information, they are automatically managed by Route 53 for private hosted zones. The administrator doesn't typically need to manually configure these records. The problem is not with the records themselves, but with the VPC's ability to use the private hosted zone.\n\n**Why option 1 is incorrect:**\nis incorrect because overlapping namespaces between private and public hosted zones can cause resolution issues, but this is a separate concern. The primary issue in the described scenario is the lack of basic VPC configuration to enable DNS resolution within the private hosted zone. While resolving namespace conflicts is good practice, it doesn't address the immediate problem of DNS queries not resolving at all.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "An enterprise uses a centralized Amazon S3 bucket to store logs and reports generated by multiple analytics services. Each service writes to and reads from a dedicated prefix (folder path) in the bucket. The company wants to enforce fine-grained access control so that each service can access only its own prefix, without being able to see or modify other services' data. The solution must support scalable and maintainable permissions management with minimal operational overhead. Which approach will best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a single S3 bucket policy that lists all object ARNs under each prefix and grants permissions accordingly. Use resource-level permissions to restrict access to individual services",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure individual S3 access points for each analytics service. Attach access point policies that restrict access to only the relevant prefix in the S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy Amazon Macie to classify the objects in the bucket by prefix and apply automated object-level access policies to each object based on service tags",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create separate IAM users for each service. Manually assign inline IAM policies to grant read/write permissions to the S3 bucket. Reference specific object names in the policy for each user",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because S3 Access Points provide a scalable and maintainable way to manage access to specific prefixes within an S3 bucket. Each Access Point can have its own policy that restricts access to a particular prefix. This allows each analytics service to access only its designated prefix without being able to see or modify other services' data. Access Points simplify permissions management by decoupling access control from the bucket policy and allowing for more granular control at the prefix level. This approach minimizes operational overhead as permissions are managed at the Access Point level, rather than through a complex bucket policy.\n\n**Why option 0 is incorrect:**\nis incorrect because creating a single S3 bucket policy with numerous object ARNs and resource-level permissions would become complex and difficult to manage as the number of services and prefixes grows. Maintaining and updating such a policy would be operationally burdensome and prone to errors. It does not scale well.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because creating separate IAM users for each service and assigning inline IAM policies is not scalable or maintainable. Managing individual IAM users and their policies for each service would be a significant operational overhead. Referencing specific object names in the policy is also impractical as the object names are likely to change frequently. This approach does not scale well and is not recommended.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "Upon a security review of your AWS account, an AWS consultant has found that a few Amazon RDS databases are unencrypted. As a Solutions Architect, what steps must be taken to encrypt the Amazon RDS databases?",
    "options": [
      {
        "id": 0,
        "text": "Enable encryption on the Amazon RDS database using the AWS Console",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ",
        "correct": false
      },
      {
        "id": 3,
        "text": "Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it outlines the standard procedure for encrypting an unencrypted RDS database. You first take a snapshot of the existing database. Then, you copy the snapshot and specify encryption for the copied snapshot. Finally, you restore a new RDS instance from the encrypted snapshot. After verifying the new instance, the original unencrypted database can be terminated. This method avoids data loss and ensures the new database is encrypted at rest.\n\n**Why option 0 is incorrect:**\nis incorrect because you cannot directly enable encryption on an existing unencrypted RDS instance. RDS encryption must be enabled at the time of database creation or during restoration from an encrypted snapshot.\n\n**Why option 1 is incorrect:**\nis incorrect because while creating an encrypted read replica and promoting it is a valid approach, it's less efficient than using snapshots. Creating a read replica involves replicating data in real-time, which can take longer and consume more resources than creating a snapshot. Also, the question asks for the steps to be taken, and this option is not the most direct or efficient.\n\n**Why option 2 is incorrect:**\nis incorrect. While Multi-AZ provides high availability, it doesn't directly address the encryption requirement. Enabling Multi-AZ doesn't automatically encrypt the standby instance. Even if the standby instance *could* be encrypted independently (which it can't, it inherits encryption settings from the primary), failing over and disabling Multi-AZ is a complex and unnecessary process compared to the snapshot method.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "An IT company wants to optimize the costs incurred on its fleet of 100 Amazon EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?",
    "options": [
      {
        "id": 0,
        "text": "Purchase 70 on-demand instances and 30 reserved instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase 70 reserved instances (RIs) and 30 spot instances",
        "correct": true
      },
      {
        "id": 2,
        "text": "Purchase 70 reserved instances and 30 on-demand instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase 70 on-demand instances and 30 spot instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\npurchasing 70 reserved instances (RIs) and 30 spot instances, is the most cost-optimal solution. Reserved Instances provide significant cost savings compared to On-Demand instances for long-term, predictable workloads. Since 70 instances need to be always available, RIs are a good fit. Spot instances are ideal for batch jobs that can tolerate interruptions, as they offer substantial discounts compared to On-Demand instances. The 30 batch processing instances can leverage spot instances to minimize costs, accepting the risk of occasional interruptions, which is acceptable given the delay tolerance.\n\n**Why option 0 is incorrect:**\npurchasing 70 on-demand instances and 30 reserved instances, is incorrect because it uses On-Demand instances for the 70 instances that require constant availability. On-Demand instances are the most expensive option for long-term workloads. Using reserved instances for the 30 instances is not the most cost-effective approach, as spot instances are cheaper for workloads that can tolerate interruptions.\n\n**Why option 2 is incorrect:**\npurchasing 70 reserved instances and 30 on-demand instances, is less cost-effective than using spot instances for the batch jobs. While RIs are appropriate for the 70 always-available instances, On-Demand instances are more expensive than Spot instances for the 30 batch job instances that can tolerate interruptions.\n\n**Why option 3 is incorrect:**\npurchasing 70 on-demand instances and 30 spot instances, is the least cost-effective option. Using On-Demand instances for the 70 instances that require constant availability is significantly more expensive than using Reserved Instances. While Spot instances are suitable for the 30 batch job instances, the overall cost will be much higher due to the On-Demand instances.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 24,
    "text": "A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience. The company is looking at alternate database options and migrating database engines if required. What would you suggest?",
    "options": [
      {
        "id": 0,
        "text": "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora",
        "correct": true
      },
      {
        "id": 1,
        "text": "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump",
        "correct": false
      },
      {
        "id": 2,
        "text": "Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nsuggests using Amazon Aurora MySQL with Multi-AZ Aurora Replicas. Aurora Replicas provide read scaling capabilities and Multi-AZ deployments enhance availability. Creating the dev database by restoring from automated backups of Aurora minimizes the impact on the production database. Aurora's backup and restore process is designed to be fast and efficient, reducing the performance impact compared to a full database copy. Aurora MySQL is a good choice for read-heavy workloads and offers improved performance compared to standard MySQL.\n\n**Why option 1 is incorrect:**\nsuggests using Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restoring the dev database via `mysqldump`. While Aurora Replicas and Multi-AZ deployment address read scaling and availability, using `mysqldump` to create the dev database is not the most efficient approach. `mysqldump` can be resource-intensive and cause performance degradation on the source database, especially during peak business hours. Restoring from automated backups is a faster and less impactful method.\n\n**Why option 2 is incorrect:**\nsuggests using Amazon RDS for SQL Server with a Multi-AZ deployment and read replicas, using the read replica as the dev database. While this addresses read scaling and availability, it doesn't solve the problem of creating the dev database without impacting production performance. Directly using a read replica as a development database can lead to issues. Development activities, such as schema changes or data modifications, could impact the read replica's ability to stay in sync with the primary database. Furthermore, the question states they are open to migrating database engines, and this option keeps them on SQL Server.\n\n**Why option 3 is incorrect:**\nsuggests using Amazon RDS for MySQL with a Multi-AZ deployment and using the standby instance as the dev database. This is incorrect because the standby instance in a Multi-AZ deployment is for failover purposes and is not intended to be used for read operations or development activities. Using the standby instance directly would compromise the high availability setup and could lead to data corruption or inconsistencies if development activities interfere with the replication process.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "A media production studio is building a content rendering and editing platform on AWS. The editing workstations and rendering tools require access to shared files over the SMB (Server Message Block) protocol. The studio wants a managed storage solution that is simple to set up, integrates easily with SMB clients, and minimizes ongoing operational tasks. Which solution will best meet the requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Set up an AWS Storage Gateway Volume Gateway in cached volume mode. Attach the volume as an iSCSI device to the application server and configure a file system with SMB sharing enabled",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch an Amazon EC2 Windows instance and manually configure a Windows file share. Use this instance to serve SMB access to application clients",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 with Transfer Acceleration enabled. Configure the application to upload and download files over HTTPS using signed URLs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an Amazon FSx for Windows File Server file system. Mount the file system using the SMB protocol on the media servers",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nprovisioning an Amazon FSx for Windows File Server file system and mounting it using the SMB protocol, is the correct answer. FSx for Windows File Server is a fully managed Windows file server service built on Windows Server. It natively supports the SMB protocol, making it easy to integrate with Windows-based editing workstations and rendering tools. It eliminates the need for manual server setup, patching, and management, significantly reducing administrative overhead. It provides features like data deduplication, snapshots, and integration with Active Directory, which are beneficial for media production workflows.\n\n**Why option 0 is incorrect:**\nsetting up an AWS Storage Gateway Volume Gateway in cached volume mode, is incorrect because it involves more administrative overhead. While it provides SMB access, it requires setting up an EC2 instance, configuring iSCSI, and managing the underlying file system. This adds complexity and operational burden compared to a fully managed solution like FSx for Windows File Server. Storage Gateway is more suitable for hybrid cloud scenarios or when integrating on-premises storage with AWS.\n\n**Why option 1 is incorrect:**\nlaunching an Amazon EC2 Windows instance and manually configuring a Windows file share, is incorrect because it requires significant manual configuration and ongoing management. This includes installing and configuring the Windows file server role, managing security, patching the operating system, and ensuring high availability. This approach does not minimize administrative overhead and is not a managed solution. It also lacks the scalability and features of a dedicated file server service like FSx for Windows File Server.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an Amazon EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone (AZ). The development team wants to improve application availability before the go-live. Given that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it deploys the web tier across two Availability Zones (AZs) behind an Elastic Load Balancer (ELB). This provides high availability for the web tier. If one AZ fails, the ELB will route traffic to the healthy AZ. Deploying the RDS MySQL database in a Multi-AZ configuration provides high availability for the database tier. In case of a failure in the primary AZ, RDS will automatically failover to the standby AZ. This setup is resource-efficient because it leverages AZs within a single region, minimizing latency for US-based users and avoiding the complexity and cost of cross-region replication.\n\n**Why option 0 is incorrect:**\nis incorrect because deploying the web tier in two regions introduces unnecessary complexity and latency for users located only in the US. Cross-region deployments are typically used for disaster recovery or global user bases. While RDS read replicas can improve read performance, they do not provide automatic failover in case of a primary database failure. Read replicas are primarily for scaling read operations, not for high availability of the primary database.\n\n**Why option 1 is incorrect:**\nis incorrect because while deploying the web tier across two AZs behind an ELB is a good practice for high availability, using RDS read replicas alone does not provide automatic failover for the database tier. If the primary RDS instance fails, the application will experience downtime until a new primary instance is promoted or restored. Multi-AZ is needed for automatic failover.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 27,
    "text": "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features. As a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
    "options": [
      {
        "id": 0,
        "text": "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because using IAM roles for EC2 instances is the AWS-recommended and most secure way to grant permissions to AWS services. When an IAM role is attached to an EC2 instance, the AWS Security Token Service (STS) automatically provides temporary credentials to the instance. These credentials are automatically rotated, eliminating the need to manage long-term credentials within the instance itself. This approach minimizes the risk of credentials being exposed or compromised. The application running on the EC2 instance can then use the AWS SDK to access S3 and DynamoDB without needing to explicitly manage credentials.\n\n**Why option 0 is incorrect:**\nis incorrect because while encrypting credentials is better than storing them in plain text, it still involves managing and storing credentials on the EC2 instance. This increases the risk of exposure if the encryption key is compromised or the instance is accessed by an unauthorized user. It's also more complex to implement and maintain than using IAM roles.\n\n**Why option 1 is incorrect:**\nis incorrect because storing AWS credentials (access key ID and secret access key) directly in a configuration file within the application code is a highly insecure practice. If the EC2 instance is compromised, or the code repository is exposed, the credentials could be easily accessed and used to gain unauthorized access to the AWS account. This violates security best practices and should be avoided at all costs.\n\n**Why option 2 is incorrect:**\nis incorrect because configuring AWS CLI with a valid IAM user's credentials on the EC2 instance and then using shell scripts to access S3 and DynamoDB is less secure than using IAM roles. It still involves managing long-term credentials on the instance, and invoking shell scripts adds complexity and potential security vulnerabilities. While slightly better than storing credentials directly in code, it's not the recommended approach.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses Amazon RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage. As a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Enable storage auto-scaling for Amazon RDS MySQL",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create read replica for Amazon RDS MySQL",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nEnabling storage auto-scaling for Amazon RDS MySQL is the most appropriate solution. RDS storage auto-scaling automatically increases storage capacity when needed, based on the database's storage consumption. This requires minimal configuration and no code changes. It directly addresses the problem of potential storage exhaustion with minimal administrative overhead. It's a built-in feature designed for this exact scenario.\n\n**Why option 1 is incorrect:**\nCreating a read replica addresses read scalability and availability, not storage capacity. Read replicas replicate data from the primary instance, but they don't automatically increase the storage capacity of the primary instance. While read replicas can offload read traffic, they don't solve the problem of the primary database running out of storage.\n\n**Why option 2 is incorrect:**\nMigrating to Amazon DynamoDB would require significant development effort to adapt the application to a NoSQL database. DynamoDB is a fundamentally different database type than MySQL, and the application would need to be rewritten to work with DynamoDB's data model and API. This contradicts the requirement for minimal development effort. While DynamoDB does automatically scale storage, the migration effort is too high.\n\n**Why option 3 is incorrect:**\nMigrating to Amazon Aurora MySQL would also require some development and testing effort, although less than migrating to DynamoDB. While Aurora offers storage auto-scaling, the migration process itself involves downtime and configuration changes, making it a more complex solution than simply enabling storage auto-scaling on the existing RDS MySQL instance. The question specifically asks for a solution that requires *minimum* development and systems administration effort.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 29,
    "text": "A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic. As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\n'Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer', is the correct answer. Application Load Balancers (ALBs) have built-in integration with Cognito User Pools. This integration allows the ALB to authenticate users before routing traffic to the backend EC2 instances. The ALB handles the authentication process, including redirecting unauthenticated users to the Cognito hosted UI for sign-in or sign-up, and then verifying the JWT tokens issued by Cognito. This approach requires minimal development effort because the authentication logic is handled by the ALB, and the EC2 instances only receive authenticated requests. The ALB can be configured to forward user information (e.g., user ID) to the EC2 instances via HTTP headers, allowing the application to personalize the user experience without handling authentication directly.\n\n**Why option 0 is incorrect:**\n'Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer', is incorrect. Cognito Identity Pools (now known as Cognito Federated Identities) are primarily used to grant temporary AWS credentials to users so they can access AWS resources directly. They don't handle user authentication in the same way as User Pools. While Identity Pools can be linked to User Pools, they are not the primary mechanism for authenticating users before they reach the application servers behind an ALB. The ALB's direct integration is with User Pools, not Identity Pools.\n\n**Why option 1 is incorrect:**\n'Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution', is incorrect. While CloudFront can be integrated with Lambda@Edge to perform custom authentication, it's not the most straightforward or minimal-effort solution for this scenario. Integrating Cognito User Pools directly with CloudFront requires more complex configurations using Lambda@Edge functions to handle authentication redirects and token validation. The ALB integration with Cognito User Pools is simpler and requires less custom code, making it a better fit for the 'minimal development effort' requirement. Also, authenticating at the ALB level ensures that all traffic reaching the EC2 instances is authenticated, regardless of the path taken (e.g., bypassing CloudFront).\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?",
    "options": [
      {
        "id": 0,
        "text": "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases",
        "correct": false
      },
      {
        "id": 1,
        "text": "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon Aurora Global Database, is the correct solution. Aurora Global Database is designed for globally distributed applications, allowing for low-latency reads in multiple regions. It replicates data with minimal latency to secondary regions, enabling fast local reads. It maintains the relational database schema, addressing the requirement of not moving away from the underlying schema. It is also more cost-effective than deploying separate MySQL instances in each region (option 0) and more suitable than migrating to a different database technology like Redshift (option 1) or DynamoDB (option 3) when the relational schema needs to be maintained.\n\n**Why option 0 is incorrect:**\nis incorrect because while it provides local reads, it involves significant overhead in managing multiple EC2 instances and MySQL databases across different regions. This is less cost-effective and more complex to manage compared to Aurora Global Database. Data synchronization between these independent databases would also be a challenge.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Redshift is a data warehouse service optimized for analytical workloads, not transactional workloads like an ACM application. Migrating the existing data to Redshift would require significant schema changes and application modifications, violating the requirement of not moving away from the underlying relational database schema. Also, Redshift is not designed for low-latency, real-time reads required by the application.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A multinational logistics company is migrating its core systems to AWS. As part of this migration, the company has built an Amazon S3–based data lake to ingest and analyze supply chain data from external carriers and vendors. While some vendors have adopted the company’s modern REST-based APIs for S3 uploads, others operate legacy systems that rely exclusively on SFTP for file transfers. These vendors are unable or unwilling to modify their workflows to support S3 APIs. The company wants to provide these vendors with an SFTP-compatible solution that allows direct uploads to Amazon S3, and must use fully managed AWS services to avoid managing any infrastructure. It must also support identity federation so that internal teams can map vendor access securely to specific S3 buckets or prefixes. Which combination of options will provide a scalable and low-maintenance solution for this use case? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 1 together provide the best solution. Option 0 leverages AWS Transfer Family with SFTP enabled to provide a fully managed SFTP endpoint. Configuring it to store uploaded files directly in an S3 bucket eliminates the need for custom infrastructure. Setting up IAM roles mapped to each vendor allows for secure bucket or prefix access, fulfilling the requirement for fine-grained permissions. Option 1 complements this by configuring S3 bucket policies to use IAM role-based access control, ensuring that only authorized vendors can access specific S3 locations. Integrating Transfer Family with an identity provider (Cognito or a custom provider) allows for federation, enabling internal teams to manage vendor access securely and map it to specific S3 buckets or prefixes. This combination provides a secure, scalable, and low-maintenance solution that meets all the requirements.\n\n**Why option 1 is correct:**\nOptions 0 and 1 together provide the best solution. Option 0 leverages AWS Transfer Family with SFTP enabled to provide a fully managed SFTP endpoint. Configuring it to store uploaded files directly in an S3 bucket eliminates the need for custom infrastructure. Setting up IAM roles mapped to each vendor allows for secure bucket or prefix access, fulfilling the requirement for fine-grained permissions. Option 1 complements this by configuring S3 bucket policies to use IAM role-based access control, ensuring that only authorized vendors can access specific S3 locations. Integrating Transfer Family with an identity provider (Cognito or a custom provider) allows for federation, enabling internal teams to manage vendor access securely and map it to specific S3 buckets or prefixes. This combination provides a secure, scalable, and low-maintenance solution that meets all the requirements.\n\n**Why option 2 is incorrect:**\nsuggests using Amazon AppFlow. While AppFlow can move data to S3, it's primarily designed for data integration from SaaS applications, not for acting as an SFTP server. It also introduces complexity with scheduling and transformation, which are not necessary given the Transfer Family solution. The requirement is for direct uploads, not extraction and transformation.\n\n**Why option 3 is incorrect:**\nsuggests using Route 53 private hosted zones for vendor-specific subdomains. While this adds a layer of organization, it doesn't directly address the core requirements of secure file transfer and identity federation. Route 53 is for DNS management, not access control or file transfer. It also doesn't contribute to the fully managed aspect of the solution.\n\n**Why option 4 is incorrect:**\nsuggests using an EC2 instance with a custom SFTP server. This violates the requirement for a fully managed solution, as it involves managing the EC2 instance, operating system, SFTP server software, and cron jobs. It also adds operational overhead and complexity, making it a less desirable option compared to AWS Transfer Family.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "The engineering manager for a content management application wants to set up Amazon RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up Amazon RDS read replicas. Which of the following would you identify as correct regarding the data transfer charges for Amazon RDS read replicas?",
    "options": [
      {
        "id": 0,
        "text": "There are data transfer charges for replicating data across AWS Regions",
        "correct": true
      },
      {
        "id": 1,
        "text": "There are data transfer charges for replicating data within the same Availability Zone (AZ)",
        "correct": false
      },
      {
        "id": 2,
        "text": "There are no data transfer charges for replicating data across AWS Regions",
        "correct": false
      },
      {
        "id": 3,
        "text": "There are data transfer charges for replicating data within the same AWS Region",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because replicating data across AWS Regions involves transferring data over the internet (or AWS backbone) between two geographically distinct locations. This data transfer incurs charges based on the amount of data transferred. AWS charges for data going *out* of a region. Replicating to a different region involves data leaving the source region, hence the charge.\n\n**Why option 1 is incorrect:**\nis incorrect because data transfer between instances within the same Availability Zone (AZ) is free. RDS read replicas within the same AZ do not incur data transfer charges.\n\n**Why option 2 is incorrect:**\nis incorrect because, as explained in the correct answer explanation, there *are* data transfer charges for replicating data across AWS Regions.\n\n**Why option 3 is incorrect:**\nis incorrect because data transfer within the same AWS Region (even across different Availability Zones) is generally free for RDS replication. The key distinction is whether the data is leaving the region or not.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves theAdministratorAccessmanaged policy. How should you proceed?",
    "options": [
      {
        "id": 0,
        "text": "For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy",
        "correct": false
      },
      {
        "id": 2,
        "text": "Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because IAM Permission Boundaries allow you to define the maximum permissions that an IAM entity (user or role) can have. By setting a permission boundary on each developer's IAM user, you can restrict the managed policies they are allowed to attach to themselves. This directly addresses the requirement of preventing privilege escalation by limiting the policies they can use, including `AdministratorAccess`. This approach provides granular control at the individual user level.\n\n**Why option 1 is incorrect:**\nis incorrect because attaching an IAM policy to the developers that prevents them from attaching the `AdministratorAccess` policy is not sufficient. A malicious developer could simply detach the policy that restricts them, and then attach the `AdministratorAccess` policy. This approach is easily circumvented and does not provide a strong security guarantee.\n\n**Why option 2 is incorrect:**\nis incorrect because applying a permission boundary to an IAM group restricts the permissions that the *group* can grant to its members. It doesn't prevent individual members from attaching policies to themselves. The question specifically asks about preventing developers from attaching policies to *themselves*, not about restricting the group's ability to grant permissions to its members. While groups are good for managing permissions, they don't solve the individual privilege escalation problem in this scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Service Control Policies (SCPs) operate at the AWS Organizations level and affect all accounts within the organization (or specific OUs). While SCPs *can* prevent users from attaching certain policies, they are typically used for broader, organization-wide governance and are not the best solution for restricting individual developers' actions within a single account. Using SCPs for this specific scenario is overkill and less granular than using permission boundaries.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 34,
    "text": "An engineering team wants to examine the feasibility of theuser datafeature of Amazon EC2 for an upcoming project. Which of the following are true about the Amazon EC2 user data configuration? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "By default, scripts entered as user data do not have root user privileges for executing",
        "correct": false
      },
      {
        "id": 1,
        "text": "When an instance is running, you can update user data by using root user credentials",
        "correct": false
      },
      {
        "id": 2,
        "text": "By default, user data is executed every time an Amazon EC2 instance is re-started",
        "correct": false
      },
      {
        "id": 3,
        "text": "By default, user data runs only during the boot cycle when you first launch an instance",
        "correct": true
      },
      {
        "id": 4,
        "text": "By default, scripts entered as user data are executed with root user privileges",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nis correct because, by default, user data scripts are designed to run only during the initial boot cycle when an EC2 instance is first launched. This allows for initial configuration and setup. Option 4 is also correct because user data scripts are executed with root user privileges by default. This is necessary to perform system-level configurations and installations.\n\n**Why option 4 is correct:**\nis correct because, by default, user data scripts are designed to run only during the initial boot cycle when an EC2 instance is first launched. This allows for initial configuration and setup. Option 4 is also correct because user data scripts are executed with root user privileges by default. This is necessary to perform system-level configurations and installations.\n\n**Why option 0 is incorrect:**\nis incorrect because, by default, scripts entered as user data *do* have root user privileges for execution. This is a fundamental aspect of how user data is designed to function, allowing for system-level changes.\n\n**Why option 1 is incorrect:**\nis incorrect because you cannot directly update the user data of a running instance. User data is processed during the instance launch. While you can simulate re-running user data scripts, you cannot directly modify the user data configuration of a running instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 35,
    "text": "A wildlife research organization uses IoT-based motion sensors attached to thousands of migrating animals to monitor their movement across regions. Every few minutes, a sensor checks for significant movement and sends updated location data to a backend application running on Amazon EC2 instances spread across multiple Availability Zones in a single AWS Region. Recently, an unexpected surge in motion data overwhelmed the application, leading to lost location records with no mechanism to replay missed data. A solutions architect must redesign the ingestion mechanism to prevent future data loss and to minimize operational overhead. What should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement an AWS IoT Core rule to route location updates directly from each sensor to Amazon SNS. Configure the application to poll the SNS topic for new messages",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue to buffer the incoming location data. Configure the backend application to poll the queue and process messages",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Data Firehose delivery stream to collect the motion data. Configure it to deliver data to an S3 bucket where the application scans and processes the files periodically",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a containerized service using Amazon ECS with an internal queue built into the application layer. Configure the motion sensors to send location updates directly to the container endpoints",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Amazon SQS provides a reliable and scalable queuing service that can buffer incoming location data. By configuring the backend application to poll the queue, it can process messages at its own pace, preventing data loss during surges. SQS offers at-least-once delivery, ensuring that messages are not lost. It also decouples the sensor data ingestion from the backend application, improving resilience. SQS is a managed service, which minimizes operational overhead.\n\n**Why option 0 is incorrect:**\nis incorrect because using Amazon SNS for this purpose is not ideal. SNS is primarily designed for fan-out messaging, where a single message is delivered to multiple subscribers. While the application could subscribe to the SNS topic, polling SNS is not a standard or efficient practice. SNS doesn't inherently provide buffering or guaranteed delivery in the same way SQS does, potentially leading to data loss during surges. Also, managing subscriptions and filtering messages in SNS can add operational complexity.\n\n**Why option 2 is incorrect:**\nis incorrect because while Amazon Data Firehose is suitable for streaming data to destinations like S3, it's not the best choice for real-time processing by an application. Firehose is designed for batch processing and analytics, not for immediate consumption by an application. The application would need to periodically scan and process files in S3, which adds latency and complexity. This approach also increases operational overhead due to the need for file management and processing logic. Furthermore, it doesn't provide a mechanism for replaying missed data easily.\n\n**Why option 3 is incorrect:**\nis incorrect because while using ECS with an internal queue might seem like a viable option, it introduces significant operational overhead. Managing a containerized service and its internal queue requires more effort than using a managed queuing service like SQS. Scaling the ECS service and ensuring the queue's capacity can be complex. Furthermore, if the ECS service fails, data in the internal queue could be lost. This option doesn't leverage the benefits of a managed service for queuing, making it less desirable than SQS.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 36,
    "text": "A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key. Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
    "options": [
      {
        "id": 0,
        "text": "Server-Side Encryption with Customer-Provided Keys (SSE-C)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Server-Side Encryption with Amazon S3 managed keys (SSE-S3)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nServer-Side Encryption with Customer-Provided Keys (SSE-C) allows the customer to provide their own encryption key when uploading objects to S3. S3 uses this key to encrypt the data at rest and then discards the key. The customer is responsible for managing and storing the encryption key. This directly addresses the requirement of keeping the encryption key within the company's on-premises application and offloading the encryption process to S3.\n\n**Why option 1 is incorrect:**\nClient-Side Encryption requires the company to encrypt the data *before* sending it to S3. While this allows the company to manage the encryption key, it doesn't offload the encryption process to S3. The question specifically mentions offloading the *encryption process* to S3. Therefore, this option does not meet the requirements.\n\n**Why option 2 is incorrect:**\nServer-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) uses AWS KMS to manage the encryption keys. The keys are stored and managed within AWS KMS, not on-premises. This violates the requirement that the encryption key must be stored in a custom application running on-premises.\n\n**Why option 3 is incorrect:**\nServer-Side Encryption with Amazon S3 managed keys (SSE-S3) uses keys managed by Amazon S3. The customer has no control over the keys, and they are stored and managed entirely by AWS. This violates the requirement that the encryption key must be stored in a custom application running on-premises.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has historically operated only in theus-east-1region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into theus-west-1AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions. Which of the following represents the best solution to address these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it leverages KMS multi-region keys and S3 replication. Creating a new bucket with replication enabled and using a KMS multi-region key ensures that the data is encrypted with the same key in both regions. Copying the existing data into the new bucket ensures that all data is replicated and encrypted correctly. This approach uses native AWS services, minimizing operational overhead and maximizing security. S3 replication handles the data transfer automatically, and the KMS multi-region key ensures consistent encryption across regions. This aligns with AWS best practices for security and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because you cannot directly 'share' a single-region KMS key across regions. KMS keys are inherently regional resources. While you can grant cross-account access to a KMS key, you cannot make a single-region key usable in another region. This violates the fundamental principle of KMS key isolation.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves a custom solution using Lambda and CloudWatch. While functional, it's less efficient and more complex than using S3 replication. It also introduces potential security vulnerabilities and operational overhead associated with managing a custom Lambda function. Furthermore, it doesn't leverage the built-in capabilities of S3 and KMS for cross-region replication and key management. This approach is less secure and less scalable than using native AWS services.\n\n**Why option 2 is incorrect:**\nis incorrect because while converting a single-region key to a multi-region key is possible, it doesn't automatically re-encrypt existing data in S3 with the new multi-region key. S3 batch replication is designed for replicating existing objects, but it doesn't inherently trigger re-encryption with the new key. The existing objects would still be encrypted with the original single-region key. Therefore, this option doesn't fully address the requirement of using the same key for all data in both regions without additional steps to re-encrypt the data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "A media publishing company is migrating its legacy content management application to AWS. Currently, the application and its MySQL database run on a single on-premises virtual machine, which creates a single point of failure and limits scalability. As traffic has increased due to growing reader engagement and video uploads, the company needs to redesign the solution to ensure automatic scaling, high availability, and separation of application and database layers. The company wants to continue using a MySQL-compatible engine and needs a cost-effective, managed solution that minimizes operational overhead. Which AWS architecture will best fulfill these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database",
        "correct": false
      },
      {
        "id": 1,
        "text": "Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it addresses all the requirements. Using an Auto Scaling group behind an Application Load Balancer (ALB) allows the application tier to automatically scale based on traffic demands, ensuring high availability. Amazon Aurora Serverless v2 for MySQL provides a managed, cost-effective, and highly available database solution that automatically scales based on the application's needs. Aurora Serverless v2 is MySQL-compatible, minimizing code changes during migration, and its serverless nature reduces operational overhead.\n\n**Why option 0 is incorrect:**\nis incorrect because while it uses an Application Load Balancer and RDS for MySQL Multi-AZ, it doesn't leverage Auto Scaling for the EC2 instances. This means the application tier will not automatically scale based on traffic, potentially leading to performance issues and increased costs if over-provisioned. While Multi-AZ provides high availability for the database, it doesn't offer the same level of scalability and cost optimization as Aurora Serverless v2.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Neptune is a graph database, not a relational database like MySQL. The company wants to continue using a MySQL-compatible engine, making Neptune unsuitable. While containerizing the application and deploying it to ECS with EC2 launch type behind an ALB provides scalability, the database choice is a deal-breaker. Also, while ECS is a good choice for containerized applications, it adds complexity compared to a simpler EC2 Auto Scaling Group if the application is not yet containerized.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon ElastiCache for Redis is an in-memory data store, primarily used for caching. It is not a suitable replacement for a persistent MySQL database. While Redis Streams can provide some persistence, it is not designed for the same use cases as a relational database. Also, a Network Load Balancer (NLB) is generally used for TCP traffic and is not the best choice for web applications, which typically use HTTP/HTTPS traffic best handled by an ALB.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 39,
    "text": "Consider the following policy associated with an IAM group containing several users: Which of the following options is correct?",
    "options": [
      {
        "id": 0,
        "text": "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200",
        "correct": false
      },
      {
        "id": 1,
        "text": "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200",
        "correct": true
      },
      {
        "id": 2,
        "text": "Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200",
        "correct": false
      },
      {
        "id": 3,
        "text": "Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because the IAM policy explicitly allows the `ec2:TerminateInstances` action in the `us-west-1` region only when the request originates from the IP address `10.200.200.200`. The `aws:SourceIp` condition in the policy restricts the action to requests coming from that specific IP address. This is a common security practice to limit access to sensitive operations based on the network location of the user.\n\n**Why option 0 is incorrect:**\nis incorrect because the policy restricts access based on the *user's* source IP address, not the IP address of the EC2 instance being terminated. The condition `aws:SourceIp` applies to the IP address from which the API request is made, not the IP address of the resource being acted upon.\n\n**Why option 2 is incorrect:**\nis incorrect because the policy explicitly allows termination in the `us-west-1` region, *if* the request originates from the IP address `10.200.200.200`. The policy does not deny termination in `us-west-1` under these conditions. It doesn't mention anything about other regions.\n\n**Why option 3 is incorrect:**\nis incorrect because the policy explicitly *allows* termination in the `us-west-1` region when the source IP is `10.200.200.200`. The condition is designed to permit the action under this specific circumstance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution. Which of the following represents the MOST cost-optimal and high-performance solution?",
    "options": [
      {
        "id": 0,
        "text": "Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution",
        "correct": true
      },
      {
        "id": 1,
        "text": "Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin",
        "correct": false
      },
      {
        "id": 2,
        "text": "Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages Amazon S3 for static website hosting and Amazon CloudFront for content delivery. S3 provides highly scalable and cost-effective storage for static assets. CloudFront, a CDN, caches the website content at edge locations globally, ensuring low latency and high availability for users worldwide. Route 53 provides DNS resolution, mapping a domain name to the CloudFront distribution. This solution is serverless, meaning there are no servers to manage, and it scales automatically to handle traffic spikes. It's also the most cost-effective option as you only pay for the storage and bandwidth used.\n\n**Why option 1 is incorrect:**\nis incorrect because hosting a static website on AWS Lambda is not a standard or cost-effective practice. Lambda is designed for event-driven compute and not for serving static content. While technically possible, it would be significantly more expensive and complex than using S3 and CloudFront. Lambda has execution time limits and would likely be throttled under high traffic, making it unsuitable for this use case.\n\n**Why option 2 is incorrect:**\nis incorrect because hosting the website on an on-premises data center instance defeats the purpose of a serverless solution and introduces significant operational overhead. It requires managing the server, network, and security, and it may not scale effectively to handle traffic spikes. Using CloudFront with a custom origin is valid, but the origin itself is not serverless and is not cost-optimal. Also, the question explicitly asks for a solution using AWS, not on-premises infrastructure.\n\n**Why option 3 is incorrect:**\nis incorrect because while using EC2 and CloudFront is a valid approach, it's not the most cost-optimal or serverless. EC2 instances require ongoing management, patching, and scaling, which adds complexity and cost. S3 is a more cost-effective and easier-to-manage solution for static website hosting. The question specifically asks for a serverless solution, and EC2 is not a serverless service.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 41,
    "text": "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system. Which of the following represents the MOST operationally efficient way to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the most operationally efficient solution. It leverages AWS DataSync to transfer data directly from the on-premises NFS file system to Amazon EFS over the Direct Connect connection. Using a PrivateLink interface VPC endpoint for EFS ensures secure and private communication without traversing the public internet. The DataSync scheduled task automates the data transfer process, ensuring that newly created video files are replicated to EFS regularly. PrivateLink provides a direct, secure connection to EFS within the AWS network, which is more efficient and secure than other options.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves transferring data to Amazon S3 first and then to EFS using a Lambda function. This adds unnecessary complexity and operational overhead. It also introduces an extra step and potential latency. While a VPC gateway endpoint for S3 is a good practice, the overall architecture is less efficient than a direct transfer to EFS.\n\n**Why option 1 is incorrect:**\nis incorrect because AWS VPC peering is not directly compatible with Amazon EFS. EFS requires an interface VPC endpoint (PrivateLink) for direct access from within a VPC. While a private VIF is appropriate for Direct Connect, the use of VPC peering is not the correct way to access EFS. Also, while a 24-hour schedule might work, it might not be frequent enough to meet the requirement of replicating *newly created* video files, depending on the creation rate.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A video conferencing platform serves users worldwide through a globally distributed deployment of Amazon EC2 instances behind Network Load Balancers (NLBs) in several AWS Regions. The platform's architecture currently allows clients to connect to any Region via public endpoints, depending on how DNS resolves. However, users in regions far from the load balancers frequently experience high latency and slow connection times, especially during session initiation. The company wants to optimize the experience for global users by reducing end-to-end latency and load time while keeping the existing NLBs and EC2-based application infrastructure in place. Which solution will best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWS’s global edge network to the closest healthy Regional NLB",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\ndeploying AWS Global Accelerator, is the best solution. AWS Global Accelerator uses AWS's global network to route traffic to the closest healthy regional endpoint (NLB in this case). This significantly reduces latency by minimizing the distance traffic travels over the public internet. It also provides static IP addresses that act as a single entry point for the application, simplifying DNS configuration and improving reliability. Global Accelerator is designed for low latency and high availability applications, making it ideal for a video conferencing platform. It integrates seamlessly with existing NLBs and requires minimal changes to the existing infrastructure, fulfilling the question's requirements.\n\n**Why option 0 is incorrect:**\nreplacing NLBs with ALBs and using cross-zone load balancing, is not the best solution. While ALBs offer more features than NLBs, replacing them would require significant changes to the existing infrastructure and might not provide the same level of performance for real-time video traffic. Cross-zone load balancing helps distribute traffic within a single region but doesn't address the global latency issue. The question explicitly states that the existing NLBs should be kept in place.\n\n**Why option 1 is incorrect:**\ndeploying Amazon CloudFront with HTTP caching, is not suitable for real-time video conferencing. CloudFront is primarily designed for caching static content and delivering it from edge locations. Video conferencing involves real-time, dynamic data streams, which are not effectively cached by CloudFront. While CloudFront can improve the delivery of static assets associated with the platform, it won't significantly reduce latency for the core video conferencing sessions. Furthermore, enabling HTTP caching for video streams would likely lead to stale or incorrect data being delivered to users.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?",
    "options": [
      {
        "id": 0,
        "text": "General Purpose",
        "correct": false
      },
      {
        "id": 1,
        "text": "Bursting Throughput",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provisioned Throughput",
        "correct": false
      },
      {
        "id": 3,
        "text": "Max I/O",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nMax I/O performance mode is designed for applications that require high throughput and low latency, such as big data analytics, media processing, and high-performance computing. It optimizes for a higher number of concurrent operations and can handle large files efficiently. This makes it the most suitable choice for improving the performance of big data processing workflows on Amazon EFS.\n\n**Why option 0 is incorrect:**\nGeneral Purpose performance mode is suitable for latency-sensitive applications, such as web serving and content management systems. While it provides good performance for a wide range of workloads, it's not optimized for the high throughput demands of big data processing.\n\n**Why option 1 is incorrect:**\nBursting Throughput is a throughput mode, not a performance mode. Bursting Throughput allows EFS to burst above its baseline throughput for short periods. While helpful, it doesn't fundamentally change the performance characteristics like Max I/O does. The question asks about performance modes, not throughput modes. Provisioned Throughput (option 2) is also a throughput mode, not a performance mode.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 44,
    "text": "An application runs big data workloads on Amazon Elastic Compute Cloud (Amazon EC2) instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?",
    "options": [
      {
        "id": 0,
        "text": "Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most cost-optimal solution. Purchasing 80 Reserved Instances (RIs) covers the typical workload demand (80% of the time). RIs offer significant cost savings compared to On-Demand instances. Using an Auto Scaling Group with a launch template allows for dynamic provisioning of additional On-Demand and Spot instances to handle workload spikes. This combination balances cost efficiency (RIs for the steady state) with flexibility and cost savings (Spot instances for spikes) and reliability (On-Demand instances for guaranteed capacity when Spot instances are unavailable).\n\n**Why option 1 is incorrect:**\nis incorrect because relying solely on Spot instances for the base workload (80 instances) is risky. Spot instances can be terminated with short notice, potentially disrupting the application's performance and availability. While Spot instances are cost-effective, they are not suitable for the consistent baseline capacity.\n\n**Why option 2 is incorrect:**\nis incorrect because purchasing only 20 On-Demand instances is insufficient to cover the typical workload demand of 80 instances. Relying heavily on Spot instances for the remaining capacity is risky and could lead to performance degradation or application unavailability if Spot instances are terminated. On-Demand instances are more expensive than RIs for consistent usage.\n\n**Why option 3 is incorrect:**\nis incorrect because purchasing 80 On-Demand instances is more expensive than purchasing 80 Reserved Instances for the baseline workload. While it provides guaranteed capacity, it doesn't leverage the cost savings offered by RIs for predictable usage. Using a mix of On-Demand and Spot instances for additional capacity is a good strategy, but the baseline should be covered by RIs.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "What does this IAM policy do?",
    "options": [
      {
        "id": 0,
        "text": "It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region",
        "correct": false
      },
      {
        "id": 1,
        "text": "It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region",
        "correct": false
      },
      {
        "id": 2,
        "text": "It allows running Amazon EC2 instances anywhere but in the eu-west-1 region",
        "correct": false
      },
      {
        "id": 3,
        "text": "It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because the policy allows the `ec2:RunInstances` action, which is required to launch EC2 instances. The `Condition` block specifies that `aws:RequestedRegion` must be equal to `eu-west-1`. This means that the EC2 instances can only be launched in the `eu-west-1` region. The policy doesn't restrict where the API call originates from, so it can be made from anywhere in the world as long as the instance is launched in eu-west-1.\n\n**Why option 0 is incorrect:**\nis incorrect because it incorrectly states that the API call must originate from the eu-west-1 region. The IAM policy only restricts the region where the EC2 instance is launched, not where the API call originates.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that EC2 instances can be launched in any region. The IAM policy explicitly restricts the launch region to `eu-west-1` using the `aws:RequestedRegion` condition.\n\n**Why option 2 is incorrect:**\nis incorrect because it incorrectly states that EC2 instances can be launched anywhere but in the eu-west-1 region. The IAM policy explicitly allows EC2 instances to be launched only in the eu-west-1 region.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A financial institution is transitioning its critical back-office systems to AWS. These systems currently rely on Microsoft SQL Server databases hosted on on-premises infrastructure. The data is highly sensitive and subject to regulatory compliance. The organization wants to enhance security and minimize database management tasks as part of the migration. Which solution will best meet these goals with the least operational burden?",
    "options": [
      {
        "id": 0,
        "text": "Move the SQL Server data into Amazon Timestream to gain time series insights. Use AWS CloudTrail to monitor access to the data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the SQL Server databases to a Multi-AZ Amazon RDS for SQL Server deployment. Enable encryption at rest by using an AWS Key Management Service (AWS KMS) managed key",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the SQL Server databases to Amazon EC2 instances with encrypted EBS volumes. Use an AWS KMS customer managed key to enable encryption",
        "correct": false
      },
      {
        "id": 3,
        "text": "Export the SQL Server databases to CSV format and store them in Amazon S3 with S3 bucket policies for access control. Use AWS Backup for data protection",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it leverages Amazon RDS for SQL Server, a managed database service, which significantly reduces operational burden. Multi-AZ deployment provides high availability and failover capabilities. Enabling encryption at rest using AWS KMS managed keys ensures data security and compliance. RDS handles patching, backups, and other maintenance tasks, minimizing administrative overhead. Using KMS managed keys simplifies key management compared to customer-managed keys, further reducing operational complexity while still providing strong encryption.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Timestream is a time-series database, which is not suitable for general-purpose relational database workloads like those typically found in back-office financial systems. Migrating to Timestream would require significant application changes and is not a direct migration path for SQL Server. While CloudTrail provides auditing, it doesn't address the core requirements of database migration and security for relational data.\n\n**Why option 2 is incorrect:**\nis incorrect because while it provides encryption using KMS, it involves managing SQL Server on EC2 instances, which significantly increases operational overhead. This includes managing the operating system, SQL Server installation, patching, backups, and other administrative tasks. This contradicts the requirement of minimizing database management tasks. While using a customer-managed key offers more control, it also adds to the operational burden.\n\n**Why option 3 is incorrect:**\nis incorrect because exporting the databases to CSV format and storing them in S3 is not a suitable solution for a relational database. It loses the relational structure and integrity of the data, making it difficult to query and maintain. While S3 bucket policies can control access, and AWS Backup provides data protection, this approach is not a viable database migration strategy and does not meet the requirements of the question. Furthermore, managing data in CSV format in S3 would be extremely inefficient for transactional workloads.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture. As a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "For security group B: Add an inbound rule that allows traffic only from security group A on port 443",
        "correct": false
      },
      {
        "id": 1,
        "text": "For security group B: Add an inbound rule that allows traffic only from all sources on port 1433",
        "correct": false
      },
      {
        "id": 2,
        "text": "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443",
        "correct": false
      },
      {
        "id": 3,
        "text": "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433",
        "correct": true
      },
      {
        "id": 4,
        "text": "For security group B: Add an inbound rule that allows traffic only from security group A on port 1433",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 3 and 4 provide the most secure configuration. \n\nOption 3:  'For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433' This allows external clients to access the web servers on port 443 (HTTPS), which is necessary for the application to function.  Crucially, it restricts outbound traffic from the web servers (security group A) to the database (security group B) to only port 1433 (MSSQL's default port). This adheres to the principle of least privilege by only allowing the necessary traffic.\n\nOption 4: 'For security group B: Add an inbound rule that allows traffic only from security group A on port 1433' This is the other half of the secure configuration. It restricts inbound traffic to the database (security group B) to only come from the web servers (security group A) on port 1433. This prevents unauthorized access to the database from any other source, significantly enhancing security.\n\n**Why option 4 is correct:**\nOptions 3 and 4 provide the most secure configuration. \n\nOption 3:  'For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433' This allows external clients to access the web servers on port 443 (HTTPS), which is necessary for the application to function.  Crucially, it restricts outbound traffic from the web servers (security group A) to the database (security group B) to only port 1433 (MSSQL's default port). This adheres to the principle of least privilege by only allowing the necessary traffic.\n\nOption 4: 'For security group B: Add an inbound rule that allows traffic only from security group A on port 1433' This is the other half of the secure configuration. It restricts inbound traffic to the database (security group B) to only come from the web servers (security group A) on port 1433. This prevents unauthorized access to the database from any other source, significantly enhancing security.\n\n**Why option 0 is incorrect:**\nOption 0: 'For security group B: Add an inbound rule that allows traffic only from security group A on port 443' is incorrect because the database (security group B) needs to receive traffic on port 1433 (MSSQL's default port), not port 443. Web servers connect to the database on port 1433. Allowing traffic on port 443 to the database is unnecessary and a security risk.\n\n**Why option 1 is incorrect:**\nOption 1: 'For security group B: Add an inbound rule that allows traffic only from all sources on port 1433' is incorrect because it opens the database (security group B) to traffic from *all* sources on port 1433. This is a major security vulnerability, as anyone could potentially connect to the database. This violates the principle of least privilege.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation. Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?",
    "options": [
      {
        "id": 0,
        "text": "Build a shared services Amazon Virtual Private Cloud (Amazon VPC)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use VPCs connected with AWS Direct Connect",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Fully meshed VPC Peering connection",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBuilding a shared services VPC is the most suitable solution. This approach centralizes common services (e.g., DNS, security tools, logging, monitoring) in a single VPC. Other VPCs in the hub-and-spoke network can access these services through the Transit Gateway. This reduces the need to deploy and manage these services in each individual VPC, thereby lowering administrative overhead and costs. The shared services VPC acts as a central point for managing and updating these shared resources, simplifying operations and improving consistency.\n\n**Why option 1 is incorrect:**\nUsing VPCs connected with AWS Direct Connect is incorrect because Direct Connect is primarily used for establishing a dedicated network connection between on-premises infrastructure and AWS. While it can be used in conjunction with Transit Gateway, it doesn't directly address the need for shared services within the AWS environment and doesn't inherently reduce administrative overhead or costs related to service deployment within the VPCs.\n\n**Why option 2 is incorrect:**\nUsing a fully meshed VPC peering connection is incorrect because it becomes complex and unmanageable as the number of VPCs increases. A fully meshed network requires n*(n-1)/2 peering connections, where n is the number of VPCs. This leads to significant administrative overhead and is not a scalable solution. The Transit Gateway is already in place, making VPC peering redundant and less efficient.\n\n**Why option 3 is incorrect:**\nUsing a Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs) is incorrect. While Transit VPC was a common pattern before Transit Gateway, Transit Gateway is the recommended and more scalable solution for connecting VPCs. Transit VPC involves using a central VPC with a VPN appliance to route traffic between other VPCs. Transit Gateway is a managed service that simplifies this process and offers better performance and scalability. The question already states that Transit Gateway is in use, making Transit VPC a step backward.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort. What will you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "correct": false
      },
      {
        "id": 1,
        "text": "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "correct": false
      },
      {
        "id": 2,
        "text": "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days",
        "correct": true
      },
      {
        "id": 3,
        "text": "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS Config managed rules can directly check for certificates nearing expiration (within 30 days) for *imported* certificates. It provides a pre-built rule that reduces the need for custom scripting. The rule can be configured to trigger an SNS notification, fulfilling the notification requirement with minimal effort. This aligns with the requirement of monitoring third-party (imported) certificates.\n\n**Why option 0 is incorrect:**\nis incorrect because it focuses on certificates *created* via ACM. The question specifically mentions *third-party* certificates, which are imported, not created within ACM. While CloudWatch can monitor ACM certificates, it doesn't directly address the requirement of monitoring *imported* certificates.\n\n**Why option 1 is incorrect:**\nis similar to option 0 in that it uses CloudWatch metrics, but it correctly identifies that the certificates are *imported* into ACM. However, using CloudWatch requires creating a custom alarm and action, which involves more configuration and maintenance compared to using an AWS Config managed rule. AWS Config provides a pre-built rule for this specific use case, making it the preferred solution for minimizing scripting and maintenance.\n\n**Why option 3 is incorrect:**\nis incorrect because it focuses on certificates *created* via ACM, not *imported* certificates. The question specifically asks about monitoring third-party certificates, which are imported into ACM.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A financial services company runs a Kubernetes-based microservices application in its on-premises data center. The application uses the Advanced Message Queuing Protocol (AMQP) to interact with a message queue. The company is experiencing rapid growth and its on-prem infrastructure cannot scale fast enough. The company wants to migrate the application to AWS with minimal code changes and reduce infrastructure management overhead. The messaging component must continue using AMQP, and the solution should offer high scalability and low operational effort. Which combination of options will together meet these requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes",
        "correct": true
      },
      {
        "id": 1,
        "text": "Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery",
        "correct": false
      },
      {
        "id": 4,
        "text": "Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because deploying the containerized application to Amazon EKS using AWS Fargate allows the company to run Kubernetes without managing the underlying EC2 nodes. Fargate provides serverless compute for containers, reducing operational overhead. Option 1 is correct because Amazon MQ is a managed message broker service that supports AMQP. This allows the company to migrate the messaging component to AWS without significant code changes, as the application can continue to use AMQP. Amazon MQ handles the infrastructure management, providing high scalability and reducing operational effort.\n\n**Why option 1 is correct:**\nThis is correct because deploying the containerized application to Amazon EKS using AWS Fargate allows the company to run Kubernetes without managing the underlying EC2 nodes. Fargate provides serverless compute for containers, reducing operational overhead. Option 1 is correct because Amazon MQ is a managed message broker service that supports AMQP. This allows the company to migrate the messaging component to AWS without significant code changes, as the application can continue to use AMQP. Amazon MQ handles the infrastructure management, providing high scalability and reducing operational effort.\n\n**Why option 2 is incorrect:**\nis incorrect because replacing AMQP with Amazon SQS would require significant code refactoring to use SQS SDKs and polling logic, violating the requirement for minimal code changes. SQS also uses a different messaging paradigm than AMQP.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying to ECS on EC2 requires managing EC2 instances, increasing operational overhead. Furthermore, using Amazon SNS as a replacement for AMQP would require significant code changes, violating the requirement for minimal code changes. SNS is a pub/sub service, not a message queue, and does not natively support AMQP.\n\n**Why option 4 is incorrect:**\nis incorrect because running the application on EC2 Auto Scaling groups and self-hosting RabbitMQ on EC2 increases operational overhead. The company would be responsible for managing the EC2 instances, RabbitMQ installation, configuration, patching, and scaling, which contradicts the requirement for low operational effort.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "To improve the performance and security of the application, the engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the Amazon EC2 instances. As a solutions architect, which of the following actions would you recommend to stop the attacks?",
    "options": [
      {
        "id": 0,
        "text": "Create a deny rule for the malicious IP in the Security Groups associated with each of the instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IP match condition in the AWS WAF to block the malicious IP address",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a ticket with AWS support to take action against the malicious IP",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because AWS WAF is already integrated with CloudFront. Creating an IP match condition in WAF allows you to block the malicious IP address at the edge, before the traffic even reaches the Application Load Balancer or the EC2 instances. This is the most efficient and effective way to mitigate the attack in this scenario. WAF is designed for this purpose and provides centralized management of security rules.\n\n**Why option 0 is incorrect:**\nis incorrect because while Security Groups can block traffic, managing Security Groups for multiple EC2 instances individually is cumbersome and less efficient than using WAF. Also, Security Groups operate at the instance level, while WAF operates at the edge (CloudFront), providing better protection and performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Network ACLs (NACLs) are stateless and operate at the subnet level. While NACLs can block traffic, they are less granular and more difficult to manage than WAF, especially when dealing with specific IP addresses and complex rules. Also, NACLs are not integrated with CloudFront, so they would not be the first line of defense. Using NACLs would also require managing rules across multiple subnets, increasing operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 53,
    "text": "A SaaS company is modernizing one of its legacy web applications by migrating it to AWS. The company aims to improve the availability of the application during both normal and peak traffic periods. Additionally, the company wants to implement protection against common web exploits and malicious traffic. The architecture must be scalable and integrate AWS WAF to secure incoming traffic. Which solution will best meet these requirements with high availability and minimal configuration complexity?",
    "options": [
      {
        "id": 0,
        "text": "Launch EC2 instances in a single Availability Zone and configure AWS Global Accelerator to route traffic to the instances. Attach AWS WAF to Global Accelerator for application protection",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto Scaling group with EC2 instances in multiple Availability Zones. Attach a Network Load Balancer (NLB) to distribute incoming traffic. Integrate AWS WAF directly with the Auto Scaling group for traffic filtering",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch two EC2 instances in separate Availability Zones and register them as targets of an Application Load Balancer. Associate the ALB with AWS WAF to filter incoming traffic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application on multiple Amazon EC2 instances in an Auto Scaling group that spans two Availability Zones. Place an Application Load Balancer (ALB) in front of the group. Associate AWS WAF with the ALB",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the best solution because it utilizes an Auto Scaling group across multiple Availability Zones for high availability and scalability. An Application Load Balancer (ALB) distributes traffic across the instances in the Auto Scaling group. Associating AWS WAF with the ALB provides the necessary protection against web exploits and malicious traffic. The ALB is designed for HTTP/HTTPS traffic and integrates well with WAF. This solution provides a good balance of scalability, availability, security, and ease of configuration. The Auto Scaling group ensures that the application can handle varying traffic loads, while the ALB distributes traffic evenly across the instances. AWS WAF protects the application from common web exploits, ensuring the security of the application.\n\n**Why option 0 is incorrect:**\nis incorrect because using a single Availability Zone violates the high availability requirement. If that Availability Zone experiences an outage, the entire application will be unavailable. While Global Accelerator can route traffic to different regions, it doesn't address the fundamental lack of redundancy within a single Availability Zone. Also, while WAF can be attached to Global Accelerator, it's more commonly used with ALB for web applications.\n\n**Why option 1 is incorrect:**\nis incorrect because while it uses an Auto Scaling group across multiple Availability Zones for high availability, integrating AWS WAF directly with the Auto Scaling group is not a standard practice. AWS WAF is typically associated with an Application Load Balancer (ALB) or CloudFront distribution. A Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and is not designed for web application traffic filtering like an ALB. NLB is more suitable for applications requiring high throughput and low latency, but it doesn't provide the same level of web application security as an ALB with WAF.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Snowball job and target a Amazon S3 Glacier Vault",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most cost-effective. Here's why:\n*   **S3 Glacier Deep Archive is the cheapest storage option for long-term archival.** It's designed for data that is rarely accessed.\n*   **Using an S3 bucket as the initial target allows for lifecycle policies.** These policies automatically transition data to Glacier Deep Archive after it's uploaded to S3, incurring minimal additional cost.\n*   **Transitioning on the same day maximizes cost savings.** The sooner the data is moved to Glacier Deep Archive, the sooner you benefit from its lower storage costs.\n*   **Snowball directly to S3 is generally faster and more flexible.** While Snowball can target Glacier Vaults, using S3 as an intermediary allows for more control and potentially faster data transfer, especially if there are any issues during the transfer process. The lifecycle policy automates the archival process.\n\n**Why option 0 is incorrect:**\nis incorrect because AWS Snowball does not directly support targeting S3 Glacier Vaults. Snowball supports importing data into S3 buckets. You would then need to use S3 lifecycle policies to transition the data to Glacier or Glacier Deep Archive.\n\n**Why option 2 is incorrect:**\nis incorrect because while it uses a similar approach to the correct answer, it transitions data to S3 Glacier instead of S3 Glacier Deep Archive. S3 Glacier Deep Archive is significantly cheaper than S3 Glacier for long-term archival, making Option 1 the more cost-effective choice.\n\n**Why option 3 is incorrect:**\nis incorrect because AWS Snowball does not directly support targeting S3 Glacier Deep Archive Vaults. Snowball supports importing data into S3 buckets. You would then need to use S3 lifecycle policies to transition the data to Glacier or Glacier Deep Archive.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B. As a Solutions Architect, which of the following will you recommend to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it implements the proper cross-account access mechanism. First, an IAM role is created in account A (where the Lambda function resides) with permissions to access the S3 bucket in account B. This role is then assigned as the execution role for the Lambda function. However, this alone is not sufficient. The S3 bucket in account B must also have a bucket policy that explicitly grants access to the IAM role created in account A. This bucket policy acts as a trust relationship, allowing the IAM role from account A to assume permissions within account B's S3 bucket. This two-pronged approach ensures secure and controlled cross-account access, following the principle of least privilege.\n\n**Why option 0 is incorrect:**\nis incorrect because making the S3 bucket public is a significant security risk. It allows anyone on the internet to access the bucket's contents, which is generally unacceptable for most use cases. It violates the principle of least privilege and exposes sensitive data to potential unauthorized access.\n\n**Why option 1 is incorrect:**\nis incorrect because while creating an IAM role for the Lambda function with S3 access permissions and assigning it as the execution role is necessary, it's not sufficient for cross-account access. The S3 bucket in the other account needs to explicitly trust the IAM role from the Lambda function's account. Without the bucket policy granting access, the Lambda function will be denied access to the S3 bucket, even with the correct IAM role.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A company is developing a global healthcare application that requires the least possible latency for database read/write operations from users in several geographies across the world. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora that offers an effective recovery point objective (RPO) of seconds and a recovery time objective (RTO) of a minute. Which of the following options would you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon Aurora provisioned Database cluster",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon Aurora Global Database cluster",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up an Amazon Aurora multi-master Database cluster",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon Aurora serverless Database cluster",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nsetting up an Amazon Aurora Global Database cluster, is the correct choice. Aurora Global Database is specifically designed for globally distributed applications. It allows you to create a single Aurora database that spans multiple AWS regions. It uses dedicated infrastructure for low-latency replication between regions, typically achieving replication lag in the single-digit milliseconds. This meets the requirement of the least possible latency for global users. In case of a regional outage, you can promote a secondary region to become the primary region with an RTO of less than a minute, fulfilling the RTO requirement. The replication lag also ensures an RPO of seconds.\n\n**Why option 0 is incorrect:**\nsetting up an Amazon Aurora provisioned Database cluster, is incorrect because a standard Aurora provisioned cluster is confined to a single AWS region. It does not provide built-in, low-latency global replication capabilities needed to minimize latency for users in different geographies. While you could implement cross-region read replicas, the replication lag would likely be higher than what's acceptable for the RPO and RTO requirements.\n\n**Why option 2 is incorrect:**\nsetting up an Amazon Aurora multi-master Database cluster, is incorrect because while multi-master allows writes to multiple instances in the *same* region, it doesn't address the global distribution requirement. It does not inherently reduce latency for users in different geographical locations. It's designed for high availability within a single region, not for global distribution and low latency across regions.\n\n**Why option 3 is incorrect:**\nsetting up an Amazon Aurora serverless Database cluster, is incorrect because while Aurora Serverless v2 offers scalability and cost-effectiveness, it doesn't inherently provide the global distribution and low-latency replication capabilities needed to meet the requirements. It's also not optimized for the very low RTO/RPO requirements specified. While you could potentially use data API to access it from different regions, the latency would be higher than with Aurora Global Database.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database. As an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
    "options": [
      {
        "id": 0,
        "text": "Activate read-through caching on the Amazon Aurora database",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a read replica and modify the application to use the appropriate endpoint",
        "correct": true
      },
      {
        "id": 2,
        "text": "Provision another Amazon Aurora database and link it to the primary database as a read replica",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the application to read from the Multi-AZ standby instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because creating a read replica allows you to direct read traffic to the replica, thus reducing the load on the primary Aurora instance. By modifying the application to use the read replica's endpoint for read operations, the write operations on the primary instance will experience less I/O contention and improved performance. This is a standard practice for scaling read capacity in database systems.\n\n**Why option 0 is incorrect:**\nis incorrect because read-through caching, while beneficial for reducing read latency, doesn't separate read traffic from the primary database instance. The primary instance still handles the initial read request and updates the cache. It doesn't address the I/O contention issue described in the question.\n\n**Why option 2 is incorrect:**\nis incorrect because it is redundant. Aurora already provides the capability to create read replicas directly from the existing primary instance. Provisioning another Aurora database and linking it as a read replica is unnecessarily complex and costly. It achieves the same result as option 1 but with more overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because the Multi-AZ standby instance in Aurora is primarily for failover purposes. It's not designed to handle read traffic directly. While it can technically serve reads in some scenarios, it's not a recommended or supported practice for read scaling. Using it for reads could interfere with its primary function of providing high availability.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed. Which of the following options represents the best solution for the given requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic File System (EFS) Standard–IA storage class",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Elastic Block Store (EBS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic File System (EFS) Standard storage class",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon EFS Standard-IA storage class is the most suitable option. EFS provides a shared file system that can be concurrently accessed by hundreds of EC2 instances. The Standard-IA storage class is designed for infrequently accessed files, offering lower storage costs compared to the Standard storage class. EFS provides immediate access to data when needed, unlike archival storage options like Glacier. The combination of EFS's shared file system capabilities and the cost-optimized Standard-IA storage class perfectly addresses the requirements.\n\n**Why option 1 is incorrect:**\nAmazon EBS is block storage, not file storage. While EBS can be attached to an EC2 instance, it cannot be concurrently accessed by multiple EC2 instances without complex configurations like clustering, which adds overhead and complexity. It's also not designed for infrequent access and is generally more expensive for storing infrequently accessed data compared to EFS Standard-IA.\n\n**Why option 2 is incorrect:**\nAmazon EFS Standard storage class provides immediate access and can be concurrently accessed by hundreds of EC2 instances, but it is more expensive than the Standard-IA storage class. Since the requirement specifies 'less frequently accessed files' and 'most cost-effective,' Standard-IA is the better choice.\n\n**Why option 3 is incorrect:**\nAmazon S3 Standard-IA is object storage, not file storage. While S3 is cost-effective for infrequently accessed data, it's not a file system that can be directly mounted and accessed by EC2 instances like EFS. Accessing S3 from EC2 instances requires using the AWS SDK or CLI, which is not as seamless as accessing a file system. Also, the question implies a need for a file system interface.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A global media agency is developing a cultural analysis project to explore how major sports stories have evolved over the last five years. The team has collected thousands of archived news bulletins and magazine spreads stored in PDF format. These documents are rich in unstructured text and come from various sources with differing layouts and font styles. The agency wants to better understand how public tone and narrative have shifted over time. The team has chosen to use Amazon Textract for its ability to accurately extract printed and scanned text from complex PDF layouts. They need a solution that can then analyze the emotional tone and subject matter of the extracted text with the least possible operational burden, using fully managed AWS services where possible. Which solution will best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages Amazon Textract for text extraction and then directly utilizes Amazon Comprehend for sentiment analysis and entity detection. Comprehend is a fully managed natural language processing (NLP) service that provides pre-trained models for sentiment analysis and entity recognition, eliminating the need for custom model training or complex data processing pipelines. Storing the results in Amazon S3 allows for easy access and visualization using other AWS services like QuickSight or custom applications. This approach minimizes operational burden by using fully managed services for both text extraction and NLP analysis.\n\n**Why option 0 is incorrect:**\nis incorrect because converting the text to CSV format and querying with Athena is not the most efficient way to perform sentiment analysis. While Athena can query text data, it doesn't provide built-in sentiment analysis capabilities. This would require additional custom scripting or integration with another service, increasing operational complexity. Also, Lambda is used for converting the text into CSV format which is an unnecessary step.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Rekognition is primarily designed for image and video analysis, not text analysis. While Rekognition can detect text in images, it's not suitable for analyzing the sentiment of large volumes of extracted text. Redshift is also not the ideal choice for this type of unstructured text analysis. Glue is useful for ETL, but it doesn't directly address the sentiment analysis requirement. This option introduces unnecessary complexity and uses services not optimized for the task.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which of the following actions meets the given requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a new trail in AWS CloudTrail from within the developer accounts with the organization trails option enabled",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to the developer accounts",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Service Control Policies (SCPs) are the primary mechanism for establishing guardrails and enforcing policies at the AWS Organizations level. An SCP can be configured to deny specific actions, such as modifying or deleting CloudTrail configurations. When attached to the developer accounts, the SCP will prevent any user, including the root user, from making changes to CloudTrail, regardless of their IAM permissions within the individual account. This ensures that the mandatory CloudTrail configuration remains intact and auditable, meeting the requirement of preventing modifications by developers.\n\n**Why option 0 is incorrect:**\nis incorrect because while an IAM policy can restrict changes to CloudTrail, it can be bypassed by the root user. The root user has the ability to modify or delete IAM policies, effectively circumventing the restriction. This option does not provide the necessary level of control to prevent modifications by the root user within the developer accounts.\n\n**Why option 1 is incorrect:**\nis incorrect because service-linked roles are used to grant AWS services permission to access other AWS resources on your behalf. While you can use conditions in service-linked role policies, they are not designed to prevent the root user within an account from modifying CloudTrail. The root user could potentially modify the service-linked role or its associated policies, rendering the restriction ineffective. This approach doesn't provide the necessary centralized control and protection against root user actions.\n\n**Why option 2 is incorrect:**\nis incorrect because while enabling organization trails in CloudTrail ensures that all events across the organization are logged to a central S3 bucket, it doesn't prevent individual developers from creating their own trails or modifying the organization trail within their own accounts if they have sufficient permissions. The question specifically requires preventing modifications to the mandatory CloudTrail configuration, which this option doesn't address.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A mobile app allows users to submit photos, which are stored in an Amazon S3 bucket. Currently, a batch of Amazon EC2 Spot Instances is launched nightly to process all the day’s uploads. Each photo requires approximately 3 minutes and 512 MB of memory to process. To improve responsiveness and minimize costs, the company wants to shift to near real-time image processing that begins as soon as an image is uploaded. Which solution will provide the MOST cost-effective and scalable architecture to meet these new requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon S3 to push events to an Amazon SQS queue. Launch a single EC2 Reserved Instance that continuously polls the queue and processes each image upon receipt",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon S3 to send event notifications to an Amazon SQS queue each time a photo is uploaded. Set up an AWS Lambda function to poll the queue and process images asynchronously",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable S3 event notifications to invoke an Amazon EventBridge rule. Configure an AWS Step Functions workflow to initiate an Fargate task in Amazon ECS to process the image",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure S3 to trigger an AWS App Runner service directly. Deploy a containerized image-processing application to App Runner to automatically process each upload",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most cost-effective and scalable solution. S3 event notifications can be configured to send messages to an SQS queue whenever a photo is uploaded. An AWS Lambda function can then be triggered by the SQS queue to process the image asynchronously. Lambda functions are cost-effective because you only pay for the compute time you consume. They also scale automatically based on the number of messages in the queue, providing near real-time processing. This approach avoids the overhead of managing EC2 instances and ensures that processing resources are only used when needed.\n\n**Why option 0 is incorrect:**\nis incorrect because using a single EC2 Reserved Instance to continuously poll the SQS queue is not cost-effective. A Reserved Instance incurs costs even when it's idle. Also, a single instance might not be able to handle the workload during peak upload times, leading to delays. Polling adds unnecessary overhead and complexity.\n\n**Why option 2 is incorrect:**\nis incorrect because using EventBridge, Step Functions, and Fargate is more complex and expensive than using Lambda. While it provides scalability, the overhead of managing a Step Functions workflow and Fargate tasks is higher. Fargate also has a higher cost per unit of compute time compared to Lambda, especially for short-lived tasks like image processing. EventBridge adds unnecessary complexity compared to directly triggering Lambda from SQS.\n\n**Why option 3 is incorrect:**\nis incorrect because while AWS App Runner offers a simplified container deployment experience, it's not the most cost-effective solution for this specific use case. App Runner is better suited for running web applications or APIs that require continuous availability. For event-driven image processing, Lambda is more cost-efficient as it only charges for the actual execution time. Also, directly triggering App Runner from S3 events might not be as straightforward or well-integrated as using SQS as an intermediary buffer.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 62,
    "text": "A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone. As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format",
        "correct": true
      },
      {
        "id": 3,
        "text": "Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an AWS Lambda function based job to delete the raw zone data after 1 day",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because transitioning the raw zone data to Amazon S3 Glacier Deep Archive after 1 day significantly reduces storage costs. Glacier Deep Archive is the cheapest storage class for long-term archival, and since the raw data is only needed for compliance and not frequent access, it's ideal. Option 2 is correct because using a compressed file format (e.g., Parquet, ORC with compression) in the refined zone reduces storage space and improves Athena query performance. Compressed columnar formats are more efficient for analytical queries as they allow Athena to read only the necessary columns and reduce the amount of data scanned.\n\n**Why option 2 is correct:**\nThis is correct because transitioning the raw zone data to Amazon S3 Glacier Deep Archive after 1 day significantly reduces storage costs. Glacier Deep Archive is the cheapest storage class for long-term archival, and since the raw data is only needed for compliance and not frequent access, it's ideal. Option 2 is correct because using a compressed file format (e.g., Parquet, ORC with compression) in the refined zone reduces storage space and improves Athena query performance. Compressed columnar formats are more efficient for analytical queries as they allow Athena to read only the necessary columns and reduce the amount of data scanned.\n\n**Why option 1 is incorrect:**\nis incorrect because CSV is not an efficient format for analytical queries. It's row-oriented and doesn't support compression or schema evolution as effectively as columnar formats like Parquet or ORC. Using CSV would lead to higher storage costs and slower query performance in Athena.\n\n**Why option 3 is incorrect:**\nis incorrect because the refined zone data is actively queried by business analysts using Athena. Transitioning it to Glacier Deep Archive after only 1 day would make it inaccessible for ad-hoc queries and defeat the purpose of the refined zone. The refined data needs to be in a storage class that allows for frequent access and efficient querying.\n\n**Why option 4 is incorrect:**\nis incorrect because the source data needs to be kept for a minimum of 5 years for compliance reasons. Deleting the raw zone data after 1 day would violate this requirement.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines. Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?",
    "options": [
      {
        "id": 0,
        "text": "Spot Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "On-Demand Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Dedicated Hosts",
        "correct": false
      },
      {
        "id": 3,
        "text": "Dedicated Instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nDedicated Instances are EC2 instances that run on hardware dedicated to a single customer. While not as isolated as Dedicated Hosts (which provide hardware-level control), they offer single-tenancy at a lower cost. They are a good balance between isolation and cost-effectiveness for meeting regulatory requirements that mandate single-tenancy. Dedicated Instances provide isolation at the hypervisor level, ensuring that no other AWS customer's instances share the same hardware.\n\n**Why option 0 is incorrect:**\nSpot Instances are a pricing mechanism that allows you to bid on unused EC2 capacity. They do not guarantee single-tenancy. Spot Instances can run on shared hardware, making them unsuitable for regulatory requirements that mandate isolation.\n\n**Why option 1 is incorrect:**\nOn-Demand Instances are a pricing model where you pay for compute capacity by the hour or second. By default, On-Demand instances run on shared hardware. While you *can* launch On-Demand instances on Dedicated Hosts, simply choosing On-Demand as a pricing model doesn't guarantee single-tenancy. Dedicated Hosts are more expensive than Dedicated Instances, making this less cost-effective than option 3.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL Amazon RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend. Which of the following is the correct outcome during the maintenance window?",
    "options": [
      {
        "id": 0,
        "text": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby database instance. This does not cause any downtime for the duration of the upgrade",
        "correct": false
      },
      {
        "id": 1,
        "text": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. This causes downtime until the upgrade is complete",
        "correct": true
      },
      {
        "id": 2,
        "text": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary database instance. This does not cause any downtime for the duration of the upgrade",
        "correct": false
      },
      {
        "id": 3,
        "text": "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment triggers both the primary and standby database instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis correct. During a database engine upgrade in a Multi-AZ deployment, RDS first upgrades the standby instance. Once the standby instance is upgraded, RDS performs a failover, promoting the upgraded standby instance to the primary. Then, the old primary instance (now the standby) is upgraded. This process minimizes downtime because the application experiences a brief interruption only during the failover. The failover process is designed to be quick, but it is not zero downtime.\n\n**Why option 0 is incorrect:**\nis incorrect because while Multi-AZ deployments are designed to minimize downtime during upgrades, they do not eliminate it entirely. There is a brief period of downtime during the failover process when the standby instance is promoted to the primary.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because upgrading both instances at the same time would not be a Multi-AZ deployment upgrade strategy. Multi-AZ deployments are designed to minimize downtime during upgrades, but there is a brief period of downtime during the failover process when the standby instance is promoted to the primary.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 65,
    "text": "Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud. What do you recommend as a replacement for the DFSR?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Storage Service (Amazon S3)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon FSx for Lustre",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx for Windows File Server is the correct answer because it provides a fully managed native Microsoft Windows file system built on Windows Server. It supports the SMB protocol, Active Directory integration, and DFS Replication, making it a direct and compatible replacement for on-premises DFSR. It allows seamless migration of Windows-based applications and file shares to AWS without requiring significant code changes or infrastructure modifications. FSx for Windows File Server offers features like data deduplication, snapshots, and encryption, providing a robust and secure file storage solution.\n\n**Why option 0 is incorrect:**\nAmazon S3 is object storage, not a file system. While S3 can store files, it doesn't support the SMB protocol or DFS Replication, and it's not a direct replacement for a Windows file server. Applications designed to work with a file system would require significant modifications to work with S3.\n\n**Why option 1 is incorrect:**\nAmazon EFS is a network file system designed for Linux-based instances. It uses the NFS protocol and is not compatible with Windows Server or DFSR. While EFS is a good option for shared file storage for Linux workloads, it's not suitable as a replacement for an on-premises Windows file server using DFSR.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  }
]