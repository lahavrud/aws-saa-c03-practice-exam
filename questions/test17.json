[
  {
    "id": 0,
    "text": "A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux \ninstances that run with elastic IP addresses to accept traffic from trusted IP sources on the \ninternet. The SFTP service is backed by shared storage that is attached to the instances. User \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n254 \naccounts are created and managed as Linux users in the SFTP servers. \n \nThe company wants a serverless option that provides high IOPS performance and highly \nconfigurable security. The company also wants to maintain control over user permissions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp- servers/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A company is developing a new machine learning (ML) model solution on AWS. The models are \ndeveloped as independent microservices that fetch approximately 1 GB of model data from \nAmazon S3 at startup and load the data into memory. Users access the models through an \nasynchronous API. Users can send a request or a batch of requests and specify where the \nresults should be sent. \n \nThe company provides models to hundreds of users. The usage patterns for the models are \nirregular. Some models could be unused for days or weeks. Other models could receive batches \nof thousands of requests at a time. \n \nWhich design should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as",
        "correct": false
      },
      {
        "id": 2,
        "text": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A solutions architect wants to use the following JSON text as an identity-based policy to grant \nspecific permissions: \n \n \n \nWhich IAM principals can the solutions architect attach this policy to? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Role",
        "correct": true
      },
      {
        "id": 1,
        "text": "Group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Organization",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Elastic Container Service (Amazon ECS) resource",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon EC2 resource",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company is running a custom application on Amazon EC2 On-Demand Instances. The \napplication has frontend nodes that need to run 24 hours a day, 7 days a week and backend \nnodes that need to run only for a short time based on workload. The number of backend nodes \nvaries during the day. \n \nThe company needs to scale out and scale in more instances based on workload. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Reserved Instances for the frontend nodes. Use AWS Fargate for the backend nodes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Spot Instances for the frontend nodes. Use Reserved Instances for the backend nodes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Spot Instances for the frontend nodes. Use AWS Fargate for the backend nodes.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company uses high block storage capacity to runs its workloads on premises. The company's \ndaily peak input and output transactions per second are not more than 15,000 IOPS. The \ncompany wants to migrate the workloads to Amazon EC2 and to provision disk performance \nindependent of storage capacity. \n \nWhich Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements \nMOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "GP2 volume type",
        "correct": false
      },
      {
        "id": 1,
        "text": "io2 volume type",
        "correct": false
      },
      {
        "id": 2,
        "text": "GP3 volume type",
        "correct": true
      },
      {
        "id": 3,
        "text": "io1 volume type",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 5,
    "text": "A company needs to store data from its healthcare application. The application's data frequently \nchanges. A new regulation requires audit access at all levels of the stored data. \n \nThe company hosts the application on an on-premises infrastructure that is running out of storage \ncapacity. A solutions architect must securely migrate the existing data to AWS while satisfying the \nnew regulation. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A solutions architect is implementing a complex Java application with a MySQL database. The \nJava application must be deployed on Apache Tomcat and must be highly available. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 7,
    "text": "A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. \nThe Lambda function needs permissions to read and write to the DynamoDB table. \n \nWhich solution will give the Lambda function access to the DynamoDB table MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "The following IAM policy is attached to an IAM group. This is the only policy applied to the group. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n258 \n \n \nWhat are the effective IAM permissions of this policy for group members?",
    "options": [
      {
        "id": 0,
        "text": "Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements",
        "correct": false
      },
      {
        "id": 1,
        "text": "Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they",
        "correct": false
      },
      {
        "id": 2,
        "text": "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. \nThese .csv files must be converted into images and must be made available as soon as possible \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n259 \nfor the automatic generation of graphical reports. \n \nThe images become irrelevant after 1 month, but the .csv files must be kept to train machine \nlearning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Design an AWS Lambda function that converts the .csv files into images and stores the images in",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. It allows S3 to automatically manage the tiering of objects based on access patterns, potentially reducing storage costs for data that is not frequently accessed. This is a common and recommended practice for cost optimization.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company has developed a new video game as a web application. The application is in a three-\ntier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players \nwill compete concurrently online. The game's developers want to display a top-10 scoreboard in \nnear-real time and offer the ability to stop and restore the game while preserving the current \nscores. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web",
        "correct": true
      },
      {
        "id": 2,
        "text": "Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "An ecommerce company wants to use machine learning (ML) algorithms to build and train \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n260 \nmodels. The company will use the models to visualize complex scenarios and to detect trends in \ncustomer data. The architecture team wants to integrate its ML models with a reporting platform \nto analyze the augmented data and use the data directly in its business intelligence dashboards. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon QuickSight to build and train models by using calculated fields. Use Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company is running its production and nonproduction environment workloads in multiple AWS \naccounts. The accounts are in an organization in AWS Organizations. The company needs to \ndesign a solution that will prevent the modification of cost usage tags. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a custom AWS Config rule to prevent tag modification except by authorized principals.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a custom trail in AWS CloudTrail to prevent tag modification.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a service control policy (SCP) to prevent tag modification except by authorized principals.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create custom Amazon CloudWatch logs to prevent tag modification.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 \ninstances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon \nDynamoDB table. The company wants to ensure the application can be made available in \nanotherAWS Region with minimal downtime. \n \nWhat should a solutions architect do to meet these requirements with the LEAST amount of \ndowntime?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS CloudFormation template to create EC2 instances, load balancers, and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS CloudFormation template to create EC2 instances and a load balancer to be",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company needs to migrate a MySQL database from its on-premises data center to AWS within \n2 weeks. The database is 20 TB in size. The company wants to complete the migration with \nminimal downtime. \n \nWhich solution will migrate the database MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service",
        "correct": true
      },
      {
        "id": 1,
        "text": "Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 15,
    "text": "A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB \ninstance. The company successfully launched a new product. The workload on the database has \nincreased. The company wants to accommodate the larger workload without adding \ninfrastructure. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 17,
    "text": "A company wants to share accounting data with an external auditor. The data is stored in an \nAmazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account \nand requires its own copy of the database. \n \nWhat is the MOST secure way for the company to share the database with the auditor?",
    "options": [
      {
        "id": 0,
        "text": "Create a read replica of the database. Configure IAM standard database authentication to grant",
        "correct": false
      },
      {
        "id": 1,
        "text": "Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A solutions architect configured a VPC that has a small range of IP addresses. The number of \nAmazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP \naddresses for future workloads. \n \nWhich solution resolves this issue with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a second VPC with additional subnets. Use a peering connection to connect the second",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A company used an Amazon RDS for MySQL DB instance during application testing. Before \nterminating the DB instance at the end of the test cycle, a solutions architect created two \nbackups. The solutions architect created the first backup by using the mysqldump utility to create \na database dump. The solutions architect created the second backup by enabling the final DB \nsnapshot option on RDS termination. \n \nThe company is now planning for a new test cycle and wants to create a new DB instance from \nthe most recent backup. The company has chosen a MySQL-compatible edition ofAmazon \nAurora to host the DB instance. \n \nWhich solutions will create the new DB instance? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Import the RDS snapshot directly into Aurora.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Upload the database dump to Amazon S3. Then import the database dump into Aurora.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 20,
    "text": "A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an \nApplication Load Balancer. The instances run in an Auto Scaling group across multiple \nAvailability Zones. The company observes that the Auto Scaling group launches more On-\nDemand Instances when the application's end users access high volumes of static web content. \nThe company wants to optimize cost. \n \nWhat should a solutions architect do to redesign the application MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function behind an Amazon API Gateway API to host the static website",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 21,
    "text": "A company stores several petabytes of data across multiple AWS accounts. The company uses \nAWS Lake Formation to manage its data lake. The company's data science team wants to \nsecurely share selective data from its accounts with the company's engineering team for \nanalytical purposes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Copy the required data to a common account. Create an IAM access role in that account. Grant",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Lake Formation permissions Grant command in each account where the data is stored to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Data Exchange to privately publish the required data to the required engineering team",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Lake Formation tag-based access control to authorize and grant cross-account permissions",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company wants to host a scalable web application on AWS. The application will be accessed \nby users from different geographic regions of the world. Application users will be able to \ndownload and upload unique data up to gigabytes in size. The development team wants a cost-\neffective solution to minimize upload and download latency and maximize performance. \n \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 with Transfer Acceleration to host the application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon S3 with CacheControl headers to host the application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company has hired a solutions architect to design a reliable architecture for its application. The \napplication consists of one Amazon RDS DB instance and two manually provisioned Amazon \nEC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. \n \nAn employee recently deleted the DB instance, and the application was unavailable for 24 hours \nas a result. The company is concerned with the overall reliability of its environment. \n \nWhat should the solutions architect do to maximize reliability of the application's infrastructure?",
    "options": [
      {
        "id": 0,
        "text": "Delete one EC2 instance and enable termination protection on the other EC2 instance. Update",
        "correct": false
      },
      {
        "id": 1,
        "text": "Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 24,
    "text": "A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in \nits corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct \nConnect connection. \n \nAfter an audit from a regulator, the company has 90 days to move the data to the cloud. The \ncompany needs to move the data efficiently and without disruption. The company still needs to be \nable to access and update the data during the transfer window. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start",
        "correct": true
      },
      {
        "id": 1,
        "text": "Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal \nrequirement to retain all new and existing data in Amazon S3 for 7 years. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Turn on the S3 Versioning feature for the S3 bucket. Configure S3 Lifecycle to delete the data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company has a stateless web application that runs on AWS Lambda functions that are invoked \nby Amazon API Gateway. The company wants to deploy the application across multiple AWS \nRegions to provide Regional failover capabilities. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n267 \n \nWhat should a solutions architect do to route traffic to multiple Regions?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Route 53 health checks for each Region. Use an active-active failover",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer in the primary Region. Set the target group to point to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company has two VPCs named Management and Production. The Management VPC uses \nVPNs through a customer gateway to connect to a single device in the data center. The \nProduction VPC uses a virtual private gateway with two attached AWS Direct Connect \nconnections. The Management and Production VPCs both use a single VPC peering connection \nto allow communication between the applications. \n \nWhat should a solutions architect do to mitigate any single point of failure in this architecture?",
    "options": [
      {
        "id": 0,
        "text": "Add a set of VPNs between the Management and Production VPCs.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a second virtual private gateway and attach it to the Management VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add a second set of VPNs to the Management VPC from a second customer gateway device.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Add a second VPC peering connection between the Management VPC and the Production VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company runs its application on an Oracle database. The company plans to quickly migrate to \nAWS because of limited resources for the database, backup administration, and data center \nmaintenance. The application uses third-party database features that require privileged access. \n \nWhich solution will help the company migrate the database to AWS MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 29,
    "text": "A company has a three-tier web application that is in a single server. The company wants to \nmigrate the application to the AWS Cloud. The company also wants the application to align with \nthe AWS Well-Architected Framework and to be consistent with AWS recommended best \npractices for security, scalability, and resiliency. \n \nWhich combination of solutions will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC across two Availability Zones with the application's existing architecture. Host the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up security groups and network access control lists (network ACLs) to control access to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC across two Availability Zones. Refactor the application to host the web tier,",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use a single Amazon RDS database. Allow database access only from the application tier",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Elastic Load Balancers in front of the web tier. Control access by using security groups",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company is migrating its applications and databases to the AWS Cloud. The company will use \nAmazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. \n \nWhich activities will be managed by the company's operational team? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Management of the Amazon RDS infrastructure layer, operating system, and platforms",
        "correct": false
      },
      {
        "id": 1,
        "text": "Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configuration of additional software components on Amazon ECS for monitoring, patch",
        "correct": false
      },
      {
        "id": 3,
        "text": "Installation of patches for all minor and major database versions for Amazon RDS",
        "correct": false
      },
      {
        "id": 4,
        "text": "Ensure the physical security of the Amazon RDS infrastructure in the data center",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 31,
    "text": "A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and \ntakes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. \nThe CPU utilization of the instance is low except for short surges during which the job uses the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n269 \nmaximum CPU available. The company wants to optimize the costs to run the job. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS App2Container (A2C) to containerize the job. Install the container in the existing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the existing schedule to stop the EC2 instance at the completion of the job and restart",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 32,
    "text": "A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 \nbuckets. Because of regulatory requirements, the company must retain backup files for a specific \ntime period. The company must not alter the files for the duration of the retention period. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A company has resources across multiple AWS Regions and accounts. A newly hired solutions \narchitect discovers a previous employee did not provide details about the resources inventory. \nThe solutions architect needs to build and map the relationship details of the various workloads \nacross all accounts. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager Inventory to generate a map view from the detailed view report.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Step Functions to collect workload details. Build architecture diagrams of the workloads",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Workload Discovery on AWS to generate architecture diagrams of the workloads.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company uses AWS Organizations. The company wants to operate some of its AWS accounts \nwith different budgets. The company wants to receive alerts and automatically prevent \nprovisioning of additional resources on AWS accounts when the allocated budget threshold is met \nduring a specific period. \n \nWhich combination of solutions will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an IAM user for AWS Budgets to run budget actions with the required permissions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM role for AWS Budgets to run budget actions with the required permissions.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Add an alert to notify the company when each account meets its budget threshold. Add a budget",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company runs applications on Amazon EC2 instances in one AWS Region. The company \nwants to back up the EC2 instances to a second Region. The company also wants to provision \nEC2 resources in the second Region and manage the EC2 instances centrally from one AWS \naccount. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a backup plan by using AWS Backup. Configure cross-Region backup to the second",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "A company that uses AWS is building an application to transfer data to a product manufacturer. \nThe company has its own identity provider (IdP). The company wants the IdP to authenticate \napplication users while the users use the application to transfer data. The company must use \nApplicability Statement 2 (AS2) protocol. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon AppFlow flows to transfer the data. Create an Amazon Elastic Container Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback \nservice. The application requires 1 GB of memory and 2 GB of storage for its computation \nresources. The application will require that the data is in a relational format. \n \nWhich additional combination ofAWS services will meet these requirements with the LEAST \nadministrative effort? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon RDS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon Elastic Kubernetes Services (Amazon EKS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging \npolicy adds department tags to AWS resources when the company creates tags. \n \nAn accounting team needs to determine spending on Amazon EC2 consumption. The accounting \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n272 \nteam must determine which departments are responsible for the costs regardless ofAWS \naccount. The accounting team has access to AWS Cost Explorer for all AWS accounts within the \norganization and needs to access all reports from Cost Explorer. \n \nWhich solution meets these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "From the Organizations management account billing console, activate a user-defined cost",
        "correct": true
      },
      {
        "id": 1,
        "text": "From the Organizations management account billing console, activate an AWS-defined cost",
        "correct": false
      },
      {
        "id": 2,
        "text": "From the Organizations member account billing console, activate a user-defined cost allocation",
        "correct": false
      },
      {
        "id": 3,
        "text": "From the Organizations member account billing console, activate an AWS-defined cost allocation",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company wants to securely exchange data between its software as a service (SaaS) \napplication Salesforce account and Amazon S3. The company must encrypt the data at rest by \nusing AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The \ncompany must also encrypt the data in transit. The company has enabled API access for the \nSalesforce account.",
    "options": [
      {
        "id": 0,
        "text": "Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Step Functions workflow. Define the task to transfer the data securely from",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a custom connector for Salesforce to transfer the data securely from Salesforce to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple \nAmazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon \nDynamoDB. The app communicates by using TCP traffic and UDP traffic between the users and \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n273 \nthe servers. The application will be used globally. The company wants to ensure the lowest \npossible latency for all users. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company has an application that processes customer orders. The company hosts the \napplication on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. \nOccasionally when traffic is high the workload does not process orders fast enough. \n \nWhat should a solutions architect do to write the orders reliably to the database as quickly as \npossible?",
    "options": [
      {
        "id": 0,
        "text": "Increase the instance size of the EC2 instance when traffic is high. Write orders to Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in",
        "correct": true
      },
      {
        "id": 2,
        "text": "Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "An IoT company is releasing a mattress that has sensors to collect data about a user's sleep. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n274 \nsensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data \nevery night for each mattress. The company must process and summarize the data for each \nmattress. The results need to be available as soon as possible. Data processing will require 1 GB \nof memory and will finish within 30 seconds. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue with a Scala job",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EMR with an Apache Spark script",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda with a Python script",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue with a PySpark job",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 43,
    "text": "A company hosts an online shopping application that stores all orders in an Amazon RDS for \nPostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and \nhas asked a solutions architect to recommend an approach to minimize database downtime \nwithout requiring any changes to the application code. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Convert the existing database instance to a Multi-AZ deployment by modifying the database",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 44,
    "text": "A company is developing an application to support customer demands. The company wants to \ndeploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability \nZone. The company also wants to give the application the ability to write to multiple block storage \nvolumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application \navailability. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n275",
    "options": [
      {
        "id": 0,
        "text": "Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company designed a stateless two-tier application that uses Amazon EC2 in a single \nAvailability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants \nto ensure the application is highly available. \n \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the application to take snapshots of the EC2 instances and send them to a different",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the application to use Amazon Route 53 latency-based routing to feed requests to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "A company uses AWS Organizations. A member account has purchased a Compute Savings \nPlan. Because of changes in the workloads inside the member account, the account no longer \nreceives the full benefit of the Compute Savings Plan commitment. The company uses less than \n50% of its purchased compute power.",
    "options": [
      {
        "id": 0,
        "text": "Turn on discount sharing from the Billing Preferences section of the account console in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on discount sharing from the Billing Preferences section of the account console in the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate additional compute workloads from another AWS account to the account that has the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Sell the excess Savings Plan commitment in the Reserved Instance Marketplace.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A company is developing a microservices application that will provide a search catalog for \ncustomers. The company must use REST APIs to present the frontend of the application to users. \nThe REST APIs must access the backend services that the company hosts in containers in \nprivate VPC subnets. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": true
      },
      {
        "id": 2,
        "text": "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company stores raw collected data in an Amazon S3 bucket. The data is used for several types \nof analytics on behalf of the company's customers. The type of analytics requested determines \nthe access pattern on the S3 objects. \n \nThe company cannot predict or control the access pattern. The company wants to reduce its S3 \ncosts. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent Access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent Access (S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use S3 Inventory to identify and transition objects that have not been accessed from S3 Standard",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company has applications hosted on Amazon EC2 instances with IPv6 addresses. The \napplications must initiate communications with other external applications using the internet. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n277 \nHowever the company's security policy states that any external service cannot initiate a \nconnection to the EC2 instances. \n \nWhat should a solutions architect recommend to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Create a NAT gateway and make it the destination of the subnet's route table",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an internet gateway and make it the destination of the subnet's route table",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a virtual private gateway and make it the destination of the subnet's route table",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an egress-only internet gateway and make it the destination of the subnet's route table",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company has a mobile chat application with a data store based in Amazon DynamoDB. Users \nwould like new messages to be read with as little latency as possible. A solutions architect needs \nto design an optimal solution that requires minimal application changes. \n \nWhich method should the solutions architect select?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code",
        "correct": true
      },
      {
        "id": 1,
        "text": "Add DynamoDB read replicas to handle the increased read load. Update the application to point",
        "correct": false
      },
      {
        "id": 2,
        "text": "Double the number of read capacity units for the new messages table in DynamoDB. Continue to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon ElastiCache for Redis cache to the application stack. Update the application to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 1 is incorrect:**\nstoring the data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect for similar reasons as S3 Standard-IA. While it offers even lower storage costs, it also has higher retrieval costs and a 30-day minimum storage duration charge. More importantly, S3 One Zone-IA stores data in a single Availability Zone, which makes it less durable than S3 Standard. While durability isn't explicitly mentioned as a primary concern, it's generally a good practice to use S3 Standard unless there's a very specific reason to use a lower-durability option. The frequent access pattern and the 30-day minimum storage duration make this option less cost-effective. Also, the loss of an AZ could impact the analytics pipeline.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The website serves static content. Website traffic is increasing, and the company is \nconcerned about a potential increase in cost.",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution to cache state files at edge locations",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second ALB in an alternative AWS Region. Route user traffic to the closest Region to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company has multiple VPCs across AWS Regions to support and run workloads that are \nisolated from workloads in other Regions. Because of a recent application launch requirement, \nthe company's VPCs must communicate with all other VPCs across all Regions. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
    "options": [
      {
        "id": 0,
        "text": "Use VPC peering to manage VPC communication in a single Region. Use VPC peering across",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transit Gateway to manage VPC communication in a single Region and Transit",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 0 is incorrect:**\nis incorrect because simple scaling policies react to alarms based on thresholds. While you *could* use simple scaling, it requires manual configuration of the scaling adjustment (how many instances to add or remove) and doesn't automatically adjust to maintain a target value. It's less sophisticated and requires more manual intervention than target tracking.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 54,
    "text": "A company is designing a containerized application that will use Amazon Elastic Container \nService (Amazon ECS). The application needs to access a shared file system that is highly \ndurable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 \nhours. The file system needs to provide a mount target m each Availability Zone within a Region. \n \nA solutions architect wants to use AWS Backup to manage the replication to another Region. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Windows File Server with a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx for NetApp ONTAP with a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic File System (Amazon EFS) with the Standard storage class",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon FSx for OpenZFS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company is expecting rapid growth in the near future. A solutions architect needs to configure \nexisting users and grant permissions to new users on AWS. The solutions architect has decided \nto create IAM groups. The solutions architect will add the new users to IAM groups based on \ndepartment. \n \nWhich additional action is the MOST secure way to grant permissions to the new users?",
    "options": [
      {
        "id": 0,
        "text": "Apply service control policies (SCPs) to manage access permissions",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create IAM roles that have least privilege permission. Attach the roles to the IAM groups",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create IAM roles. Associate the roles with a permissions boundary that defines the maximum",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A group requires permissions to list an Amazon S3 bucket and delete objects from that bucket. \nAn administrator has created the following IAM policy to provide access to the bucket and applied \nthat policy to the group. The group is not able to delete objects in the bucket. The company \nfollows least-privilege access rules. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n280 \n \n \nWhich statement should a solutions architect add to the policy to correct bucket access?",
    "options": [
      {
        "id": 0,
        "text": "B.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Answer: D",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A law firm needs to share information with the public. The information includes hundreds of files \nthat must be publicly readable. Modifications or deletions of the files by anyone before a \ndesignated future date are prohibited. \n \nWhich solution will meet these requirements in the MOST secure way?",
    "options": [
      {
        "id": 0,
        "text": "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Grant read-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new Amazon S3 bucket with S3 Versioning enabled. Configure an event trigger to run",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Select the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SNS is a publish/subscribe messaging service that does not guarantee message order. While Lambda can process the updates, SNS is not the right choice for ordered processing. Also, running a SQL database on an EC2 instance increases management overhead compared to a managed database service like DynamoDB or RDS.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS is a message queuing service that does not guarantee message order unless using FIFO queues, which are not mentioned in the option. Also, processing messages from SQS using EC2 instances increases management overhead. While RDS MySQL is a viable database option, DynamoDB is generally preferred for high scalability and availability in this type of scenario, and the EC2 processing is a negative.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company is making a prototype of the infrastructure for its new website by manually \nprovisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an \nApplication Load Balancer and an Amazon RDS database. After the configuration has been \nthoroughly validated, the company wants the capability to immediately deploy the infrastructure \nfor development and production use in two Availability Zones in an automated fashion. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager to replicate and provision the prototype infrastructure in two",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Config to record the inventory of resources that are used in the prototype infrastructure.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object \nstorage. The chief information security officer has directed that no application traffic between the \ntwo services should traverse the public internet. \n \nWhich capability should the solutions architect use to meet the compliance requirements?",
    "options": [
      {
        "id": 0,
        "text": "AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "VPC endpoint",
        "correct": true
      },
      {
        "id": 2,
        "text": "Private subnet",
        "correct": false
      },
      {
        "id": 3,
        "text": "Virtual private gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company hosts a three-tier web application in the AWS Cloud. A Multi-AZAmazon RDS for \nMySQL server forms the database layer Amazon ElastiCache forms the cache layer. The \ncompany wants a caching strategy that adds or updates data in the cache when a customer adds \nan item to the database. The data in the cache must always match the data in the database. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement the lazy loading caching strategy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement the write-through caching strategy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement the adding TTL caching strategy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement the AWS AppConfig caching strategy",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A company wants to migrate 100 GB of historical data from an on-premises location to an \nAmazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on \npremises. The company needs to encrypt the data in transit to the S3 bucket. The company will \nstore new data directly in Amazon S3. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Snowball to move the data to an S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company containerized a Windows job that runs on .NET 6 Framework under a Windows \ncontainer. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. \nThe job's runtime varies between 1 minute and 3 minutes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function based on the container image of the job. Configure Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 63,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n284 \nA company wants to move from many standalone AWS accounts to a consolidated, multi-account \narchitecture. The company plans to create many new AWS accounts for different business units. \nThe company needs to authenticate access to these AWS accounts by using a centralized \ncorporate directory service. \n \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a new organization in AWS Organizations with all features turned on. Create the new",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new organization in AWS Organizations. Configure the organization's authentication",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM",
        "correct": false
      },
      {
        "id": 0,
        "text": "By creating a new organization in AWS Organizations, you can establish a consolidated multi-",
        "correct": false
      },
      {
        "id": 4,
        "text": "Setting up AWS IAM Identity Center (AWS Single Sign-On) within the organization enables",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nOption 4: Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3. While VPN connections provide secure communication, they often introduce overhead and latency, potentially slowing down upload speeds compared to using the public internet with S3 Transfer Acceleration. Also, setting up and managing multiple VPN connections across different regions can be complex and costly. It's not the most cost-effective solution for improving S3 upload speeds.\n\n**Why option 4 is incorrect:**\nOption 4: Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3. While VPN connections provide secure communication, they often introduce overhead and latency, potentially slowing down upload speeds compared to using the public internet with S3 Transfer Acceleration. Also, setting up and managing multiple VPN connections across different regions can be complex and costly. It's not the most cost-effective solution for improving S3 upload speeds.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company is looking for a solution that can store video archives in AWS from old news footage. \nThe company needs to minimize costs and will rarely need to restore these files. When the files \nare needed, they must be available in a maximum of five minutes. \n \nWhat is the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Store the video archives in Amazon S3 Glacier and use Expedited retrievals.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the video archives in Amazon S3 Glacier and use Standard retrievals.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.",
    "domain": "Design Cost-Optimized Architectures"
  }
]