[
  {
    "id": 0,
    "text": "A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux \ninstances that run with elastic IP addresses to accept traffic from trusted IP sources on the \ninternet. The SFTP service is backed by shared storage that is attached to the instances. User \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n254 \naccounts are created and managed as Linux users in the SFTP servers. \n \nThe company wants a serverless option that provides high IOPS performance and highly \nconfigurable security. The company also wants to maintain control over user permissions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/blogs/storage/use-ip-whitelisting-to-secure-your-aws-transfer-for-sftp- servers/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A company is developing a new machine learning (ML) model solution on AWS. The models are \ndeveloped as independent microservices that fetch approximately 1 GB of model data from \nAmazon S3 at startup and load the data into memory. Users access the models through an \nasynchronous API. Users can send a request or a batch of requests and specify where the \nresults should be sent. \n \nThe company provides models to hundreds of users. The usage patterns for the models are \nirregular. Some models could be unused for days or weeks. Other models could receive batches \nof thousands of requests at a time. \n \nWhich design should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as",
        "correct": false
      },
      {
        "id": 2,
        "text": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://aws.amazon.com/blogs/containers/amazon-elastic-container-service-ecs-auto-scaling- using-custom-metrics/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A solutions architect wants to use the following JSON text as an identity-based policy to grant \nspecific permissions: \n \n \n \nWhich IAM principals can the solutions architect attach this policy to? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Role",
        "correct": true
      },
      {
        "id": 1,
        "text": "Group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Organization",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Elastic Container Service (Amazon ECS) resource",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon EC2 resource",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company is running a custom application on Amazon EC2 On-Demand Instances. The \napplication has frontend nodes that need to run 24 hours a day, 7 days a week and backend \nnodes that need to run only for a short time based on workload. The number of backend nodes \nvaries during the day. \n \nThe company needs to scale out and scale in more instances based on workload. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Reserved Instances for the frontend nodes. Use AWS Fargate for the backend nodes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Spot Instances for the frontend nodes. Use Reserved Instances for the backend nodes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Spot Instances for the frontend nodes. Use AWS Fargate for the backend nodes.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company uses high block storage capacity to runs its workloads on premises. The company's \ndaily peak input and output transactions per second are not more than 15,000 IOPS. The \ncompany wants to migrate the workloads to Amazon EC2 and to provision disk performance \nindependent of storage capacity. \n \nWhich Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements \nMOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "GP2 volume type",
        "correct": false
      },
      {
        "id": 1,
        "text": "io2 volume type",
        "correct": false
      },
      {
        "id": 2,
        "text": "GP3 volume type",
        "correct": true
      },
      {
        "id": 3,
        "text": "io1 volume type",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nBoth GP2 and GP3 has max IOPS 16000 but GP3 is cost effective. https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and- save-up-to-20-on-costs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 5,
    "text": "A company needs to store data from its healthcare application. The application's data frequently \nchanges. A new regulation requires audit access at all levels of the stored data. \n \nThe company hosts the application on an on-premises infrastructure that is running out of storage \ncapacity. A solutions architect must securely migrate the existing data to AWS while satisfying the \nnew regulation. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/datasync/latest/userguide/encryption-in-transit.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 6,
    "text": "A solutions architect is implementing a complex Java application with a MySQL database. The \nJava application must be deployed on Apache Tomcat and must be highly available. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Elastic Beanstalk provides an easy and quick way to deploy, manage, and scale applications. It supports a variety of platforms, including Java and Apache Tomcat. By using Elastic Beanstalk, the solutions architect can upload the Java application and configure the environment to run Apache Tomcat.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 7,
    "text": "A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. \nThe Lambda function needs permissions to read and write to the DynamoDB table. \n \nWhich solution will give the Lambda function access to the DynamoDB table MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOption B suggests creating an IAM role that includes Lambda as a trusted service, meaning the role is specifically designed for Lambda functions. The role should have a policy attached to it that grants the required read and write access to the DynamoDB table.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "The following IAM policy is attached to an IAM group. This is the only policy applied to the group. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n258 \n \n \nWhat are the effective IAM permissions of this policy for group members?",
    "options": [
      {
        "id": 0,
        "text": "Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements",
        "correct": false
      },
      {
        "id": 1,
        "text": "Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they",
        "correct": false
      },
      {
        "id": 2,
        "text": "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. \nThese .csv files must be converted into images and must be made available as soon as possible \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n259 \nfor the automatic generation of graphical reports. \n \nThe images become irrelevant after 1 month, but the .csv files must be kept to train machine \nlearning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Design an AWS Lambda function that converts the .csv files into images and stores the images in",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html https://aws.amazon.com/jp/about-aws/whats-new/2021/11/amazon-s3-glacier-storage-class- amazon-s3-glacier-flexible-retrieval/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company has developed a new video game as a web application. The application is in a three-\ntier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players \nwill compete concurrently online. The game's developers want to display a top-10 scoreboard in \nnear-real time and offer the ability to stop and restore the game while preserving the current \nscores. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web",
        "correct": true
      },
      {
        "id": 2,
        "text": "Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/jp/blogs/news/building-a-real-time-gaming-leaderboard-with-amazon- elasticache-for-redis/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "An ecommerce company wants to use machine learning (ML) algorithms to build and train \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n260 \nmodels. The company will use the models to visualize complex scenarios and to detect trends in \ncustomer data. The architecture team wants to integrate its ML models with a reporting platform \nto analyze the augmented data and use the data directly in its business intelligence dashboards. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon QuickSight to build and train models by using calculated fields. Use Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon SageMaker is a fully managed service that provides a complete set of tools and capabilities for building, training, and deploying ML models. It simplifies the end-to-end ML workflow and reduces operational overhead by handling infrastructure provisioning, model training, and deployment. To visualize the data and integrate it into business intelligence dashboards, Amazon QuickSight can be used. QuickSight is a cloud-native business intelligence service that allows users to easily create interactive visualizations, reports, and dashboards from various data sources, including the augmented data generated by the ML models.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company is running its production and nonproduction environment workloads in multiple AWS \naccounts. The accounts are in an organization in AWS Organizations. The company needs to \ndesign a solution that will prevent the modification of cost usage tags. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a custom AWS Config rule to prevent tag modification except by authorized principals.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a custom trail in AWS CloudTrail to prevent tag modification.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a service control policy (SCP) to prevent tag modification except by authorized principals.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create custom Amazon CloudWatch logs to prevent tag modification.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/organizations/latest/userguide/orgs_manage_policies_scps_e xamples_tagging.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 \ninstances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon \nDynamoDB table. The company wants to ensure the application can be made available in \nanotherAWS Region with minimal downtime. \n \nWhat should a solutions architect do to meet these requirements with the LEAST amount of \ndowntime?",
    "options": [
      {
        "id": 0,
        "text": "Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS CloudFormation template to create EC2 instances, load balancers, and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS CloudFormation template to create EC2 instances and a load balancer to be",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company needs to migrate a MySQL database from its on-premises data center to AWS within \n2 weeks. The database is 20 TB in size. The company wants to complete the migration with \nminimal downtime. \n \nWhich solution will migrate the database MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service",
        "correct": true
      },
      {
        "id": 1,
        "text": "Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 15,
    "text": "A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB \ninstance. The company successfully launched a new product. The workload on the database has \nincreased. The company wants to accommodate the larger workload without adding \ninfrastructure. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\n\"without adding infrastructure\" means scaling vertically and choosing larger instance. \"MOST cost-effectively\" reserved instances\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 16,
    "text": "A company operates an ecommerce website on Amazon EC2 instances behind an Application \nLoad Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues \nrelated to a high request rate from illegitimate external systems with changing IP addresses. The \nsecurity team is worried about potential DDoS attacks against the website. The company must \nblock the illegitimate incoming requests in a way that has a minimal impact on legitimate users. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon Inspector and associate it with the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy rules to the network ACLs associated with the ALB to block the incomingtraffic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS WAF (Web Application Firewall) is a service that provides protection for web applications against common web exploits. By associating AWS WAF with the Application Load Balancer (ALB), you can inspect incoming traffic and define rules to allow or block requests based on various criteria.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A company wants to share accounting data with an external auditor. The data is stored in an \nAmazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account \nand requires its own copy of the database. \n \nWhat is the MOST secure way for the company to share the database with the auditor?",
    "options": [
      {
        "id": 0,
        "text": "Create a read replica of the database. Configure IAM standard database authentication to grant",
        "correct": false
      },
      {
        "id": 1,
        "text": "Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe most secure way for the company to share the database with the auditor is option D: Create an encrypted snapshot of the database, share the snapshot with the auditor, and allow access to the AWS Key Management Service (AWS KMS) encryption key. By creating an encrypted snapshot, the company ensures that the database data is protected at rest. Sharing the encrypted snapshot with the auditor allows them to have their own copy of the Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A solutions architect configured a VPC that has a small range of IP addresses. The number of \nAmazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP \naddresses for future workloads. \n \nWhich solution resolves this issue with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a second VPC with additional subnets. Use a peering connection to connect the second",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nYou assign a single CIDR IP address range as the primary CIDR block when you create a VPC and can add up to four secondary CIDR blocks after creation of the VPC.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A company used an Amazon RDS for MySQL DB instance during application testing. Before \nterminating the DB instance at the end of the test cycle, a solutions architect created two \nbackups. The solutions architect created the first backup by using the mysqldump utility to create \na database dump. The solutions architect created the second backup by enabling the final DB \nsnapshot option on RDS termination. \n \nThe company is now planning for a new test cycle and wants to create a new DB instance from \nthe most recent backup. The company has chosen a MySQL-compatible edition ofAmazon \nAurora to host the DB instance. \n \nWhich solutions will create the new DB instance? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Import the RDS snapshot directly into Aurora.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Upload the database dump to Amazon S3. Then import the database dump into Aurora.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.RD SMySQL.Import.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 20,
    "text": "A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an \nApplication Load Balancer. The instances run in an Auto Scaling group across multiple \nAvailability Zones. The company observes that the Auto Scaling group launches more On-\nDemand Instances when the application's end users access high volumes of static web content. \nThe company wants to optimize cost. \n \nWhat should a solutions architect do to redesign the application MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function behind an Amazon API Gateway API to host the static website",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nBy leveraging Amazon CloudFront, you can cache and serve the static web content from edge locations worldwide, reducing the load on your EC2 instances. This can help lower the number of On-Demand Instances required to handle high volumes of static web content requests. Storing the static content in an Amazon S3 bucket and using CloudFront as a content delivery network (CDN) improves performance and reduces costs by reducing the load on your EC2 instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 21,
    "text": "A company stores several petabytes of data across multiple AWS accounts. The company uses \nAWS Lake Formation to manage its data lake. The company's data science team wants to \nsecurely share selective data from its accounts with the company's engineering team for \nanalytical purposes. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Copy the required data to a common account. Create an IAM access role in that account. Grant",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Lake Formation permissions Grant command in each account where the data is stored to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Data Exchange to privately publish the required data to the required engineering team",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Lake Formation tag-based access control to authorize and grant cross-account permissions",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nBy utilizing Lake Formation's tag-based access control, you can define tags and tag-based policies to grant selective access to the required data for the engineering team accounts. This approach allows you to control access at a granular level without the need to copy or move the Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company wants to host a scalable web application on AWS. The application will be accessed \nby users from different geographic regions of the world. Application users will be able to \ndownload and upload unique data up to gigabytes in size. The development team wants a cost-\neffective solution to minimize upload and download latency and maximize performance. \n \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 with Transfer Acceleration to host the application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon S3 with CacheControl headers to host the application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/transfer-acceleration.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company has hired a solutions architect to design a reliable architecture for its application. The \napplication consists of one Amazon RDS DB instance and two manually provisioned Amazon \nEC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. \n \nAn employee recently deleted the DB instance, and the application was unavailable for 24 hours \nas a result. The company is concerned with the overall reliability of its environment. \n \nWhat should the solutions architect do to maximize reliability of the application's infrastructure?",
    "options": [
      {
        "id": 0,
        "text": "Delete one EC2 instance and enable termination protection on the other EC2 instance. Update",
        "correct": false
      },
      {
        "id": 1,
        "text": "Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUpdate the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 24,
    "text": "A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in \nits corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct \nConnect connection. \n \nAfter an audit from a regulator, the company has 90 days to move the data to the cloud. The \ncompany needs to move the data efficiently and without disruption. The company still needs to be \nable to access and update the data during the transfer window. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start",
        "correct": true
      },
      {
        "id": 1,
        "text": "Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy leveraging AWS DataSync in combination with AWS Direct Connect, the company can efficiently and securely transfer its 700 terabytes of data to an Amazon S3 bucket without disruption. The solution allows continued access and updates to the data during the transfer window, ensuring business continuity throughout the migration process.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal \nrequirement to retain all new and existing data in Amazon S3 for 7 years. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Turn on the S3 Versioning feature for the S3 bucket. Configure S3 Lifecycle to delete the data",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo replicate existing object/data in S3 Bucket to bring them to compliance, optionally we use \"S3 Batch Replication\", so option D is the most appropriate, especially if we have big data in S3.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company has a stateless web application that runs on AWS Lambda functions that are invoked \nby Amazon API Gateway. The company wants to deploy the application across multiple AWS \nRegions to provide Regional failover capabilities. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n267 \n \nWhat should a solutions architect do to route traffic to multiple Regions?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Route 53 health checks for each Region. Use an active-active failover",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Application Load Balancer in the primary Region. Set the target group to point to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company has two VPCs named Management and Production. The Management VPC uses \nVPNs through a customer gateway to connect to a single device in the data center. The \nProduction VPC uses a virtual private gateway with two attached AWS Direct Connect \nconnections. The Management and Production VPCs both use a single VPC peering connection \nto allow communication between the applications. \n \nWhat should a solutions architect do to mitigate any single point of failure in this architecture?",
    "options": [
      {
        "id": 0,
        "text": "Add a set of VPNs between the Management and Production VPCs.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a second virtual private gateway and attach it to the Management VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add a second set of VPNs to the Management VPC from a second customer gateway device.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Add a second VPC peering connection between the Management VPC and the Production VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nRedundant VPN connections: Instead of relying on a single device in the data center, the Management VPC should have redundant VPN connections established through multiple customer gateways. This will ensure high availability and fault tolerance in case one of the VPN connections or customer gateways fails.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company runs its application on an Oracle database. The company plans to quickly migrate to \nAWS because of limited resources for the database, backup administration, and data center \nmaintenance. The application uses third-party database features that require privileged access. \n \nWhich solution will help the company migrate the database to AWS MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 29,
    "text": "A company has a three-tier web application that is in a single server. The company wants to \nmigrate the application to the AWS Cloud. The company also wants the application to align with \nthe AWS Well-Architected Framework and to be consistent with AWS recommended best \npractices for security, scalability, and resiliency. \n \nWhich combination of solutions will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC across two Availability Zones with the application's existing architecture. Host the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up security groups and network access control lists (network ACLs) to control access to the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a VPC across two Availability Zones. Refactor the application to host the web tier,",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use a single Amazon RDS database. Allow database access only from the application tier",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Elastic Load Balancers in front of the web tier. Control access by using security groups",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company is migrating its applications and databases to the AWS Cloud. The company will use \nAmazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. \n \nWhich activities will be managed by the company's operational team? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Management of the Amazon RDS infrastructure layer, operating system, and platforms",
        "correct": false
      },
      {
        "id": 1,
        "text": "Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configuration of additional software components on Amazon ECS for monitoring, patch",
        "correct": false
      },
      {
        "id": 3,
        "text": "Installation of patches for all minor and major database versions for Amazon RDS",
        "correct": false
      },
      {
        "id": 4,
        "text": "Ensure the physical security of the Amazon RDS infrastructure in the data center",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 31,
    "text": "A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and \ntakes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. \nThe CPU utilization of the instance is low except for short surges during which the job uses the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n269 \nmaximum CPU available. The company wants to optimize the costs to run the job. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS App2Container (A2C) to containerize the job. Install the container in the existing",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the existing schedule to stop the EC2 instance at the completion of the job and restart",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 32,
    "text": "A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 \nbuckets. Because of regulatory requirements, the company must retain backup files for a specific \ntime period. The company must not alter the files for the duration of the retention period. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A company has resources across multiple AWS Regions and accounts. A newly hired solutions \narchitect discovers a previous employee did not provide details about the resources inventory. \nThe solutions architect needs to build and map the relationship details of the various workloads \nacross all accounts. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager Inventory to generate a map view from the detailed view report.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Step Functions to collect workload details. Build architecture diagrams of the workloads",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Workload Discovery on AWS to generate architecture diagrams of the workloads.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nWorkload Discovery on AWS is a service that helps visualize and understand the architecture of your workloads across multiple AWS accounts and Regions. It automatically discovers and maps the relationships between resources, providing an accurate representation of the architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company uses AWS Organizations. The company wants to operate some of its AWS accounts \nwith different budgets. The company wants to receive alerts and automatically prevent \nprovisioning of additional resources on AWS accounts when the allocated budget threshold is met \nduring a specific period. \n \nWhich combination of solutions will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an IAM user for AWS Budgets to run budget actions with the required permissions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM role for AWS Budgets to run budget actions with the required permissions.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Add an alert to notify the company when each account meets its budget threshold. Add a budget",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/ja_jp/awsaccountbilling/latest/aboutv2/view-billing-dashboard.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company runs applications on Amazon EC2 instances in one AWS Region. The company \nwants to back up the EC2 instances to a second Region. The company also wants to provision \nEC2 resources in the second Region and manage the EC2 instances centrally from one AWS \naccount. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a backup plan by using AWS Backup. Configure cross-Region backup to the second",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing AWS Backup, you can create backup plans that automate the backup process for your EC2 instances. By configuring cross-Region backup, you can ensure that backups are replicated Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 36,
    "text": "A company that uses AWS is building an application to transfer data to a product manufacturer. \nThe company has its own identity provider (IdP). The company wants the IdP to authenticate \napplication users while the users use the application to transfer data. The company must use \nApplicability Statement 2 (AS2) protocol. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon AppFlow flows to transfer the data. Create an Amazon Elastic Container Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nTo authenticate your users, you can use your existing identity provider with AWS Transfer Family. You integrate your identity provider using an AWS Lambda function, which authenticates and authorizes your users for access to Amazon S3 or Amazon Elastic File System (Amazon EFS). https://docs.aws.amazon.com/transfer/latest/userguide/custom-identity-provider-users.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback \nservice. The application requires 1 GB of memory and 2 GB of storage for its computation \nresources. The application will require that the data is in a relational format. \n \nWhich additional combination ofAWS services will meet these requirements with the LEAST \nadministrative effort? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon RDS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon Elastic Kubernetes Services (Amazon EKS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\n\"The application will require that the data is in a relational format\" so DynamoDB is out. RDS is the choice. Lambda is severless.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging \npolicy adds department tags to AWS resources when the company creates tags. \n \nAn accounting team needs to determine spending on Amazon EC2 consumption. The accounting \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n272 \nteam must determine which departments are responsible for the costs regardless ofAWS \naccount. The accounting team has access to AWS Cost Explorer for all AWS accounts within the \norganization and needs to access all reports from Cost Explorer. \n \nWhich solution meets these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "From the Organizations management account billing console, activate a user-defined cost",
        "correct": true
      },
      {
        "id": 1,
        "text": "From the Organizations management account billing console, activate an AWS-defined cost",
        "correct": false
      },
      {
        "id": 2,
        "text": "From the Organizations member account billing console, activate a user-defined cost allocation",
        "correct": false
      },
      {
        "id": 3,
        "text": "From the Organizations member account billing console, activate an AWS-defined cost allocation",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy activating a user-defined cost allocation tag named \"department\" and creating a cost report in Cost Explorer that groups by the tag name and filters by EC2, the accounting team will be able to track and attribute costs to specific departments across all AWS accounts within the organization. This approach allows for consistent cost allocation and reporting regardless of the AWS account structure.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company wants to securely exchange data between its software as a service (SaaS) \napplication Salesforce account and Amazon S3. The company must encrypt the data at rest by \nusing AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The \ncompany must also encrypt the data in transit. The company has enabled API access for the \nSalesforce account.",
    "options": [
      {
        "id": 0,
        "text": "Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Step Functions workflow. Define the task to transfer the data securely from",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a custom connector for Salesforce to transfer the data securely from Salesforce to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon AppFlow is a fully managed integration service that allows you to securely transfer data between different SaaS applications and AWS services. It provides built-in encryption options and supports encryption in transit using SSL/TLS protocols. With AppFlow, you can configure the data transfer flow from Salesforce to Amazon S3, ensuring data encryption at rest by utilizing AWS KMS CMKs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple \nAmazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon \nDynamoDB. The app communicates by using TCP traffic and UDP traffic between the users and \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n273 \nthe servers. The application will be used globally. The company wants to ensure the lowest \npossible latency for all users. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Global Accelerator is a better solution for the mobile gaming app than CloudFront.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company has an application that processes customer orders. The company hosts the \napplication on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. \nOccasionally when traffic is high the workload does not process orders fast enough. \n \nWhat should a solutions architect do to write the orders reliably to the database as quickly as \npossible?",
    "options": [
      {
        "id": 0,
        "text": "Increase the instance size of the EC2 instance when traffic is high. Write orders to Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in",
        "correct": true
      },
      {
        "id": 2,
        "text": "Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy decoupling the write operation from the processing operation using SQS, you ensure that the orders are reliably stored in the queue, regardless of the processing capacity of the EC2 instances. This allows the processing to be performed at a scalable rate based on the available EC2 instances, improving the overall reliability and speed of order processing.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "An IoT company is releasing a mattress that has sensors to collect data about a user's sleep. The \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n274 \nsensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data \nevery night for each mattress. The company must process and summarize the data for each \nmattress. The results need to be available as soon as possible. Data processing will require 1 GB \nof memory and will finish within 30 seconds. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue with a Scala job",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EMR with an Apache Spark script",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda with a Python script",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue with a PySpark job",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Lambda charges you based on the number of invocations and the execution time of your function. Since the data processing job is relatively small (2 MB of data), Lambda is a cost- effective choice. You only pay for the actual usage without the need to provision and maintain infrastructure.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 43,
    "text": "A company hosts an online shopping application that stores all orders in an Amazon RDS for \nPostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and \nhas asked a solutions architect to recommend an approach to minimize database downtime \nwithout requiring any changes to the application code. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Convert the existing database instance to a Multi-AZ deployment by modifying the database",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCompared to other solutions that involve creating new instances, restoring snapshots, or setting up replication manually, converting to a Multi-AZ deployment is a simpler and more streamlined approach with lower overhead. Overall, option A offers a cost-effective and efficient way to minimize database downtime without requiring significant changes or additional complexities.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 44,
    "text": "A company is developing an application to support customer demands. The company wants to \ndeploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability \nZone. The company also wants to give the application the ability to write to multiple block storage \nvolumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application \navailability. \n \nWhich solution will meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n275",
    "options": [
      {
        "id": 0,
        "text": "Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nMulti-Attach is supported exclusively on Provisioned IOPS SSD (io1 and io2) volumes.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company designed a stateless two-tier application that uses Amazon EC2 in a single \nAvailability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants \nto ensure the application is highly available. \n \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the application to take snapshots of the EC2 instances and send them to a different",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the application to use Amazon Route 53 latency-based routing to feed requests to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy combining Multi-AZ EC2 Auto Scaling and an Application Load Balancer, you achieve high availability for the EC2 instances hosting your stateless two-tier application.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "A company uses AWS Organizations. A member account has purchased a Compute Savings \nPlan. Because of changes in the workloads inside the member account, the account no longer \nreceives the full benefit of the Compute Savings Plan commitment. The company uses less than \n50% of its purchased compute power.",
    "options": [
      {
        "id": 0,
        "text": "Turn on discount sharing from the Billing Preferences section of the account console in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on discount sharing from the Billing Preferences section of the account console in the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate additional compute workloads from another AWS account to the account that has the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Sell the excess Savings Plan commitment in the Reserved Instance Marketplace.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 47,
    "text": "A company is developing a microservices application that will provide a search catalog for \ncustomers. The company must use REST APIs to present the frontend of the application to users. \nThe REST APIs must access the backend services that the company hosts in containers in \nprivate VPC subnets. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": false
      },
      {
        "id": 1,
        "text": "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": true
      },
      {
        "id": 2,
        "text": "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-private-integration.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company stores raw collected data in an Amazon S3 bucket. The data is used for several types \nof analytics on behalf of the company's customers. The type of analytics requested determines \nthe access pattern on the S3 objects. \n \nThe company cannot predict or control the access pattern. The company wants to reduce its S3 \ncosts. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent Access",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent Access (S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use S3 Inventory to identify and transition objects that have not been accessed from S3 Standard",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company has applications hosted on Amazon EC2 instances with IPv6 addresses. The \napplications must initiate communications with other external applications using the internet. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n277 \nHowever the company's security policy states that any external service cannot initiate a \nconnection to the EC2 instances. \n \nWhat should a solutions architect recommend to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Create a NAT gateway and make it the destination of the subnet's route table",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an internet gateway and make it the destination of the subnet's route table",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a virtual private gateway and make it the destination of the subnet's route table",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an egress-only internet gateway and make it the destination of the subnet's route table",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAn egress-only internet gateway (EIGW) is specifically designed for IPv6-only VPCs and provides outbound IPv6 internet access while blocking inbound IPv6 traffic. It satisfies the requirement of preventing external services from initiating connections to the EC2 instances while allowing the instances to initiate outbound communications.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A company is creating an application that runs on containers in a VPC. The application stores \nand accesses data in an Amazon S3 bucket. During the development phase, the application will \nstore and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs \nand wants to prevent traffic from traversing the internet whenever possible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable S3 Intelligent-Tiering for the S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable S3 Transfer Acceleration for the S3 bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nPrevent traffic from traversing the internet = Gateway VPC endpoint for S3.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company has a mobile chat application with a data store based in Amazon DynamoDB. Users \nwould like new messages to be read with as little latency as possible. A solutions architect needs \nto design an optimal solution that requires minimal application changes. \n \nWhich method should the solutions architect select?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code",
        "correct": true
      },
      {
        "id": 1,
        "text": "Add DynamoDB read replicas to handle the increased read load. Update the application to point",
        "correct": false
      },
      {
        "id": 2,
        "text": "Double the number of read capacity units for the new messages table in DynamoDB. Continue to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon ElastiCache for Redis cache to the application stack. Update the application to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 52,
    "text": "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The website serves static content. Website traffic is increasing, and the company is \nconcerned about a potential increase in cost.",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution to cache state files at edge locations",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second ALB in an alternative AWS Region. Route user traffic to the closest Region to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon CloudFront: CloudFront is a content delivery network (CDN) service that caches content at edge locations worldwide. By creating a CloudFront distribution, static content from the website can be cached at edge locations, reducing the load on the EC2 instances and improving the overall performance. Caching Static Files: Since the website serves static content, caching these files at CloudFront edge locations can significantly reduce the number of requests forwarded to the EC2 instances. This helps to lower the overall cost by offloading traffic from the instances and reducing the data transfer costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 53,
    "text": "A company has multiple VPCs across AWS Regions to support and run workloads that are \nisolated from workloads in other Regions. Because of a recent application launch requirement, \nthe company's VPCs must communicate with all other VPCs across all Regions. \n \nWhich solution will meet these requirements with the LEAST amount of administrative effort?",
    "options": [
      {
        "id": 0,
        "text": "Use VPC peering to manage VPC communication in a single Region. Use VPC peering across",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transit Gateway to manage VPC communication in a single Region and Transit",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Transit Gateway: Transit Gateway is a highly scalable service that simplifies network connectivity between VPCs and on-premises networks. By using a Transit Gateway in a single Region, you can centralize VPC communication management and reduce administrative effort. Transit Gateway Peering: Transit Gateway supports peering connections across AWS Regions, allowing you to establish connectivity between VPCs in different Regions without the need for complex VPC peering configurations. This simplifies the management of VPC communications across Regions. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 54,
    "text": "A company is designing a containerized application that will use Amazon Elastic Container \nService (Amazon ECS). The application needs to access a shared file system that is highly \ndurable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 \nhours. The file system needs to provide a mount target m each Availability Zone within a Region. \n \nA solutions architect wants to use AWS Backup to manage the replication to another Region. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx for Windows File Server with a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx for NetApp ONTAP with a Multi-AZ deployment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic File System (Amazon EFS) with the Standard storage class",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon FSx for OpenZFS",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/efs/faq/ Q: What is Amazon EFS Replication? EFS Replication can replicate your file system data to another Region or within the same Region without requiring additional infrastructure or a custom process. Amazon EFS Replication automatically and transparently replicates your data to a second file system in a Region or AZ of your choice. You can use the Amazon EFS console, AWS CLI, and APIs to activate replication on an existing file system. EFS Replication is continual and provides a recovery point objective (RPO) and a recovery time objective (RTO) of minutes, helping you meet your compliance and business continuity goals.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company is expecting rapid growth in the near future. A solutions architect needs to configure \nexisting users and grant permissions to new users on AWS. The solutions architect has decided \nto create IAM groups. The solutions architect will add the new users to IAM groups based on \ndepartment. \n \nWhich additional action is the MOST secure way to grant permissions to the new users?",
    "options": [
      {
        "id": 0,
        "text": "Apply service control policies (SCPs) to manage access permissions",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create IAM roles that have least privilege permission. Attach the roles to the IAM groups",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create IAM roles. Associate the roles with a permissions boundary that defines the maximum",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_manage_attach-policy.html Attaching a policy to an IAM user group.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A group requires permissions to list an Amazon S3 bucket and delete objects from that bucket. \nAn administrator has created the following IAM policy to provide access to the bucket and applied \nthat policy to the group. The group is not able to delete objects in the bucket. The company \nfollows least-privilege access rules. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n280 \n \n \nWhich statement should a solutions architect add to the policy to correct bucket access?",
    "options": [
      {
        "id": 0,
        "text": "B.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Answer: D",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A law firm needs to share information with the public. The information includes hundreds of files \nthat must be publicly readable. Modifications or deletions of the files by anyone before a \ndesignated future date are prohibited. \n \nWhich solution will meet these requirements in the MOST secure way?",
    "options": [
      {
        "id": 0,
        "text": "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Grant read-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new Amazon S3 bucket with S3 Versioning enabled. Configure an event trigger to run",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Select the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company is making a prototype of the infrastructure for its new website by manually \nprovisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an \nApplication Load Balancer and an Amazon RDS database. After the configuration has been \nthoroughly validated, the company wants the capability to immediately deploy the infrastructure \nfor development and production use in two Availability Zones in an automated fashion. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager to replicate and provision the prototype infrastructure in two",
        "correct": false
      },
      {
        "id": 1,
        "text": "Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Config to record the inventory of resources that are used in the prototype infrastructure.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS CloudFormation is a service that allows you to define and provision infrastructure as code. This means that you can create a template that describes the resources you want to create, and then use CloudFormation to deploy those resources in an automated fashion. In this case, the solutions architect should define the infrastructure as a template by using the prototype infrastructure as a guide. The template should include resources for an Auto Scaling group, an Application Load Balancer, and an Amazon RDS database. Once the template is created, the solutions architect can use CloudFormation to deploy the infrastructure in two Availability Zones.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 59,
    "text": "A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object \nstorage. The chief information security officer has directed that no application traffic between the \ntwo services should traverse the public internet. \n \nWhich capability should the solutions architect use to meet the compliance requirements?",
    "options": [
      {
        "id": 0,
        "text": "AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "VPC endpoint",
        "correct": true
      },
      {
        "id": 2,
        "text": "Private subnet",
        "correct": false
      },
      {
        "id": 3,
        "text": "Virtual private gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nA VPC endpoint enables you to privately access AWS services without requiring internet gateways, NAT gateways, VPN connections, or AWS Direct Connect connections. It allows you to connect your VPC directly to supported AWS services, such as Amazon S3, over a private connection within the AWS network. By creating a VPC endpoint for Amazon S3, the traffic between your EC2 instances and S3 will stay within the AWS network and won't traverse the public internet. This provides a more secure and compliant solution, as the data transfer remains within the private network boundaries.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company hosts a three-tier web application in the AWS Cloud. A Multi-AZAmazon RDS for \nMySQL server forms the database layer Amazon ElastiCache forms the cache layer. The \ncompany wants a caching strategy that adds or updates data in the cache when a customer adds \nan item to the database. The data in the cache must always match the data in the database. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement the lazy loading caching strategy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement the write-through caching strategy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement the adding TTL caching strategy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement the AWS AppConfig caching strategy",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIn the write-through caching strategy, when a customer adds or updates an item in the database, Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A company wants to migrate 100 GB of historical data from an on-premises location to an \nAmazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on \npremises. The company needs to encrypt the data in transit to the S3 bucket. The company will \nstore new data directly in Amazon S3. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Snowball to move the data to an S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS DataSync is a fully managed data transfer service that simplifies and automates the process of moving data between on-premises storage and Amazon S3. It provides secure and efficient data transfer with built-in encryption, ensuring that the data is encrypted in transit. By using AWS DataSync, the company can easily migrate the 100 GB of historical data from their on-premises location to an S3 bucket. DataSync will handle the encryption of data in transit and ensure secure transfer.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company containerized a Windows job that runs on .NET 6 Framework under a Windows \ncontainer. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. \nThe job's runtime varies between 1 minute and 3 minutes. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function based on the container image of the job. Configure Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nBy leveraging AWS Fargate and ECS, you can achieve cost-effective scaling and resource allocation for your containerized Windows job running on .NET 6 Framework in the AWS Cloud. The serverless nature of Fargate ensures that you only pay for the actual resources consumed by your containers, allowing for efficient cost management.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 63,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n284 \nA company wants to move from many standalone AWS accounts to a consolidated, multi-account \narchitecture. The company plans to create many new AWS accounts for different business units. \nThe company needs to authenticate access to these AWS accounts by using a centralized \ncorporate directory service. \n \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a new organization in AWS Organizations with all features turned on. Create the new",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new organization in AWS Organizations. Configure the organization's authentication",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM",
        "correct": false
      },
      {
        "id": 0,
        "text": "By creating a new organization in AWS Organizations, you can establish a consolidated multi-",
        "correct": false
      },
      {
        "id": 4,
        "text": "Setting up AWS IAM Identity Center (AWS Single Sign-On) within the organization enables",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nA. By creating a new organization in AWS Organizations, you can establish a consolidated multi- account architecture. This allows you to create and manage multiple AWS accounts for different business units under a single organization. E. Setting up AWS IAM Identity Center (AWS Single Sign-On) within the organization enables you to integrate it with the company's corporate directory service. This integration allows for centralized authentication, where users can sign in using their corporate credentials and access the AWS accounts within the organization. Together, these actions create a centralized, multi-account architecture that leverages AWS Organizations for account management and AWS IAM Identity Center (AWS Single Sign-On) for authentication and access control.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company is looking for a solution that can store video archives in AWS from old news footage. \nThe company needs to minimize costs and will rarely need to restore these files. When the files \nare needed, they must be available in a maximum of five minutes. \n \nWhat is the MOST cost-effective solution?",
    "options": [
      {
        "id": 0,
        "text": "Store the video archives in Amazon S3 Glacier and use Expedited retrievals.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the video archives in Amazon S3 Glacier and use Standard retrievals.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy choosing Expedited retrievals in Amazon S3 Glacier, you can reduce the retrieval time to minutes, making it suitable for scenarios where quick access is required. Expedited retrievals come with a higher cost per retrieval compared to standard retrievals but provide faster access to your archived data.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  }
]