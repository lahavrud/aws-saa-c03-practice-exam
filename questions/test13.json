[
  {
    "id": 0,
    "text": "Much of your company's data does not need to be accessed often, and can take several hours for \nretrieval time, so it's stored on Amazon Glacier. However someone within your organization has \nexpressed concerns that his data is more sensitive than the other data, and is wondering whether \nthe high level of encryption that he knows is on S3 is also used on the much cheaper Glacier \nservice. Which of the following statements would be most applicable in regards to this concern?",
    "options": [
      {
        "id": 0,
        "text": "There is no encryption on Amazon Glacier, that's why it is cheaper.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Glacier automatically encrypts the data using AES-256, the same as Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon Glacier automatically encrypts data at rest using AES-256, which is the same encryption standard used by Amazon S3. This ensures that data stored in Glacier is protected with a strong encryption algorithm, addressing the user's concern about data security.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon Glacier does provide encryption at rest. The lower cost of Glacier compared to S3 is primarily due to the storage class characteristics (e.g., retrieval times, availability), not the absence of encryption.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while Glacier does encrypt data at rest, it uses AES-256 encryption, not AES-128. AES-256 is a stronger encryption algorithm than AES-128.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "Your EBS volumes do not seem to be performing as expected and your team leader has \nrequested you look into improving their performance. Which of the following is not a true \nstatement relating to the performance of your EBS volumes?",
    "options": [
      {
        "id": 0,
        "text": "Frequent snapshots provide a higher level of data durability and they will not degrade the",
        "correct": true
      },
      {
        "id": 1,
        "text": "General Purpose (SSD) and Provisioned IOPS (SSD) volumes have a throughput limit of 128",
        "correct": false
      },
      {
        "id": 2,
        "text": "There is a relationship between the maximum performance of your EBS volumes, the amount of",
        "correct": false
      },
      {
        "id": 3,
        "text": "There is a 5 to 50 percent reduction in IOPS when you first access each block of data on a newly",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis statement is not true because while frequent snapshots do provide a higher level of data durability, they *can* degrade the performance of your EBS volumes, especially if taken during peak usage times. Snapshots create a point-in-time copy of the volume, and the process of creating this copy can consume I/O resources, leading to a temporary performance reduction. Therefore, the statement that snapshots *will not* degrade performance is incorrect.\n\n**Why option 1 is incorrect:**\nThis statement is incorrect because General Purpose (SSD) and Provisioned IOPS (SSD) volumes have a throughput limit that can be higher than 128 MiB/s, depending on the volume size and type. gp2 volumes have a maximum throughput of 250 MiB/s, gp3 volumes can reach 1000 MiB/s, and io1/io2 volumes can also reach higher throughput limits. Therefore, the statement that they are limited to 128 MiB/s is not true for all cases.\n\n**Why option 2 is incorrect:**\nThis statement is incorrect because there is indeed a relationship between the maximum performance of your EBS volumes, the amount of provisioned IOPS (for io1/io2 volumes), and the instance type you are using. The instance type needs to be able to support the bandwidth and IOPS that the EBS volume is capable of delivering. If the instance type is a bottleneck, the EBS volume's performance will be limited. Therefore, the statement is true.\n\n**Why option 3 is incorrect:**\nThis statement is incorrect because it accurately describes the 'first-touch' penalty. When you first access a block of data on a newly created or restored EBS volume, there can be a performance reduction (5-50%) as the block is retrieved from storage and loaded into the volume's cache. This is known as the 'first-touch' penalty or 'initialization tax'. Therefore, the statement is true.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "You are building infrastructure for a data warehousing solution and an extra request has come \nthrough that there will be a lot of business reporting queries running all the time and you are not \nsure if your current DB instance will be able to handle it.  \nWhat would be the best solution for this?",
    "options": [
      {
        "id": 0,
        "text": "DB Parameter Groups",
        "correct": false
      },
      {
        "id": 1,
        "text": "Read Replicas",
        "correct": true
      },
      {
        "id": 2,
        "text": "Multi-AZ DB Instance deployment",
        "correct": false
      },
      {
        "id": 3,
        "text": "Database Snapshots",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the problem of heavy read workloads by creating copies of the primary database. These copies, known as read replicas, can handle the business reporting queries, thus offloading the primary instance and preventing performance degradation. This allows the primary instance to focus on write operations and other critical tasks, ensuring optimal performance for the data warehousing solution.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it only involves configuring database parameters. While parameter groups are useful for tuning database performance, they don't provide a mechanism to offload read traffic from the primary database instance. They cannot address the core issue of the primary instance being overloaded by reporting queries.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because Multi-AZ deployment is primarily for high availability and disaster recovery. While it provides a standby instance in case of failure, it doesn't actively offload read traffic from the primary instance. The standby instance is only used during failover, not for handling reporting queries.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because database snapshots are used for backup and recovery purposes. They do not provide a mechanism for handling real-time read traffic or offloading queries from the primary database. Snapshots are point-in-time copies of the data and are not suitable for serving live reporting queries.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "You've created your first load balancer and have registered your EC2 instances with the load \nbalancer. Elastic Load Balancing routinely performs health checks on all the registered EC2 \ninstances and automatically distributes all incoming requests to the DNS name of your load \nbalancer across your registered, healthy EC2 instances. By default, the load balancer uses the \n___ protocol for checking the health of your instances.",
    "options": [
      {
        "id": 0,
        "text": "HTTPS",
        "correct": false
      },
      {
        "id": 1,
        "text": "HTTP",
        "correct": true
      },
      {
        "id": 2,
        "text": "ICMP",
        "correct": false
      },
      {
        "id": 3,
        "text": "IPv6",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because by default, Elastic Load Balancing (ELB), specifically Application Load Balancers (ALB) and Classic Load Balancers, use the HTTP protocol for performing health checks on registered instances. While HTTPS can be configured, it is not the default.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while HTTPS can be configured for health checks, it is not the default protocol used by Elastic Load Balancing. The default is HTTP.\n\n**Why option 2 is incorrect:**\nThis is incorrect because ICMP (Internet Control Message Protocol) is primarily used for network diagnostics like pinging and is not used by Elastic Load Balancing for health checks. Health checks typically involve application-level protocols like HTTP to verify the application's responsiveness.\n\n**Why option 3 is incorrect:**\nThis is incorrect because IPv6 is an addressing protocol and not a protocol used for application health checks. While ELB supports IPv6, it's not the default protocol for health checks.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 4,
    "text": "A major finance organisation has engaged your company to set up a large data mining \napplication. Using AWS you decide the best service for this is Amazon Elastic MapReduce(EMR) \nwhich you know uses Hadoop. Which of the following statements best describes Hadoop?",
    "options": [
      {
        "id": 0,
        "text": "Hadoop is 3rd Party software which can be installed using AMI",
        "correct": false
      },
      {
        "id": 1,
        "text": "Hadoop is an open source python web framework",
        "correct": false
      },
      {
        "id": 2,
        "text": "Hadoop is an open source Java software framework",
        "correct": true
      },
      {
        "id": 3,
        "text": "Hadoop is an open source javascript framework",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Hadoop is indeed an open-source software framework written in Java. It is designed for distributed storage and processing of large datasets using the MapReduce programming model. Amazon EMR leverages Hadoop as a core component for its big data processing capabilities.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while Hadoop *can* be installed on EC2 instances using an AMI, this statement doesn't accurately describe what Hadoop *is*. It focuses on a deployment method rather than its fundamental nature as a software framework. EMR simplifies the deployment and management of Hadoop clusters, making the AMI approach less common in many EMR use cases.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Hadoop is not a Python web framework. Python is often used in conjunction with Hadoop for data analysis and scripting, but Hadoop itself is a Java-based framework.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Hadoop is not a Javascript framework. Javascript is primarily used for front-end web development, while Hadoop is a Java-based framework for distributed data processing.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A company wants to host a scalable web application on AWS. \nThe application will be accessed by users from different geographic regions of the world. \nApplication users will be able to download and upload unique data up to gigabytes in size. \nThe development team wants a cost-effective solution to minimize upload and download latency \nand maximize performance. \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 with Transfer Acceleration to host the application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon S3 with CacheControl headers to host the application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most suitable solution because Amazon S3 provides scalable and cost-effective storage for large files. S3 Transfer Acceleration utilizes the globally distributed AWS edge locations to accelerate uploads to S3. By routing uploads through these edge locations, data is transferred over optimized network paths to the S3 bucket, reducing latency and improving upload speed, especially for users located far from the S3 bucket's region. This directly addresses the requirements of minimizing upload latency and maximizing performance for global users, while leveraging S3's cost-effectiveness for storage.\n\n**Why option 1 is incorrect:**\nWhile Cache-Control headers can improve performance by caching static content, they do not address the core requirement of accelerating large file uploads. Cache-Control headers are primarily used for controlling how browsers and CDNs cache content, which is more relevant for static web assets than for user-generated data uploads. It doesn't help with the initial upload latency.\n\n**Why option 2 is incorrect:**\nUsing EC2 with Auto Scaling and CloudFront is a viable solution for hosting the application itself and caching static content. However, it doesn't directly address the requirement of optimizing large file uploads to the application. While CloudFront can cache downloads, the upload latency to the EC2 instances in a specific region would still be a bottleneck, especially for users far from that region. Furthermore, managing EC2 instances and Auto Scaling groups is more complex and potentially more expensive than using S3 with Transfer Acceleration for storing and uploading large files.\n\n**Why option 3 is incorrect:**\nAmazon ElastiCache is an in-memory data store that improves application performance by caching frequently accessed data. While it can enhance the application's responsiveness, it doesn't address the core requirement of minimizing latency for large file uploads and downloads. ElastiCache is not designed for storing gigabytes of user-generated data; it's better suited for caching database query results or session data. Using EC2 with Auto Scaling and ElastiCache would also be more complex and potentially more expensive than using S3 with Transfer Acceleration for this specific use case.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company captures clickstream data from multiple websites and analyzes it using batch \nprocessing. \nThe data is loaded nightly into Amazon Redshift and is consumed by business analysts. \nThe company wants to move towards near-real-time data processing for timely insights. \nThe solution should process the streaming data with minimal effort and operational overhead. \nWhich combination of AWS services are MOST cost-effective for this solution? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Kinesis Data Firehose",
        "correct": true
      },
      {
        "id": 4,
        "text": "Amazon Kinesis Data Analytics",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of near-real-time data processing with minimal operational overhead. Kinesis Data Firehose automatically delivers streaming data to destinations like Amazon Redshift without requiring you to write or manage any code. It handles tasks like data buffering, compression, and transformation, making it a cost-effective and low-maintenance option for loading data into Redshift.\n\n**Why option 0 is incorrect:**\nUsing EC2 instances would require significant manual configuration and management, including setting up and maintaining streaming data processing applications. This increases operational overhead and cost compared to managed services like Kinesis Data Firehose and Kinesis Data Analytics. It doesn't align with the requirement of minimal effort and operational overhead.\n\n**Why option 1 is incorrect:**\nWhile Lambda can process streaming data, it requires more configuration and coding than Kinesis Data Firehose for direct delivery to Redshift. Lambda functions would need to be triggered by a stream (e.g., from Kinesis Data Streams), process the data, and then load it into Redshift. This adds complexity and operational overhead compared to using Kinesis Data Firehose directly. Also, Lambda alone doesn't handle the ingestion of streaming data.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company is migrating a three-tier application to AWS. \nThe application requires a MySQL database. In the past, the application users reported poor \napplication performance when creating new entries. \nThese performance issues were caused by users generating different real-time reports from the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n138 \napplication duringworking hours. \nWhich solution will improve the performance of the application when it is moved to AWS?",
    "options": [
      {
        "id": 0,
        "text": "Import the data into an Amazon DynamoDB table with provisioned capacity.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create the database on a compute optimized Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the performance bottleneck caused by real-time report generation. Aurora MySQL Multi-AZ provides high availability, while read replicas offload read traffic from the primary database instance. This allows the primary instance to focus on write operations (creating new entries), improving application performance during peak hours when reports are generated. The Multi-AZ deployment also ensures resilience and availability in case of failures.\n\n**Why option 0 is incorrect:**\nWhile DynamoDB is a fast NoSQL database, migrating the data to DynamoDB would require significant application code changes, which is not ideal. Also, the question states the application currently uses a MySQL database, suggesting a relational data model. DynamoDB is not a direct replacement for a relational database without significant architectural changes. Provisioned capacity might improve performance, but it doesn't address the core issue of read contention on the primary database.\n\n**Why option 1 is incorrect:**\nUsing a compute-optimized EC2 instance for the database might provide some performance improvement, but it doesn't address the root cause of the problem, which is read contention. The real-time reports are consuming resources on the database server, impacting the application's ability to create new entries. Simply increasing the compute power of the database server will not solve the problem efficiently. It also doesn't provide high availability like a Multi-AZ deployment.\n\n**Why option 3 is incorrect:**\nCreating an Aurora MySQL Multi-AZ DB cluster provides high availability and failover capabilities, but it doesn't directly address the performance issue caused by read contention from real-time reports. Without read replicas, the reports will still impact the performance of the primary database instance, affecting the application's ability to create new entries.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A start-up company has a web application based in the us-east-1 Region with multiple Amazon \nEC2 instances running behind an Application Load Balancer across multiple Availability Zones. \nAs the company's user base grows in the us-west-1 Region, it needs a solution with low latency \nand high availability. \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Provision EC2 instances in us-west-1.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision EC2 instances and an Application Load Balancer in us-west-1.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision EC2 instances and configure an Application Load Balancer in us-west-1.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision EC2 instances and configure an Application Load Balancer in us-west-1.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by creating a separate infrastructure stack in the us-west-1 region. Provisioning EC2 instances in us-west-1 places the compute resources closer to the users in that region, thereby reducing latency. Configuring an Application Load Balancer in us-west-1 distributes traffic across these EC2 instances, ensuring high availability and fault tolerance within the region. This approach allows for independent scaling and management of the application in each region.\n\n**Why option 0 is incorrect:**\nProvisioning only EC2 instances in us-west-1 without an Application Load Balancer would not provide high availability. If one of the EC2 instances fails, users might experience service disruptions. Furthermore, it doesn't address how traffic will be distributed to these instances.\n\n**Why option 1 is incorrect:**\nWhile provisioning EC2 instances and an Application Load Balancer in us-west-1 is a step in the right direction, it's incomplete. The ALB needs to be *configured* to properly route traffic to the EC2 instances. Simply provisioning the resources without configuring them will not result in a functional and available application.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company must generate sales reports at the beginning of every month. \nThe reporting process launches 20 Amazon EC2 instances on the first of the month. \nThe process runs for 7 days and cannot be interrupted. The company wants to minimize costs. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n139 \nWhich pricing model should the company choose?",
    "options": [
      {
        "id": 0,
        "text": "Reserved Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Spot Block Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "On-Demand Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Scheduled Reserved Instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the most cost-effective solution because it allows you to reserve EC2 instances for a specific recurring schedule. The company knows that it needs 20 instances for 7 days at the beginning of each month. Scheduled Reserved Instances provide a guaranteed capacity reservation for that specific time period, ensuring the instances are available when needed and at a lower cost than On-Demand instances. The fact that the process cannot be interrupted aligns perfectly with the guaranteed availability of Scheduled Reserved Instances.\n\n**Why option 0 is incorrect:**\nReserved Instances are a good choice for long-term, consistent usage. However, they are not ideal for a short, recurring period like the first 7 days of each month. While they provide cost savings compared to On-Demand, the company would be paying for the instances for the entire term of the reservation (1 or 3 years), even when they are not being used. This makes them less cost-effective than Scheduled Reserved Instances for this specific use case.\n\n**Why option 1 is incorrect:**\nSpot Block Instances (now simply Spot Instances with defined duration) could offer significant cost savings, but they are not suitable for a process that cannot be interrupted. Spot Instances can be terminated by AWS with a short notice (2 minutes) if the spot price exceeds the company's bid. This risk of interruption makes Spot Instances unsuitable for this scenario, as the reporting process must run for the full 7 days without interruption.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company's web application uses an Amazon RDS PostgreSQL DB instance to store its \napplication data. \nDuring the financial closing period at the start of every month. Accountants run large queries that \nimpact the database's performance due to high usage. \nThe company wants to minimize the impact that the reporting activity has on the web application. \nWhat should a solutions architect do to reduce the impact on the database with the LEAST \namount of effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a read replica and direct reporting traffic to the replica.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Multi-AZ database and direct reporting traffic to the standby.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a cross-Region read replica and direct reporting traffic to the replica.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Redshift database and direct reporting traffic to the Amazon Redshift",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most straightforward and efficient solution. Creating a read replica allows the reporting queries to be executed against the replica, thus isolating the workload from the primary RDS instance that serves the web application. This minimizes the performance impact on the web application without requiring significant changes to the existing architecture or data migration. Read replicas are designed for read-heavy workloads and are relatively easy to set up and manage.\n\n**Why option 1 is incorrect:**\nWhile a Multi-AZ database provides high availability and failover capabilities, it does not inherently offload read traffic. The standby instance in a Multi-AZ configuration is primarily for failover purposes and is not designed to handle read traffic directly. Directing reporting traffic to the standby instance is not a supported or recommended practice, and it could potentially interfere with the failover process. Furthermore, it doesn't address the need to separate the reporting workload from the primary database.\n\n**Why option 2 is incorrect:**\nCreating a cross-Region read replica would certainly offload the reporting workload. However, it introduces additional complexity and latency due to the geographical distance between the primary and replica. This adds unnecessary overhead and cost compared to a same-region read replica, especially since the question emphasizes minimizing effort. The added latency might also be unacceptable for the reporting requirements.\n\n**Why option 3 is incorrect:**\nCreating an Amazon Redshift database and migrating the reporting data to it would be a more complex and time-consuming solution. It involves significant data migration, schema changes, and potentially application code modifications to point the reporting queries to Redshift. While Redshift is well-suited for analytical workloads, it is overkill for this scenario, where a simple read replica can effectively address the performance bottleneck with minimal effort.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "A company has application running on Amazon EC2 instances in a VPC. \nOne of the applications needs to call an Amazon S3 API to store and read objects. \nThe company's security policies restrict any internet-bound traffic from the applications. \nWhich action will fulfill these requirements and maintain security?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 interface endpoint.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 gateway endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an S3 bucket in a private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket in the same Region as the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating a route within the VPC that allows traffic destined for S3 to be routed directly to S3 without traversing the internet. A gateway endpoint is designed specifically for S3 and DynamoDB, providing a cost-effective and highly available connection. It modifies the VPC route table to direct S3-bound traffic to the endpoint, ensuring no internet gateway or NAT gateway is required.\n\n**Why option 0 is incorrect:**\nThis is incorrect because an S3 interface endpoint uses AWS PrivateLink, which provides private connectivity to S3 using elastic network interfaces (ENIs) with private IP addresses in your VPC. While this does provide private connectivity, it's more complex and generally more expensive than a gateway endpoint for S3. The question doesn't necessitate the features of PrivateLink (like DNS resolution within the VPC), making the gateway endpoint the more appropriate and cost-effective solution.\n\n**Why option 2 is incorrect:**\nThis is incorrect because the location of the S3 bucket (private subnet or not) does not affect the network path taken by the EC2 instances to access S3. The EC2 instances still need a way to reach S3, and placing the bucket in a private subnet doesn't provide that connectivity. The bucket's accessibility is controlled by IAM policies and bucket policies, not the subnet it resides in (which it doesn't reside in anyway).\n\n**Why option 3 is incorrect:**\nThis is incorrect because the Region of the S3 bucket, while important for latency and data residency, doesn't directly address the requirement of restricting internet-bound traffic. Even if the S3 bucket is in the same Region as the EC2 instances, the traffic will still attempt to go through the internet unless a VPC endpoint is configured.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A website runs a web application that receives a burst of traffic each day at noon. The users \nupload new pictures and content daily, but have been complaining of timeouts. The architecture \nuses Amazon EC2 Auto Scaling groups, and the custom application consistently takes 1 minute \nto initiate upon boot up before responding to user requests. \nHow should a solutions architect redesign the architecture to better respond to changing traffic?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Network Load Balancer with a slow start configuration.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS ElastiCache for Redis to offload direct requests to the servers.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Auto Scaling step scaling policy with an instance warmup condition.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudFront to use an Application Load Balancer as the origin.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the problem of instance initialization time. By configuring an Auto Scaling step scaling policy with an instance warmup condition, newly launched instances are given time to initialize before being considered healthy and receiving traffic. This prevents requests from being routed to instances that are still starting up, thus avoiding timeouts. The step scaling policy ensures that the Auto Scaling group can quickly add capacity in response to the traffic burst, while the instance warmup condition ensures that the new instances are ready to handle traffic before being put into service.\n\n**Why option 0 is incorrect:**\nA Network Load Balancer (NLB) with a slow start configuration is not the most effective solution for this scenario. While slow start gradually increases the traffic sent to new instances, it doesn't address the underlying issue of the 1-minute application initialization time. Instances will still be unavailable for the first minute, potentially causing timeouts during the traffic burst. NLB slow start is more suitable for gradual traffic increases, not sudden spikes with a known instance startup delay.\n\n**Why option 1 is incorrect:**\nWhile ElastiCache can improve performance by caching frequently accessed data, it doesn't directly address the problem of instance initialization time. Offloading requests to ElastiCache might reduce the load on the application servers, but new instances will still take 1 minute to initialize, and users will still experience timeouts during that period. ElastiCache is more suitable for improving overall application performance and reducing database load, not for mitigating instance startup delays.\n\n**Why option 3 is incorrect:**\nCloudFront with an Application Load Balancer (ALB) as the origin can improve performance by caching static content and distributing traffic globally. However, it doesn't directly address the problem of the 1-minute application initialization time. While CloudFront can cache content, the initial requests to the ALB will still be subject to the instance startup delay, potentially causing timeouts for uncached content. CloudFront is more suitable for improving content delivery and reducing latency for geographically dispersed users, not for mitigating instance startup delays.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company hosts its website on Amazon S3. The website serves petabytes of outbound traffic \nmonthly, which accounts for most of the company's AWS costs. \nWhat should a solutions architect do to reduce costs?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudFront with the existing website as the origin.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Move the website to Amazon EC2 with Amazon EBS volumes for storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Global Accelerator and specify the existing website as the endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Rearchitect the website to run on a combination of Amazon API Gateway and AWS Lambda.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-effective solution because Amazon CloudFront caches the website's content at edge locations globally. This reduces the amount of data transferred directly from the S3 bucket, significantly lowering outbound data transfer costs. CloudFront also offers compression and other optimizations that can further reduce bandwidth usage and improve performance.\n\n**Why option 1 is incorrect:**\nMoving the website to Amazon EC2 with EBS volumes would likely increase costs. EC2 instances and EBS volumes have associated costs, and data transfer out from EC2 is generally more expensive than data transfer out from CloudFront when serving large amounts of static content. This option also adds operational overhead for managing the EC2 instances and EBS volumes.\n\n**Why option 2 is incorrect:**\nAWS Global Accelerator improves the performance of applications by routing traffic through AWS's global network. While it can improve performance, it primarily focuses on improving network latency and reliability, not on reducing data transfer costs. It doesn't cache content like CloudFront, so it won't significantly reduce the outbound traffic from S3. It also adds another layer of cost without directly addressing the primary cost driver.\n\n**Why option 3 is incorrect:**\nRearchitecting the website to run on API Gateway and Lambda would be a significant undertaking and is unlikely to be cost-effective for serving static website content. API Gateway and Lambda are better suited for dynamic content and serverless applications. Serving static content through API Gateway and Lambda would introduce unnecessary overhead and costs compared to using S3 and CloudFront.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 14,
    "text": "A company currently stores symmetric encryption keys in a hardware security module (HSM). A \nsolution architect must design a solution to migrate key management to AWS. The solution \nshould allow for key rotation and support the use of customer provided keys. Where should the \nkey material be stored to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n141",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Secrets Manager",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Systems Manager Parameter store",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Key Management Service (AWS KMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because AWS Secrets Manager allows you to store, rotate, and manage secrets, including encryption keys. While it doesn't directly support customer-provided keys in the same way as KMS, you can store the customer-provided key as a secret and manage its rotation within Secrets Manager. It is also more suitable for storing secrets that are not necessarily encryption keys, making it a more general-purpose solution for managing sensitive information, including customer-provided keys.\n\n**Why option 0 is incorrect:**\nAmazon S3 is object storage and is not designed for storing and managing encryption keys. It does not provide key rotation capabilities or specific support for customer-provided keys.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nAWS Systems Manager Parameter Store is suitable for storing configuration data and secrets, but it is not primarily designed for managing encryption keys with rotation capabilities and customer-provided key support. While you *could* store keys there, it's not the best practice or most secure approach compared to dedicated key management services.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company needs to implement a relational database with a multi-Region disaster recovery \nRecovery Point Objective (RPO) of 1 second and an Recovery Time Objective (RTO) of 1 minute. \nWhich AWS solution can achieve this?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Aurora Global Database",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB global tables.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon RDS for MySQL with Multi-AZ enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon RDS for MySQL with a cross-Region snapshot copy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon Aurora Global Database is specifically designed for globally distributed applications requiring low latency reads in multiple regions and disaster recovery with a Recovery Point Objective (RPO) of less than 1 second and a Recovery Time Objective (RTO) of less than 1 minute. It replicates data to multiple AWS Regions with low latency, enabling fast failover in case of a regional outage.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon DynamoDB global tables, while providing multi-region replication, are a NoSQL database solution and do not fulfill the requirement of a relational database. Also, while DynamoDB provides fast replication, it is not designed to be a relational database.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon RDS for MySQL with Multi-AZ enabled provides high availability within a single region, not cross-region disaster recovery. It does not address the multi-region requirement or the stringent RPO/RTO specified in the question.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon RDS for MySQL with a cross-Region snapshot copy involves taking snapshots of the database and copying them to another region. This process is not real-time and would not meet the RPO of 1 second and RTO of 1 minute. The time to copy and restore a snapshot would significantly exceed the allowed RTO.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company is designing a new service that will run on Amazon EC2 instance behind an Elastic \nLoad Balancer. However, many of the web service clients can only reach IP addresses \nwhitelisted on their firewalls. \nWhat should a solution architect recommend to meet the clients' needs?",
    "options": [
      {
        "id": 0,
        "text": "A Network Load Balancer with an associated Elastic IP address.",
        "correct": false
      },
      {
        "id": 1,
        "text": "An Application Load Balancer with an a associated Elastic IP address",
        "correct": false
      },
      {
        "id": 2,
        "text": "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address",
        "correct": true
      },
      {
        "id": 3,
        "text": "An EC2 instance with a public IP address running as a proxy in front of the load balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by providing a static IP address that clients can whitelist. An Elastic IP address is a static, public IPv4 address designed for dynamic cloud computing. By associating an Elastic IP address with an EC2 instance, and then configuring an A record in Route 53 to point to that Elastic IP, clients can whitelist the Elastic IP address. The EC2 instance acts as a proxy, forwarding traffic to the load balancer. This allows the clients to access the service through a known, static IP address, fulfilling the whitelisting requirement.\n\n**Why option 0 is incorrect:**\nWhile a Network Load Balancer (NLB) provides static IP addresses per Availability Zone, it doesn't directly solve the whitelisting problem in a manageable way for a large number of clients. NLBs are designed for high performance and TCP/UDP traffic, and are not typically used for web traffic that requires HTTP/HTTPS inspection. Also, the question implies the need for a single, static IP address for clients to whitelist, not multiple per AZ. Furthermore, directly exposing the NLB's IP addresses to external clients for whitelisting is not a standard or recommended practice.\n\n**Why option 1 is incorrect:**\nAn Application Load Balancer (ALB) does not provide static IP addresses. ALBs use dynamic IP addresses that can change, making it impossible for clients to whitelist them. ALBs are designed for HTTP/HTTPS traffic and provide advanced features like content-based routing, but they are not suitable for scenarios requiring static IP addresses for whitelisting.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A company's packaged application dynamically creates and returns single-use text files in \nresponse to user requests. The company is using Amazon CloudFront for distribution, but wants \nto future reduce data transfer costs. The company modify the application's source code. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n142 \nWhat should a solution architect do to reduce costs?",
    "options": [
      {
        "id": 0,
        "text": "Use Lambda@Edge to compress the files as they are sent to users.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable Amazon S3 Transfer Acceleration to reduce the response times.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable caching on the CloudFront distribution to store generated files at the edge.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 multipart uploads to move the files to Amazon S3 before returning them to users.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Lambda@Edge allows you to execute code at CloudFront edge locations. By compressing the text files using Lambda@Edge before they are sent to the users, the amount of data transferred is reduced, leading to lower data transfer costs. This approach doesn't require modifying the application's origin server code, which aligns with the problem statement.\n\n**Why option 1 is incorrect:**\nThis is incorrect because S3 Transfer Acceleration speeds up uploads *to* S3, not downloads *from* CloudFront to users. The problem focuses on reducing data transfer costs *from* CloudFront to the users, not speeding up uploads to S3. Using S3 Transfer Acceleration would not address the cost reduction requirement.\n\n**Why option 2 is incorrect:**\nThis is incorrect because the files are single-use, meaning caching them at the edge would not be effective. CloudFront caching is beneficial for frequently accessed content, but since each file is unique, caching would result in a very low cache hit ratio, negating any potential cost savings. Furthermore, caching single-use files could potentially lead to security issues if not handled carefully.\n\n**Why option 3 is incorrect:**\nThis is incorrect because using S3 multipart uploads is designed for uploading large files to S3 efficiently and reliably. It does not directly reduce data transfer costs from CloudFront to the users. The question focuses on reducing the cost of delivering the files to the users, not on how the files are stored or uploaded.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "An application running on an Amazon EC2 instance in VPC-A needs to access files in another \nEC2 instance in VPC-B. Both are in separate AWS accounts. \nThe network administrator needs to design a solution to enable secure access to EC2 instance in \nVPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth \nconcerns. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up a VPC peering connection between VPC-A and VPC-B.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Attach a virtual private gateway to VPC-B and enable routing from VPC-A.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because VPC peering allows direct network connectivity between two VPCs, even if they are in different AWS accounts. It establishes a private connection without routing traffic through the public internet, ensuring secure communication. VPC peering does not have a single point of failure as it is a distributed service. Also, VPC peering connections do not introduce bandwidth limitations, allowing for high-throughput communication between the EC2 instances.\n\n**Why option 1 is incorrect:**\nThis is incorrect because VPC gateway endpoints are used to provide access to AWS services like S3 and DynamoDB, not to connect to EC2 instances in other VPCs. Gateway endpoints do not facilitate general network connectivity between VPCs.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a virtual private gateway (VGW) is used to establish a VPN connection between a VPC and an on-premises network. While it can be used to connect two VPCs, it requires a customer gateway on the other side, which is not the scenario described in the question. Furthermore, VPN connections can introduce bandwidth limitations and are more complex to manage than VPC peering. The question specifies that both EC2 instances are in separate AWS accounts, so a VGW is not the most appropriate solution.\n\n**Why option 3 is incorrect:**\nThis is incorrect because a Private Virtual Interface (VIF) is used with Direct Connect to establish a private connection between an on-premises network and AWS. It does not facilitate connectivity between VPCs in different AWS accounts. Direct Connect is also an over-engineered solution for this scenario, as it's designed for dedicated, high-bandwidth connections between on-premises infrastructure and AWS, not for connecting two EC2 instances.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company stores user data in AWS. The data is used continuously with peak usage during \nbusiness hours. Access patterns vary, with some data not being used for months at a time. A \nsolutions architect must choose a cost-effective solution that maintains the highest level of \ndurability while maintaining high availability. \n \nWhich storage solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 One Zone-Infequent Access (S3 One Zone-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution automatically moves data between different storage tiers based on access patterns. It moves frequently accessed data to the frequent access tier and infrequently accessed data to the infrequent access tier, optimizing costs without compromising performance. This addresses the requirement for cost-effectiveness and handles varying access patterns. S3 Intelligent-Tiering stores data in multiple Availability Zones, ensuring high durability and availability.\n\n**Why option 0 is incorrect:**\nWhile this option provides high durability and availability, it doesn't inherently address the cost-effectiveness requirement for data that is infrequently accessed. Storing all data in standard S3, regardless of access frequency, would be more expensive than using a tiered storage solution.\n\n**Why option 2 is incorrect:**\nThis option is designed for long-term archival of data that is rarely accessed. While it is the most cost-effective storage option, retrieving data from Glacier Deep Archive can take several hours, which does not meet the requirement for continuous data usage and high availability.\n\n**Why option 3 is incorrect:**\nThis option offers lower availability than standard S3 because it stores data in a single Availability Zone. While it is more cost-effective than standard S3, it does not meet the requirement for high durability, as data loss is possible if the single Availability Zone becomes unavailable.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect is creating an application that will handle batch processing of large amounts \nof data. The input data will be held in Amazon S3 and the output data will be stored in a different \nS3 bucket. For processing, the application will transfer the data over the network between \nmultiple Amazon EC2 instances. \n \nWhat should the solutions architect do to reduce the overall data transfer costs?",
    "options": [
      {
        "id": 0,
        "text": "Place all the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Place all the EC2 instances in the same AWS Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Place all the EC2 instances in the same Availability Zone.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Place all the EC2 instances in private subnets in multiple Availability Zones.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because data transfer within the same Availability Zone is free. By placing all EC2 instances in the same Availability Zone, the application avoids incurring data transfer charges for the network traffic between the instances during the batch processing. This directly addresses the requirement of reducing overall data transfer costs.\n\n**Why option 0 is incorrect:**\nPlacing EC2 instances in an Auto Scaling group primarily focuses on scaling the application based on demand or other metrics. While Auto Scaling can help optimize costs by adjusting the number of instances, it doesn't directly reduce the data transfer costs between instances. The data transfer costs will still be incurred regardless of whether the instances are part of an Auto Scaling group. Therefore, this option does not directly address the problem of minimizing data transfer costs.\n\n**Why option 1 is incorrect:**\nPlacing all EC2 instances in the same AWS Region reduces data transfer costs compared to transferring data between different Regions, which is significantly more expensive. However, data transfer between Availability Zones within the same Region still incurs charges. Therefore, while this option is better than using multiple Regions, it's not the most cost-effective solution for minimizing data transfer costs within the application's architecture.\n\n**Why option 3 is incorrect:**\nPlacing EC2 instances in private subnets in multiple Availability Zones increases data transfer costs. Data transfer between Availability Zones incurs charges. Using private subnets doesn't affect the data transfer costs; it only affects network accessibility from the public internet. This option is the opposite of what the question asks for, as it increases data transfer costs instead of reducing them.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 21,
    "text": "A company has recently updated its internal security standards. \nThe company must now ensure all Amazon S3 buckets and Amazon Elastic Block Store (Amazon \nEBS) volumes are encrypted with keys created and periodically rotated by internal security \nspecialists. \nThe company is looking for a native, software-based AWS service to accomplish this goal. \nWhat should a solutions architect recommend as a solution?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Secrets Manager with customer master keys (CMKs) to store master key material and",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Key Management Service (AWS KMS) with customer master keys (CMKs) to store",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an AWS CloudHSM cluster with customer master keys (CMKs) to store master key material",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Parameter Store with customer master keys (CMKs) keys to store",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by utilizing AWS KMS with customer managed keys (CMKs). AWS KMS is a native AWS service designed for key management. Using CMKs allows the company's internal security specialists to create, manage, and rotate the encryption keys used for S3 and EBS encryption. This fulfills the security and compliance requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nWhile AWS Secrets Manager can store secrets, including encryption keys, it's primarily designed for managing database credentials, API keys, and other secrets used by applications. It's not the primary service for managing encryption keys used to encrypt S3 buckets and EBS volumes. AWS KMS is the more appropriate service for this purpose.\n\n**Why option 2 is incorrect:**\nAWS CloudHSM provides dedicated hardware security modules within the AWS cloud. While it offers the highest level of security and control over cryptographic keys, it also introduces significant operational overhead and complexity. For the stated requirements of encrypting S3 and EBS with internally managed and rotated keys, AWS KMS provides a more suitable and less complex solution. CloudHSM is generally chosen when regulatory compliance mandates the use of dedicated hardware.\n\n**Why option 3 is incorrect:**\nAWS Systems Manager Parameter Store is designed for storing configuration data and secrets. While it can store encryption keys, it's not specifically designed for managing cryptographic keys used for encrypting S3 buckets and EBS volumes. It lacks the key management features and integration with AWS services that AWS KMS provides. Therefore, it's not the optimal solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "An application is running on Amazon EC2 instances Sensitive information required for the \napplication is stored in an Amazon S3 bucket. \nThe bucket needs to be protected from internet access while only allowing services within the \nVPC access to the bucket. \nWhich combination of actions should a solutions archived take to accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC endpoint for Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable server access logging on the bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Apply a bucket policy to restrict access to the S3 endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an S3 ACL to the bucket that has sensitive information",
        "correct": false
      },
      {
        "id": 4,
        "text": "Restrict users using the IAM policy to use the specific bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a VPC endpoint for S3 allows resources within the VPC to access S3 without traversing the public internet. It provides a private connection between the VPC and S3, ensuring that traffic remains within the AWS network. This directly addresses the requirement of preventing internet access to the S3 bucket.\n\n**Why option 1 is incorrect:**\nThis is incorrect because enabling server access logging on the bucket only records access requests made to the bucket. It does not restrict or control access to the bucket itself. While logging is a good security practice, it doesn't address the core requirement of preventing internet access.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while a bucket policy can restrict access, it needs to be used in conjunction with a VPC endpoint. A bucket policy alone cannot prevent access from the internet if there is no VPC endpoint in place. The question asks for a combination of actions, and using only a bucket policy is insufficient to meet the requirement.\n\n**Why option 3 is incorrect:**\nThis is incorrect because S3 ACLs (Access Control Lists) are a legacy access control mechanism and are not the recommended way to manage access to S3 buckets. Bucket policies offer more granular control and are the preferred method. Furthermore, ACLs alone cannot restrict access based on the source VPC.\n\n**Why option 4 is incorrect:**\nThis is incorrect because restricting users using IAM policies controls what actions users can perform on the S3 bucket, but it does not prevent access from outside the VPC. IAM policies focus on authentication and authorization of users and roles, not network-level access control.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A company relies on an application that needs at least 4 Amazon EC2 instances during regular \ntraffic and must scale up to 12 EC2 instances during peak loads. \nThe application is critical to the business and must be highly available. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using an Auto Scaling group. Auto Scaling groups allow you to define a minimum capacity (4 instances in this case), a desired capacity, and a maximum capacity (12 instances). The Auto Scaling group automatically launches and terminates EC2 instances based on scaling policies, ensuring that the application has the necessary resources to handle traffic fluctuations. Furthermore, by deploying instances across multiple Availability Zones (which is implied by the need for high availability and is a default behavior of Auto Scaling), the application remains available even if one Availability Zone experiences an outage.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it lacks specific details about how the Auto Scaling group is configured to meet the requirements. While using an Auto Scaling group is the right approach, this option doesn't mention the crucial aspects of setting minimum, desired, and maximum capacity, or the deployment across multiple Availability Zones for high availability. Without these details, the solution is incomplete and may not fully meet the requirements.\n\n**Why option 1 is incorrect:**\nThis option is incorrect for the same reasons as option 0. It mentions using an Auto Scaling group but omits the necessary configuration details to ensure minimum capacity, scalability to peak loads, and high availability across multiple Availability Zones.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company recently deployed a two-tier application in two Availability Zones in the us-east-1 \nRegion. The databases are deployed in a private subnet while the web servers are deployed in a \npublic subnet. \nAn internet gateway is attached to the VPC. The application and database run on Amazon EC2 \ninstances. The database servers are unable to access patches on the internet. \nA solutions architect needs to design a solution that maintains database security with the least \noperational overhead. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n145 \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a NAT gateway inside the public subnet for each Availability Zone and associate it with an",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy a NAT gateway inside the private subnet for each Availability Zone and associate it with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy two NAT instances inside the public subnet for each Availability Zone and associate them",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy two NAT instances inside the private subnet for each Availability Zone and associate them",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by providing a managed NAT gateway in the public subnet. The NAT gateway allows instances in the private subnet to initiate outbound traffic to the internet without allowing inbound traffic from the internet. Deploying a NAT gateway in each Availability Zone ensures high availability. This approach minimizes operational overhead because NAT Gateways are managed services, reducing the administrative burden compared to NAT instances.\n\n**Why option 1 is incorrect:**\nDeploying a NAT gateway in the private subnet would not provide the necessary internet access. NAT gateways need to be in a public subnet to route traffic to the internet gateway. The database servers are already in the private subnet, so placing the NAT gateway there doesn't solve the problem of outbound internet access.\n\n**Why option 2 is incorrect:**\nDeploying NAT instances requires more operational overhead than using NAT Gateways. NAT instances require manual configuration, patching, and scaling. While this solution would provide internet access, it doesn't meet the requirement of minimizing operational overhead. Also, deploying two NAT instances per AZ doesn't necessarily provide HA without additional configuration like routing tables and health checks.\n\n**Why option 3 is incorrect:**\nDeploying NAT instances in the private subnet would not provide the necessary internet access. NAT instances need to be in a public subnet to route traffic to the internet gateway. The database servers are already in the private subnet, so placing the NAT instances there doesn't solve the problem of outbound internet access.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company has an on-premises data center that is running out of storage capacity. The company \nwants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution \nmust allow for immediate retrieval of data at no additional cost. \nHow can these requirements be met?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon S3 Glacier Vault and enable expedited retrieval.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Storage Gateway using cached volumes.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy AWS Storage Gateway using stored volumes to store data locally.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy AWS Direct Connect to connect with the on-premises data center.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by storing the entire dataset on-premises using AWS Storage Gateway's stored volumes. This minimizes bandwidth costs because data is primarily accessed locally. Only changes are replicated to AWS for backup and disaster recovery. Immediate retrieval is possible because the data is stored locally, fulfilling the 'immediate retrieval' requirement. Stored volumes ensure the primary copy of the data resides on-premises, which is crucial for minimizing bandwidth costs when frequent access is needed.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon S3 Glacier Vault is designed for long-term archival storage and retrieval is not immediate. Expedited retrieval comes with additional costs, violating the 'no additional cost' requirement. Glacier is not suitable for frequent, immediate data access.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Storage Gateway using cached volumes stores only frequently accessed data locally. The remaining data resides in S3. While it can reduce latency for frequently accessed data, it doesn't guarantee immediate retrieval of all data without incurring bandwidth costs for retrieving data from S3. The question requires immediate retrieval of *all* data at no additional cost, which cached volumes cannot guarantee.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Direct Connect establishes a dedicated network connection between the on-premises data center and AWS. While it can improve network performance and reduce bandwidth costs compared to using the public internet, it does not directly address the storage capacity issue or guarantee immediate retrieval of data. It also incurs costs for the Direct Connect service itself. The question requires a solution that minimizes bandwidth costs *and* allows for immediate retrieval at no additional cost, which Direct Connect alone does not provide.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 26,
    "text": "A company recently implemented hybrid cloud connectivity using AWS Direct Connect and is \nmigrating data to Amazon S3. \nThe company is looking for a fully managed solution that will automate and accelerate the \nreplication of data between the on-premises storage systems and AWS storage services. \nWhich solution should a solutions architect recommend to keep the data private?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS DataSync agent tor the on-premises environment.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an AWS DataSync agent for the on-premises environment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway volume gateway for the on-premises environment.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Storage Gateway file gateway for the on-premises environment.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because AWS DataSync is a fully managed data transfer service that automates and accelerates moving data between on-premises storage and AWS storage services like S3. Deploying a DataSync agent on-premises allows it to securely connect to the on-premises storage and transfer data to S3 over the Direct Connect link. DataSync encrypts data in transit and at rest, ensuring data privacy. It's designed for large-scale data migration and replication, making it suitable for the company's needs.\n\n**Why option 1 is incorrect:**\nThis option is a duplicate of the correct answer. While deploying an AWS DataSync agent is the correct approach, this option doesn't provide any new information or context.\n\n**Why option 2 is incorrect:**\nAWS Storage Gateway volume gateway presents on-premises applications with cloud-backed iSCSI block storage volumes. While it can be used for hybrid cloud storage, it's not primarily designed for large-scale data migration and replication like DataSync. It's more suitable for disaster recovery, backup, and tiered storage scenarios. It doesn't directly address the requirements of automated and accelerated data replication to S3 as effectively as DataSync.\n\n**Why option 3 is incorrect:**\nAWS Storage Gateway file gateway provides a file interface into S3, allowing on-premises applications to store files as objects in S3. While it can be used for hybrid cloud storage, it's not optimized for large-scale data migration and replication. It's more suited for scenarios where on-premises applications need to access S3 as a file share. It doesn't offer the same level of automation and acceleration for data replication as DataSync.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A solutions architect is designing the storage architecture for a new web application used for \nstonng and viewing engineering drawings. All application components will be deployed on the \nAWS infrastructure. \nThe application design must support caching to minimize the amount of time that users wait for \nthe engineering drawings to load. The application must be able to store petabytes of data. \nWhich combination of storage and caching should the solutions architect use?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 with Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon S3 Glacier with Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway with Amazon ElastiCache",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct solution because Amazon S3 provides scalable object storage suitable for petabytes of data. Amazon CloudFront is a content delivery network (CDN) that caches content closer to users, reducing latency and improving the speed at which engineering drawings load. This combination directly addresses both the storage and caching requirements.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon S3 Glacier is designed for long-term archival storage and retrieval is slow and costly. It is not suitable for frequently accessed engineering drawings. ElastiCache is a caching service, but it's not effectively paired with Glacier for this use case.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon EBS volumes are block storage and are typically used for operating systems and databases. They are not designed for storing petabytes of data for a web application serving static files like engineering drawings. While CloudFront can cache content from EBS, EBS itself is not the right storage solution for this scenario.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Storage Gateway connects on-premises storage infrastructure to AWS. While it can be used to store data in AWS, it doesn't inherently provide a scalable storage solution for petabytes of data directly accessible by a web application. It's more suited for hybrid cloud scenarios or integrating existing on-premises storage with AWS. ElastiCache is a caching service, but Storage Gateway is not the most appropriate storage solution for this use case.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "An operations team has a standard that states IAM policies should not be applied directly to \nusers. Some new members have not been following this standard. \nThe operation manager needs a way to easily identify the users with attached policies. \nWhat should a solutions architect do to accomplish this? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n147",
    "options": [
      {
        "id": 0,
        "text": "Monitor using AWS CloudTrail",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Config rule to run daily",
        "correct": true
      },
      {
        "id": 2,
        "text": "Publish IAM user changes lo Amazon SNS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run AWS Lambda when a user is modified",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by continuously evaluating the configuration of IAM users against the defined standard. AWS Config rules can be created to check for specific configurations, such as whether an IAM user has policies directly attached. The rule can run daily (or more frequently) and identify non-compliant users, providing an easy way for the operations manager to identify the users violating the standard. Config provides a historical view of configuration changes, allowing for auditing and remediation.\n\n**Why option 0 is incorrect:**\nWhile CloudTrail logs API calls related to IAM, it doesn't provide a built-in mechanism to easily identify users with attached policies. Analyzing CloudTrail logs would require custom scripting and parsing, making it a more complex and less efficient solution than using AWS Config. It's also a reactive approach, requiring analysis after the policy has been attached, rather than proactively identifying violations.\n\n**Why option 2 is incorrect:**\nPublishing IAM user changes to Amazon SNS would require additional logic to process the messages and determine if a user has policies attached. This adds unnecessary complexity compared to using AWS Config, which is specifically designed for configuration compliance. SNS by itself doesn't provide the capability to evaluate compliance against a standard.\n\n**Why option 3 is incorrect:**\nRunning AWS Lambda when a user is modified could work, but it requires more development and maintenance overhead than using AWS Config. You would need to write the Lambda function to check for directly attached policies and then trigger some kind of alert or notification. AWS Config provides a managed service for configuration compliance, making it a simpler and more scalable solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company is building applications in containers. \nThe company wants to migrate its on-premises development and operations services from its on-\npremises data center to AWS. \nManagement states that production system must be cloud agnostic and use the same \nconfiguration and administrator tools across production systems. \nA solutions architect needs to design a managed solution that will align open-source software. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch the containers on Amazon EC2 with EC2 instance worker nodes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch the containers on Amazon Elastic Kubernetes Service (Amazon EKS) and EKS workers",
        "correct": true
      },
      {
        "id": 2,
        "text": "Launch the containers on Amazon Elastic Containers service (Amazon ECS) with AWS Fargate",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch the containers on Amazon Elastic Container Service (Amazon EC) with Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution aligns with the requirement for cloud agnosticism because Amazon EKS uses Kubernetes, an open-source container orchestration platform. Kubernetes is widely adopted and can be run on various cloud providers and on-premises, allowing the company to maintain the same configuration and administrator tools across different environments. EKS provides a managed Kubernetes service, reducing the operational overhead of managing the Kubernetes control plane.\n\n**Why option 0 is incorrect:**\nLaunching containers directly on Amazon EC2 instances, while possible, does not provide a managed container orchestration solution. It requires the company to manage the container lifecycle, scaling, and networking themselves, which increases operational overhead and does not directly address the requirement for consistent tooling across environments. It also doesn't inherently provide cloud agnosticism.\n\n**Why option 2 is incorrect:**\nAmazon ECS, while a managed container service, is AWS-specific and does not provide the cloud agnosticism required by the question. Using Fargate with ECS further ties the solution to AWS, making it difficult to migrate to other cloud providers or on-premises environments without significant changes. While ECS is a valid container orchestration service, it doesn't meet the cloud agnostic requirement.\n\n**Why option 3 is incorrect:**\nSimilar to option 2, Amazon ECS is an AWS-specific container orchestration service and does not provide the cloud agnosticism required. Using EC2 with ECS still ties the solution to AWS, making it difficult to migrate to other cloud providers or on-premises environments without significant changes. While ECS is a valid container orchestration service, it doesn't meet the cloud agnostic requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 30,
    "text": "A solutions architect is performing a security review of a recently migrated workload. \nThe workload is a web application that consists of Amazon EC2 instances in an Auto Scaling \ngroup behind an Application Load Balancer. \nThe solutions architect must improve the security posture and minimize the impact of a DDoS \nattack on resources. \nWhich solution is MOST effective?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS WAF ACL with rate-based rules.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a custom AWS Lambda function that adds identified attacks into a common vulnerability",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable VPC Flow Logs and store them in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable Amazon GuardDuty and configure findings written to Amazon CloudWatch.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most effective solution because AWS WAF with rate-based rules allows you to identify and block malicious traffic based on the rate of requests. This helps mitigate DDoS attacks by limiting the number of requests from a single source within a specified time period, preventing the application from being overwhelmed. It directly addresses the requirement of minimizing the impact of a DDoS attack on resources and improves the security posture.\n\n**Why option 1 is incorrect:**\nWhile a custom Lambda function could potentially identify attacks, it would be more complex to implement and maintain than using AWS WAF's built-in rate-based rules. Also, adding identified attacks to a common vulnerability database doesn't directly mitigate the ongoing DDoS attack. The Lambda function would need to actively block or mitigate the traffic, which is what WAF does more efficiently and directly.\n\n**Why option 2 is incorrect:**\nEnabling VPC Flow Logs and storing them in S3 provides valuable network traffic data for analysis and auditing, but it doesn't actively mitigate a DDoS attack. It's a passive monitoring solution, not a preventative or reactive one. While useful for post-incident analysis, it doesn't directly address the requirement of minimizing the impact of a DDoS attack on resources.\n\n**Why option 3 is incorrect:**\nEnabling Amazon GuardDuty and configuring findings written to CloudWatch provides threat detection and security monitoring capabilities. GuardDuty can identify suspicious activity, including potential DDoS attacks, but it doesn't directly mitigate the attack in real-time. It's a detection service, not a prevention or mitigation service. While valuable for security monitoring, it doesn't directly address the requirement of minimizing the impact of a DDoS attack on resources.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company is creating a prototype of an ecommerce website on AWS. The website consists of an \nApplication Load Balancer, an Auto Scaling group of Amazon EC2 instances for web servers, and \nan Amazon RDS for MySQL DB instance that runs with the Single-AZ configuration. \nThe website is slow to respond during searches of the product catalog. The product catalog is a \ngroup of tables in the MySQL database that the company does not update frequently. A solutions \narchitect has determined that the CPU utilization on the DB instance is high when product catalog \nsearches occur. \nWhat should the solutions architect recommend to improve the performance of the website during \nsearches of the product catalog?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the product catalog to an Amazon Redshift database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement an Amazon ElastiCache for Redis cluster to cache the product catalog.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Add an additional scaling policy to the Auto Scaling group to launch additional EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on the Multi-AZ configuration for the DB instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the performance bottleneck by caching the product catalog data. Since the product catalog is not frequently updated, caching it in ElastiCache for Redis will significantly reduce the load on the MySQL database during product catalog searches. Redis is an in-memory data store, offering very fast read performance, which will improve the website's responsiveness during searches. This directly addresses the high CPU utilization on the database instance.\n\n**Why option 0 is incorrect:**\nMigrating the product catalog to Amazon Redshift is not the best solution for this scenario. Redshift is designed for large-scale data warehousing and analytical workloads, not for serving real-time product catalog searches. While Redshift could handle the data, it would be overkill and likely more complex and expensive than necessary. The problem is high CPU utilization on the MySQL database due to frequent read operations, which is better solved with a caching solution.\n\n**Why option 2 is incorrect:**\nAdding more EC2 instances to the Auto Scaling group will not directly address the bottleneck, which is the high CPU utilization on the RDS instance. The web servers are likely waiting for data from the database, so scaling the web tier will not alleviate the database load. The bottleneck is at the data layer, not the application layer.\n\n**Why option 3 is incorrect:**\nTurning on Multi-AZ for the RDS instance improves availability and durability by providing a standby instance in a different Availability Zone. However, it does not directly address the performance issue of high CPU utilization during product catalog searches. Multi-AZ is primarily for disaster recovery and failover, not for improving read performance. While it can help in case of an AZ failure, it won't reduce the load on the primary database instance during normal operation.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "A company's application is running on Amazon EC2 instances within an Auto Scaling group \nbehind an Elastic Load Balancer. Based on the application's history, the company anticipates a \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n149 \nspike in traffic during a holiday each year. A solutions architect must design a strategy to ensure \nthat the Auto Scaling group proactively increases capacity to minimize any performance impact \non application users. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a recurring scheduled action to scale up the Auto Scaling group before the expected",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirement of proactively increasing capacity. Scheduled actions allow you to define a schedule to automatically adjust the desired, minimum, and maximum capacity of the Auto Scaling group. By scheduling an action to increase capacity before the holiday traffic spike, the application can handle the increased load without performance degradation. This is the most effective way to handle predictable, recurring traffic patterns.\n\n**Why option 0 is incorrect:**\nThis approach relies on reactive scaling based on CPU utilization. While CPU utilization is a valid metric for scaling, it's not proactive. The Auto Scaling group will only scale up *after* the CPU utilization exceeds the defined threshold, which means there will be a period of time when the application is under stress and potentially experiencing performance issues. This contradicts the requirement to minimize performance impact.\n\n**Why option 2 is incorrect:**\nIncreasing the minimum and maximum number of EC2 instances in the Auto Scaling group provides a higher baseline capacity, but it doesn't proactively scale *before* the expected traffic spike. It simply allows the Auto Scaling group to scale to a higher level if needed. It also potentially leads to unnecessary costs during periods of normal traffic, as you'll be running more instances than required. It doesn't address the proactive scaling requirement.\n\n**Why option 3 is incorrect:**\nSNS notifications are used for alerting and monitoring, not for scaling. While an SNS notification could be triggered by a CloudWatch alarm (e.g., high CPU utilization), it doesn't directly address the requirement of proactively increasing capacity. It's a reactive approach that relies on a threshold being breached before any action is taken.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company is creating an application that runs on containers in a VPC. The application stores \nand accesses data in an Amazon S3 bucket. During the development phase, the application will \nstore and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs \nand wants to prevent traffic from traversing the internet whenever possible. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable S3 Intelligent-Tiering for the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable S3 Transfer Acceleration for the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by creating a gateway VPC endpoint for S3. A gateway endpoint allows resources within the VPC to access S3 without using public IPs or requiring internet gateways, NAT devices, or virtual private gateways. This keeps the traffic within the AWS network, preventing it from traversing the internet. Gateway endpoints are also free of charge for data transfer, which helps minimize costs. Associating the endpoint with all route tables ensures that all subnets within the VPC can access S3 through the endpoint.\n\n**Why option 0 is incorrect:**\nWhile S3 Intelligent-Tiering optimizes storage costs by automatically moving data between different access tiers based on usage patterns, it doesn't address the requirement of preventing traffic from traversing the internet. It focuses on storage cost optimization, not network connectivity.\n\n**Why option 1 is incorrect:**\nS3 Transfer Acceleration uses Amazon CloudFront's globally distributed edge locations to accelerate data transfers to S3. While it can improve transfer speeds, it relies on the public internet and does not prevent traffic from traversing the internet. It also incurs additional costs, which contradicts the requirement of minimizing costs.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 34,
    "text": "A solutions architect is tasked with transferring 750 TB of data from an on-premises network-\nattached file system located at a branch office Amazon S3 Glacier. \nThe migration must not saturate the on-premises 1 Mbps internet connection. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS site-to-site VPN tunnel to an Amazon S3 bucket and transfer the files directly.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order 10 AWS Snowball Edge Storage Optimized devices, and select an S3 Glacier vault as the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Mount the network-attached file system to an S3 bucket, and copy the files directly.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Order 10 AWS Snowball Edge Storage Optimized devices, and select an Amazon S3 bucket as",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by using AWS Snowball Edge Storage Optimized devices for offline data transfer. Ordering 10 devices allows for parallel data transfer, reducing the overall migration time. Selecting an Amazon S3 bucket as the destination is correct because Snowball Edge directly integrates with S3. After the data is transferred to S3, a lifecycle policy can be configured to move the data to S3 Glacier for cost-effective long-term storage. This avoids saturating the 1 Mbps internet connection during the initial data transfer.\n\n**Why option 0 is incorrect:**\nEstablishing a site-to-site VPN and transferring the files directly over the 1 Mbps connection would take an extremely long time and saturate the available bandwidth. Transferring 750 TB over a 1 Mbps connection is impractical and would likely disrupt other network operations. This option does not meet the requirement of avoiding saturation of the internet connection.\n\n**Why option 1 is incorrect:**\nWhile using Snowball Edge is a good approach for large data transfers, selecting an S3 Glacier vault directly as the destination during the Snowball Edge import process is not the standard or recommended approach. Snowball Edge is designed to import data into S3. After the data resides in S3, lifecycle policies can be used to transition the data to S3 Glacier. Directly importing to Glacier is not supported and would likely involve a more complex and less efficient process.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company's website handles millions of requests each day, and the number of requests \ncontinues to increase. A solutions architect needs to improve the response time of the web \napplication. The solutions architect determines that the application needs to decrease latency \nwhen retrieving product details from the \nAmazon DynamoDB table. \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Set up a DynamoDB Accelerator (DAX) cluster.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up Amazon ElastiCache for Redis between the DynamoDB table and the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon ElastiCache for Memcached between the DynamoDB table and the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon DynamoDB Streams on the table, and have AWS Lambda read from the table",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB. It's designed specifically to reduce read latency for DynamoDB tables. DAX integrates seamlessly with DynamoDB, requiring minimal application code changes. Since it's fully managed, it significantly reduces operational overhead compared to setting up and managing a separate caching solution like ElastiCache. DAX is optimized for DynamoDB workloads, providing better performance than generic caching solutions in this specific scenario.\n\n**Why option 1 is incorrect:**\nWhile ElastiCache for Redis can be used for caching, it requires more manual configuration and management than DAX. You would need to implement the caching logic in your application code, including cache invalidation strategies. This increases operational overhead. Also, Redis is a general-purpose caching solution, while DAX is specifically designed and optimized for DynamoDB, making DAX a better fit for this scenario.\n\n**Why option 2 is incorrect:**\nSimilar to Redis, ElastiCache for Memcached requires more manual configuration and management compared to DAX. You would need to implement the caching logic in your application code, including cache invalidation strategies. This increases operational overhead. Memcached is also a simpler caching solution than Redis and might not be as suitable for complex caching scenarios. DAX provides a more seamless and optimized caching experience for DynamoDB.\n\n**Why option 3 is incorrect:**\nDynamoDB Streams and Lambda are used for capturing data changes in DynamoDB and triggering actions based on those changes. They are not designed for caching data to reduce read latency. This option would add complexity and operational overhead without addressing the primary requirement of improving response time for product detail retrieval.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A media company collects and analyzes user activity data on premises. The company wants to \nmigrate this capability to AWS. The user activity data store will continue to grow and will be \npetabytes in size. The company needs to build a highly available data ingestion solution that \nfacilitates on-demand analytics of existing data and new data with SQL. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Send activity data to an Amazon Kinesis data stream.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Send activity data to an Amazon Kinesis Data Firehose delivery stream.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Place activity data in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by providing a managed service for data ingestion. Kinesis Data Firehose can directly load data into destinations like Amazon S3, Amazon Redshift, or Amazon OpenSearch Service. This eliminates the need for managing EC2 instances or custom ingestion services, reducing operational overhead. Firehose automatically scales to handle the data volume and provides built-in error handling and retry mechanisms, ensuring high availability. The data can then be queried using services like Amazon Athena (for S3) or directly within Redshift or OpenSearch, enabling on-demand SQL-based analytics.\n\n**Why option 0 is incorrect:**\nWhile Kinesis Data Streams can handle high-volume data ingestion, it requires additional processing and storage infrastructure to make the data available for analysis. It doesn't directly load data into a data store suitable for SQL queries. This would necessitate building and managing additional services like Kinesis Data Analytics or custom EC2-based processing to transform and load the data, increasing operational overhead.\n\n**Why option 2 is incorrect:**\nSimply placing the data in an S3 bucket addresses the storage requirement but doesn't provide a built-in mechanism for ingestion or on-demand analytics. While Amazon Athena can query data in S3, a separate ingestion process would still need to be built and managed, adding operational overhead. Furthermore, directly dumping data into S3 without a proper ingestion pipeline might lead to data quality issues and make it difficult to perform efficient analytics.\n\n**Why option 3 is incorrect:**\nCreating an ingestion service on EC2 instances introduces significant operational overhead. It requires managing the EC2 instances, scaling them to handle the data volume, ensuring fault tolerance, and implementing error handling and retry mechanisms. This approach is more complex and less cost-effective compared to using a managed service like Kinesis Data Firehose.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company is using a centralized AWS account to store log data in various Amazon S3 buckets. \nA solutions architect needs to ensure that the data is encrypted at rest before the data is \nuploaded to the S3 buckets. The data also must be encrypted in transit. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create bucket policies that require the use of server-side encryption with S3 managed encryption",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because client-side encryption encrypts the data *before* it is transmitted to S3. This ensures that the data is encrypted at rest before it even reaches the S3 bucket. Using HTTPS for the upload process handles encryption in transit. This satisfies both requirements of the question.\n\n**Why option 1 is incorrect:**\nThis is incorrect because server-side encryption encrypts the data *after* it has been uploaded to S3. The requirement is to encrypt the data *before* it's uploaded.\n\n**Why option 2 is incorrect:**\nThis is incorrect because bucket policies can enforce server-side encryption, but the encryption still happens *after* the data is uploaded. It doesn't address the requirement of encrypting the data *before* upload.\n\n**Why option 3 is incorrect:**\nThis is incorrect because enabling a default AWS Key for S3 bucket encryption only applies to server-side encryption. It doesn't encrypt the data *before* it's uploaded to S3.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "A company has an on-premises business application that generates hundreds of files each day. \nThese files are stored on an SMB file share and require a low- latency connection to the \napplication servers. A new company policy states all application-generated files must be copied to \nAWS. There is already a VPN connection to AWS. \nThe application development team does not have time to make the necessary code modifications \nto move the application to AWS. \nWhich service should a solutions architect recommend to allow the application to copy files to \nAWS?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by providing a bridge between the on-premises SMB file share and AWS. AWS Storage Gateway, specifically the File Gateway type, can be deployed on-premises and configured to present an SMB interface to the existing application. The File Gateway then asynchronously copies the files to Amazon S3 in AWS. This approach avoids the need to modify the application code, leverages the existing VPN connection, and allows the application to continue writing to the SMB share as before. The low-latency requirement is met because the application is still writing to a local SMB share, and the gateway handles the transfer to AWS in the background.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Amazon EFS is a fully managed NFS file system service. While it's suitable for applications running in AWS, it's not designed to directly integrate with on-premises SMB file shares without significant application modifications. Migrating the files to EFS would require changes to the application to use NFS instead of SMB, which violates the requirement of not modifying the application.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because Amazon FSx for Windows File Server is a fully managed Windows file server in AWS. While it supports SMB, it would require migrating the application to AWS or reconfiguring the application to use the FSx file share in AWS, which would require application modifications. The question specifically states that the application development team does not have time to make the necessary code modifications to move the application to AWS. Also, while FSx can be accessed from on-premises, it doesn't directly address the need to copy files from an existing on-premises SMB share without application changes.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because AWS Snowball is a physical device used for transferring large amounts of data to AWS. While it could be used to initially migrate the existing files, it's not a suitable solution for the ongoing daily transfer of hundreds of files. It would require manual intervention to ship the device back and forth, which is not practical for a daily process. It also doesn't address the low-latency requirement for the application.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A company has an ordering application that stores customer information in Amazon RDS for \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n152 \nMySQL. During regular business hours, employees run one-time queries for reporting purposes. \nTimeouts are occurring during order processing because the reporting queries are taking a long \ntime to run. The company needs to eliminate the timeouts without preventing employees from \nperforming queries. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a read replica. Move reporting queries to the read replica.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a read replica. Distribute the ordering application to the primary DB instance and the read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the ordering application to Amazon DynamoDB with on-demand capacity.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Schedule the reporting queries for non-peak hours.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct solution because creating a read replica allows you to offload read-heavy reporting queries from the primary RDS instance. The read replica will handle the reporting workload, freeing up resources on the primary instance for order processing. This directly addresses the performance issues and prevents timeouts without restricting employees' ability to run reports.\n\n**Why option 1 is incorrect:**\nDistributing the ordering application across the primary and read replica is not a standard or recommended practice. Read replicas are primarily for read-only operations, such as reporting. Attempting to write to a read replica directly would lead to data inconsistencies and application errors. The primary database is designed to handle write operations, and the application should continue to use it for order processing.\n\n**Why option 2 is incorrect:**\nMigrating to DynamoDB might be a viable long-term solution, but it involves significant application changes and data migration. It doesn't directly address the immediate problem of timeouts caused by reporting queries on the RDS instance. Also, DynamoDB is a NoSQL database, and the question states that the application uses MySQL, which is a relational database. A complete migration to DynamoDB would require a significant rewrite of the application, which is not the most efficient solution for the stated problem.\n\n**Why option 3 is incorrect:**\nScheduling reporting queries for non-peak hours might reduce the frequency of timeouts, but it doesn't eliminate them entirely. It also restricts employees' ability to run reports whenever they need to, which violates the requirement of not preventing employees from performing queries. This is a workaround, not a proper solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A company runs a web application that is backed by Amazon RDS. A new database administrator \ncaused data loss by accidentally editing information in a database table. To help recover from this \ntype of incident, the company wants the ability to restore the database to its state from 5 minutes \nbefore any change within the last 30 days. \nWhich feature should the solutions architect include in the design to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Read replicas",
        "correct": false
      },
      {
        "id": 1,
        "text": "Manual snapshots",
        "correct": false
      },
      {
        "id": 2,
        "text": "Automated backups",
        "correct": true
      },
      {
        "id": 3,
        "text": "Multi-AZ deployments",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by enabling point-in-time recovery (PITR). Automated backups, when enabled for an RDS instance, allow you to restore the database to any point in time within the specified retention period (up to 35 days). The question specifies a need to restore to a point 5 minutes before the data loss, which is achievable with automated backups and PITR. The automated backups create transaction logs that are used to replay transactions up to the desired point in time.\n\n**Why option 0 is incorrect:**\nRead replicas are primarily used for offloading read traffic from the primary database instance. While they can provide some level of data redundancy, they don't directly address the requirement of restoring the database to a specific point in time before data loss. Changes made on the primary instance are replicated to the read replica, so the data loss would also be replicated.\n\n**Why option 1 is incorrect:**\nManual snapshots allow you to create backups of your database at a specific point in time. However, they do not provide the granularity required to restore the database to a point 5 minutes before the data loss. Restoring from a manual snapshot would restore the database to the exact time the snapshot was taken, which may not be close enough to the desired recovery point. Furthermore, relying solely on manual snapshots requires proactive action before the data loss occurs, which is not a reliable recovery strategy.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 41,
    "text": "A company uses Amazon RDS for PostgreSQL databases for its data tier. The company must \nimplement password rotation for the databases. \n \nWhich solution meets this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the password in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the password in AWS Systems Manager Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the password in AWS Systems Manager Parameter Store.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the password in AWS Key Management Service (AWS KMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Secrets Manager is specifically designed for managing secrets, including database credentials. It offers built-in functionality for automatic password rotation for RDS databases, including PostgreSQL. This automation significantly reduces operational overhead compared to manually rotating passwords or implementing custom solutions.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while AWS Systems Manager Parameter Store can store passwords, it doesn't provide built-in automatic password rotation functionality for RDS databases. Implementing password rotation with Parameter Store would require creating and managing custom scripts or Lambda functions, increasing operational overhead.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while AWS Systems Manager Parameter Store can store passwords, it doesn't provide built-in automatic password rotation functionality for RDS databases. Implementing password rotation with Parameter Store would require creating and managing custom scripts or Lambda functions, increasing operational overhead.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS KMS is used for encryption key management, not for storing and rotating secrets like database passwords. While KMS can encrypt the password, it doesn't provide any built-in mechanism for automatic password rotation. Using KMS for password rotation would require significant custom development and management, resulting in high operational overhead.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "A company's facility has badge readers at every entrance throughout the building. When badges \nare scanned, the readers send a message over HTTPS to indicate who attempted to access that \nparticular entrance. \n \nA solutions architect must design a system to process these messages from the sensors. The \nsolution must be highly available, and the results must be made available for the company's \nsecurity team to analyze. \n \nWhich system architecture should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an HTTPS endpoint in Amazon API Gateway.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a gateway VPC endpoint for Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing a managed, scalable, and highly available HTTPS endpoint. Amazon API Gateway can handle a large number of concurrent requests from the badge readers. It can then integrate with other AWS services like Lambda or Kinesis to process the messages and store the data for analysis. API Gateway handles authentication, authorization, and request validation, which are important for security. It also provides monitoring and logging capabilities, which are essential for operational visibility.\n\n**Why option 0 is incorrect:**\nThis is incorrect because launching a single EC2 instance introduces a single point of failure, which violates the high availability requirement. While you could implement high availability with EC2 instances behind a load balancer, it requires more manual configuration and management compared to using API Gateway. Also, managing the HTTPS endpoint, scaling, and security becomes the responsibility of the user, adding operational overhead.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Route 53 is a DNS service and cannot directly receive HTTPS requests. Route 53 is used for routing traffic to different endpoints based on DNS records, but it doesn't act as an HTTP endpoint itself. While you could use Route 53 to direct traffic to an API Gateway endpoint, it doesn't fulfill the requirement of creating the HTTPS endpoint.\n\n**Why option 3 is incorrect:**\nThis is incorrect because a gateway VPC endpoint for Amazon S3 allows resources within a VPC to access S3 without traversing the public internet. It does not provide an HTTPS endpoint for receiving messages from the badge readers. It's relevant for accessing S3 from within a VPC, but not for receiving external HTTPS requests.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have \noutbound internet access, but the EC2 instance needs the ability to download monthly security \nupdates from an outside vendor. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an internet gateway, and attach it to the VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a NAT gateway, and place it in a public subnet.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a NAT instance, and place it in the same subnet where the EC2 instance is located.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an internet gateway, and attach it to the VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution allows instances in the private subnet to initiate outbound traffic to the internet while preventing inbound traffic initiated from the internet. The NAT gateway, placed in a public subnet, acts as an intermediary. The EC2 instance sends traffic to the NAT gateway, which then forwards it to the internet. The NAT gateway translates the private IP address of the EC2 instance to its own public IP address, allowing the EC2 instance to receive responses from the internet. The NAT gateway only forwards traffic initiated by the EC2 instance, thus maintaining security.\n\n**Why option 0 is incorrect:**\nCreating an internet gateway and attaching it to the VPC only provides a path for traffic to and from the internet. It doesn't, by itself, allow instances in private subnets to access the internet. To enable internet access for an instance in a private subnet using an internet gateway, you would need to associate a public IP address with the instance, which is not desirable in this scenario as it would expose the instance directly to the internet.\n\n**Why option 2 is incorrect:**\nPlacing a NAT instance in the same subnet as the EC2 instance would not provide the required outbound internet access. NAT instances need to be in a public subnet to forward traffic to the internet via an internet gateway. Also, managing and scaling NAT instances requires more operational overhead than using a NAT gateway.\n\n**Why option 3 is incorrect:**\nCreating an internet gateway, and attaching it to the VPC only provides a path for traffic to and from the internet. It doesn't, by itself, allow instances in private subnets to access the internet. To enable internet access for an instance in a private subnet using an internet gateway, you would need to associate a public IP address with the instance, which is not desirable in this scenario as it would expose the instance directly to the internet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company has been running a web application with an Oracle relational database in an on-\npremises data center for the past 15 years. The company must migrate the database to AWS. \nThe company needs to reduce operational overhead without having to modify the application's \ncode. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the database servers to Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 instances to migrate and operate the database servers.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the database servers to Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Snowball Edge Storage Optimized device to migrate the data from Oracle to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution leverages AWS Database Migration Service (DMS) to migrate the database to Amazon RDS for Oracle. RDS is a managed database service, which significantly reduces operational overhead because AWS handles tasks like patching, backups, and infrastructure management. Migrating to RDS for Oracle allows the company to continue using Oracle without significant code changes, as the application can connect to the RDS instance in a similar way it connected to the on-premises database. DMS facilitates the migration process itself, ensuring data is moved efficiently and reliably to the new RDS instance.\n\n**Why option 1 is incorrect:**\nUsing Amazon EC2 instances to migrate and operate the database servers would not reduce operational overhead. It would essentially be a lift-and-shift of the database to AWS, requiring the company to manage the operating system, database software, patching, backups, and other administrative tasks, similar to the on-premises environment. This defeats the purpose of reducing operational overhead.\n\n**Why option 2 is incorrect:**\nThis option is incomplete. While DMS can migrate the database, it needs a target. Migrating to EC2 instances doesn't reduce operational overhead as the database still needs to be managed by the company.\n\n**Why option 3 is incorrect:**\nAWS Snowball Edge is primarily used for large-scale data transfers when network bandwidth is limited. While it can be used to move data to AWS, it doesn't address the operational overhead requirement. It only handles the initial data transfer. After the data is on AWS, it still needs to be loaded into a database service, which would likely involve EC2 and manual management, increasing operational overhead. Also, it doesn't directly migrate the Oracle database to a managed service.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company is running an application on Amazon EC2 instances. Traffic to the workload increases \nsubstantially during business hours and decreases afterward. The CPU utilization of an EC2 \ninstance is a strong indicator of end-user demand on the application. The company has \nconfigured an Auto Scaling group to have a minimum group size of 2 EC2 instances and a \nmaximum group size of 10 EC2 instances. \n \nThe company is concerned that the current scaling policy that is associated with the Auto Scaling \ngroup might not be correct. The company must avoid over-provisioning EC2 instances and \nincurring unnecessary costs. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EC2 Auto Scaling to use a scheduled scaling plan and launch an additional 8",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS Auto Scaling to use a scaling plan that enables predictive scaling.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a step scaling policy to add 4 EC2 instances at 50% CPU utilization and add another 4",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Auto Scaling to have a desired capacity of 5 EC2 instances, and disable any",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of avoiding over-provisioning and unnecessary costs by leveraging predictive scaling. Predictive scaling analyzes historical data to forecast future traffic patterns and proactively adjusts the Auto Scaling group's capacity. This allows the company to scale up before the business hours traffic surge and scale down afterward, minimizing idle resources and optimizing costs. It is more dynamic and responsive than scheduled scaling, which relies on fixed schedules and may not accurately reflect real-time demand fluctuations.\n\n**Why option 0 is incorrect:**\nScheduled scaling relies on predefined schedules and does not adapt to unexpected changes in traffic patterns. While it can address the general increase during business hours, it might not be flexible enough to handle variations in demand or unexpected spikes. This can lead to either over-provisioning (if the schedule is set too high) or under-provisioning (if the schedule is set too low). It doesn't dynamically adjust based on real-time metrics like CPU utilization.\n\n**Why option 2 is incorrect:**\nStep scaling policies react to current CPU utilization, adding or removing instances based on predefined thresholds. While this can help manage traffic, it's reactive rather than proactive. Adding a fixed number of instances (4 in this case) at a specific CPU utilization level might not be optimal for all scenarios. It could lead to over-provisioning if the demand increase is less than expected or under-provisioning if the demand increase is higher. It also doesn't address the need to scale down efficiently during off-peak hours. The question specifies avoiding over-provisioning, and this option doesn't guarantee that.\n\n**Why option 3 is incorrect:**\nSetting a desired capacity of 5 EC2 instances and disabling scaling policies would not address the fluctuating traffic patterns. It would likely lead to under-provisioning during peak hours, resulting in poor application performance and a negative user experience. It also contradicts the requirement of dynamically adjusting the capacity based on demand. Disabling scaling policies defeats the purpose of Auto Scaling and eliminates the ability to automatically respond to changes in traffic.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "A company wants to use a custom distributed application that calculates various profit and loss \nscenarios. To achieve this goal, the company needs to provide a network connection between its \nAmazon EC2 instances. The connection must minimize latency and must maximize throughput \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision the application to use EC2 Dedicated Hosts of the same instance type.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a placement group for EC2 instances that have the same instance type.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use multiple AWS elastic network interfaces and link aggregation.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS PrivateLink for the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by grouping EC2 instances close together within an AWS Region. Placement groups offer low latency and high throughput network connectivity. Specifically, cluster placement groups are designed for applications that need tight inter-node communication and are ideal for minimizing latency and maximizing the network performance between instances. The requirement for the same instance type is a constraint of cluster placement groups, which are the best choice for low latency and high throughput.\n\n**Why option 0 is incorrect:**\nWhile Dedicated Hosts provide dedicated hardware, they don't inherently guarantee low latency and high throughput between instances. Dedicated Hosts primarily address compliance and licensing requirements. The instances could still be placed far apart within the data center, negating the benefits of proximity. The cost is also significantly higher than using placement groups.\n\n**Why option 2 is incorrect:**\nWhile using multiple Elastic Network Interfaces (ENIs) and link aggregation can increase throughput, it doesn't inherently minimize latency. Link aggregation primarily focuses on increasing bandwidth, not reducing the time it takes for data to travel between instances. Placement groups are more effective for minimizing latency.\n\n**Why option 3 is incorrect:**\nAWS PrivateLink is designed for securely accessing AWS services or services hosted by other AWS accounts over the AWS network. It's not intended for direct communication between EC2 instances within the same account and region. PrivateLink adds an additional layer of network hops, which would increase latency, not minimize it. It's more suitable for accessing services across VPC boundaries or from on-premises environments.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company needs to run a critical application on AWS. The company needs to use Amazon EC2 \nfor the applications database. The database must be highly available and must fail over \nautomatically if a disruptive event occurs. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirements by providing redundancy across Availability Zones. Launching two EC2 instances in different Availability Zones ensures that if one Availability Zone experiences a disruptive event, the application can failover to the instance in the other Availability Zone. Installing and configuring a database replication mechanism between the two instances allows for data synchronization. Implementing automatic failover mechanisms, such as using a floating IP address or a load balancer with health checks, ensures that the application automatically switches to the healthy instance in case of a failure. This setup provides both high availability and automatic failover.\n\n**Why option 1 is incorrect:**\nThis solution does not provide high availability. If the single EC2 instance fails, the application will experience downtime. Using an Elastic IP address allows you to remap the address to another instance but does not provide automatic failover. Manual intervention would be required to remap the Elastic IP, leading to downtime.\n\n**Why option 2 is incorrect:**\nWhile launching EC2 instances in different Regions provides disaster recovery capabilities, it does not directly address the requirement for automatic failover in the event of a disruptive event. Failover between Regions typically involves more complex configurations and longer recovery times compared to failover within the same Region across Availability Zones. The question specifically asks for automatic failover, which is more easily achieved within a single Region.\n\n**Why option 3 is incorrect:**\nThis solution suffers from the same problem as option 1. It does not provide high availability or automatic failover. While Elastic Load Balancing (ELB) can distribute traffic, it requires at least two healthy instances to provide high availability. With only one instance, if that instance fails, the application will become unavailable.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company hosts its application on AWS. The company uses Amazon Cognito to manage users. \nWhen users log in to the application, the application fetches required data from Amazon \nDynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an \nAWS managed solution that will control access to the REST API to reduce development efforts. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user",
        "correct": false
      },
      {
        "id": 1,
        "text": "For each user, create and assign an API key that must be sent with each request. Validate the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Send the users email address in the header with every request. Invoke an AWS Lambda function",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution directly integrates Amazon Cognito user pools with API Gateway for authentication and authorization. API Gateway handles the validation of the user's identity token against the Cognito user pool. This eliminates the need for custom authorizers (Lambda functions) or manual API key management, significantly reducing operational overhead. It leverages the existing Cognito infrastructure, making it a fully managed and efficient solution.\n\n**Why option 0 is incorrect:**\nWhile a Lambda authorizer can provide fine-grained control, it requires writing and maintaining custom code for authentication and authorization logic. This increases development effort and operational overhead compared to using a built-in Cognito authorizer, which is a managed service.\n\n**Why option 1 is incorrect:**\nCreating and assigning API keys for each user introduces significant operational overhead. It requires managing the API keys, distributing them to users, and handling key rotation and revocation. This approach is less scalable and more complex than using a Cognito user pool authorizer, which is a managed service.\n\n**Why option 2 is incorrect:**\nSending the user's email address in the header is not a secure or reliable method for authentication. It's easily spoofed and doesn't provide any real security. Furthermore, relying on a Lambda function to validate the email address against a database or user store adds unnecessary complexity and operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company is developing a marketing communications service that targets mobile app users. The \ncompany needs to send confirmation messages with Short Message Service (SMS) to its users. \nThe users must be able to reply to the SMS messages. The company must store the responses \nfor a year for analysis. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by leveraging Amazon Pinpoint's capabilities for sending SMS messages and managing user engagement. Using a Pinpoint journey allows for orchestrated messaging campaigns and tracking of user interactions. Configuring Pinpoint to send events to an Amazon Kinesis Data Firehose delivery stream enables the capture of SMS responses and other relevant data. Kinesis Data Firehose can then be configured to deliver the data to Amazon S3 for long-term storage (one year as required) and subsequent analysis. This provides a complete solution for sending SMS, receiving replies, and storing the data for analysis.\n\n**Why option 0 is incorrect:**\nAmazon Connect is primarily designed for contact center solutions, not for general SMS marketing campaigns. While it can send SMS, it's not the most efficient or cost-effective solution for this scenario, especially considering the requirement for storing responses for analysis. The integration with Lambda to store responses would require custom coding and management, making it more complex than using Pinpoint.\n\n**Why option 2 is incorrect:**\nAmazon SQS is a message queuing service and is not directly used for sending SMS messages. It can be used as part of a larger system, but it doesn't provide the SMS sending functionality itself. The question specifically asks for a solution to send SMS messages, and SQS only handles message queuing. The SMS sending would need to be handled by another service, adding unnecessary complexity.\n\n**Why option 3 is incorrect:**\nAmazon SNS FIFO topics are designed for ordered message delivery and are not the most suitable option for sending SMS messages. While SNS can send SMS, it doesn't provide the same level of features and analytics as Amazon Pinpoint for marketing communications. Subscribing an Amazon SQS queue to the SNS topic would allow you to capture the SMS messages, but it doesn't address the requirement of receiving replies from users. Additionally, storing the responses from SQS would require additional infrastructure and management.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "The customers of a finance company request appointments with financial advisors by sending \ntext messages. A web application that runs on Amazon EC2 instances accepts the appointment \nrequests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) \nqueue through the web application. Another application that runs on EC2 instances then sends \nmeeting invitations and meeting confirmation email messages to the customers. After successful \nscheduling, this application stores the meeting information in an Amazon DynamoDB database. \nAs the company expands, customers report that their meeting invitations are taking longer to \narrive. \nWhat should a solutions architect recommend to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon API Gateway API in front of the web application that accepts the appointment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the performance bottleneck by allowing the application that sends meeting invitations to scale automatically based on demand. By adding an Auto Scaling group, the number of EC2 instances running the application can increase during peak times, reducing the processing time for each message in the SQS queue and thus speeding up the delivery of meeting invitations. This directly addresses the reported issue of increasing delays as the company expands.\n\n**Why option 0 is incorrect:**\nDynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB. While it can improve read performance, the problem is not explicitly stated to be related to DynamoDB read performance. The bottleneck is in the processing of messages and sending invitations, not necessarily in retrieving meeting information from DynamoDB. Therefore, adding DAX might not significantly improve the overall performance of sending meeting invitations.\n\n**Why option 1 is incorrect:**\nAdding an API Gateway in front of the web application primarily addresses concerns related to API management, security, and request routing. While it can provide benefits like throttling and request validation, it doesn't directly address the bottleneck in the application that sends meeting invitations. The problem is not related to the web application accepting requests, but rather the processing of those requests and sending invitations.\n\n**Why option 2 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) used to cache and deliver static and dynamic content closer to users. It is primarily used to improve the performance of web applications by caching content at edge locations. In this scenario, the bottleneck is not related to the delivery of the web application itself, but rather the processing of appointment requests and sending meeting invitations. Therefore, adding CloudFront would not directly address the reported issue.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company offers a food delivery service that is growing rapidly. Because of the growth, the \ncompanys order processing system is experiencing scaling problems during peak traffic hours. \nThe current architecture includes the following: \n \n- A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect \norders from the application \n- Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders \n \nThe order collection process occurs quickly, but the order fulfillment process can take longer. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n158 \nData must not be lost because of a scaling event. \nA solutions architect must ensure that the order collection process and the order fulfillment \nprocess can both scale properly during peak traffic hours. The solution must optimize utilization of \nthe companys AWS resources. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by introducing two SQS queues. One queue handles order collection, allowing the order collection Auto Scaling group to quickly offload orders without being bottlenecked by the slower fulfillment process. The second queue handles order fulfillment, allowing the fulfillment Auto Scaling group to process orders at its own pace. SQS ensures that no data is lost during scaling events because messages are persisted in the queue until they are successfully processed. This decoupling optimizes resource utilization by allowing each Auto Scaling group to scale independently based on its specific workload.\n\n**Why option 0 is incorrect:**\nMonitoring CPU utilization alone, while important, does not directly address the scaling problem or prevent data loss. It only provides information about resource usage, but doesn't decouple the order collection and fulfillment processes. It also doesn't guarantee data persistence in case of scaling events.\n\n**Why option 1 is incorrect:**\nMonitoring CPU utilization alone, while important, does not directly address the scaling problem or prevent data loss. It only provides information about resource usage, but doesn't decouple the order collection and fulfillment processes. It also doesn't guarantee data persistence in case of scaling events.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company hosts multiple production applications. One of the applications consists of resources \nfrom Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon \nSNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All \ncompany resources are tagged with a tag name of application and a value that corresponds to \neach application. A solutions architect must provide the quickest solution for identifying all of the \ntagged components. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudTrail to generate a list of resources with the application tag.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the AWS CLI to query each service across all Regions to report the tagged components.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run a query in Amazon CloudWatch Logs Insights to report on the components with the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using AWS Resource Groups Tag Editor. The Tag Editor is specifically designed to centrally manage and query tags across multiple AWS services and Regions. It provides a single pane of glass to search for resources based on their tags, making it the quickest and most efficient solution for identifying all tagged components globally.\n\n**Why option 0 is incorrect:**\nThis is incorrect because AWS CloudTrail primarily logs API calls and events. While you might be able to infer resource creation and tagging events from CloudTrail logs, it's not designed for directly querying resources based on their tags. Analyzing CloudTrail logs for this purpose would be complex, slow, and inefficient compared to using the Tag Editor.\n\n**Why option 1 is incorrect:**\nThis is incorrect because using the AWS CLI to query each service across all Regions would be a very time-consuming and complex process. It would require writing separate scripts for each service, iterating through all Regions, and aggregating the results. This approach is not the quickest solution and is prone to errors and inconsistencies. Furthermore, it requires detailed knowledge of each service's CLI syntax and API structure.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon CloudWatch Logs Insights is designed for querying log data, not for querying resource tags. While some resources might log information that includes their tags, it's not a reliable or efficient way to identify all tagged components across all services and Regions. CloudWatch Logs Insights is not designed for this purpose.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company needs to export its database once a day to Amazon S3 for other teams to access. \nThe exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n159 \nvariable and changes rapidly. The data must be immediately available and must remain \naccessible for up to 3 months. The company needs the most cost-effective solution that will not \nincrease retrieval time. \nWhich S3 storage class should the company use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 1,
        "text": "S3 Glacier Instant Retrieval",
        "correct": false
      },
      {
        "id": 2,
        "text": "S3 Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-effective solution because S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on changing access patterns. It ensures immediate availability when the data is accessed frequently and reduces costs when the data is accessed infrequently or not at all. This aligns perfectly with the variable and rapidly changing access pattern described in the question, and the requirement for immediate availability.\n\n**Why option 1 is incorrect:**\nThis is incorrect because S3 Glacier Instant Retrieval is designed for archiving data that is rarely accessed but requires immediate retrieval. While it offers low storage costs, it is not optimized for variable access patterns and might be more expensive than S3 Intelligent-Tiering if the data is accessed frequently. Also, Intelligent-Tiering automatically manages the tiers, whereas Glacier Instant Retrieval requires manual configuration or lifecycle policies to move data to it.\n\n**Why option 2 is incorrect:**\nThis is incorrect because S3 Standard is designed for frequently accessed data and has higher storage costs than S3 Intelligent-Tiering. While it provides immediate availability, it is not cost-effective for data with variable access patterns, especially when the data might be infrequently accessed.\n\n**Why option 3 is incorrect:**\nThis is incorrect because S3 Standard-IA is designed for infrequently accessed data and has lower storage costs than S3 Standard. However, it has retrieval fees, which can increase the overall cost if the data is accessed frequently. The question specifies that the solution should not increase retrieval time, and although S3 Standard-IA offers immediate retrieval, the retrieval fees make S3 Intelligent-Tiering a more suitable and cost-effective option for variable access patterns.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A company is developing a new mobile app. The company must implement proper traffic filtering \nto protect its Application Load Balancer (ALB) against common application-level attacks, such as \ncross-site scripting or SQL injection. The company has minimal infrastructure and operational \nstaff. The company needs to reduce its share of the responsibility in managing, updating, and \nsecuring servers for its AWS environment. \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS WAF rules and associate them with the ALB.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application using Amazon S3 with public hosting enabled.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy AWS Shield Advanced and add the ALB as a protected resource.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall,",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS WAF is a web application firewall that allows you to monitor the HTTP and HTTPS requests that are forwarded to an Application Load Balancer, Amazon API Gateway, or Amazon CloudFront. WAF lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, WAF allows you to block or allow requests. This directly addresses the need for traffic filtering against application-level attacks. Associating the rules with the ALB provides the necessary protection at the application layer without requiring the company to manage additional infrastructure or servers.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon S3 with public hosting is primarily for static content and does not provide any protection against application-level attacks like XSS or SQL injection. It also doesn't address the need for traffic filtering for an ALB.\n\n**Why option 2 is incorrect:**\nThis is incorrect because AWS Shield Advanced provides protection against Distributed Denial of Service (DDoS) attacks, which operate at the network and transport layers (Layers 3 & 4). While Shield Advanced can protect against some application-layer DDoS attacks, it doesn't provide the granular application-level filtering and rule-based protection against XSS and SQL injection that AWS WAF offers. Also, it's a more expensive solution than WAF and might be overkill for the stated requirements.\n\n**Why option 3 is incorrect:**\nThis is incorrect because creating a new ALB and routing traffic to an EC2 instance running a third-party firewall would introduce significant operational overhead. The company would be responsible for managing, patching, and securing the EC2 instance and the third-party firewall software. This contradicts the requirement to reduce the management burden.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A companys reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. \nThe company must convert these files to Apache Parquet format and must store the files in a \ntransformed data bucket. \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function to transform the data and output the data to the transformed",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements efficiently. AWS Glue provides a managed ETL (Extract, Transform, Load) service. The crawler automatically discovers the schema of the CSV files. The extract, transform, and load (ETL) job can then be configured to read the CSV files, convert them to Parquet format, and write them to the transformed data bucket. Glue minimizes development effort because it provides a visual interface and pre-built connectors for common data sources and formats, automating much of the ETL process.\n\n**Why option 0 is incorrect:**\nWhile an EMR cluster with Spark can perform the conversion, it involves significantly more development effort. Setting up and managing an EMR cluster, writing a Spark application to read CSV, convert to Parquet, and write to S3 requires considerable coding and operational overhead compared to AWS Glue.\n\n**Why option 2 is incorrect:**\nUsing AWS Batch with Bash scripting is possible, but it would require writing custom scripts to parse the CSV files, convert them to Parquet format (potentially using command-line tools or libraries), and then upload the Parquet files to S3. This involves more development effort than using AWS Glue, which provides built-in functionality for data transformation.\n\n**Why option 3 is incorrect:**\nWhile AWS Lambda can be used for data transformation, it's not the most suitable option for processing hundreds of CSV files daily. Lambda functions have execution time limits and memory constraints, making them less efficient and potentially more complex to manage for this scenario. Furthermore, writing the code to parse CSV, convert to Parquet, and handle potential errors would require more development effort than using AWS Glue.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A company has a serverless website with millions of objects in an Amazon S3 bucket. The \ncompany uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company \ndid not set encryption on the S3 bucket before the objects were loaded. A solutions architect \nneeds to enable encryption for all existing objects and for all objects that are added to the S3 \nbucket in the future. \nWhich solution will meet these requirements with the LEAST amount of effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Navigate to Amazon S3 in the AWS Management Console. Browse the S3 buckets objects. Sort",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by enabling default encryption on the existing S3 bucket. S3 default encryption automatically encrypts all new objects added to the bucket. The S3 Inventory feature can then be used to identify all existing unencrypted objects. A subsequent process (e.g., S3 Batch Operations or a script) can then be used to encrypt the identified unencrypted objects. This approach minimizes effort by leveraging built-in S3 features and avoids the need to migrate data to a new bucket.\n\n**Why option 0 is incorrect:**\nCreating a new S3 bucket and migrating all objects is a valid approach, but it involves significantly more effort than necessary. It requires copying millions of objects, updating the CloudFront distribution to point to the new bucket, and potentially dealing with downtime during the migration. The question specifically asks for the solution with the LEAST amount of effort.\n\n**Why option 2 is incorrect:**\nCreating a new KMS key is a valid step if you want to use KMS encryption, but it doesn't automatically encrypt existing objects or future objects. You would still need to implement a mechanism to encrypt existing objects and configure the bucket to use the new KMS key for future objects. This option doesn't provide a complete solution with the least amount of effort. It also doesn't leverage the default encryption feature, which simplifies future object encryption.\n\n**Why option 3 is incorrect:**\nBrowsing the S3 bucket through the AWS Management Console and manually sorting objects is not a scalable or efficient solution for millions of objects. It would be extremely time-consuming and error-prone to identify and encrypt each object individually. This approach is not practical and does not meet the requirement of minimal effort.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company has a web server running on an Amazon EC2 instance in a public subnet with an \nElastic IP address. The default security group is assigned to the EC2 instance. The default \nnetwork ACL has been modified to block all traffic. A solutions architect needs to make the web \nserver accessible from everywhere on port 443. \nWhich combination of steps will accomplish this task? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because security groups act as a virtual firewall for EC2 instances, controlling inbound and outbound traffic. Creating a security group rule that allows TCP port 443 from source 0.0.0.0/0 (any IP address) will allow inbound HTTPS traffic to the web server. Since the question states the default security group is assigned, creating a new security group with the correct rule and assigning it to the EC2 instance will allow the traffic. The default security group is restrictive, so adding a new one with the correct rule is the best approach.\n\n**Why option 1 is incorrect:**\nThis is incorrect because security group rules define the source of inbound traffic and the destination of outbound traffic. Specifying 0.0.0.0/0 as the *destination* for port 443 would be relevant for *outbound* traffic, not inbound traffic to the web server. The requirement is to allow *inbound* traffic on port 443.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while network ACLs control traffic at the subnet level, the question states that the default network ACL has been modified to block all traffic. While this option would allow inbound traffic on port 443, it doesn't address the outbound traffic requirement. Network ACLs are stateless, meaning that rules must be configured for both inbound and outbound traffic. Furthermore, the question asks for a *combination* of steps, implying that both a security group and network ACL configuration are needed. This option only addresses the network ACL.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while updating the network ACL to allow both inbound and outbound traffic on port 443 is necessary due to its stateless nature, it doesn't address the security group configuration. The question asks for a *combination* of steps, implying that both a security group and network ACL configuration are needed. This option only addresses the network ACL.\n\n**Why option 4 is incorrect:**\nThis is incorrect because while updating the network ACL to allow both inbound and outbound traffic on port 443 is necessary due to its stateless nature, it doesn't address the security group configuration. The question asks for a *combination* of steps, implying that both a security group and network ACL configuration are needed. This option only addresses the network ACL.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A solutions architect is designing a new API using Amazon API Gateway that will receive \nrequests from users. The volume of requests is highly variable; several hours can pass without \nreceiving a single request. The data processing will take place asynchronously, but should be \ncompleted within a few seconds after a request is made. \nWhich compute service should the solutions architect have the API invoke to deliver the \nrequirements at the lowest cost?",
    "options": [
      {
        "id": 0,
        "text": "An AWS Glue job",
        "correct": false
      },
      {
        "id": 1,
        "text": "An AWS Lambda function",
        "correct": true
      },
      {
        "id": 2,
        "text": "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "A containerized service hosted in Amazon ECS with Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the most cost-effective option because AWS Lambda functions are billed based on actual usage (number of requests and execution duration). Since the API receives requests sporadically, Lambda functions will only incur costs when they are invoked. There are no costs when the function is idle. This aligns perfectly with the requirement of minimizing costs during periods of inactivity. Lambda also provides the low latency required for asynchronous processing within a few seconds.\n\n**Why option 0 is incorrect:**\nAWS Glue is designed for ETL (Extract, Transform, Load) operations and data processing at scale. While Glue can be triggered by API Gateway, it's not optimized for low-latency, near real-time processing of individual requests. Glue jobs typically involve larger datasets and longer processing times, making them less suitable and more expensive for this scenario.\n\n**Why option 2 is incorrect:**\nAmazon EKS is a managed Kubernetes service for running containerized applications. While EKS offers flexibility and scalability, it involves significant overhead in terms of infrastructure management and cost. Even when the API is idle, the EKS cluster will incur costs for the underlying EC2 instances or Fargate nodes. This makes it a less cost-effective option compared to Lambda for handling infrequent requests.\n\n**Why option 3 is incorrect:**\nAmazon ECS with EC2 requires managing EC2 instances, which incur costs even when the API is not receiving requests. While ECS with Fargate could be considered, it's still generally more expensive than Lambda for infrequent invocations due to the minimum resource allocation and associated costs, even when idle. Lambda's pay-per-use model makes it a better fit for the cost optimization requirement.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 59,
    "text": "A company runs an application on a group of Amazon Linux EC2 instances. For compliance \nreasons, the company must retain all application log files for 7 years. The log files will be \nanalyzed by a reporting tool that must be able to access all the files concurrently. \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon EC2 instance store",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the most cost-effective solution for long-term storage with concurrent access. Amazon S3 offers durable, scalable, and cost-effective object storage. S3's standard storage class is suitable for frequently accessed data, while S3 Glacier or S3 Glacier Deep Archive can be used for less frequently accessed data to further reduce costs after an initial period. S3 allows for concurrent access from multiple reporting tools, and its lifecycle policies can be used to automate the transition of logs to cheaper storage tiers after a certain period, optimizing cost.\n\n**Why option 0 is incorrect:**\nThis is not the most cost-effective solution for long-term storage of log files. EBS volumes are block storage devices primarily used for operating systems and application data that require high performance and low latency. While EBS can be used to store log files, it is more expensive than S3 for long-term archival, and managing EBS volumes across multiple instances can be complex. EBS is also tied to a specific Availability Zone, which can impact availability.\n\n**Why option 1 is incorrect:**\nWhile this provides shared file storage accessible by multiple EC2 instances, it is generally more expensive than S3 for long-term archival of log files. EFS is designed for applications that require shared file system access and is suitable for frequently accessed data. For long-term storage of log files that are primarily accessed for analysis, S3 is a more cost-effective option, especially when leveraging S3's storage classes like Glacier or Glacier Deep Archive.\n\n**Why option 2 is incorrect:**\nThis is not a suitable solution for long-term storage or concurrent access. Instance store provides temporary block-level storage for EC2 instances. Data stored in instance store is lost when the instance is stopped, terminated, or fails. Therefore, it does not meet the requirement for 7-year retention. Also, instance store is not designed for concurrent access from multiple reporting tools.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company has hired an external vendor to perform work in the companys AWS account. The \nvendor uses an automated tool that is hosted in an AWS account that the vendor owns. The \nvendor does not have IAM access to the companys AWS account. \nHow should a solutions architect grant this access to the vendor?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role in the companys account to delegate access to the vendors IAM role. Attach",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM user in the companys account with a password that meets the password",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM group in the companys account. Add the tools IAM user from the vendor account",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new identity provider by choosing AWS account as the provider type in the IAM",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. The company creates an IAM role in their account with permissions to access the necessary resources. The trust policy of this role is configured to allow the vendor's IAM role (or user) in their account to assume it. This allows the vendor's tool to temporarily assume the role in the company's account and perform the required actions without needing permanent credentials in the company's account. This approach adheres to the principle of least privilege and provides a secure way to grant access to external parties.\n\n**Why option 1 is incorrect:**\nThis is incorrect because creating an IAM user for the vendor in the company's account is less secure and harder to manage than using IAM roles for cross-account access. It requires managing credentials (passwords, access keys) for the vendor, which increases the risk of credential leakage and management overhead. It also violates the principle of least privilege, as the vendor would have permanent credentials in the company's account.\n\n**Why option 2 is incorrect:**\nThis is incorrect because adding the vendor's IAM user to an IAM group in the company's account does not establish a trust relationship between the two accounts. It would require the vendor's IAM user to have credentials in the company's account, which is less secure than using IAM roles for cross-account access. It also doesn't address the need for the vendor's tool to assume a role with specific permissions.\n\n**Why option 3 is incorrect:**\nThis is incorrect because creating a new identity provider with \"AWS account\" as the provider type is not the correct way to grant cross-account access. Identity providers are typically used for federating access from external identity providers (e.g., SAML, OpenID Connect). While you *could* use an AWS account as an identity provider, it's not the standard or recommended approach for cross-account access. Using IAM roles with trust policies is the simpler and more direct method.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic \nKubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an \nAmazon DynamoDB table. A solutions architect must ensure that the application can interact with \nthe DynamoDB table without exposing traffic to the internet. \nWhich combination of steps should the solutions architect take to accomplish this goal? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Attach an IAM role that has sufficient privileges to the EKS pod.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an IAM user that has sufficient privileges to the EKS pod.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow outbound connectivity to the DynamoDB table through the private subnets network ACLs.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a VPC endpoint for DynamoDB.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Embed the access keys in the Java Spring Boot code.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because using IAM roles for service accounts allows you to assign specific permissions to the EKS pod. This eliminates the need to manage long-term credentials within the application code or the EKS cluster itself. The application can then assume this role and interact with DynamoDB using the AWS SDK without needing explicit access keys. This follows the principle of least privilege and enhances security.\n\n**Why option 1 is incorrect:**\nIAM users are designed for human users, not applications. Attaching an IAM user to an EKS pod is not a standard or secure practice. IAM roles for service accounts are the recommended approach for granting permissions to applications running in EKS.\n\n**Why option 2 is incorrect:**\nWhile network ACLs control traffic at the subnet level, allowing outbound connectivity through them doesn't inherently prevent traffic from going to the internet. If the DynamoDB endpoint resolves to a public IP address, the traffic will still traverse the internet. A VPC endpoint is needed to keep the traffic within the AWS network.\n\n**Why option 3 is incorrect:**\nCreating a VPC endpoint for DynamoDB is the correct approach to ensure that traffic to DynamoDB remains within the AWS network and does not traverse the internet. This provides a private connection to DynamoDB without exposing the application to the public internet.\n\n**Why option 4 is incorrect:**\nEmbedding access keys directly in the application code is a highly insecure practice. If the code is compromised, the access keys could be exposed, granting unauthorized access to DynamoDB. This violates security best practices and should be avoided at all costs. IAM roles for service accounts provide a much more secure and manageable alternative.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company recently migrated its web application to AWS by rehosting the application on Amazon \nEC2 instances in a single AWS Region. The company wants to redesign its application \narchitecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances \nrandomly. \nWhich combination of steps should the company take to meet these requirements? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n163",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Route 53 failover routing policy.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Route 53 weighted routing policy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Route 53 multivalue answer routing policy.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Launch three EC2 instances: two instances in one Availability Zone and one instance in another",
        "correct": false
      },
      {
        "id": 4,
        "text": "Launch four EC2 instances: two instances in one Availability Zone and two instances in another",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because a multivalue answer routing policy in Route 53 returns multiple IP addresses in response to a DNS query. The client then randomly chooses one of the returned IP addresses to connect to. This provides a degree of randomness in traffic distribution and improves availability because if one instance is unavailable, the client can try another IP address returned by Route 53. This option also promotes fault tolerance by allowing clients to retry with different IP addresses if one instance fails.\n\n**Why option 0 is incorrect:**\nThis is incorrect because a failover routing policy in Route 53 is designed for active-passive failover scenarios. It directs traffic to a primary resource and only switches to a secondary resource when the primary resource becomes unavailable. This does not meet the requirement of randomly distributing traffic to all running EC2 instances.\n\n**Why option 1 is incorrect:**\nThis is incorrect because a weighted routing policy in Route 53 allows you to assign weights to different resources, controlling the proportion of traffic that each resource receives. While this can be used to distribute traffic, it does not inherently provide the random distribution requested in the scenario. It also doesn't inherently provide fault tolerance as it requires manual adjustment of weights in case of instance failure.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company collects data from thousands of remote devices by using a RESTful web services \napplication that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, \ntransforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote \ndevices will increase into the millions soon. The company needs a highly scalable solution that \nminimizes operational overhead. \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue to process the raw data in Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 to route traffic to different EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add more EC2 instances to accommodate the increasing amount of incoming data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Glue is a fully managed ETL (extract, transform, load) service. It can directly process data stored in Amazon S3, allowing the company to transform the raw data at scale without managing EC2 instances for data processing. This significantly reduces operational overhead and provides automatic scaling based on the data volume.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Route 53 is a DNS service and is primarily used for routing traffic to different endpoints. While it can be used for load balancing across multiple EC2 instances, it doesn't address the core issue of scaling the data processing pipeline itself. The bottleneck is the data transformation step, not just the routing of incoming requests.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while adding more EC2 instances can increase capacity, it introduces significant operational overhead. The company would need to manage the EC2 instances, scale them manually, and handle potential failures. This approach doesn't minimize operational overhead as required.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while SQS can decouple the data ingestion from the processing, it still requires EC2 instances to consume messages from the queue and process the data. This doesn't fully address the scalability and operational overhead requirements. While it's a good practice for decoupling, it doesn't provide a complete solution.\n\n**Why option 4 is incorrect:**\nThis is incorrect because while API Gateway and Kinesis Data Streams are good choices for ingesting high-velocity data, they don't address the data transformation requirement. Kinesis Data Streams primarily focuses on real-time data ingestion and streaming, not batch processing and transformation of data already stored in S3. It would add complexity to the architecture without directly solving the scaling and operational overhead issues for the existing data in S3.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 64,
    "text": "A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing \nCloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. \nThe CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy \nis in place to delete current objects after 3 years. \nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of \nobjects has continued to rise. However, the number of new CloudTrail logs that are delivered to \nthe S3 bucket has remained consistent. \nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Configure the organizations centralized CloudTrail trail to expire objects after 3 years.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the S3 Lifecycle policy to delete previous versions as well as current versions.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the parent account as the owner of all objects that are delivered to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because S3 Versioning keeps all versions of an object. The initial lifecycle policy only deletes the *current* version after 3 years. To completely remove objects older than 3 years, the lifecycle policy must be configured to also delete *previous* versions. This is the most cost-effective way to manage object retention in S3 with versioning enabled, as it leverages the built-in lifecycle management features.\n\n**Why option 0 is incorrect:**\nThis is incorrect because CloudTrail itself doesn't have a built-in expiration mechanism. CloudTrail delivers logs to an S3 bucket, and the expiration is managed by S3 lifecycle policies. Configuring CloudTrail to expire objects is not a valid option.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while a Lambda function could be used to delete objects, it is significantly more complex and costly than using S3 Lifecycle policies. It would require writing and maintaining code, managing Lambda execution, and incurring Lambda invocation costs, as well as S3 API call costs for listing and deleting objects. S3 Lifecycle policies are designed for this purpose and are much more cost-effective.\n\n**Why option 3 is incorrect:**\nThis is incorrect because the object ownership does not affect the lifecycle policy's ability to delete objects. S3 Lifecycle policies apply regardless of object ownership. While setting the parent account as the owner might be relevant for access control in some scenarios, it doesn't solve the problem of deleting old object versions.",
    "domain": "Design Cost-Optimized Architectures"
  }
]