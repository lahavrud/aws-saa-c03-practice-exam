[
  {
    "id": 0,
    "text": "Much of your company's data does not need to be accessed often, and can take several hours for \nretrieval time, so it's stored on Amazon Glacier. However someone within your organization has \nexpressed concerns that his data is more sensitive than the other data, and is wondering whether \nthe high level of encryption that he knows is on S3 is also used on the much cheaper Glacier \nservice. Which of the following statements would be most applicable in regards to this concern?",
    "options": [
      {
        "id": 0,
        "text": "There is no encryption on Amazon Glacier, that's why it is cheaper.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Glacier automatically encrypts the data using AES-256, the same as Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Glacier automatically encrypts the data using AES-128 a lesser encryption method than",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nLike Amazon S3, the Amazon Glacier service provides low-cost, secure, and durable storage. But where S3 is designed for rapid retrieval, Glacier is meant to be used as an archival service for data that is not accessed often, and for which retrieval times of several hours are suitable. Amazon Glacier automatically encrypts the data using AES-256 and stores it durably in an immutable form. Amazon Glacier is designed to provide average annual durability of 99.999999999% for an archive. It stores each archive in multiple facilities and multiple devices. Unlike traditional systems which can require laborious data verification and manual repair, Glacier performs regular, systematic data integrity checks, and is built to be automatically self-healing. Reference: http://d0.awsstatic.com/whitepapers/Security/AWS%20Security%20Whitepaper.pdf\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "Your EBS volumes do not seem to be performing as expected and your team leader has \nrequested you look into improving their performance. Which of the following is not a true \nstatement relating to the performance of your EBS volumes?",
    "options": [
      {
        "id": 0,
        "text": "Frequent snapshots provide a higher level of data durability and they will not degrade the",
        "correct": true
      },
      {
        "id": 1,
        "text": "General Purpose (SSD) and Provisioned IOPS (SSD) volumes have a throughput limit of 128",
        "correct": false
      },
      {
        "id": 2,
        "text": "There is a relationship between the maximum performance of your EBS volumes, the amount of",
        "correct": false
      },
      {
        "id": 3,
        "text": "There is a 5 to 50 percent reduction in IOPS when you first access each block of data on a newly",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSeveral factors can affect the performance of Amazon EBS volumes, such as instance configuration, I/O characteristics, workload demand, and storage configuration. Frequent snapshots provide a higher level of data durability, but they may slightly degrade the performance of your application while the snapshot is in progress. This trade off becomes critical when you have data that changes rapidly. Whenever possible, plan for snapshots to occur during off-peak times in order to minimize workload impact. Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "You are building infrastructure for a data warehousing solution and an extra request has come \nthrough that there will be a lot of business reporting queries running all the time and you are not \nsure if your current DB instance will be able to handle it.  \nWhat would be the best solution for this?",
    "options": [
      {
        "id": 0,
        "text": "DB Parameter Groups",
        "correct": false
      },
      {
        "id": 1,
        "text": "Read Replicas",
        "correct": true
      },
      {
        "id": 2,
        "text": "Multi-AZ DB Instance deployment",
        "correct": false
      },
      {
        "id": 3,
        "text": "Database Snapshots",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nRead Replicas make it easy to take advantage of MySQL's built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. There are a variety of scenarios where deploying one or more Read Replicas for a given source DB Instance may make sense. Common reasons for deploying a Read Replica include: Scaling beyond the compute or I/O capacity of a single DB Instance for read-heavy database workloads. This excess read traffic can be directed to one or more Read Replicas. Serving read traffic while the source DB Instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your Read Replica(s). For this use case, keep in mind that the data on the Read Replica may be \"stale\" since the source DB Instance is unavailable. Business reporting or data warehousing scenarios; you may want business reporting queries to run against a Read Replica, rather than your primary, production DB Instance. Reference: https://aws.amazon.com/rds/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "You've created your first load balancer and have registered your EC2 instances with the load \nbalancer. Elastic Load Balancing routinely performs health checks on all the registered EC2 \ninstances and automatically distributes all incoming requests to the DNS name of your load \nbalancer across your registered, healthy EC2 instances. By default, the load balancer uses the \n___ protocol for checking the health of your instances.",
    "options": [
      {
        "id": 0,
        "text": "HTTPS",
        "correct": false
      },
      {
        "id": 1,
        "text": "HTTP",
        "correct": true
      },
      {
        "id": 2,
        "text": "ICMP",
        "correct": false
      },
      {
        "id": 3,
        "text": "IPv6",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIn Elastic Load Balancing a health configuration uses information such as protocol, ping port, ping path (URL), response timeout period, and health check interval to determine the health state of the instances registered with the load balancer. Currently, HTTP on port 80 is the default health check. Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/TerminologyandKeyCo ncepts.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 4,
    "text": "A major finance organisation has engaged your company to set up a large data mining \napplication. Using AWS you decide the best service for this is Amazon Elastic MapReduce(EMR) \nwhich you know uses Hadoop. Which of the following statements best describes Hadoop?",
    "options": [
      {
        "id": 0,
        "text": "Hadoop is 3rd Party software which can be installed using AMI",
        "correct": false
      },
      {
        "id": 1,
        "text": "Hadoop is an open source python web framework",
        "correct": false
      },
      {
        "id": 2,
        "text": "Hadoop is an open source Java software framework",
        "correct": true
      },
      {
        "id": 3,
        "text": "Hadoop is an open source javascript framework",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon EMR uses Apache Hadoop as its distributed data processing engine. Hadoop is an open source, Java software framework that supports data-intensive distributed applications running on Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A company wants to host a scalable web application on AWS. \nThe application will be accessed by users from different geographic regions of the world. \nApplication users will be able to download and upload unique data up to gigabytes in size. \nThe development team wants a cost-effective solution to minimize upload and download latency \nand maximize performance. \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 with Transfer Acceleration to host the application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon S3 with CacheControl headers to host the application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe maximum size of a single file that can be delivered through Amazon CloudFront is 20 GB. This limit applies to all Amazon CloudFront distributions.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company captures clickstream data from multiple websites and analyzes it using batch \nprocessing. \nThe data is loaded nightly into Amazon Redshift and is consumed by business analysts. \nThe company wants to move towards near-real-time data processing for timely insights. \nThe solution should process the streaming data with minimal effort and operational overhead. \nWhich combination of AWS services are MOST cost-effective for this solution? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Kinesis Data Firehose",
        "correct": true
      },
      {
        "id": 4,
        "text": "Amazon Kinesis Data Analytics",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://d0.awsstatic.com/whitepapers/whitepaper-streaming-data-solutions-on-aws-with- amazonkinesis.pdf (9) https://aws.amazon.com/kinesis/#Evolve_from_batch_to_real-time_analytics\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A company is migrating a three-tier application to AWS. \nThe application requires a MySQL database. In the past, the application users reported poor \napplication performance when creating new entries. \nThese performance issues were caused by users generating different real-time reports from the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n138 \napplication duringworking hours. \nWhich solution will improve the performance of the application when it is moved to AWS?",
    "options": [
      {
        "id": 0,
        "text": "Import the data into an Amazon DynamoDB table with provisioned capacity.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create the database on a compute optimized Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Aurora MySQL Multi-AZ DB cluster.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe MySQL-compatible edition of Aurora delivers up to 5X the throughput of standard MySQL running on the same hardware, and enables existing MySQL applications and tools to run without requiring modification. https://aws.amazon.com/rds/aurora/mysql-features/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A start-up company has a web application based in the us-east-1 Region with multiple Amazon \nEC2 instances running behind an Application Load Balancer across multiple Availability Zones. \nAs the company's user base grows in the us-west-1 Region, it needs a solution with low latency \nand high availability. \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Provision EC2 instances in us-west-1.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision EC2 instances and an Application Load Balancer in us-west-1.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision EC2 instances and configure an Application Load Balancer in us-west-1.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision EC2 instances and configure an Application Load Balancer in us-west-1.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\n\"ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions [...] AWS Global Accelerator complements ELB by extending these capabilities beyond a single AWS Region, allowing you to provision a global interface for your applications in any number of Regions. If you have workloads that cater to a global client base, we recommend that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources.\" https://aws.amazon.com/global-accelerator/faqs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company must generate sales reports at the beginning of every month. \nThe reporting process launches 20 Amazon EC2 instances on the first of the month. \nThe process runs for 7 days and cannot be interrupted. The company wants to minimize costs. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n139 \nWhich pricing model should the company choose?",
    "options": [
      {
        "id": 0,
        "text": "Reserved Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Spot Block Instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "On-Demand Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Scheduled Reserved Instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nScheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them. Scheduled Instances are a good choice for workloads that do not run continuously, but do run on a regular schedule. For example, you can use Scheduled Instances for an application that runs during business hours or for batch processing that runs at the end of the week. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company's web application uses an Amazon RDS PostgreSQL DB instance to store its \napplication data. \nDuring the financial closing period at the start of every month. Accountants run large queries that \nimpact the database's performance due to high usage. \nThe company wants to minimize the impact that the reporting activity has on the web application. \nWhat should a solutions architect do to reduce the impact on the database with the LEAST \namount of effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a read replica and direct reporting traffic to the replica.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Multi-AZ database and direct reporting traffic to the standby.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a cross-Region read replica and direct reporting traffic to the replica.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Redshift database and direct reporting traffic to the Amazon Redshift",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon RDS uses the MariaDB, MySQL, Oracle, PostgreSQL, and Microsoft SQL Server DB engines' built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "A company has application running on Amazon EC2 instances in a VPC. \nOne of the applications needs to call an Amazon S3 API to store and read objects. \nThe company's security policies restrict any internet-bound traffic from the applications. \nWhich action will fulfill these requirements and maintain security?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 interface endpoint.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 gateway endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an S3 bucket in a private subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 bucket in the same Region as the EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nGateway Endpoint for S3 and DynamoDB https://medium.com/tensult/aws-vpc-endpoints-introduction-ef2bf85c4422 https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A website runs a web application that receives a burst of traffic each day at noon. The users \nupload new pictures and content daily, but have been complaining of timeouts. The architecture \nuses Amazon EC2 Auto Scaling groups, and the custom application consistently takes 1 minute \nto initiate upon boot up before responding to user requests. \nHow should a solutions architect redesign the architecture to better respond to changing traffic?",
    "options": [
      {
        "id": 0,
        "text": "Configure a Network Load Balancer with a slow start configuration.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS ElastiCache for Redis to offload direct requests to the servers.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Auto Scaling step scaling policy with an instance warmup condition.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudFront to use an Application Load Balancer as the origin.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nIf you are creating a step policy, you can specify the number of seconds that it takes for a newly launched instance to warm up. Until its specified warm-up time has expired, an instance is not counted toward the aggregated metrics of the Auto Scaling group. https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html#as-step- scaling-warmup\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company hosts its website on Amazon S3. The website serves petabytes of outbound traffic \nmonthly, which accounts for most of the company's AWS costs. \nWhat should a solutions architect do to reduce costs?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon CloudFront with the existing website as the origin.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Move the website to Amazon EC2 with Amazon EBS volumes for storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Global Accelerator and specify the existing website as the endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Rearchitect the website to run on a combination of Amazon API Gateway and AWS Lambda.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nA textbook case for CloudFront. The data transfer cost in CloudFront is lower than in S3. With heavy read operations of static content, it's more economical to add CloudFront in front of you S3 bucket.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 14,
    "text": "A company currently stores symmetric encryption keys in a hardware security module (HSM). A \nsolution architect must design a solution to migrate key management to AWS. The solution \nshould allow for key rotation and support the use of customer provided keys. Where should the \nkey material be stored to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n141",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Secrets Manager",
        "correct": true
      },
      {
        "id": 2,
        "text": "AWS Systems Manager Parameter store",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Key Management Service (AWS KMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. https://aws.amazon.com/secrets-manager/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company needs to implement a relational database with a multi-Region disaster recovery \nRecovery Point Objective (RPO) of 1 second and an Recovery Time Objective (RTO) of 1 minute. \nWhich AWS solution can achieve this?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Aurora Global Database",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB global tables.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon RDS for MySQL with Multi-AZ enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon RDS for MySQL with a cross-Region snapshot copy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCross-Region Disaster Recovery If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company is designing a new service that will run on Amazon EC2 instance behind an Elastic \nLoad Balancer. However, many of the web service clients can only reach IP addresses \nwhitelisted on their firewalls. \nWhat should a solution architect recommend to meet the clients' needs?",
    "options": [
      {
        "id": 0,
        "text": "A Network Load Balancer with an associated Elastic IP address.",
        "correct": false
      },
      {
        "id": 1,
        "text": "An Application Load Balancer with an a associated Elastic IP address",
        "correct": false
      },
      {
        "id": 2,
        "text": "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address",
        "correct": true
      },
      {
        "id": 3,
        "text": "An EC2 instance with a public IP address running as a proxy in front of the load balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nRoute 53 routes end users to Internet applications so the correct answer is C. Map one of the whitelisted IP addresses using an A record to the Elastic IP address.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A company's packaged application dynamically creates and returns single-use text files in \nresponse to user requests. The company is using Amazon CloudFront for distribution, but wants \nto future reduce data transfer costs. The company modify the application's source code. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n142 \nWhat should a solution architect do to reduce costs?",
    "options": [
      {
        "id": 0,
        "text": "Use Lambda@Edge to compress the files as they are sent to users.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable Amazon S3 Transfer Acceleration to reduce the response times.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable caching on the CloudFront distribution to store generated files at the edge.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 multipart uploads to move the files to Amazon S3 before returning them to users.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nB seems more expensive; C does not seem right because they are single use files and will not be needed again from the cache; D multipart mainly for large files and will not reduce data and cost; A seems the best: change the application code to compress the files and reduce the amount of data transferred to save costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "An application running on an Amazon EC2 instance in VPC-A needs to access files in another \nEC2 instance in VPC-B. Both are in separate AWS accounts. \nThe network administrator needs to design a solution to enable secure access to EC2 instance in \nVPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth \nconcerns. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up a VPC peering connection between VPC-A and VPC-B.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Attach a virtual private gateway to VPC-B and enable routing from VPC-A.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The traffic remains in the private IP space. All inter-region traffic is encrypted with no single point of failure, or bandwidth bottleneck. https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company stores user data in AWS. The data is used continuously with peak usage during \nbusiness hours. Access patterns vary, with some data not being used for months at a time. A \nsolutions architect must choose a cost-effective solution that maintains the highest level of \ndurability while maintaining high availability. \n \nWhich storage solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon S3 Glacier Deep Archive",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 One Zone-Infequent Access (S3 One Zone-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIntelligent tearing moves data between storage classes based on its current degree of usage.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect is creating an application that will handle batch processing of large amounts \nof data. The input data will be held in Amazon S3 and the output data will be stored in a different \nS3 bucket. For processing, the application will transfer the data over the network between \nmultiple Amazon EC2 instances. \n \nWhat should the solutions architect do to reduce the overall data transfer costs?",
    "options": [
      {
        "id": 0,
        "text": "Place all the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Place all the EC2 instances in the same AWS Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Place all the EC2 instances in the same Availability Zone.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Place all the EC2 instances in private subnets in multiple Availability Zones.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe transfer is between EC2 instances and not just between S3 and EC2. Also, be aware of inter-Availability Zones data transfer charges between Amazon EC2 instances, even within the same region. If possible, the instances in a development or test environment that need to communicate with each other should be co-located within the same Availability Zone to avoid data transfer charges. (This doesn't apply to production workloads which will most likely need to span multiple Availability Zones for high availability.) https://aws.amazon.com/blogs/mt/using-aws-cost-explorer-to-analyze-data-transfer-costs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 21,
    "text": "A company has recently updated its internal security standards. \nThe company must now ensure all Amazon S3 buckets and Amazon Elastic Block Store (Amazon \nEBS) volumes are encrypted with keys created and periodically rotated by internal security \nspecialists. \nThe company is looking for a native, software-based AWS service to accomplish this goal. \nWhat should a solutions architect recommend as a solution?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Secrets Manager with customer master keys (CMKs) to store master key material and",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Key Management Service (AWS KMS) with customer master keys (CMKs) to store",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an AWS CloudHSM cluster with customer master keys (CMKs) to store master key material",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Parameter Store with customer master keys (CMKs) keys to store",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Secrets Manager provides full lifecycle management for secrets within your environment. In this post, Maitreya and I will show you how to use Secrets Manager to store, deliver, and rotate SSH keypairs used for communication within compute clusters. Rotation of these keypairs is a security best practice, and sometimes a regulatory requirement. Traditionally, these keypairs have been associated with a number of tough challenges. For example, synchronizing key Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "An application is running on Amazon EC2 instances Sensitive information required for the \napplication is stored in an Amazon S3 bucket. \nThe bucket needs to be protected from internet access while only allowing services within the \nVPC access to the bucket. \nWhich combination of actions should a solutions archived take to accomplish this? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC endpoint for Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable server access logging on the bucket",
        "correct": false
      },
      {
        "id": 2,
        "text": "Apply a bucket policy to restrict access to the S3 endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an S3 ACL to the bucket that has sensitive information",
        "correct": false
      },
      {
        "id": 4,
        "text": "Restrict users using the IAM policy to use the specific bucket",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nACL is a property at object level not at bucket level .Also by just adding ACL you can't let the services in VPC allow access to the bucket .\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "A company relies on an application that needs at least 4 Amazon EC2 instances during regular \ntraffic and must scale up to 12 EC2 instances during peak loads. \nThe application is critical to the business and must be highly available. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy the EC2 instances in an Auto Scaling group.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nIt requires HA and if one AZ is down then at least 4 instances will be active in another AZ which is key for this\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company recently deployed a two-tier application in two Availability Zones in the us-east-1 \nRegion. The databases are deployed in a private subnet while the web servers are deployed in a \npublic subnet. \nAn internet gateway is attached to the VPC. The application and database run on Amazon EC2 \ninstances. The database servers are unable to access patches on the internet. \nA solutions architect needs to design a solution that maintains database security with the least \noperational overhead. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n145 \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a NAT gateway inside the public subnet for each Availability Zone and associate it with an",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy a NAT gateway inside the private subnet for each Availability Zone and associate it with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy two NAT instances inside the public subnet for each Availability Zone and associate them",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy two NAT instances inside the private subnet for each Availability Zone and associate them",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nVPC with public and private subnets (NAT) The configuration for this scenario includes a virtual private cloud (VPC) with a public subnet and a private subnet. We recommend this scenario if you want to run a public-facing web application, while maintaining back-end servers that aren't publicly accessible. A common example is a multi- tier website, with the web servers in a public subnet and the database servers in a private subnet. You can set up security and routing so that the web servers can communicate with the database servers. The instances in the public subnet can send outbound traffic directly to the Internet, whereas the instances in the private subnet can't. Instead, the instances in the private subnet can access the Internet by using a network address translation (NAT) gateway that resides in the public subnet. The database servers can connect to the Internet for software updates using the NAT gateway, but the Internet cannot establish connections to the database servers. https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "A company has an on-premises data center that is running out of storage capacity. The company \nwants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution \nmust allow for immediate retrieval of data at no additional cost. \nHow can these requirements be met?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon S3 Glacier Vault and enable expedited retrieval.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Storage Gateway using cached volumes.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy AWS Storage Gateway using stored volumes to store data locally.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy AWS Direct Connect to connect with the on-premises data center.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nVolume Gateway provides an iSCSI target, which enables you to create block storage volumes and mount them as iSCSI devices from your on-premises or EC2 application servers. The Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 26,
    "text": "A company recently implemented hybrid cloud connectivity using AWS Direct Connect and is \nmigrating data to Amazon S3. \nThe company is looking for a fully managed solution that will automate and accelerate the \nreplication of data between the on-premises storage systems and AWS storage services. \nWhich solution should a solutions architect recommend to keep the data private?",
    "options": [
      {
        "id": 0,
        "text": "Deploy an AWS DataSync agent tor the on-premises environment.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an AWS DataSync agent for the on-premises environment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an AWS Storage Gateway volume gateway for the on-premises environment.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Storage Gateway file gateway for the on-premises environment.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nYou can use AWS DataSync with your Direct Connect link to access public service endpoints or private VPC endpoints. When using VPC endpoints, data transferred between the DataSync agent and AWS services does not traverse the public internet or need public IP addresses, increasing the security of data as it is copied over the network.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A solutions architect is designing the storage architecture for a new web application used for \nstonng and viewing engineering drawings. All application components will be deployed on the \nAWS infrastructure. \nThe application design must support caching to minimize the amount of time that users wait for \nthe engineering drawings to load. The application must be able to store petabytes of data. \nWhich combination of storage and caching should the solutions architect use?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 with Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon S3 Glacier with Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway with Amazon ElastiCache",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCloudFront for caching and S3 as the origin. Glacier is used for archiving which is not the case for this scenario.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "An operations team has a standard that states IAM policies should not be applied directly to \nusers. Some new members have not been following this standard. \nThe operation manager needs a way to easily identify the users with attached policies. \nWhat should a solutions architect do to accomplish this? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n147",
    "options": [
      {
        "id": 0,
        "text": "Monitor using AWS CloudTrail",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Config rule to run daily",
        "correct": true
      },
      {
        "id": 2,
        "text": "Publish IAM user changes lo Amazon SNS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run AWS Lambda when a user is modified",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nA new AWS Config rule is deployed in the account after you enable AWS Security Hub. The AWS Config rule reacts to resource configuration and compliance changes and send these change items to AWS CloudWatch. When AWS CloudWatch receives the compliance change, a CloudWatch event rule triggers the AWS Lambda function.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company is building applications in containers. \nThe company wants to migrate its on-premises development and operations services from its on-\npremises data center to AWS. \nManagement states that production system must be cloud agnostic and use the same \nconfiguration and administrator tools across production systems. \nA solutions architect needs to design a managed solution that will align open-source software. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch the containers on Amazon EC2 with EC2 instance worker nodes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch the containers on Amazon Elastic Kubernetes Service (Amazon EKS) and EKS workers",
        "correct": true
      },
      {
        "id": 2,
        "text": "Launch the containers on Amazon Elastic Containers service (Amazon ECS) with AWS Fargate",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch the containers on Amazon Elastic Container Service (Amazon EC) with Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nWhen talking about containerized applications, the leading technologies which will always come up during the conversation are Kubernetes and Amazon ECS (Elastic Container Service). While Kubernetes is an open-sourced container orchestration platform that was originally developed by Google, Amazon ECS is AWS' proprietary, managed container orchestration service.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 30,
    "text": "A solutions architect is performing a security review of a recently migrated workload. \nThe workload is a web application that consists of Amazon EC2 instances in an Auto Scaling \ngroup behind an Application Load Balancer. \nThe solutions architect must improve the security posture and minimize the impact of a DDoS \nattack on resources. \nWhich solution is MOST effective?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS WAF ACL with rate-based rules.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a custom AWS Lambda function that adds identified attacks into a common vulnerability",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable VPC Flow Logs and store them in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable Amazon GuardDuty and configure findings written to Amazon CloudWatch.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS WAF is a web application firewall that helps detect and mitigate web application layer DDoS attacks by inspecting traffic inline. Application layer DDoS attacks use well-formed but malicious requests to evade mitigation and consume application resources. You can define custom security rules (also called web ACLs) that contain a set of conditions, rules, and actions to block attacking traffic. After you define web ACLs, you can apply them to CloudFront distributions, and web ACLs are evaluated in the priority order you specified when you configured them. Real-time metrics and sampled web requests are provided for each web ACL. https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos- attacks-by-using-amazon-cloudfront-and-amazon-route-53/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company is creating a prototype of an ecommerce website on AWS. The website consists of an \nApplication Load Balancer, an Auto Scaling group of Amazon EC2 instances for web servers, and \nan Amazon RDS for MySQL DB instance that runs with the Single-AZ configuration. \nThe website is slow to respond during searches of the product catalog. The product catalog is a \ngroup of tables in the MySQL database that the company does not update frequently. A solutions \narchitect has determined that the CPU utilization on the DB instance is high when product catalog \nsearches occur. \nWhat should the solutions architect recommend to improve the performance of the website during \nsearches of the product catalog?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the product catalog to an Amazon Redshift database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement an Amazon ElastiCache for Redis cluster to cache the product catalog.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Add an additional scaling policy to the Auto Scaling group to launch additional EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on the Multi-AZ configuration for the DB instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCommon ElastiCache Use Cases and How ElastiCache Can Help : Whether serving the latest news, a top-10 leaderboard, a product catalog, or selling tickets to an event, speed is the name of the game. The success of your website and business is greatly affected by the speed at which you deliver content. https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-use-cases.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "A company's application is running on Amazon EC2 instances within an Auto Scaling group \nbehind an Elastic Load Balancer. Based on the application's history, the company anticipates a \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n149 \nspike in traffic during a holiday each year. A solutions architect must design a strategy to ensure \nthat the Auto Scaling group proactively increases capacity to minimize any performance impact \non application users. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a recurring scheduled action to scale up the Auto Scaling group before the expected",
        "correct": true
      },
      {
        "id": 2,
        "text": "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon EC2 Auto Scaling supports sending Amazon SNS notifications when the following events occur. https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company is creating an application that runs on containers in a VPC. The application stores \nand accesses data in an Amazon S3 bucket. During the development phase, the application will \nstore and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs \nand wants to prevent traffic from traversing the internet whenever possible. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable S3 Intelligent-Tiering for the S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable S3 Transfer Acceleration for the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 34,
    "text": "A solutions architect is tasked with transferring 750 TB of data from an on-premises network-\nattached file system located at a branch office Amazon S3 Glacier. \nThe migration must not saturate the on-premises 1 Mbps internet connection. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS site-to-site VPN tunnel to an Amazon S3 bucket and transfer the files directly.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order 10 AWS Snowball Edge Storage Optimized devices, and select an S3 Glacier vault as the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Mount the network-attached file system to an S3 bucket, and copy the files directly.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Order 10 AWS Snowball Edge Storage Optimized devices, and select an Amazon S3 bucket as",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo upload existing data to Amazon S3 Glacier (S3 Glacier), you might consider using one of the AWS Snowball device types to import data into Amazon S3, and then move it to the S3 Glacier storage class for archival using lifecycle rules. When you transition Amazon S3 objects to the S3 Glacier storage class, Amazon S3 internally uses S3 Glacier for durable storage at lower cost. Although the objects are stored in S3 Glacier, they remain Amazon S3 objects that you manage in Amazon S3, and you cannot access them directly through S3 Glacier. https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company's website handles millions of requests each day, and the number of requests \ncontinues to increase. A solutions architect needs to improve the response time of the web \napplication. The solutions architect determines that the application needs to decrease latency \nwhen retrieving product details from the \nAmazon DynamoDB table. \nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Set up a DynamoDB Accelerator (DAX) cluster.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up Amazon ElastiCache for Redis between the DynamoDB table and the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon ElastiCache for Memcached between the DynamoDB table and the web",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon DynamoDB Streams on the table, and have AWS Lambda read from the table",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon DynamoDB is designed for scale and performance. In most cases, the DynamoDB response times can be measured in single-digit milliseconds. However, there are certain use cases that require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A media company collects and analyzes user activity data on premises. The company wants to \nmigrate this capability to AWS. The user activity data store will continue to grow and will be \npetabytes in size. The company needs to build a highly available data ingestion solution that \nfacilitates on-demand analytics of existing data and new data with SQL. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Send activity data to an Amazon Kinesis data stream.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Send activity data to an Amazon Kinesis Data Firehose delivery stream.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Place activity data in an Amazon S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon Kinesis Data Firehose is a data transfer service for loading streaming data into Amazon S3, Splunk, ElasticSearch, and RedShift. https://www.whizlabs.com/blog/aws-kinesis-data-streams-vs-aws-kinesis-data-firehose/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company is using a centralized AWS account to store log data in various Amazon S3 buckets. \nA solutions architect needs to ensure that the data is encrypted at rest before the data is \nuploaded to the S3 buckets. The data also must be encrypted in transit. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create bucket policies that require the use of server-side encryption with S3 managed encryption",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nData protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit using Secure Socket Layer/Transport Layer Security (SSL/TLS) or client-side encryption. https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "A company has an on-premises business application that generates hundreds of files each day. \nThese files are stored on an SMB file share and require a low- latency connection to the \napplication servers. A new company policy states all application-generated files must be copied to \nAWS. There is already a VPN connection to AWS. \nThe application development team does not have time to make the necessary code modifications \nto move the application to AWS. \nWhich service should a solutions architect recommend to allow the application to copy files to \nAWS?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe files will be on the storgare gateway with low latency and copied to AWS as a second copy. FSx in AWS will not provide low latency for the on prem apps over a vpn to the FSx file system.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A company has an ordering application that stores customer information in Amazon RDS for \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n152 \nMySQL. During regular business hours, employees run one-time queries for reporting purposes. \nTimeouts are occurring during order processing because the reporting queries are taking a long \ntime to run. The company needs to eliminate the timeouts without preventing employees from \nperforming queries. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a read replica. Move reporting queries to the read replica.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a read replica. Distribute the ordering application to the primary DB instance and the read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the ordering application to Amazon DynamoDB with on-demand capacity.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Schedule the reporting queries for non-peak hours.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nReporting is OK to run on replicated data with some delay in replication.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A company runs a web application that is backed by Amazon RDS. A new database administrator \ncaused data loss by accidentally editing information in a database table. To help recover from this \ntype of incident, the company wants the ability to restore the database to its state from 5 minutes \nbefore any change within the last 30 days. \nWhich feature should the solutions architect include in the design to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Read replicas",
        "correct": false
      },
      {
        "id": 1,
        "text": "Manual snapshots",
        "correct": false
      },
      {
        "id": 2,
        "text": "Automated backups",
        "correct": true
      },
      {
        "id": 3,
        "text": "Multi-AZ deployments",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nRDS creates automated backups of your volume snapshot in which you can recover to a specific point-in-time recovery. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBac kups.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 41,
    "text": "A company uses Amazon RDS for PostgreSQL databases for its data tier. The company must \nimplement password rotation for the databases. \n \nWhich solution meets this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the password in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the password in AWS Systems Manager Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the password in AWS Systems Manager Parameter Store.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the password in AWS Key Management Service (AWS KMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "A company's facility has badge readers at every entrance throughout the building. When badges \nare scanned, the readers send a message over HTTPS to indicate who attempted to access that \nparticular entrance. \n \nA solutions architect must design a system to process these messages from the sensors. The \nsolution must be highly available, and the results must be made available for the company's \nsecurity team to analyze. \n \nWhich system architecture should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an HTTPS endpoint in Amazon API Gateway.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a gateway VPC endpoint for Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nDeploy Amazon API Gateway as an HTTPS endpoint and AWS Lambda to process and save the messages to an Amazon DynamoDB table. This option provides a highly available and scalable solution that can easily handle large amounts of data. It also integrates with other AWS services, making it easier to analyze and visualize the data for the security team.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have \noutbound internet access, but the EC2 instance needs the ability to download monthly security \nupdates from an outside vendor. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an internet gateway, and attach it to the VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a NAT gateway, and place it in a public subnet.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a NAT instance, and place it in the same subnet where the EC2 instance is located.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an internet gateway, and attach it to the VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis approach will allow the EC2 instance to access the internet and download the monthly security updates while still being located in a private subnet. By creating a NAT gateway and placing it in a public subnet, it will allow the instances in the private subnet to access the internet through the NAT gateway. And then, configure the private subnet route table to use the NAT gateway as the default route. This will ensure that all outbound traffic is directed through the NAT gateway, allowing the EC2 instance to access the internet while still maintaining the security of the private subnet.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company has been running a web application with an Oracle relational database in an on-\npremises data center for the past 15 years. The company must migrate the database to AWS. \nThe company needs to reduce operational overhead without having to modify the application's \ncode. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the database servers to Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 instances to migrate and operate the database servers.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the database servers to Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Snowball Edge Storage Optimized device to migrate the data from Oracle to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nDMS can be used for database migration(supports cross database migration too). RDS supports MySQL, PostgreSQL, Microsoft SQL Server, Oracle. https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises- oracle-database-to-amazon-rds-for-oracle.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company is running an application on Amazon EC2 instances. Traffic to the workload increases \nsubstantially during business hours and decreases afterward. The CPU utilization of an EC2 \ninstance is a strong indicator of end-user demand on the application. The company has \nconfigured an Auto Scaling group to have a minimum group size of 2 EC2 instances and a \nmaximum group size of 10 EC2 instances. \n \nThe company is concerned that the current scaling policy that is associated with the Auto Scaling \ngroup might not be correct. The company must avoid over-provisioning EC2 instances and \nincurring unnecessary costs. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EC2 Auto Scaling to use a scheduled scaling plan and launch an additional 8",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS Auto Scaling to use a scaling plan that enables predictive scaling.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure a step scaling policy to add 4 EC2 instances at 50% CPU utilization and add another 4",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Auto Scaling to have a desired capacity of 5 EC2 instances, and disable any",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nPredictive Scaling, now natively supported as an EC2 Auto Scaling policy, uses machine learning to schedule the right number of EC2 instances in anticipation of approaching traffic changes. Predictive Scaling predicts future traffic, including regularly-occurring spikes, and provisions the right number of EC2 instances in advance. Predictive Scaling's machine learning algorithms detect changes in daily and weekly patterns, automatically adjusting their forecasts. This removes the need for manual adjustment of Auto Scaling parameters as cyclicality changes over time, making Auto Scaling simpler to configure. Auto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 46,
    "text": "A company wants to use a custom distributed application that calculates various profit and loss \nscenarios. To achieve this goal, the company needs to provide a network connection between its \nAmazon EC2 instances. The connection must minimize latency and must maximize throughput \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision the application to use EC2 Dedicated Hosts of the same instance type.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a placement group for EC2 instances that have the same instance type.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use multiple AWS elastic network interfaces and link aggregation.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS PrivateLink for the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement- groups-cluster\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company needs to run a critical application on AWS. The company needs to use Amazon EC2 \nfor the applications database. The database must be highly available and must fail over \nautomatically if a disruptive event occurs. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nConfigure the EC2 instances as a cluster) Cluster consist of one or more DB instances and a cluster volume that manages the data for those DB instances. Cluster Volume is a VIRTUAL DATABASE storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the DB cluster data. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company hosts its application on AWS. The company uses Amazon Cognito to manage users. \nWhen users log in to the application, the application fetches required data from Amazon \nDynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an \nAWS managed solution that will control access to the REST API to reduce development efforts. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user",
        "correct": false
      },
      {
        "id": 1,
        "text": "For each user, create and assign an API key that must be sent with each request. Validate the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Send the users email address in the header with every request. Invoke an AWS Lambda function",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUse the Amazon Cognito console, CLI/SDK, or API to create a user poolor use one that's owned by another AWS account. https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with- cognito.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company is developing a marketing communications service that targets mobile app users. The \ncompany needs to send confirmation messages with Short Message Service (SMS) to its users. \nThe users must be able to reply to the SMS messages. The company must store the responses \nfor a year for analysis. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/pinpoint/product-details/sms/ Two-Way Messaging: Receive SMS messages from your customers and reply back to them in a chat-like interactive experience. With Amazon Pinpoint, you can create automatic responses when customers send you messages that contain certain keywords. You can even use Amazon Lex to create conversational bots. A majority of mobile phone users read incoming SMS messages almost immediately after receiving them. If you need to be able to provide your customers with urgent or important information, SMS messaging may be the right solution for you. You can use Amazon Pinpoint to create targeted groups of customers, and then send them campaign-based messages. You can also use Amazon Pinpoint to send direct messages, such as appointment confirmations, order updates, and one-time passwords.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "The customers of a finance company request appointments with financial advisors by sending \ntext messages. A web application that runs on Amazon EC2 instances accepts the appointment \nrequests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) \nqueue through the web application. Another application that runs on EC2 instances then sends \nmeeting invitations and meeting confirmation email messages to the customers. After successful \nscheduling, this application stores the meeting information in an Amazon DynamoDB database. \nAs the company expands, customers report that their meeting invitations are taking longer to \narrive. \nWhat should a solutions architect recommend to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon API Gateway API in front of the web application that accepts the appointment",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo resolve the issue of longer delivery times for meeting invitations, the solutions architect can recommend adding an Auto Scaling group for the application that sends meeting invitations and configuring the Auto Scaling group to scale based on the depth of the SQS queue. This will allow the application to scale up as the number of appointment requests increases, improving the performance and delivery times of the meeting invitations.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company offers a food delivery service that is growing rapidly. Because of the growth, the \ncompanys order processing system is experiencing scaling problems during peak traffic hours. \nThe current architecture includes the following: \n \n- A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect \norders from the application \n- Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders \n \nThe order collection process occurs quickly, but the order fulfillment process can take longer. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n158 \nData must not be lost because of a scaling event. \nA solutions architect must ensure that the order collection process and the order fulfillment \nprocess can both scale properly during peak traffic hours. The solution must optimize utilization of \nthe companys AWS resources. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe number of instances in your Auto Scaling group can be driven by how long it takes to process a message and the acceptable amount of latency (queue delay). The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company hosts multiple production applications. One of the applications consists of resources \nfrom Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon \nSNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All \ncompany resources are tagged with a tag name of application and a value that corresponds to \neach application. A solutions architect must provide the quickest solution for identifying all of the \ntagged components. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudTrail to generate a list of resources with the application tag.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the AWS CLI to query each service across all Regions to report the tagged components.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Run a query in Amazon CloudWatch Logs Insights to report on the components with the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html Tags are words or phrases that act as metadata that you can use to identify and organize your AWS resources. A resource can have up to 50 user-applied tags. It can also have read-only system tags. Each tag consists of a key and one optional value.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company needs to export its database once a day to Amazon S3 for other teams to access. \nThe exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n159 \nvariable and changes rapidly. The data must be immediately available and must remain \naccessible for up to 3 months. The company needs the most cost-effective solution that will not \nincrease retrieval time. \nWhich S3 storage class should the company use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 1,
        "text": "S3 Glacier Instant Retrieval",
        "correct": false
      },
      {
        "id": 2,
        "text": "S3 Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nS3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier and after 90 days of no access to the Archive Instant Access tier.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A company is developing a new mobile app. The company must implement proper traffic filtering \nto protect its Application Load Balancer (ALB) against common application-level attacks, such as \ncross-site scripting or SQL injection. The company has minimal infrastructure and operational \nstaff. The company needs to reduce its share of the responsibility in managing, updating, and \nsecuring servers for its AWS environment. \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS WAF rules and associate them with the ALB.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application using Amazon S3 with public hosting enabled.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy AWS Shield Advanced and add the ALB as a protected resource.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall,",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nA solutions architect should recommend option A, which is to configure AWS WAF rules and associate them with the ALB. This will allow the company to apply traffic filtering at the application layer, which is necessary for protecting the ALB against common application-level attacks such as cross-site scripting or SQL injection. AWS WAF is a managed service that makes it easy to protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. The company can easily manage and update the rules to ensure the security of its application.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A companys reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. \nThe company must convert these files to Apache Parquet format and must store the files in a \ntransformed data bucket. \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function to transform the data and output the data to the transformed",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types- for-converting-data-to-apache-parquet.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "A company has a serverless website with millions of objects in an Amazon S3 bucket. The \ncompany uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company \ndid not set encryption on the S3 bucket before the objects were loaded. A solutions architect \nneeds to enable encryption for all existing objects and for all objects that are added to the S3 \nbucket in the future. \nWhich solution will meet these requirements with the LEAST amount of effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Navigate to Amazon S3 in the AWS Management Console. Browse the S3 buckets objects. Sort",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nStep 1: S3 inventory to get object list Step 2 (If needed): Use S3 Select to filter Step 3: S3 object operations to encrypt the unencrypted objects. On the going object use default encryption. https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company has a web server running on an Amazon EC2 instance in a public subnet with an \nElastic IP address. The default security group is assigned to the EC2 instance. The default \nnetwork ACL has been modified to block all traffic. A solutions architect needs to make the web \nserver accessible from everywhere on port 443. \nWhich combination of steps will accomplish this task? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTo enable the connection to a service running on an instance, the associated network ACL must allow both: - Inbound traffic on the port that the service is listening on - Outbound traffic to ephemeral ports https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A solutions architect is designing a new API using Amazon API Gateway that will receive \nrequests from users. The volume of requests is highly variable; several hours can pass without \nreceiving a single request. The data processing will take place asynchronously, but should be \ncompleted within a few seconds after a request is made. \nWhich compute service should the solutions architect have the API invoke to deliver the \nrequirements at the lowest cost?",
    "options": [
      {
        "id": 0,
        "text": "An AWS Glue job",
        "correct": false
      },
      {
        "id": 1,
        "text": "An AWS Lambda function",
        "correct": true
      },
      {
        "id": 2,
        "text": "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "A containerized service hosted in Amazon ECS with Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAPI Gateway + Lambda is the perfect solution for modern applications with serverless architecture.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 59,
    "text": "A company runs an application on a group of Amazon Linux EC2 instances. For compliance \nreasons, the company must retain all application log files for 7 years. The log files will be \nanalyzed by a reporting tool that must be able to access all the files concurrently. \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon EC2 instance store",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon S3 - Requests to Amazon S3 can be authenticated or anonymous. Authenticated access requires credentials that AWS can use to authenticate your requests. When making REST API calls directly from your code, you create a signature using valid credentials and include the signature in your request. Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world. Reference: Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company has hired an external vendor to perform work in the companys AWS account. The \nvendor uses an automated tool that is hosted in an AWS account that the vendor owns. The \nvendor does not have IAM access to the companys AWS account. \nHow should a solutions architect grant this access to the vendor?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role in the companys account to delegate access to the vendors IAM role. Attach",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM user in the companys account with a password that meets the password",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM group in the companys account. Add the tools IAM user from the vendor account",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new identity provider by choosing AWS account as the provider type in the IAM",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic \nKubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an \nAmazon DynamoDB table. A solutions architect must ensure that the application can interact with \nthe DynamoDB table without exposing traffic to the internet. \nWhich combination of steps should the solutions architect take to accomplish this goal? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Attach an IAM role that has sufficient privileges to the EKS pod.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an IAM user that has sufficient privileges to the EKS pod.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow outbound connectivity to the DynamoDB table through the private subnets network ACLs.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a VPC endpoint for DynamoDB.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Embed the access keys in the Java Spring Boot code.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints- dynamodb.html https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-eks-adds-support-to-assign-iam- permissions-to-kubernetes-service-accounts/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company recently migrated its web application to AWS by rehosting the application on Amazon \nEC2 instances in a single AWS Region. The company wants to redesign its application \narchitecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances \nrandomly. \nWhich combination of steps should the company take to meet these requirements? (Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n163",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Route 53 failover routing policy.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Route 53 weighted routing policy.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Route 53 multivalue answer routing policy.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Launch three EC2 instances: two instances in one Availability Zone and one instance in another",
        "correct": false
      },
      {
        "id": 4,
        "text": "Launch four EC2 instances: two instances in one Availability Zone and two instances in another",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/multivalue-versus-simple-policies/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company collects data from thousands of remote devices by using a RESTful web services \napplication that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, \ntransforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote \ndevices will increase into the millions soon. The company needs a highly scalable solution that \nminimizes operational overhead. \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue to process the raw data in Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Route 53 to route traffic to different EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add more EC2 instances to accommodate the increasing amount of incoming data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\n\"RESTful web services\" => API Gateway. \"EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket\" => GLUE with (Extract - Transform - Load)\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 64,
    "text": "A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing \nCloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. \nThe CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy \nis in place to delete current objects after 3 years. \nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of \nobjects has continued to rise. However, the number of new CloudTrail logs that are delivered to \nthe S3 bucket has remained consistent. \nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Configure the organizations centralized CloudTrail trail to expire objects after 3 years.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the S3 Lifecycle policy to delete previous versions as well as current versions.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the parent account as the owner of all objects that are delivered to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nTo delete objects that are older than 3 years in the most cost-effective manner, the company should configure the S3 Lifecycle policy to delete previous versions as well as current versions. This will ensure that all versions of the objects, including the previous versions, are deleted after 3 years.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  }
]