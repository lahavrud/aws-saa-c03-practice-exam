[
  {
    "id": 0,
    "text": "A medical company wants to perform transformations on a large amount of clinical trial data that \ncomes from several customers. The company must extract the data from a relational database \nthat contains the customer data. Then the company will transform the data by using a series of \ncomplex rules. The company will load the data to Amazon S3 when the transformations are \ncomplete. \n \nAll data must be encrypted where it is processed before the company stores the data in Amazon \nS3. All data must be encrypted by using customer-specific keys. \n \nWhich solution will meet these requirements with the LEAST amount of operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create one AWS Glue job for each customer. Attach a security configuration to each job that uses",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create one AWS Glue job for each customer. Attach a security configuration to each job that uses",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. \nThe analytics application is highly resilient and is designed to run in stateless mode. \n \nThe company notices that the application is showing signs of performance degradation during \nbusy times and is presenting 5xx errors. The company needs to make the application scale \nseamlessly. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n507 \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Machine Image (AMI) of the web application. Apply the AMI to a launch",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 2,
    "text": "A company runs an environment where data is stored in an Amazon S3 bucket. The objects are \naccessed frequently throughout the day. The company has strict da ta encryption requirements \nfor data that is stored in the S3 bucket. The company currently uses AWS Key Management \nService (AWS KMS) for encryption. \n \nThe company wants to optimize costs associated with encrypting S3 objects without making \nadditional calls to AWS KMS. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use server-side encryption with Amazon S3 managed keys (SSE-S3).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an S3 Bucket Key for server-side encryption with AWS KMS keys (SSE-KMS) on the new",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use client-side encryption with AWS KMS customer managed keys.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use server-side encryption with customer-provided keys (SSE-C) stored in AWS KMS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning after only 7 days might be too soon. The question states high access for the first few days, but it doesn't explicitly state that access drops to almost zero immediately after a week. Transitioning after 7 days might result in unnecessary transitions if there's still some level of frequent access between 7 and 30 days. Also, while S3 One Zone-IA is a good choice, the timing is premature.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "A company runs multiple workloads on virtual machines (VMs) in an on-premises data center. \nThe company is expanding rapidly. The on-premises data center is not able to scale fast enough \nto meet business needs. The company wants to migrate the workloads to AWS. \n \nThe migration is time sensitive. The company wants to use a lift-and-shift strategy for non-critical \nworkloads. \n \nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Application Migration Service. Install the AWS Replication Agent on the VMs.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Complete the initial replication of the VMs. Launch test instances to perform acceptance tests on",
        "correct": false
      },
      {
        "id": 3,
        "text": "Stop all operations on the VMs. Launch a cutover instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS App2Container (A2C) to collect data about the VMs.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "A company hosts an application in a private subnet. The company has already integrated the \napplication with Amazon Cognito. The company uses an Amazon Cognito user pool to \nauthenticate users. \n \nThe company needs to modify the application so the application can securely store user \ndocuments in an Amazon S3 bucket. \n \nWhich combination of steps will securely integrate Amazon S3 with the application? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Cognito identity pool to generate secure Amazon S3 access tokens for users",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the existing Amazon Cognito user pool to generate Amazon S3 access tokens for users when",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 VPC endpoint in the same VPC where the company hosts the application.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a NAT gateway in the VPC where the company hosts the application. Assign a policy to the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Attach a policy to the S3 bucket that allows access only from the users' IP addresses.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because Amazon Neptune is a graph database service. While it provides low latency for graph-based queries, it is not the best choice for a simple leaderboard application. The data structure and query patterns of a leaderboard are better suited for key-value stores or sorted sets, which are offered by ElastiCache for Redis or DynamoDB with DAX. Neptune is optimized for complex relationship analysis, which is not a primary requirement for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company has a three-tier web application that processes orders from customers. The web tier \nconsists of Amazon EC2 instances behind an Application Load Balancer. The processing tier \nconsists of EC2 instances. The company decoupled the web tier and processing tier by using \nAmazon Simple Queue Service (Amazon SQS). The storage layer uses Amazon DynamoDB. \n \nAt peak times, some users report order processing delays and halls. The company has noticed \nthat during these delays, the EC2 instances are running at 100% CPU usage, and the SQS \nqueue fills up. The peak times are variable and unpredictable. \n \nThe company needs to improve the performance of the application. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use scheduled scaling for Amazon EC2 Auto Scaling to scale out the processing tier instances for",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon ElastiCache for Redis in front of the DynamoDB backend tier. Use target utilization",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution to cache the responses for the web tier. Use HTTP latency",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon EC2 Auto Scaling target tracking policy to scale out the processing tier instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A company's production environment consists of Amazon EC2 On-Demand Instances that run \nconstantly between Monday and Saturday. The instances must run for only 12 hours on Sunday \nand cannot tolerate interruptions. The company wants to cost-optimize the production \nenvironment. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Purchase Scheduled Reserved Instances for the EC2 instances that run for only 12 hours on",
        "correct": true
      },
      {
        "id": 1,
        "text": "Purchase Convertible Reserved Instances for the EC2 instances that run for only 12 hours on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS is the most expensive due to the provisioned storage model, not the least expensive.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 7,
    "text": "A digital image processing company wants to migrate its on-premises monolithic application to \nthe AWS Cloud. The company processes thousands of images and generates large files as part \nof the processing workflow. \n \nThe company needs a solution to manage the growing number of image processing jobs. The \nsolution must also reduce the manual tasks in the image processing workflow. The company \ndoes not want to manage the underlying infrastructure of the solution. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 Spot Instances to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Batch jobs to process the images. Use AWS Step Functions to orchestrate the workflow.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Lambda functions and Amazon EC2 Spot Instances to process the images. Store the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a group of Amazon EC2 instances to process the images. Use AWS Step Functions to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because you *can* place a retention period on an object version through a bucket default setting. This is a valid configuration option, although explicit settings on the object version will override the bucket default.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company's image-hosting website gives users around the world the ability to up load, view, and \ndownload images from their mobile devices. The company currently hosts the static website in an \nAmazon S3 bucket. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n510 \nBecause of the website's growing popularity, the website's performance has decreased. Users \nhave reported latency issues when they upload and download images. \n \nThe company must improve the performance of the website. \n \nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon CloudFront distribution for the S3 bucket to improve the download",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure Amazon EC2 instances of the right sizes in multiple AWS Regions. Migrate the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon CloudFront distribution that uses the S3 bucket as an origin to improve the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure AWS Global Accelerator for the S3 bucket to improve network performance. Create an",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A company runs an application in a private subnet behind an Application Load Balancer (ALB) in \na VPC. The VPC has a NAT gateway and an internet gateway. The application calls the Amazon \nS3 API to store objects. \n \nAccording to the company's security policy, traffic from the application must not travel across the \ninternet. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure an S3 interface endpoint. Create a security group that allows outbound traffic to Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 gateway endpoint. Update the VPC route table to use the endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an S3 bucket policy to allow traffic from the Elastic IP address that is assigned to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a second NAT gateway in the same subnet where the legacy application is deployed.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company has an application that runs on an Amazon Elastic Kubernetes Service (Amazon \nEKS) cluster on Amazon EC2 instances. The application has a UI that uses Amazon DynamoDB \nand data services that use Amazon S3 as part of the application deployment. \n \nThe company must ensure that the EKS Pods for the UI can access only Amazon DynamoDB \nand that the EKS Pods for the data services can access only Amazon S3. The company uses \nAWS Identity and Access Management (IAM). \n \nWhich solution meals these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n511",
    "options": [
      {
        "id": 0,
        "text": "Create separate IAM policies for Amazon S3 and DynamoDB access with the required",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create separate IAM policies for Amazon S3 and DynamoDB access with the required",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create separate Kubernetes service accounts for the UI and data services to assume an IAM role.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create separate Kubernetes service accounts for the UI and data services to assume an IAM role.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "A company needs to give a globally distributed development team secure access to the \ncompany's AWS resources in a way that complies with security policies. \n \nThe company currently uses an on-premises Active Directory for internal authentication. The \ncompany uses AWS Organizations to manage multiple AWS accounts that support multiple \nprojects. \n \nThe company needs a solution to integrate with the existing infrastructure to provide centralized \nidentity management and access control. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Directory Service to create an AWS managed Microsoft Active Directory on AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM user for each developer. Manually manage permissions for each IAM user based",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AD Connector in AWS Directory Service to connect to the on-premises Active Directory.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Cognito to deploy an identity federation solution. Integrate the identity federation",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company is developing an application in the AWS Cloud. The application's HTTP API contains \ncritical information that is published in Amazon API Gateway. The critical information must be \naccessible from only a limited set of trusted IP addresses that belong to the company's internal \nnetwork. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n512",
    "options": [
      {
        "id": 0,
        "text": "Set up an API Gateway private integration to restrict access to a predefined set of IP addresses.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a resource policy for the API that denies access to any IP address that is not specifically",
        "correct": true
      },
      {
        "id": 2,
        "text": "Directly deploy the API in a private subnet. Create a network ACL. Set up rules to allow the traffic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the security group that is attached to API Gateway to allow inbound traffic from only the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Secure Architectures"
  }
]