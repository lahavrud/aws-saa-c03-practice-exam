[
  {
    "id": 1,
    "text": "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process. As a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EC2 user data to customize the dynamic installation parts at boot time",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the installation files in Amazon S3 so they can be quickly retrieved",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Elastic Beanstalk deployment caching feature",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon EC2 user data to install the application at boot time",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they address the core problem of lengthy installation time by pre-baking static components and efficiently handling dynamic components.\n\n*   **Option 0: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup:** This is a crucial step. By creating a Golden AMI, you pre-install all the static components of the application directly into the AMI. When Elastic Beanstalk launches new instances, it uses this AMI, significantly reducing the installation time because the static parts are already in place. This aligns with the principle of pre-baking infrastructure to improve deployment speed.\n\n*   **Option 1: Use Amazon EC2 user data to customize the dynamic installation parts at boot time:** User data scripts are executed when an EC2 instance (and thus, an Elastic Beanstalk instance) is launched. By using user data, you can handle the dynamic parts of the installation process. This could include fetching configuration files, generating unique IDs, or performing other tasks that need to be done on each instance. User data is a standard and efficient way to customize instances at boot time.\n\n**Why option 1 is correct:**\nThese are correct because they address the core problem of lengthy installation time by pre-baking static components and efficiently handling dynamic components.\n\n*   **Option 0: Create a Golden Amazon Machine Image (AMI) with the static installation components already setup:** This is a crucial step. By creating a Golden AMI, you pre-install all the static components of the application directly into the AMI. When Elastic Beanstalk launches new instances, it uses this AMI, significantly reducing the installation time because the static parts are already in place. This aligns with the principle of pre-baking infrastructure to improve deployment speed.\n\n*   **Option 1: Use Amazon EC2 user data to customize the dynamic installation parts at boot time:** User data scripts are executed when an EC2 instance (and thus, an Elastic Beanstalk instance) is launched. By using user data, you can handle the dynamic parts of the installation process. This could include fetching configuration files, generating unique IDs, or performing other tasks that need to be done on each instance. User data is a standard and efficient way to customize instances at boot time.\n\n**Why option 2 is incorrect:**\nOption 2: Store the installation files in Amazon S3 so they can be quickly retrieved. While storing installation files in S3 is a good practice for availability and versioning, it doesn't fundamentally address the 45-minute installation time. Retrieving files from S3 still takes time, and the installation process itself would still need to run. It might improve the speed slightly, but not enough to meet the 2-minute requirement.\n\n**Why option 3 is incorrect:**\nOption 3: Use AWS Elastic Beanstalk deployment caching feature. Elastic Beanstalk deployment caching is useful for speeding up deployments *after* the initial instance is set up. It caches application versions and configurations to reduce deployment time for subsequent updates. However, it doesn't help with the initial instance creation time, which is the focus of this question.\n\n**Why option 4 is incorrect:**\nOption 4: Use Amazon EC2 user data to install the application at boot time. While technically possible, installing the *entire* application using user data would likely still take a significant amount of time, especially given the 45-minute initial installation time. The goal is to reduce the installation time drastically, and installing everything via user data would be inefficient compared to pre-baking the static components into a Golden AMI.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 2,
    "text": "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system. As a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon EC2 user data to speed up the installation process",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the installation files in Amazon S3 for quicker retrieval",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the correct answer. Creating an Amazon Machine Image (AMI) after installing and configuring the software captures the entire application stack in a pre-configured state. Copying this AMI to all relevant AWS Regions ensures that a ready-to-use image is available in each region. When a disaster occurs, launching EC2 instances from the Region-specific AMI significantly reduces the recovery time, as the lengthy installation process is bypassed. This approach allows for meeting the 5-minute RTO requirement. The use of Region-specific AMIs is crucial because AMIs are Region-locked by default, and copying them ensures availability in the target DR regions.\n\n**Why option 0 is incorrect:**\nis incorrect because while creating an AMI helps, it doesn't address the need for regional availability. If the AMI is only in one region, the recovery process in other regions would still require copying the AMI first, which would take time and violate the RTO. It also doesn't explicitly mention copying the AMI to other regions.\n\n**Why option 2 is incorrect:**\nis incorrect because while EC2 user data can automate some configuration tasks, it doesn't eliminate the initial installation time. User data scripts are executed after the instance is launched, meaning the 45-minute installation process would still need to complete, violating the RTO. It only automates configuration after the base OS is up and running.\n\n**Why option 3 is incorrect:**\nis incorrect because storing installation files in Amazon S3 only addresses the retrieval of the files. It doesn't eliminate the need to install and configure the software, which still takes 45 minutes. While S3 provides fast retrieval, the installation process itself is the bottleneck.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly. Which of the following will you recommend as the best-fit solution?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon DynamoDB table with a global secondary index",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up Amazon DynamoDB table in the on-demand capacity mode",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon DynamoDB global table in the provisioned capacity mode",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nsetting up the DynamoDB table in on-demand capacity mode, is the best solution. On-demand capacity mode automatically scales up or down based on the actual workload, without requiring any capacity planning. It handles unpredictable traffic spikes effectively and charges only for the read and write capacity consumed. This is ideal for scenarios where traffic is unpredictable and includes periods of inactivity, as the company will not be charged when the table is not in use during the night hours.\n\n**Why option 0 is incorrect:**\nsetting up a DynamoDB table with a global secondary index (GSI), doesn't directly address the problem of unpredictable traffic and cost optimization. While GSIs can improve query performance, they don't automatically scale capacity or reduce costs during periods of inactivity. The question's primary concern is capacity management, not query optimization.\n\n**Why option 2 is incorrect:**\nsetting up a DynamoDB table in provisioned capacity mode with auto-scaling enabled, is a viable solution, but it's not as optimal as on-demand capacity mode in this scenario. Provisioned capacity requires you to estimate the initial capacity and configure auto-scaling rules. While auto-scaling can adjust capacity based on traffic, it might not react as quickly as on-demand capacity mode to sudden spikes. Furthermore, even with auto-scaling, you're still paying for the minimum provisioned capacity, even during the night hours when the table is not in use. This makes it less cost-effective than on-demand capacity.\n\n**Why option 3 is incorrect:**\nsetting up a DynamoDB global table in provisioned capacity mode, is designed for multi-region replication and disaster recovery. While it provides high availability and low latency access to data in different regions, it doesn't directly address the problem of unpredictable traffic spikes and cost optimization within a single region. It also adds complexity and cost compared to on-demand capacity mode. The question doesn't mention a requirement for multi-region replication.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains. The rule *.example.com matches which of the following?",
    "options": [
      {
        "id": 0,
        "text": "EXAMPLE.COM",
        "correct": false
      },
      {
        "id": 1,
        "text": "example.test.com",
        "correct": false
      },
      {
        "id": 2,
        "text": "test.example.com",
        "correct": true
      },
      {
        "id": 3,
        "text": "example.com",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\n'test.example.com', is the correct answer because the wildcard '*' in '*.example.com' matches any string of characters at the beginning of the domain before '.example.com'. Therefore, 'test' is a valid match for the wildcard. This allows the ELB to route traffic based on the subdomain 'test'.\n\n**Why option 0 is incorrect:**\n'EXAMPLE.COM', is incorrect because while domain names are case-insensitive, the wildcard '*' requires a subdomain to be present. 'EXAMPLE.COM' is the base domain itself, and the wildcard requires something *before* the 'example.com' part.\n\n**Why option 1 is incorrect:**\n'example.test.com', is incorrect because the wildcard '*.example.com' only matches subdomains of 'example.com'. 'example.test.com' is a subdomain of 'test.com', not 'example.com'. The wildcard only applies to the part of the domain immediately preceding '.example.com'.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 5,
    "text": "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements. Which of the following options represents a valid cost-optimization solution?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most comprehensive and effective solution. AWS Cost Optimization Hub provides a centralized view of cost optimization recommendations across different AWS services, including identifying idle or underutilized EC2 instances. AWS Compute Optimizer then analyzes the workload characteristics of those instances and suggests optimal instance types, potentially leading to significant cost savings by downsizing or switching to more cost-effective instance families. This directly addresses the issue of high EC2 costs, which are often a major contributor to overall AWS spend.\n\n**Why option 0 is incorrect:**\nis partially correct but incomplete. While AWS Compute Optimizer is useful for instance type recommendations and purchasing options, it doesn't directly address the issue of identifying idle or low-utilization instances in the first place. It also doesn't cover RDS or S3 cost optimization. Focusing solely on purchasing options without addressing utilization is less effective.\n\n**Why option 2 is incorrect:**\nfocuses solely on S3 cost optimization. While S3 storage class analysis and lifecycle policies are important for managing storage costs, they don't address the potential inefficiencies in EC2 and RDS, which are likely larger cost drivers for a compute-heavy workload. The question asks for a general cost optimization solution, and this option is too narrowly focused.\n\n**Why option 3 is incorrect:**\nis also partially correct but incomplete. AWS Trusted Advisor can help with RI utilization and identify idle RDS instances, but it doesn't provide instance type recommendations or address S3 cost optimization. Furthermore, Trusted Advisor doesn't automatically renew RIs; it only provides recommendations. This option is less comprehensive than option 1.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity. Which of the following options will maximize the VPN throughput?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Global Accelerator for the VPN connection to maximize the throughput",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Transfer Acceleration for the VPN connection to maximize the throughput",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a virtual private gateway with equal cost multipath routing and multiple channels",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\ncreating an AWS Transit Gateway with equal cost multipath (ECMP) routing and adding additional VPN tunnels, is the correct solution. Transit Gateway acts as a central hub for connecting multiple VPCs and on-premises networks. ECMP allows traffic to be distributed across multiple VPN tunnels, effectively increasing the overall bandwidth available for the VPN connection. By adding additional tunnels, the company can scale the VPN throughput to accommodate the increased traffic. Transit Gateway is designed for this type of hub-and-spoke network topology and provides a scalable and manageable solution for connecting multiple networks.\n\n**Why option 1 is incorrect:**\nusing AWS Global Accelerator for the VPN connection, is incorrect. AWS Global Accelerator is designed to improve the performance of applications by routing traffic through the AWS global network. While it can improve latency for internet-facing applications, it does not directly increase the throughput of a Site-to-Site VPN connection. Global Accelerator is more suitable for improving the user experience for applications accessed over the public internet, not for increasing VPN bandwidth.\n\n**Why option 2 is incorrect:**\nusing Transfer Acceleration for the VPN connection, is incorrect. Transfer Acceleration is a feature of Amazon S3 that enables fast, easy, and secure transfers of files over long distances between your client and your S3 bucket. It is not applicable to increasing the throughput of a Site-to-Site VPN connection.\n\n**Why option 3 is incorrect:**\ncreating a virtual private gateway with equal cost multipath routing and multiple channels, is incorrect. While a VGW can support multiple tunnels, Transit Gateway is the recommended solution for connecting multiple VPCs and on-premises networks in a hub-and-spoke topology. Transit Gateway provides better scalability and management capabilities compared to using multiple VGWs. Also, the term 'multiple channels' is not standard terminology in the context of AWS VPNs.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs. What do you recommend? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Dedicated hosts for the minimum capacity",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set the minimum capacity to 3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Reserved Instances (RIs) for the minimum capacity",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set the minimum capacity to 2",
        "correct": true
      },
      {
        "id": 4,
        "text": "Set the minimum capacity to 1",
        "correct": false
      }
    ],
    "correctAnswers": [
      2,
      3
    ],
    "explanation": "**Why option 2 is correct:**\nUse Reserved Instances (RIs) for the minimum capacity) is correct because RIs provide significant cost savings compared to On-Demand instances when used for predictable, steady-state workloads. By using RIs for the minimum capacity, the company can reduce costs for the baseline infrastructure required for high availability. Option 3 (Set the minimum capacity to 2) is also correct. Setting the minimum capacity to 2 ensures that even if one instance fails in one AZ, there is still one instance running in another AZ, maintaining high availability. Since the infrastructure is spread across 3 AZs, a minimum of 2 instances provides a reasonable level of redundancy without being overly expensive during idle periods. This allows the ASG to quickly scale up when tournaments start.\n\n**Why option 3 is correct:**\nUse Reserved Instances (RIs) for the minimum capacity) is correct because RIs provide significant cost savings compared to On-Demand instances when used for predictable, steady-state workloads. By using RIs for the minimum capacity, the company can reduce costs for the baseline infrastructure required for high availability. Option 3 (Set the minimum capacity to 2) is also correct. Setting the minimum capacity to 2 ensures that even if one instance fails in one AZ, there is still one instance running in another AZ, maintaining high availability. Since the infrastructure is spread across 3 AZs, a minimum of 2 instances provides a reasonable level of redundancy without being overly expensive during idle periods. This allows the ASG to quickly scale up when tournaments start.\n\n**Why option 0 is incorrect:**\nUse Dedicated hosts for the minimum capacity) is incorrect. Dedicated Hosts are the most expensive EC2 purchasing option and are typically used for compliance or licensing reasons, not for general cost optimization. While they provide instance isolation, they don't directly contribute to the cost-effective scaling strategy required in this scenario. Using Dedicated Hosts for minimum capacity would be an unnecessary expense.\n\n**Why option 1 is incorrect:**\nSet the minimum capacity to 3) is incorrect. While setting the minimum capacity to 3 would provide higher availability, it might be overkill and increase costs during idle periods. With the infrastructure spread across 3 AZs, a minimum of 2 instances is generally sufficient for high availability, allowing the ASG to scale up as needed during tournaments. Setting the minimum to 3 would increase the baseline cost without a significant improvement in availability, especially considering the rapid scaling capabilities of ASG.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 8,
    "text": "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes. As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
    "options": [
      {
        "id": 0,
        "text": "Use a wildcard Secure Sockets Layer certificate (SSL certificate)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Secure Sockets Layer certificate (SSL certificate) with SNI",
        "correct": true
      },
      {
        "id": 2,
        "text": "Change the Elastic Load Balancing (ELB) SSL Security Policy",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an HTTP to HTTPS redirect",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\n'Use Secure Sockets Layer certificate (SSL certificate) with SNI', is the correct answer. SNI (Server Name Indication) is an extension to the TLS protocol that allows a server to present multiple SSL certificates on the same IP address and port. This is crucial for hosting multiple HTTPS websites on a single load balancer. The ALB can use SNI to determine which certificate to present to the client based on the hostname requested in the TLS handshake. This minimizes configuration effort because you only need one ALB and SNI handles the certificate selection for each domain/path. Using a single certificate with SNI is more efficient and manageable than using multiple load balancers or complex routing configurations.\n\n**Why option 0 is incorrect:**\n'Use a wildcard Secure Sockets Layer certificate (SSL certificate)', is not the best solution. While a wildcard certificate (*.mycorp.com) would cover www.mycorp.com and checkout.mycorp.com, it wouldn't cover yourcorp.com. To cover all the domains, you would need a separate certificate for yourcorp.com or a multi-domain (SAN) certificate. SNI is a more flexible and scalable solution, especially when dealing with multiple distinct domains. Also, wildcard certificates are generally considered less secure than specific certificates.\n\n**Why option 2 is incorrect:**\n'Change the Elastic Load Balancing (ELB) SSL Security Policy', is incorrect. SSL Security Policies define the ciphers and protocols that the load balancer uses for SSL/TLS negotiation. While important for security, changing the security policy doesn't address the core requirement of serving different certificates for different domains. It's a separate concern from handling multiple HTTPS endpoints.\n\n**Why option 3 is incorrect:**\n'Use an HTTP to HTTPS redirect', is incorrect. While redirecting HTTP to HTTPS is a good security practice, it doesn't solve the problem of serving different domains/paths with HTTPS. It only ensures that users are redirected to the HTTPS version of the site. You still need to configure the load balancer to handle HTTPS requests and present the correct certificate for each domain.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures. Which combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon EC2 Spot Instances",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 Reserved Instances (RIs)",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon EC2 On-Demand Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answers are Amazon EC2 Spot Instances and Amazon Simple Queue Service (Amazon SQS).\n\n*   **Amazon EC2 Spot Instances:** Spot Instances offer significant cost savings compared to On-Demand or Reserved Instances. Since the application can tolerate interruptions (the job can be retried), Spot Instances are a suitable choice for cost optimization. The company is willing to wait for the response, so interruptions are acceptable.\n*   **Amazon Simple Queue Service (Amazon SQS):** SQS provides a reliable and scalable message queueing service. It allows for asynchronous processing by decoupling the image submission from the compression process. SQS also supports retries in case of failures, ensuring that jobs are eventually processed. The messages will be stored in the queue until they are successfully processed by the EC2 instances.\n\n**Why option 1 is correct:**\nThe correct answers are Amazon EC2 Spot Instances and Amazon Simple Queue Service (Amazon SQS).\n\n*   **Amazon EC2 Spot Instances:** Spot Instances offer significant cost savings compared to On-Demand or Reserved Instances. Since the application can tolerate interruptions (the job can be retried), Spot Instances are a suitable choice for cost optimization. The company is willing to wait for the response, so interruptions are acceptable.\n*   **Amazon Simple Queue Service (Amazon SQS):** SQS provides a reliable and scalable message queueing service. It allows for asynchronous processing by decoupling the image submission from the compression process. SQS also supports retries in case of failures, ensuring that jobs are eventually processed. The messages will be stored in the queue until they are successfully processed by the EC2 instances.\n\n**Why option 2 is incorrect:**\nAmazon Simple Notification Service (Amazon SNS) is primarily used for pub/sub messaging and notifications. While it can be integrated with SQS, it doesn't directly address the requirement of queuing and processing jobs asynchronously. It's more suitable for notifying subscribers about events, not for managing a queue of tasks to be processed.\n\n**Why option 3 is incorrect:**\nAmazon EC2 Reserved Instances (RIs) offer cost savings for long-term, predictable workloads. While they are cheaper than On-Demand Instances, they don't provide the same level of cost optimization as Spot Instances, especially when the workload can tolerate interruptions. RIs are a good choice when you need guaranteed capacity for a long period of time. In this case, the workload can tolerate interruptions, so Spot Instances are more cost-effective.\n\n**Why option 4 is incorrect:**\nAmazon EC2 On-Demand Instances provide flexibility but are the most expensive option. Since the company is cost-conscious and can tolerate interruptions, On-Demand Instances are not the best choice.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one. The users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
    "options": [
      {
        "id": 0,
        "text": "The Time To Live (TTL) is still in effect",
        "correct": true
      },
      {
        "id": 1,
        "text": "The health checks are failing",
        "correct": false
      },
      {
        "id": 2,
        "text": "The Alias Record is misconfigured",
        "correct": false
      },
      {
        "id": 3,
        "text": "The CNAME Record is misconfigured",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe Time To Live (TTL) value in the Route 53 record determines how long DNS resolvers (like those used by ISPs) cache the DNS record. When the record is updated, these resolvers may still be serving the old IP address (associated with the old Load Balancer) from their cache until the TTL expires. This is the most likely reason why users are not being redirected to the new Load Balancer immediately after the DNS record update. Reducing the TTL *before* the switch is a common mitigation strategy.\n\n**Why option 1 is incorrect:**\nWhile failing health checks could prevent traffic from being routed to *healthy* instances behind the load balancer, it wouldn't explain why users are still being directed to the *old* load balancer. Health checks are relevant for ensuring traffic is routed to healthy endpoints *within* the new load balancer's target group, not for the initial DNS resolution to the new load balancer itself.\n\n**Why option 2 is incorrect:**\nAn Alias record is used to map a domain name to an AWS resource, such as an ELB or CloudFront distribution. If the record type was incorrect, the DNS resolution would likely fail entirely, or return an error. The question states that users are being directed to the *old* load balancer, implying that the DNS resolution is working, just not pointing to the correct location. Therefore, a misconfigured Alias record is less likely than the TTL issue.\n\n**Why option 3 is incorrect:**\nA CNAME record maps a domain name to another domain name. Similar to the Alias record explanation, if the CNAME record was misconfigured, the DNS resolution would likely fail entirely or return an error. The question states that users are being directed to the *old* load balancer, implying that the DNS resolution is working, just not pointing to the correct location. Therefore, a misconfigured CNAME record is less likely than the TTL issue.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 11,
    "text": "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size10.0.1.0/24and the Auto Scaling group is deployed in a subnet of size10.0.4.0/22. As a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
    "options": [
      {
        "id": 0,
        "text": "Add a rule to authorize the security group of the Application Load Balancer",
        "correct": true
      },
      {
        "id": 1,
        "text": "Add a rule to authorize the CIDR 10.0.1.0/24",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add a rule to authorize the security group of the Auto Scaling group",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add a rule to authorize the CIDR 10.0.4.0/22",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because security groups in AWS are stateful firewalls that control inbound and outbound traffic at the instance level. By adding a rule to the EC2 instance's security group that authorizes traffic from the ALB's security group, you are explicitly allowing only traffic originating from the ALB to reach the EC2 instances. This approach is more secure and dynamic than using CIDR blocks because it automatically adjusts as the ALB's IP addresses change or if the ALB is scaled. It leverages the inherent security features of AWS and aligns with the principle of least privilege.\n\n**Why option 1 is incorrect:**\nis incorrect because while it might seem to work initially, relying on the CIDR block of the ALB's subnet is less secure and less maintainable. The ALB's IP addresses within that subnet could change, potentially breaking the security rule. Furthermore, it's possible that other resources within the same subnet might inadvertently gain access to the EC2 instances, violating the principle of least privilege.\n\n**Why option 2 is incorrect:**\nis incorrect because the Auto Scaling group itself doesn't have a security group. The security group is applied to the EC2 instances launched by the ASG. Authorizing the ASG's security group on itself would not restrict traffic to only come from the ALB.\n\n**Why option 3 is incorrect:**\nis incorrect because the CIDR block 10.0.4.0/22 represents the subnet where the EC2 instances are located, not the ALB. Authorizing this CIDR block would essentially allow all traffic from the EC2 instances' subnet to access the EC2 instances, which defeats the purpose of restricting traffic to only the ALB.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A fintech startup hosts its real-time transaction metadata in Amazon DynamoDB tables. During a recent system maintenance event, a junior engineer accidentally deleted a production table, resulting in major service downtime and irreversible data loss. Leadership has mandated an immediate solution that prevents future data loss from human error, while requiring minimal ongoing maintenance or manual effort from the engineering team. Which approach best addresses these requirements with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Enable deletion protection on DynamoDB tables",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable point-in-time recovery (PITR) on each DynamoDB table",
        "correct": false
      },
      {
        "id": 3,
        "text": "Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nEnabling deletion protection on DynamoDB tables directly prevents accidental deletion of tables. This feature adds a layer of protection that requires explicit disabling before a table can be deleted. This significantly reduces the risk of accidental deletion by human error. It has minimal operational overhead because it's a simple configuration setting.\n\n**Why option 1 is incorrect:**\nWhile this option provides auditing and automated recovery, it's more complex and has higher operational overhead than deletion protection. It involves configuring CloudTrail, EventBridge, Lambda, and S3, which requires more management and monitoring. The recovery process also takes time, leading to potential downtime. It also relies on having backup data already available.\n\n**Why option 2 is incorrect:**\nPoint-in-time recovery (PITR) allows you to restore a table to any point in time within the past 35 days. While it helps recover from data corruption or accidental writes, it doesn't prevent table deletion. If a table is deleted, PITR can be used to restore it, but the table is still gone initially, causing downtime. It also has a continuous cost associated with it.\n\n**Why option 3 is incorrect:**\nManually exporting tables to S3 on a weekly basis is a valid backup strategy, but it's not the best solution for preventing data loss from accidental deletion with minimal operational overhead. It requires manual intervention for both backup and recovery, which increases the risk of human error and adds to the operational burden. The weekly backup frequency also means that data loss could occur for transactions made since the last backup.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 13,
    "text": "A healthcare provider is experiencing rapid data growth in its on-premises servers due to increased patient imaging and record retention requirements. The organization wants to extend its storage capacity to AWS in a way that preserves quick access to critical records, including from its local file systems. The company must optimize bandwidth usage during migration and avoid any retrieval fees or delays when accessing the data in the cloud. The provider wants a hybrid cloud solution that requires minimal application reconfiguration, allows frequent local access to key datasets, and ensures that cloud storage costs remain predictable without paying extra for data retrieval. Which AWS solution best meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\ndeploying AWS Storage Gateway using cached volumes, is the best solution. Storage Gateway in cached mode stores frequently accessed data locally, providing low-latency access for on-premises applications. All data is asynchronously backed up to Amazon S3, satisfying the data growth and retention requirements. Because the data is written to S3, there are no retrieval fees when accessing the data, only the standard S3 storage costs. This approach minimizes application reconfiguration since the on-premises servers can continue to access data through a local interface. The asynchronous write to S3 also helps optimize bandwidth usage during migration and ongoing operations.\n\n**Why option 0 is incorrect:**\nis incorrect because while S3 Standard-IA is cost-effective for infrequently accessed data, it incurs retrieval fees. The question specifically states that the provider wants to avoid retrieval fees. Also, replicating changes to S3 using DataSync, while useful for migration, doesn't provide the low-latency local access required for frequently accessed data. It also doesn't address the need for a local file system interface.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon FSx for Windows File Server provides a fully managed Windows file system in the cloud, mounting it over a VPN connection introduces latency and may not provide the required quick access to critical records, especially for frequently accessed data. Also, it doesn't address the on-premises data growth issue directly, as it essentially moves the entire file system to the cloud, potentially requiring significant bandwidth for initial migration and ongoing operations. It also doesn't explicitly address the requirement for minimal application reconfiguration, as applications may need to be reconfigured to access the file system over the VPN.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A genomics research firm is processing sporadic bursts of data-intensive workloads using Amazon EC2 instances. The shared storage must support unpredictable spikes in file operations, but the average daily throughput demand remains relatively low. The team has selected Amazon Elastic File System (EFS) for its scalability and wants to ensure optimal cost and performance during bursts without provisioning throughput manually. Which approach should the team take to best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode",
        "correct": false
      },
      {
        "id": 3,
        "text": "Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages EFS's burst throughput mode, which is the default and most cost-effective option for workloads with occasional bursts of activity. The General Purpose performance mode is suitable for a wide range of workloads, and the EFS Standard storage class provides the necessary performance for the bursts. By using the default burst throughput, the team avoids the cost and complexity of provisioning throughput manually, which is not needed given the low average daily throughput.\n\n**Why option 0 is incorrect:**\nis incorrect because provisioning throughput manually would incur unnecessary costs when the average daily throughput is low. Provisioned throughput is more suitable for workloads with consistently high throughput requirements, not sporadic bursts. It also requires manual configuration and monitoring, which the question aims to avoid.\n\n**Why option 2 is incorrect:**\nis incorrect because while EFS Infrequent Access (IA) reduces storage costs for infrequently accessed files, it doesn't directly enhance or impact the burst throughput mode. IA is a storage tier, not a throughput mode. The question is primarily concerned with handling burst throughput efficiently and cost-effectively, not just reducing storage costs. While cost reduction is a factor, the primary focus is on performance during bursts.\n\n**Why option 3 is incorrect:**\nis incorrect because switching to EFS One Zone reduces cost by storing data in a single Availability Zone, which makes it less resilient. While it might be cheaper, it doesn't automatically enable burst throughput mode. Burst throughput is a feature of EFS itself, independent of the storage class (Standard or One Zone). More importantly, reducing availability might not be acceptable for a research firm's data.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 15,
    "text": "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure. As a Solutions Architect, which deployment do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use a Cluster placement group",
        "correct": true
      },
      {
        "id": 1,
        "text": "Optimize the Amazon EC2 kernel using EC2 User Data",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Spot Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Spread placement group",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing a Cluster placement group is the correct choice because it is designed to provide low latency and high network throughput between instances within the group. Cluster placement groups pack instances close together inside a single Availability Zone. This reduces latency and increases network throughput, which is crucial for distributed applications that require high network performance, such as the described Big Data processing framework. This directly addresses the requirement of high network performance between processing machines.\n\n**Why option 1 is incorrect:**\nOptimizing the Amazon EC2 kernel using EC2 User Data might provide some performance improvements, but it doesn't directly address the network proximity requirement. Kernel optimization is a general performance tuning technique and doesn't guarantee low latency or high throughput between specific instances. It's also a more complex and potentially less impactful solution compared to using placement groups.\n\n**Why option 2 is incorrect:**\nUsing Spot Instances is a cost-saving strategy, but it doesn't guarantee network performance. Spot Instances can be interrupted, which can negatively impact the performance of a distributed application. Furthermore, Spot Instances can be launched in different Availability Zones, potentially increasing network latency. The question specifically prioritizes performance over cost.\n\n**Why option 3 is incorrect:**\nUsing a Spread placement group is designed for high availability by spreading instances across distinct underlying hardware. While it provides fault tolerance, it doesn't optimize for network performance. In fact, spreading instances can increase network latency compared to keeping them close together. This contradicts the requirement of high network performance.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead. As a solutions architect, which of the following would you recommend to meet the given requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nmigrating the data to Amazon RDS for SQL Server in a Multi-AZ deployment, is the correct answer. RDS Multi-AZ provides automatic failover to a standby replica in a different Availability Zone in case of an infrastructure failure. This significantly increases database availability. Furthermore, RDS is a managed service, which reduces the operational overhead associated with managing the database server, backups, patching, and other administrative tasks. This aligns perfectly with the requirements of maximum availability and minimal operational overhead.\n\n**Why option 1 is incorrect:**\nmigrating the data to Amazon RDS for SQL Server in a cross-region read-replica configuration, is incorrect. While cross-region read replicas provide disaster recovery capabilities, they are primarily designed for read scaling and disaster recovery, not immediate high availability. Failover to a read replica is not automatic and involves manual intervention, increasing recovery time objective (RTO). The question specifically asks for maximum availability and minimizing operational overhead, which a simple read replica setup doesn't fully address for immediate failover.\n\n**Why option 2 is incorrect:**\nmigrating the data to an Amazon EC2 instance hosted SQL Server database deployed in a Multi-AZ configuration, is incorrect. While deploying SQL Server on EC2 instances in a Multi-AZ configuration can provide high availability, it significantly increases operational overhead. The company would be responsible for managing the operating system, SQL Server installation, patching, backups, and failover mechanisms. This contradicts the requirement of minimizing operational and management overhead. RDS handles these tasks automatically.\n\n**Why option 3 is incorrect:**\nmigrating the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment, is incorrect. While this option provides both disaster recovery (cross-region) and high availability (Multi-AZ), it is more complex and potentially more expensive than simply using a Multi-AZ deployment within a single region. For the stated requirements of *maximum possible availability* and *minimizing operational and management overhead*, the simpler Multi-AZ deployment within a single region is sufficient and more cost-effective. Cross-region deployments are typically reserved for scenarios with specific disaster recovery requirements that are not explicitly mentioned in the question.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other. Which of the following is the MOST cost-effective solution for this use-case?",
    "options": [
      {
        "id": 0,
        "text": "Use a Network Address Translation gateway (NAT gateway)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a VPC peering connection",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an AWS Direct Connect connection",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Internet Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nVPC peering is the most cost-effective solution for connecting VPCs within the same AWS account. It allows direct network connectivity between VPCs, enabling instances in different VPCs to communicate as if they were within the same network. VPC peering is generally free (you only pay for the data transferred between the VPCs), making it significantly cheaper than other options like NAT Gateways (which have hourly charges and data processing fees), Direct Connect (which is for connecting on-premises networks), or using the Internet Gateway (which would require public IPs and incur data transfer costs). VPC peering is also relatively simple to set up and manage for this small-scale scenario.\n\n**Why option 0 is incorrect:**\nA NAT Gateway allows instances in a private subnet to connect to the internet or other AWS services, but it does not enable direct communication between VPCs. It also incurs hourly charges and data processing fees, making it a less cost-effective solution for VPC-to-VPC communication within the same account.\n\n**Why option 2 is incorrect:**\nAWS Direct Connect is used to establish a dedicated network connection from your on-premises environment to AWS. It's not intended for connecting VPCs within the same AWS account and is significantly more expensive and complex than VPC peering. It is an overkill for this scenario.\n\n**Why option 3 is incorrect:**\nAn Internet Gateway allows instances in a VPC to connect to the internet. While it could be used to facilitate communication between VPCs, it would require instances to have public IP addresses and route traffic through the public internet, which is less secure and less cost-effective than VPC peering. Also, data transfer costs over the internet can be substantial.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 18,
    "text": "A transportation logistics company runs a shipment tracking application on Amazon EC2 instances with an Amazon Aurora MySQL database cluster. The application is experiencing rapid growth due to increased demand from mobile app users querying package delivery statuses. Although the compute layer (EC2) has remained stable, the Aurora DB cluster is under growing read pressure, especially from frequent repeated queries about package locations and delivery history. The company added an Aurora read replica, which temporarily alleviated the load, but read traffic continues to spike as user queries grow. The company wants to reduce the repeated reads pressure on the DB cluster. Which solution will best meet these requirements in a cost-effective manner?",
    "options": [
      {
        "id": 0,
        "text": "Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas",
        "correct": false
      },
      {
        "id": 1,
        "text": "Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database",
        "correct": true
      },
      {
        "id": 2,
        "text": "Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nintegrating Amazon ElastiCache for Redis between the application and Aurora, is the most suitable solution. Redis is an in-memory data store that can cache frequently accessed query results. By caching the results of repeated queries about package locations and delivery history, the application can retrieve the data from Redis instead of querying the Aurora database, significantly reducing the read load on the database. This approach is cost-effective because Redis is generally cheaper than scaling the database tier, and it specifically addresses the problem of repeated queries.\n\n**Why option 0 is incorrect:**\nAdding another Aurora read replica (Option 0) might provide some temporary relief, but it doesn't fundamentally address the problem of repeated queries. Read replicas still need to query the database, and scaling read replicas indefinitely can become expensive. Client-side load balancing adds complexity to the application without directly solving the caching issue.\n\n**Why option 2 is incorrect:**\nConverting to a multi-writer setup (Option 2) is not relevant to the problem. The issue is read pressure, not write contention. Multi-writer setups are designed for scenarios where multiple applications need to write to the database simultaneously, which is not the case here. It also increases complexity and cost unnecessarily.\n\n**Why option 3 is incorrect:**\nEnabling Aurora Serverless v2 (Option 3) would automatically scale the database capacity, but it might not be the most cost-effective solution for repeated read queries. While it can handle spikes in traffic, it doesn't specifically address the caching of frequently accessed data. Furthermore, Serverless v2 can be more expensive than using a caching layer like ElastiCache for scenarios with highly repetitive read patterns.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 19,
    "text": "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect  Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability. Which of the following would you identify as correct regarding Amazon CloudFront? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Use field level encryption in Amazon CloudFront to protect sensitive data for specific content",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon CloudFront can route to multiple origins based on the price class",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use geo restriction to configure Amazon CloudFront for high-availability and failover",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon CloudFront can route to multiple origins based on the content type",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content",
        "correct": false
      },
      {
        "id": 5,
        "text": "Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      3,
      5
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0, 3, and 5 are correct.\n\n*   **Option 0 (Use field level encryption in Amazon CloudFront to protect sensitive data for specific content):** Field-level encryption in CloudFront allows you to encrypt specific data fields (like credit card numbers or personal information) at the edge, ensuring that only authorized applications can decrypt and access the data. This is a crucial security feature for protecting sensitive information.\n*   **Option 3 (Amazon CloudFront can route to multiple origins based on the content type):** CloudFront can be configured to route requests to different origins based on various factors, including content type. This is achieved through behaviors and path patterns. For example, requests for images (.jpg, .png) can be routed to an S3 bucket optimized for image storage, while requests for dynamic content (.php, .jsp) can be routed to an EC2 instance or an Application Load Balancer.\n*   **Option 5 (Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover):** Origin groups provide high availability and failover capabilities. You can configure a primary origin and a secondary origin. If the primary origin becomes unavailable (e.g., returns 5xx errors), CloudFront automatically switches to the secondary origin, ensuring continuous content delivery.\n\n**Why option 3 is correct:**\nOptions 0, 3, and 5 are correct.\n\n*   **Option 0 (Use field level encryption in Amazon CloudFront to protect sensitive data for specific content):** Field-level encryption in CloudFront allows you to encrypt specific data fields (like credit card numbers or personal information) at the edge, ensuring that only authorized applications can decrypt and access the data. This is a crucial security feature for protecting sensitive information.\n*   **Option 3 (Amazon CloudFront can route to multiple origins based on the content type):** CloudFront can be configured to route requests to different origins based on various factors, including content type. This is achieved through behaviors and path patterns. For example, requests for images (.jpg, .png) can be routed to an S3 bucket optimized for image storage, while requests for dynamic content (.php, .jsp) can be routed to an EC2 instance or an Application Load Balancer.\n*   **Option 5 (Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover):** Origin groups provide high availability and failover capabilities. You can configure a primary origin and a secondary origin. If the primary origin becomes unavailable (e.g., returns 5xx errors), CloudFront automatically switches to the secondary origin, ensuring continuous content delivery.\n\n**Why option 5 is correct:**\nOptions 0, 3, and 5 are correct.\n\n*   **Option 0 (Use field level encryption in Amazon CloudFront to protect sensitive data for specific content):** Field-level encryption in CloudFront allows you to encrypt specific data fields (like credit card numbers or personal information) at the edge, ensuring that only authorized applications can decrypt and access the data. This is a crucial security feature for protecting sensitive information.\n*   **Option 3 (Amazon CloudFront can route to multiple origins based on the content type):** CloudFront can be configured to route requests to different origins based on various factors, including content type. This is achieved through behaviors and path patterns. For example, requests for images (.jpg, .png) can be routed to an S3 bucket optimized for image storage, while requests for dynamic content (.php, .jsp) can be routed to an EC2 instance or an Application Load Balancer.\n*   **Option 5 (Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover):** Origin groups provide high availability and failover capabilities. You can configure a primary origin and a secondary origin. If the primary origin becomes unavailable (e.g., returns 5xx errors), CloudFront automatically switches to the secondary origin, ensuring continuous content delivery.\n\n**Why option 1 is incorrect:**\nAmazon CloudFront can route to multiple origins based on the price class) is incorrect. CloudFront's price class determines the edge locations used for caching content. It affects performance and cost, but it doesn't directly influence routing to different origins. Routing is based on behaviors and path patterns, not the price class.\n\n**Why option 2 is incorrect:**\nUse geo restriction to configure Amazon CloudFront for high-availability and failover) is incorrect. Geo restriction (also known as geo blocking) is a security feature that allows you to control which geographic locations can access your content. It does not provide high availability or failover capabilities. High availability is achieved through origin groups.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably. As a Solutions Architect, which database do you recommend for their requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Relational Database Service (Amazon RDS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Neptune",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon DynamoDB is the best choice because it's a fully managed, serverless NoSQL database service that offers single-digit millisecond performance at any scale. It automatically scales up or down based on demand, eliminating the need for manual capacity provisioning. DynamoDB is designed for high-throughput, low-latency applications, making it ideal for high-frequency lookups of GPS coordinates. Its serverless nature removes the operational overhead of managing database servers.\n\n**Why option 0 is incorrect:**\nAmazon ElastiCache is an in-memory data store and cache service. While it provides very low latency, it is primarily used for caching frequently accessed data to improve the performance of existing databases. It's not a suitable primary database for storing all GPS coordinates, especially considering the need for persistence and scalability beyond caching.\n\n**Why option 2 is incorrect:**\nAmazon Relational Database Service (Amazon RDS) is a managed relational database service. While RDS offers various database engines (e.g., MySQL, PostgreSQL), relational databases are generally not as well-suited as NoSQL databases for high-frequency lookups and horizontal scalability in this specific scenario. Also, while some RDS options offer auto-scaling, they are not inherently serverless in the same way as DynamoDB.\n\n**Why option 3 is incorrect:**\nAmazon Neptune is a graph database service. While it's suitable for applications with complex relationships between data points, it's not the best choice for storing and retrieving simple GPS coordinates. The scenario doesn't indicate a need for graph-based queries or relationships.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams. As a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams",
        "correct": true
      },
      {
        "id": 2,
        "text": "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nLeveraging AWS Database Migration Service (DMS) is the fastest way to achieve this. While DMS is typically used for database migrations, it can also be used for continuous data replication from S3 to Kinesis Data Streams. DMS supports S3 as a source and Kinesis Data Streams as a target. It can handle both the initial load of existing data and ongoing changes (file updates) in S3. This approach minimizes the need for custom code and infrastructure management, making it the fastest solution.\n\n**Why option 0 is incorrect:**\nConfiguring EventBridge and Lambda is a viable solution, but it's not the *fastest*. It requires writing and deploying Lambda code, configuring EventBridge rules, and handling potential scaling issues. This involves more development and operational overhead compared to DMS.\n\n**Why option 2 is incorrect:**\nSimilar to option 0, using S3 event notifications and Lambda is a valid approach, but it's not the *fastest*. It also requires writing and deploying Lambda code, and it might be less efficient for handling the initial load of existing data compared to DMS. S3 event notifications are primarily designed for reacting to individual file events, not for bulk data transfer.\n\n**Why option 3 is incorrect:**\nUsing SNS as an intermediary is inefficient and unnecessary. SNS is primarily for notifications, not for streaming data. It would introduce additional complexity and latency without providing any significant benefit. Furthermore, SNS has message size limitations that would make it unsuitable for transferring large data files.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 22,
    "text": "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud. As a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
    "options": [
      {
        "id": 0,
        "text": "Pilot Light",
        "correct": false
      },
      {
        "id": 1,
        "text": "Warm Standby",
        "correct": true
      },
      {
        "id": 2,
        "text": "Multi Site",
        "correct": false
      },
      {
        "id": 3,
        "text": "Backup and Restore",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nWarm Standby is the correct answer. In a Warm Standby disaster recovery strategy, a scaled-down but fully functional environment is continuously running in AWS. This environment includes critical services and data. When a disaster occurs in the primary on-premise data center, the Warm Standby environment can be quickly scaled up to handle the full production workload, minimizing recovery time. This aligns perfectly with the question's requirements of a scaled-down, always-running environment and minimal recovery time.\n\n**Why option 0 is incorrect:**\nPilot Light involves replicating data to AWS and having minimal core services running. While it's faster than Backup and Restore, it still requires provisioning and configuring resources during a disaster, leading to a longer recovery time than Warm Standby. It doesn't fully meet the requirement of a scaled-down, *fully functional* environment always running.\n\n**Why option 2 is incorrect:**\nMulti-Site (also known as Active-Active) involves running the application in multiple active locations simultaneously. This provides the fastest recovery time but is also the most expensive and complex to implement and maintain. While it offers minimal recovery time, it's more than what's strictly required by the question, and the question implies a cost-conscious approach by mentioning a 'scaled-down' environment. Also, the question mentions an on-premise to AWS migration, making Multi-Site less relevant as the primary site is not in AWS.\n\n**Why option 3 is incorrect:**\nBackup and Restore is the simplest and least expensive disaster recovery strategy. However, it involves backing up data and applications to AWS and restoring them in case of a disaster. This process can take a significant amount of time, making it unsuitable for scenarios requiring minimal recovery time. It doesn't meet the requirement of an always-running environment.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies. Which is the only resource-based policy that the IAM service supports?",
    "options": [
      {
        "id": 0,
        "text": "Access control list (ACL)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Trust policy",
        "correct": true
      },
      {
        "id": 2,
        "text": "Permissions boundary",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Organizations Service Control Policies (SCP)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nTrust policies are the only resource-based policies that the IAM service directly supports. These policies are attached to IAM roles and define which principals (AWS accounts, IAM users, or AWS services) are allowed to assume the role. This allows resources to grant permissions to other entities to access them. Trust policies are essential for cross-account access and service-linked roles.\n\n**Why option 0 is incorrect:**\nAccess Control Lists (ACLs) are resource-based policies, but they are primarily used with services like Amazon S3 and Amazon Glacier, *not* directly within IAM itself. While they control access to resources, they are not an IAM service feature.\n\n**Why option 2 is incorrect:**\nPermissions boundaries are identity-based policies that define the maximum permissions that an IAM identity (user or role) can have. They do not grant access to resources directly, but rather limit the scope of what an identity can do, regardless of the permissions granted by other policies. They are not resource-based.\n\n**Why option 3 is incorrect:**\nAWS Organizations Service Control Policies (SCPs) are used to manage permissions across an entire AWS organization or organizational unit (OU). They are not resource-based policies in the same way as trust policies. SCPs limit the maximum permissions that can be delegated to IAM identities within the organization or OU, but they don't directly grant access to individual resources. They operate at a higher level of abstraction than resource-based policies.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints. As a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Amazon Kinesis",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon DynamoDB",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Amazon DynamoDB) and 4 (Amazon S3) are the correct answers because they are the only two AWS services that support Gateway Endpoints. Gateway Endpoints operate at Layer 3 (the network layer) and are used to provide private connectivity to S3 and DynamoDB within a VPC. This means traffic to these services from within the VPC does not traverse the public internet, enhancing security and reducing latency. The VPC route tables are modified to route traffic destined for S3 or DynamoDB through the Gateway Endpoint.\n\n**Why option 4 is correct:**\nOptions 1 (Amazon DynamoDB) and 4 (Amazon S3) are the correct answers because they are the only two AWS services that support Gateway Endpoints. Gateway Endpoints operate at Layer 3 (the network layer) and are used to provide private connectivity to S3 and DynamoDB within a VPC. This means traffic to these services from within the VPC does not traverse the public internet, enhancing security and reducing latency. The VPC route tables are modified to route traffic destined for S3 or DynamoDB through the Gateway Endpoint.\n\n**Why option 0 is incorrect:**\nAmazon Kinesis) is incorrect because Amazon Kinesis does not support Gateway Endpoints. It requires Interface Endpoints (powered by PrivateLink) to access it from within a VPC without traversing the public internet.\n\n**Why option 2 is incorrect:**\nAmazon Simple Notification Service (Amazon SNS)) is incorrect because Amazon SNS does not support Gateway Endpoints. It requires Interface Endpoints (powered by PrivateLink) to access it from within a VPC without traversing the public internet.\n\n**Why option 3 is incorrect:**\nAmazon Simple Queue Service (Amazon SQS)) is incorrect because Amazon SQS does not support Gateway Endpoints. It requires Interface Endpoints (powered by PrivateLink) to access it from within a VPC without traversing the public internet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 25,
    "text": "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture. As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold",
        "correct": true
      },
      {
        "id": 1,
        "text": "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions",
        "correct": false
      },
      {
        "id": 2,
        "text": "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources",
        "correct": true
      },
      {
        "id": 3,
        "text": "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images",
        "correct": false
      },
      {
        "id": 4,
        "text": "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code",
        "correct": true
      },
      {
        "id": 5,
        "text": "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0, 2, and 4 are correct. \n\n*   **Option 0:** Monitoring Lambda function metrics like `ConcurrentExecutions` and `Invocations` is crucial for identifying potential bottlenecks or unexpected behavior. CloudWatch Alarms are the standard way to trigger notifications when these metrics exceed predefined thresholds, allowing the team to proactively address issues and maintain the performance of the serverless application. This aligns with operational excellence and reliability pillars of the AWS Well-Architected Framework.\n*   **Option 2:** By default, Lambda functions do not have access to resources within a VPC. When a Lambda function is configured to access resources within a VPC (e.g., a database), it needs to be associated with subnets within that VPC. To access public internet resources or public AWS APIs from within the VPC, the Lambda function needs a route through a NAT Gateway or NAT Instance in a public subnet. This is because Lambda functions deployed in private subnets do not have direct internet access. This ensures secure and controlled access to resources.\n*   **Option 4:** AWS Lambda Layers provide a mechanism for sharing code across multiple Lambda functions. This promotes code reuse, reduces deployment package size, and simplifies maintenance. By creating a Lambda Layer for reusable code, the engineering team can avoid duplicating code across multiple functions, making the application more modular and easier to update. This aligns with the principle of DRY (Don't Repeat Yourself) and improves code maintainability.\n\n**Why option 2 is correct:**\nOptions 0, 2, and 4 are correct. \n\n*   **Option 0:** Monitoring Lambda function metrics like `ConcurrentExecutions` and `Invocations` is crucial for identifying potential bottlenecks or unexpected behavior. CloudWatch Alarms are the standard way to trigger notifications when these metrics exceed predefined thresholds, allowing the team to proactively address issues and maintain the performance of the serverless application. This aligns with operational excellence and reliability pillars of the AWS Well-Architected Framework.\n*   **Option 2:** By default, Lambda functions do not have access to resources within a VPC. When a Lambda function is configured to access resources within a VPC (e.g., a database), it needs to be associated with subnets within that VPC. To access public internet resources or public AWS APIs from within the VPC, the Lambda function needs a route through a NAT Gateway or NAT Instance in a public subnet. This is because Lambda functions deployed in private subnets do not have direct internet access. This ensures secure and controlled access to resources.\n*   **Option 4:** AWS Lambda Layers provide a mechanism for sharing code across multiple Lambda functions. This promotes code reuse, reduces deployment package size, and simplifies maintenance. By creating a Lambda Layer for reusable code, the engineering team can avoid duplicating code across multiple functions, making the application more modular and easier to update. This aligns with the principle of DRY (Don't Repeat Yourself) and improves code maintainability.\n\n**Why option 4 is correct:**\nOptions 0, 2, and 4 are correct. \n\n*   **Option 0:** Monitoring Lambda function metrics like `ConcurrentExecutions` and `Invocations` is crucial for identifying potential bottlenecks or unexpected behavior. CloudWatch Alarms are the standard way to trigger notifications when these metrics exceed predefined thresholds, allowing the team to proactively address issues and maintain the performance of the serverless application. This aligns with operational excellence and reliability pillars of the AWS Well-Architected Framework.\n*   **Option 2:** By default, Lambda functions do not have access to resources within a VPC. When a Lambda function is configured to access resources within a VPC (e.g., a database), it needs to be associated with subnets within that VPC. To access public internet resources or public AWS APIs from within the VPC, the Lambda function needs a route through a NAT Gateway or NAT Instance in a public subnet. This is because Lambda functions deployed in private subnets do not have direct internet access. This ensures secure and controlled access to resources.\n*   **Option 4:** AWS Lambda Layers provide a mechanism for sharing code across multiple Lambda functions. This promotes code reuse, reduces deployment package size, and simplifies maintenance. By creating a Lambda Layer for reusable code, the engineering team can avoid duplicating code across multiple functions, making the application more modular and easier to update. This aligns with the principle of DRY (Don't Repeat Yourself) and improves code maintainability.\n\n**Why option 1 is incorrect:**\nis incorrect because while Lambda allocates compute power proportionally to the memory allocated, over-provisioning timeout settings doesn't directly improve performance. Timeout settings should be based on the expected execution time of the function, with a small buffer for unexpected delays. Setting excessively long timeouts can lead to unnecessary costs if the function encounters an error and doesn't complete within a reasonable timeframe. The statement that AWS recommends over-provisioning timeout settings is also incorrect.\n\n**Why option 3 is incorrect:**\nis incorrect because AWS Lambda supports packaging and deploying functions as container images. This allows developers to use familiar container tooling and workflows to build and deploy Lambda functions, especially when dealing with larger dependencies or custom runtimes. This provides more flexibility in managing dependencies and deployment environments.\n\n**Why option 5 is incorrect:**\nis incorrect because while it's true that larger deployment packages can increase cold start times, AWS recommends using Lambda Layers to separate dependencies from the function code. This allows Lambda to cache the layer content, reducing the cold start time for subsequent invocations. The statement is partially correct but the recommendation to package dependencies as a separate package from the actual Lambda package is precisely what Lambda Layers are for.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 26,
    "text": "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud. What is an optimal solution that meets these requirements while keeping the costs to a minimum?",
    "options": [
      {
        "id": 0,
        "text": "Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Tape Gateway, a feature of AWS Storage Gateway, allows you to replace physical tapes with virtual tapes in AWS without changing your existing backup workflows. It integrates seamlessly with existing backup applications. The virtual tapes are stored in Amazon S3, and you can use S3 Glacier or S3 Glacier Deep Archive for cost-effective long-term storage. This directly addresses the requirements of maintaining existing workflows and minimizing costs for archiving petabytes of data.\n\n**Why option 1 is incorrect:**\nis incorrect because AWS DataSync is primarily used for online data transfer between on-premises storage and AWS. While it can move large amounts of data, it doesn't directly address the requirement of maintaining existing tape backup workflows. DataSync would require a separate process to extract data from the tapes before transferring it, adding complexity and potentially disrupting the current workflow. Also, DataSync is not specifically designed to work with tape backups.\n\n**Why option 2 is incorrect:**\nis incorrect because AWS Direct Connect provides a dedicated network connection between on-premises and AWS, which can improve transfer speeds and security. However, it doesn't directly address the requirement of maintaining existing tape backup workflows. Direct Connect is a networking solution, not a tape backup solution. It would still require a separate process to extract data from the tapes and transfer it to S3, which would not preserve the existing tape workflow. While S3 can be used for cost-effective storage, Direct Connect itself does not solve the tape backup integration problem.\n\n**Why option 3 is incorrect:**\nis incorrect because while a VPN connection can provide secure connectivity between on-premises and AWS, and EFS is a scalable file system, neither directly addresses the requirement of maintaining existing tape backup workflows. EFS is a file system, not a tape backup solution. It would require a separate process to extract data from the tapes and store it on EFS, which would not preserve the existing tape workflow. Also, EFS is generally more expensive than S3 Glacier or S3 Glacier Deep Archive, making it less cost-effective for archiving.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 27,
    "text": "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage. How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS CloudFormation to manage Amazon RDS databases",
        "correct": true
      },
      {
        "id": 2,
        "text": "Attach an IAM policy to interns preventing them from creating an Amazon RDS database",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store your recommendations in a custom AWS Trusted Advisor rule",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS CloudFormation, is the correct answer. CloudFormation allows you to define your infrastructure as code. By creating a CloudFormation template that includes the desired configuration for your RDS databases, you can ensure that all databases created using that template adhere to your defined best practices. This promotes consistency and reduces the risk of misconfigurations. CloudFormation also supports features like drift detection, which can help identify when resources have deviated from the template's configuration. You can also use CloudFormation Guard to enforce policies on your templates.\n\n**Why option 0 is incorrect:**\ncreating a Lambda function to send emails when misconfigurations are found, is a reactive approach. While it can help identify issues, it doesn't prevent them from occurring in the first place. The question asks for a proactive solution to incorporate best practices into reusable templates. This option only alerts after a misconfiguration has already happened.\n\n**Why option 2 is incorrect:**\nattaching an IAM policy to interns preventing them from creating RDS databases, is a restrictive approach. While it might prevent misconfigurations by interns, it doesn't address the underlying problem of ensuring best practices are followed by all users. It also doesn't provide a reusable template for creating RDS databases correctly. It's a control, not a solution for incorporating best practices into infrastructure as code.\n\n**Why option 3 is incorrect:**\nstoring recommendations in a custom AWS Trusted Advisor rule, is helpful for identifying potential issues, but it doesn't enforce best practices during the creation of the RDS database. It's a monitoring and advisory tool, not a mechanism for incorporating best practices into reusable infrastructure templates. It's also a reactive approach, identifying issues after they exist, rather than preventing them.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity. Which of the following represents the MOST scalable solution with minimal configuration changes?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator",
        "correct": true
      },
      {
        "id": 1,
        "text": "Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Global Accelerator provides static IP addresses that act as a single point of entry for applications distributed across multiple AWS Regions. By registering the ALBs with Global Accelerator, the on-premises firewall only needs to allow the static IP addresses associated with the Global Accelerator. This simplifies firewall management and provides a scalable solution, as the Global Accelerator handles routing traffic to the appropriate ALB based on health checks and proximity. The static IPs provided by Global Accelerator remain constant, even if the underlying ALB IP addresses change, fulfilling the requirement of minimal configuration changes.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves developing a Lambda script to periodically retrieve the IP addresses of the ALBs. This introduces complexity and operational overhead. The Lambda function would need to be scheduled, monitored, and maintained. Furthermore, there's a potential for race conditions if the ALB IP addresses change between the Lambda function's execution and the firewall update. This solution is not scalable and requires significant configuration changes and ongoing maintenance.\n\n**Why option 2 is incorrect:**\nis incorrect because Network Load Balancers (NLBs) are regional and cannot directly route traffic to private IP addresses of ALBs in different regions. While it's possible to peer VPCs and route traffic, this adds significant complexity and doesn't provide a scalable or easily manageable solution. Registering private IPs across regions with an NLB is not a standard or recommended practice. The NLB would also need to be in the same VPC as one of the ALBs, creating a dependency.\n\n**Why option 3 is incorrect:**\nis incorrect because migrating all ALBs to NLBs is a significant architectural change that may not be feasible or desirable. ALBs and NLBs have different features and capabilities, and a wholesale migration could impact application functionality. Furthermore, while NLBs support Elastic IPs, you would still need to manage multiple Elastic IPs across different regions, which doesn't provide the desired scalability and minimal configuration changes compared to Global Accelerator.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs):AZ-AandAZ-B. Cross-zone load balancing is disabled.AZ-Ahas four targets andAZ-Bhas six targets. Which of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
    "options": [
      {
        "id": 0,
        "text": "Each of the four targets in AZ-A receives 12.5% of the traffic",
        "correct": true
      },
      {
        "id": 1,
        "text": "Each of the six targets in AZ-B receives 10% of the traffic",
        "correct": false
      },
      {
        "id": 2,
        "text": "Each of the four targets in AZ-A receives 10% of the traffic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Each of the four targets in AZ-A receives 8% of the traffic",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis correct. Since cross-zone load balancing is disabled, each NLB only distributes traffic to targets within its own AZ. Route 53 sends 50% of the traffic to the NLB in AZ-A. This 50% is then evenly distributed among the four targets in AZ-A. Therefore, each target receives 50% / 4 = 12.5% of the total traffic.\n\n**Why option 1 is incorrect:**\nis incorrect because it focuses on AZ-B. While each target in AZ-B *does* receive an equal share of the traffic sent to the NLB in AZ-B, the question specifically asks about AZ-A. Also, the NLB in AZ-B receives 50% of the traffic, and that 50% is distributed among 6 targets, meaning each target in AZ-B receives 50%/6 = 8.33% of the *total* traffic, not 10%.\n\n**Why option 2 is incorrect:**\nis incorrect. If each target in AZ-A received 10% of the traffic, the total traffic handled by AZ-A would be 4 * 10% = 40%. However, AZ-A's NLB receives 50% of the traffic from Route 53.\n\n**Why option 3 is incorrect:**\nis incorrect. If each target in AZ-A received 8% of the traffic, the total traffic handled by AZ-A would be 4 * 8% = 32%. However, AZ-A's NLB receives 50% of the traffic from Route 53.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 30,
    "text": "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database. The architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon RDS Read Replicas",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon RDS Multi-AZ feature",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing Amazon RDS Read Replicas is the most suitable solution. Read Replicas allow you to create one or more copies of your primary RDS instance. These replicas can handle read traffic, offloading the read load from the primary database. Since the application is read-heavy, directing read requests to the Read Replicas will significantly improve performance and scalability without requiring changes to the application's core logic. RDS Read Replicas are designed for exactly this scenario: scaling read capacity for relational databases.\n\n**Why option 1 is incorrect:**\nUsing Amazon DynamoDB would require significant changes to the application's data model and code. DynamoDB is a NoSQL database and is not compatible with the existing RDS PostgreSQL database schema. Migrating to DynamoDB would involve rewriting the data access layer of the application, which contradicts the requirement of avoiding changes to the application's core logic.\n\n**Why option 2 is incorrect:**\nUsing Amazon ElastiCache would be beneficial for caching frequently accessed data, but it doesn't directly address the read load on the RDS database. While caching can reduce the number of reads to the database, it requires application-level changes to implement the caching logic. The question specifically asks for a solution that doesn't require changes to the application's core logic. ElastiCache would be a good *supplement* to Read Replicas, but not a replacement.\n\n**Why option 3 is incorrect:**\nUsing Amazon RDS Multi-AZ feature enhances the availability and durability of the database by providing a standby instance in a different Availability Zone. While Multi-AZ provides failover capabilities, it does not increase read throughput. The standby instance is only used in case of a failure of the primary instance. It does not serve read requests. Therefore, it does not address the requirement of increasing read throughput.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A global pharmaceutical company operates a hybrid cloud network. Its primary AWS workloads run in the us-west-2 Region, connected to its on-premises data center via an AWS Direct Connect connection. After acquiring a biotech firm headquartered in Europe, the company must integrate the biotechs workloads, which are hosted in several VPCs in the eu-central-1 Region and connected to the biotech's on-premises facility through a separate Direct Connect link. All CIDR blocks are non-overlapping, and the business requires full connectivity between both data centers and all VPCs across the two Regions. The company also wants a scalable solution that minimizes manual network configuration and long-term operational overhead. Which solution will best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication",
        "correct": false
      },
      {
        "id": 1,
        "text": "Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages AWS Direct Connect Gateway (DXGW) to enable transitive routing between the two Regions and the on-premises networks. DXGW simplifies the network architecture by acting as a central hub for Direct Connect connections. By attaching the virtual private gateways (VGWs) of both Regions to the DXGW, the Direct Connect connections can be shared, and traffic can flow between the on-premises networks and the VPCs in both Regions. This solution is scalable, reduces manual configuration compared to peering or VPNs, and minimizes operational overhead by centralizing routing management.\n\n**Why option 0 is incorrect:**\nis incorrect because establishing inter-Region VPC peering between *each* VPC in both regions creates a complex and unscalable mesh network. With multiple VPCs in each region, the number of peering connections required grows rapidly, leading to significant management overhead. Static routing also adds to the complexity and manual configuration effort. While VPC peering provides connectivity, it doesn't scale well for this scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because private VIFs are associated with a single VGW or DXGW. You cannot directly associate them with foreign-region VPCs. Also, VPC endpoints are used for accessing AWS services, not for routing traffic between VPCs or on-premises networks. BGP is used for routing information exchange, but it doesn't solve the fundamental limitation of VIF association.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying EC2-based VPN appliances in each VPC and configuring a full mesh VPN topology is complex and resource-intensive. It requires managing and maintaining a large number of VPN connections, which increases operational overhead. While it provides connectivity, it's not the most scalable or cost-effective solution, especially when Direct Connect is already in place.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\". As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
    "options": [
      {
        "id": 0,
        "text": "Amazon OpenSearch Service",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Redshift",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Neptune",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Aurora",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon Neptune is a fully managed graph database service. It is designed to store and query highly connected data. The query 'What are the number of likes on the videos that have been posted by friends of a user A?' involves traversing relationships (friendships, video postings, likes). Neptune is optimized for such graph traversals, making it the best choice for this use case. It supports popular graph query languages like Gremlin and SPARQL, allowing developers to efficiently query the relationships between users, videos, and likes.\n\n**Why option 0 is incorrect:**\nAmazon OpenSearch Service is a search and analytics engine. While it can handle large volumes of data, it is not optimized for complex relationship queries like the one described in the question. It's primarily used for log analytics, full-text search, and application monitoring, not for managing and querying graph-like relationships.\n\n**Why option 1 is incorrect:**\nAmazon Redshift is a data warehouse service designed for large-scale data warehousing and analytics. It is optimized for analytical queries on structured data, typically using SQL. While Redshift can handle complex queries, it is not the best choice for querying relationships between entities in a graph-like structure. The performance would be significantly worse compared to a graph database like Neptune. Additionally, Redshift is more suited for historical data analysis rather than real-time relationship queries.\n\n**Why option 3 is incorrect:**\nAmazon Aurora is a relational database service compatible with MySQL and PostgreSQL. While Aurora can handle relational data and complex SQL queries, it is not optimized for graph-based relationships. Implementing the required query in Aurora would involve complex joins and potentially slow performance, especially as the data scales. It's not the right tool for the job when a graph database is a much better fit.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation. As a Solutions Architect, what do you propose to reduce the costs?",
    "options": [
      {
        "id": 0,
        "text": "Change the Amazon EC2 instance type to something much smaller",
        "correct": false
      },
      {
        "id": 1,
        "text": "Keep the Amazon EBS volume to io1 and reduce the IOPS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Convert the Amazon EC2 instance EBS volume to gp2",
        "correct": true
      },
      {
        "id": 3,
        "text": "Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\n'Convert the Amazon EC2 instance EBS volume to gp2', is the correct answer. The scenario states that the io1 volume is underutilized but experiences occasional I/O bursts. gp2 volumes provide a good balance of price and performance for most workloads and offer the ability to burst to higher IOPS for short periods. Since the current io1 volume is underutilized, switching to gp2 will likely reduce costs significantly without negatively impacting performance, especially considering the occasional burst pattern. gp2 volumes are also suitable for boot volumes and general-purpose workloads, making them a good fit for this scenario.\n\n**Why option 0 is incorrect:**\n'Change the Amazon EC2 instance type to something much smaller', might seem like a cost-saving measure, but the question states that the EBS volume accounts for 90% of the cost. While reducing the instance size could save some money, it doesn't address the primary cost driver, which is the over-provisioned io1 volume. Furthermore, reducing the instance size could negatively impact application performance if the instance is already sized appropriately for the workload's CPU and memory requirements.\n\n**Why option 1 is incorrect:**\n'Keep the Amazon EBS volume to io1 and reduce the IOPS', is incorrect because while reducing IOPS would lower the cost of the io1 volume, it doesn't address the fundamental issue of over-provisioning. io1 volumes are designed for applications that require consistent, high IOPS performance. Since the volume is underutilized and only experiences occasional bursts, gp2 is a more cost-effective option. Staying with io1, even with reduced IOPS, will still be more expensive than switching to gp2.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs).AZ-Ahas 3 Amazon EC2 instances andAZ-Bhas 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm. What will happen under the default Auto Scaling Group configuration?",
    "options": [
      {
        "id": 0,
        "text": "A random instance in the AZ-A will be terminated",
        "correct": false
      },
      {
        "id": 1,
        "text": "A random instance will be terminated in AZ-B",
        "correct": false
      },
      {
        "id": 2,
        "text": "An instance in the AZ-A will be created",
        "correct": false
      },
      {
        "id": 3,
        "text": "The instance with the oldest launch template or launch configuration will be terminated in AZ-B",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because the default termination policy for an Auto Scaling Group prioritizes maintaining balance across Availability Zones. When scaling in, the ASG first looks for the AZ with the most instances. In this case, AZ-B has 4 instances while AZ-A has 3. Within AZ-B, the default termination policy then targets the instance with the oldest launch template or launch configuration. This ensures that the ASG attempts to remove instances created with older configurations first, promoting the use of the latest configurations.\n\n**Why option 0 is incorrect:**\nis incorrect because while the ASG aims to balance instances across AZs, it will first target the AZ with more instances. AZ-A has fewer instances than AZ-B, so it's less likely to be chosen for termination first.\n\n**Why option 1 is incorrect:**\nis incorrect because while a random instance *could* be terminated in AZ-B, the default termination policy is more deterministic. It prioritizes the AZ with the most instances and then uses other criteria (like oldest launch configuration) to select the instance to terminate. A random termination would not be the default behavior.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 35,
    "text": "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet. Which architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a Classic Load Balancer with an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Application Load Balancer with an Auto Scaling Group",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Network Load Balancer with an Auto Scaling Group",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an Auto Scaling Group with Dynamic Elastic IPs attachment",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing a Network Load Balancer (NLB) with an Auto Scaling Group, is the correct choice. NLBs provide static IP addresses per Availability Zone, which the bank can whitelist. NLBs are designed for high performance and can handle millions of requests per second. The Auto Scaling Group ensures high availability by automatically launching new instances if existing ones fail and scaling the number of instances based on demand, up to the required 10 instances. NLBs operate at Layer 4 (TCP/UDP), making them suitable for a wide range of applications and protocols.\n\n**Why option 0 is incorrect:**\nusing a Classic Load Balancer (CLB) with an Auto Scaling Group, is incorrect. CLBs do not provide static IP addresses per Availability Zone. While they can provide a single Elastic IP, this is not ideal for high availability and can become a single point of failure. Also, CLBs are considered legacy and are not recommended for new deployments.\n\n**Why option 1 is incorrect:**\nusing an Application Load Balancer (ALB) with an Auto Scaling Group, is incorrect. ALBs also do not provide static IP addresses per Availability Zone. They provide a DNS name that resolves to multiple IP addresses, which can change. This makes it difficult for the bank to whitelist a specific IP address. ALBs operate at Layer 7 (HTTP/HTTPS) and are designed for web applications, which is not necessarily the requirement in this scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use a user policy to grant permission to users in its account as well as to users in another account",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use permissions boundary to grant permission to users in its account as well as to users in another account",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a bucket policy to grant permission to users in its account as well as to users in another account",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing a bucket policy, is the correct answer. Bucket policies are resource-based policies that are attached directly to the S3 bucket. They allow you to specify who (principals) has access to the bucket and what actions they can perform. Bucket policies can grant permissions to IAM users and roles within the same AWS account, as well as to IAM users and roles in *other* AWS accounts. This makes them ideal for cross-account access scenarios. The policy would specify the ARN of the IAM user or role in the other account as the principal.\n\n**Why option 0 is incorrect:**\nusing a user policy, is incorrect. User policies are attached to IAM users or roles. While a user policy can grant a user access to an S3 bucket, it cannot directly grant access to users in *another* AWS account. The user in the other account would still need appropriate permissions within their own account to assume a role or otherwise access the bucket. While a user policy *could* allow the user to assume a role in the target account that has access to the S3 bucket, this is a more complex approach than directly granting access via the bucket policy.\n\n**Why option 1 is incorrect:**\nstating that either a bucket policy or a user policy can be used, is partially correct but ultimately misleading. While bucket policies *can* be used, user policies cannot directly grant access to users in another account as explained above. The option is therefore incorrect because it implies user policies are a direct alternative for granting cross-account access, which they are not.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "You have an Amazon S3 bucket that contains files in two different folders -s3://my-bucket/imagesands3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ). How can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it addresses the final archival requirement for both images and thumbnails after 180 days. Amazon S3 Glacier is a low-cost storage option suitable for archiving data that is infrequently accessed. Option 4 is correct because it transitions the image files (using a prefix to differentiate them from thumbnails) to Amazon S3 Standard IA after 45 days. Standard IA is cheaper than Standard for infrequently accessed data and still provides high availability. Using a prefix ensures that only the image files are transitioned, as required.\n\n**Why option 4 is correct:**\nThis is correct because it addresses the final archival requirement for both images and thumbnails after 180 days. Amazon S3 Glacier is a low-cost storage option suitable for archiving data that is infrequently accessed. Option 4 is correct because it transitions the image files (using a prefix to differentiate them from thumbnails) to Amazon S3 Standard IA after 45 days. Standard IA is cheaper than Standard for infrequently accessed data and still provides high availability. Using a prefix ensures that only the image files are transitioned, as required.\n\n**Why option 0 is incorrect:**\nis incorrect because it transitions *all* objects to Standard IA after 45 days. This is not cost-effective for the thumbnails, which are accessed more frequently than the images after 45 days. Thumbnails should remain in Standard storage for faster access.\n\n**Why option 1 is incorrect:**\nis incorrect because it transitions objects to Amazon S3 One Zone IA. One Zone IA stores data in a single Availability Zone, which violates the requirement for high availability and resilience against AZ failures.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database. You would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Lambda in a VPC",
        "correct": false
      },
      {
        "id": 1,
        "text": "Attach an AWS Identity and Access Management (IAM) role to AWS Lambda",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL",
        "correct": true
      },
      {
        "id": 3,
        "text": "Restrict the Amazon RDS database security group to the AWS Lambda's security group",
        "correct": false
      },
      {
        "id": 4,
        "text": "Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because attaching an IAM role to the Lambda function allows the function to assume temporary credentials. These credentials are automatically managed by AWS and rotated regularly, eliminating the need to store or manage long-term secrets within the Lambda function. Option 2 is correct because IAM authentication for RDS allows the Lambda function to authenticate directly with the RDS instance using the IAM role attached to the Lambda function. This eliminates the need for storing database credentials within the Lambda function code or environment variables. RDS will validate the IAM role's permissions to access the database.\n\n**Why option 2 is correct:**\nThis is correct because attaching an IAM role to the Lambda function allows the function to assume temporary credentials. These credentials are automatically managed by AWS and rotated regularly, eliminating the need to store or manage long-term secrets within the Lambda function. Option 2 is correct because IAM authentication for RDS allows the Lambda function to authenticate directly with the RDS instance using the IAM role attached to the Lambda function. This eliminates the need for storing database credentials within the Lambda function code or environment variables. RDS will validate the IAM role's permissions to access the database.\n\n**Why option 0 is incorrect:**\nis incorrect because deploying Lambda in a VPC is important for network isolation and connectivity to resources within the VPC, such as the RDS database. However, it doesn't directly address the problem of securing database credentials. While a VPC is often a prerequisite for accessing RDS, it doesn't inherently provide short-lived credentials or IAM-based authentication.\n\n**Why option 3 is incorrect:**\nis incorrect because restricting the RDS security group to the Lambda's security group is a good security practice for network access control, but it doesn't address the authentication mechanism. It only controls which resources can connect to the RDS instance, not how they authenticate. Even with a restricted security group, the Lambda function would still need to authenticate using some form of credentials.\n\n**Why option 4 is incorrect:**\nis incorrect because while credential rotation is a good practice, embedding the logic within the Lambda function and retrieving credentials from SSM adds unnecessary complexity and potential vulnerabilities. IAM roles and IAM authentication for RDS provide a more secure and managed solution for short-lived credentials.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A ride-hailing startup has launched a mobile app that matches passengers with nearby drivers based on real-time GPS coordinates. The application backend uses an Amazon RDS for PostgreSQL instance with read replicas to store the latitude and longitude of drivers and passengers. As the service scales, the backend experiences performance bottlenecks during peak hours, especially when thousands of updates and reads occur per second to keep location data current. The company expects its user base to double in the next few months and needs a high-performance, scalable solution that can handle frequent write and read operations with minimal latency. What do you recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards",
        "correct": false
      },
      {
        "id": 2,
        "text": "Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nis the correct answer. Placing an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database provides a fast, in-memory data store that can handle frequent read and write operations with minimal latency. By caching recent location reads and updates in Redis, the application can reduce the load on the PostgreSQL database, improving performance and scalability. The TTL-based eviction strategy ensures that the cached data remains relatively fresh and prevents the cache from growing indefinitely. Redis is well-suited for this use case because it is designed for high-throughput, low-latency operations, making it an ideal caching layer for real-time data.\n\n**Why option 0 is incorrect:**\nis incorrect because while adding read replicas can help with read scalability, it doesn't address the write performance bottleneck. The primary RDS instance still needs to handle all write operations, and creating and managing read replica Auto Scaling policies can add complexity. RDS proxy helps with connection management, but it doesn't inherently solve the performance issue of high write volume to the primary database.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon OpenSearch Service is suitable for geospatial indexing and searching, migrating all location data to OpenSearch might be overkill for this scenario. OpenSearch is better suited for complex search queries and analytics, rather than simple, frequent read and write operations of real-time location data. Also, migrating the entire dataset and changing the application architecture would be a more complex and time-consuming solution compared to implementing a caching layer. The visualization aspect using OpenSearch Dashboards is not a primary requirement mentioned in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website. What could be the reason the instances are being marked as unhealthy? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "You need to attach elastic IP address (EIP) to the Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "The route for the health check is misconfigured",
        "correct": true
      },
      {
        "id": 2,
        "text": "Your web-app has a runtime that is not supported by the Application Load Balancer",
        "correct": false
      },
      {
        "id": 3,
        "text": "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted",
        "correct": false
      },
      {
        "id": 4,
        "text": "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nThe route for the health check is misconfigured) is correct because the ELB performs health checks on a specific path or port. If this path is incorrect or the application is not responding correctly on that path, the ELB will mark the instance as unhealthy, even if the main website is accessible. For example, the health check might be configured to check `/health` but the application only responds to `/`. Option 4 (The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer) is also correct. The ELB needs to be able to communicate with the EC2 instances on the health check port. If the EC2 instance's security group doesn't allow inbound traffic from the ELB's security group (or the ELB's IP addresses, though using security groups is the best practice), the health checks will fail, and the instances will be marked unhealthy.\n\n**Why option 4 is correct:**\nThe route for the health check is misconfigured) is correct because the ELB performs health checks on a specific path or port. If this path is incorrect or the application is not responding correctly on that path, the ELB will mark the instance as unhealthy, even if the main website is accessible. For example, the health check might be configured to check `/health` but the application only responds to `/`. Option 4 (The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer) is also correct. The ELB needs to be able to communicate with the EC2 instances on the health check port. If the EC2 instance's security group doesn't allow inbound traffic from the ELB's security group (or the ELB's IP addresses, though using security groups is the best practice), the health checks will fail, and the instances will be marked unhealthy.\n\n**Why option 0 is incorrect:**\nYou need to attach elastic IP address (EIP) to the Amazon EC2 instances) is incorrect. EIPs are not required for EC2 instances behind an ELB. The ELB manages the traffic distribution, and the EC2 instances can have private IP addresses. The ELB uses the private IP addresses of the instances in the target group.\n\n**Why option 2 is incorrect:**\nYour web-app has a runtime that is not supported by the Application Load Balancer) is incorrect. The Application Load Balancer (ALB) operates at Layer 7 (HTTP/HTTPS) and is agnostic to the runtime of the web application as long as the application responds to HTTP/HTTPS requests. If the runtime was truly incompatible, the website wouldn't be accessible directly either.\n\n**Why option 3 is incorrect:**\nThe Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted) is incorrect. While improperly mounted EBS volumes can cause application issues, they wouldn't directly cause the ELB health checks to fail if the application is still responding to HTTP/HTTPS requests on the configured health check path. The application might be malfunctioning, but the ELB health check would still succeed if it receives a healthy response.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A digital media platform is preparing to launch a new interactive content service that is expected to receive sudden spikes in user engagement, especially during live events and media releases. The backend uses an Amazon Aurora PostgreSQL Serverless v2 cluster to handle dynamic workloads. The architecture must be capable of scaling both compute and storage performance to maintain low latency and avoid bottlenecks under load. The engineering team is evaluating storage configuration options and wants a solution that will scale automatically with traffic, optimize I/O performance, and remain cost-effective without manual provisioning or tuning. Which configuration will best meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Auroras autoscaling to handle demand spikes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nconfiguring the Aurora cluster to use Aurora I/O-Optimized storage, is the best solution. Aurora I/O-Optimized is specifically designed for I/O-intensive applications, offering high throughput and low-latency I/O performance. It provides predictable pricing, eliminating I/O-based charges, which aligns with the requirement for cost-effectiveness. This storage type automatically scales with the workload, meeting the need for automatic scaling without manual provisioning or tuning. It is optimized for workloads that require high I/O performance, making it suitable for the described scenario.\n\n**Why option 0 is incorrect:**\nis incorrect because while Provisioned IOPS (io1) allows for specifying the number of IOPS, it requires manual adjustment based on expected traffic. This contradicts the requirement for automatic scaling and minimal manual intervention. Also, io1 is generally more expensive than Aurora I/O-Optimized for I/O-intensive workloads in Aurora.\n\n**Why option 1 is incorrect:**\nis incorrect because Magnetic (Standard) storage is the slowest and least performant storage option. It is not suitable for workloads that require low latency and high throughput, especially during traffic spikes. Relying solely on Aurora's autoscaling with Magnetic storage will likely lead to performance bottlenecks and a poor user experience. While it minimizes baseline storage costs, the performance trade-off is unacceptable given the application's requirements.\n\n**Why option 2 is incorrect:**\nis incorrect because while General Purpose SSD (gp2) is a reasonable storage option, it doesn't provide the same level of I/O optimization as Aurora I/O-Optimized. Scaling database compute capacity can help to some extent, but it's not the most efficient way to address IOPS bottlenecks. Aurora I/O-Optimized is designed to handle I/O-intensive workloads more effectively and cost-efficiently than relying solely on scaling compute capacity with gp2 storage.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic. Which technology allows you to get a managed message broker that supports the MQTT protocol?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Kinesis Data Streams",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon MQ",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes it easy to set up and operate message brokers in the cloud. It supports industry-standard protocols, including MQTT, AMQP, STOMP, OpenWire, and JMS. By using Amazon MQ, the company can migrate their existing message broker to AWS without needing to rewrite their application logic that relies on the MQTT protocol. Amazon MQ provides a managed service, reducing the operational overhead of managing the message broker infrastructure.\n\n**Why option 0 is incorrect:**\nAmazon SNS (Simple Notification Service) is a fully managed messaging service for application-to-application (A2A) and application-to-person (A2P) communication. While it is a messaging service, it primarily focuses on push notifications and doesn't directly support the MQTT protocol. It's more suitable for fan-out scenarios rather than acting as a general-purpose message broker.\n\n**Why option 1 is incorrect:**\nAmazon SQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS uses a pull-based model and does not directly support the MQTT protocol. It's designed for asynchronous message queuing, not real-time message brokering with protocol compatibility like MQTT.\n\n**Why option 2 is incorrect:**\nAmazon Kinesis Data Streams is a massively scalable and durable real-time data streaming service. It is used for collecting, processing, and analyzing streaming data. It is not a message broker and does not support the MQTT protocol. Kinesis is designed for high-throughput data ingestion and processing, not general-purpose message brokering.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 43,
    "text": "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes. What would you recommend as an AWS Certified Solutions Architect - Associate?",
    "options": [
      {
        "id": 0,
        "text": "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time",
        "correct": false
      },
      {
        "id": 1,
        "text": "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages the Kinesis suite of services effectively. Kinesis Data Streams is used to ingest the high-velocity data from the website, providing decoupling from the website and allowing for scalable ingestion. Kinesis Data Analytics is then used to perform real-time analysis on the data stream, generating the product recommendations. Finally, Kinesis Data Firehose is used to persist the analyzed data to S3 for long-term storage and potential batch analytics. This architecture addresses all the requirements: decoupling (Kinesis Streams), scalability (Kinesis Streams and Analytics), real-time analysis (Kinesis Analytics), and no data loss (Kinesis Streams' ordered and durable data ingestion and Firehose's data persistence).\n\n**Why option 0 is incorrect:**\nis incorrect because while it uses Kinesis Data Streams and Firehose for data ingestion and persistence, it uses Athena for real-time analysis. Athena is a query service for data in S3 and is not designed for real-time, continuous analysis of streaming data. Athena is more suitable for ad-hoc queries and batch processing.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon QuickSight is a business intelligence service for visualizing data. While it can connect to data sources and provide insights, it's not designed for real-time, continuous analysis of streaming data like Kinesis Data Analytics. QuickSight is better suited for creating dashboards and reports from existing data sources.\n\n**Why option 3 is incorrect:**\nis incorrect because while SQS can decouple the website from the analytics processing, it's not the best choice for real-time analytics. SQS is a message queuing service, and while EC2 instances can process messages from the queue, performing real-time analytics using a third-party library on EC2 instances is complex to manage, scale, and maintain compared to using a managed service like Kinesis Data Analytics. Also, it does not guarantee the same level of data durability as Kinesis Data Streams.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 44,
    "text": "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource. Which feature of Amazon Route 53 can help achieve this functionality?",
    "options": [
      {
        "id": 0,
        "text": "Latency-based routing",
        "correct": false
      },
      {
        "id": 1,
        "text": "Geolocation routing",
        "correct": false
      },
      {
        "id": 2,
        "text": "Geoproximity routing",
        "correct": true
      },
      {
        "id": 3,
        "text": "Weighted routing",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nGeoproximity routing allows you to route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic to resources that are closer to your users. Crucially, it allows you to specify a bias, which effectively expands or contracts the geographic region associated with a resource. This directly addresses the requirement of dynamically altering the size of a geographic area.\n\n**Why option 0 is incorrect:**\nLatency-based routing routes traffic to the resource that provides the lowest latency for the user. While it considers geographic location indirectly through latency, it doesn't allow you to explicitly define or dynamically alter the size of a geographic area. It's based on performance, not geographic boundaries.\n\n**Why option 1 is incorrect:**\nGeolocation routing routes traffic based on the geographic location from which the DNS query originates. While it allows you to route traffic to different resources based on country or continent, it doesn't provide the capability to dynamically alter the *size* of the geographic area. It's based on fixed geographic regions, not adjustable boundaries.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A financial services company is implementing two separate data retention policies to comply with regulatory standards: Policy A: Critical transaction records must be immediately available for audit and must not be deleted or overwritten for 7 years. Policy B: Archived compliance data must be stored in a low-cost, long-term storage solution and locked from deletion or modification for at least 10 years. As a solutions architect, which combination of AWS features should you recommend to enforce these policies effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the correct answer. Amazon S3 Object Lock in Compliance mode is ideal for Policy A because it ensures that objects cannot be deleted or overwritten for a specified retention period, meeting the 7-year requirement. Compliance mode is stricter than Governance mode and cannot be overridden by any user, including the root user, ensuring immutability. S3 Glacier Vault Lock is the perfect solution for Policy B. It allows you to lock a vault policy, preventing any modifications to the policy itself, which ensures that the data stored in the vault cannot be deleted or modified for the specified retention period (10 years). S3 Glacier is also a low-cost, long-term storage solution, fulfilling the cost-effectiveness requirement.\n\n**Why option 0 is incorrect:**\nis incorrect because while S3 Standard with Lifecycle policies can handle the 7-year retention for Policy A, it doesn't inherently provide immutability. Lifecycle policies primarily manage object transitions between storage classes and eventual deletion, but they don't prevent accidental or malicious deletion before the lifecycle rule is triggered. Also, while S3 Glacier Flexible Retrieval is suitable for long-term storage, it doesn't inherently provide the immutability required for Policy B without Vault Lock.\n\n**Why option 2 is incorrect:**\nis incorrect because S3 Object Lock in Governance mode allows users with specific IAM permissions to override the retention settings. This contradicts the requirement that data cannot be deleted prematurely. Governance mode is less strict than Compliance mode and doesn't guarantee immutability.\n\n**Why option 3 is incorrect:**\nis incorrect because using S3 Glacier Vault Lock for both policies would be inefficient and costly for Policy A. Policy A requires immediate availability, which S3 Glacier doesn't provide. S3 Glacier is designed for infrequent access and has retrieval times that are not suitable for audit purposes requiring immediate access. Also, it's not cost-effective to store frequently accessed data in Glacier.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC. As a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables",
        "correct": false
      },
      {
        "id": 1,
        "text": "Contact AWS support to map your VPC with subnet",
        "correct": false
      },
      {
        "id": 2,
        "text": "Check if the security groups allow ping from the source",
        "correct": true
      },
      {
        "id": 3,
        "text": "Disable Source / Destination check on the Amazon EC2 instance",
        "correct": false
      },
      {
        "id": 4,
        "text": "Check if the route table is configured with internet gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because security groups act as virtual firewalls for EC2 instances. If the security group associated with the EC2 instance does not allow inbound ICMP traffic (used by ping), the ping requests will be blocked. Checking and modifying the security group to allow ICMP traffic from the source (e.g., your local machine's IP address or a wider range like 0.0.0.0/0 for testing purposes) is a crucial troubleshooting step.\n\nOption 4 is correct because the route table associated with the public subnet must have a route that directs traffic destined for the internet (0.0.0.0/0) to the Internet Gateway. Without this route, traffic from the EC2 instance cannot reach the internet, and responses to ping requests cannot be routed back to the instance. Verifying that the route table is correctly configured with the Internet Gateway as the target for internet-bound traffic is essential.\n\n**Why option 4 is correct:**\nThis is correct because security groups act as virtual firewalls for EC2 instances. If the security group associated with the EC2 instance does not allow inbound ICMP traffic (used by ping), the ping requests will be blocked. Checking and modifying the security group to allow ICMP traffic from the source (e.g., your local machine's IP address or a wider range like 0.0.0.0/0 for testing purposes) is a crucial troubleshooting step.\n\nOption 4 is correct because the route table associated with the public subnet must have a route that directs traffic destined for the internet (0.0.0.0/0) to the Internet Gateway. Without this route, traffic from the EC2 instance cannot reach the internet, and responses to ping requests cannot be routed back to the instance. Verifying that the route table is correctly configured with the Internet Gateway as the target for internet-bound traffic is essential.\n\n**Why option 0 is incorrect:**\nis incorrect because creating a secondary Internet Gateway and moving the existing one to the private subnet is not a standard or necessary solution for this problem. A single Internet Gateway is sufficient for a VPC to communicate with the internet. Creating multiple Internet Gateways would add unnecessary complexity and cost. The issue is more likely related to misconfigured security groups or route tables, not the number of Internet Gateways.\n\n**Why option 1 is incorrect:**\nis incorrect because AWS support does not handle VPC subnet mapping. VPC and subnet configurations are the responsibility of the user. Contacting AWS support for this issue would be inappropriate and would not resolve the problem. The user needs to troubleshoot and configure the VPC, subnets, route tables, and security groups themselves.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high. How can you easily reduce the costs while improving performance, with minimal changes?",
    "options": [
      {
        "id": 0,
        "text": "Enable Amazon API Gateway Caching",
        "correct": true
      },
      {
        "id": 1,
        "text": "Switch to using an Application Load Balancer",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add Amazon Aurora Read Replicas",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Lambda In Memory Caching",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nEnabling Amazon API Gateway caching is the most effective solution. API Gateway caching allows you to store the API responses for a specified time-to-live (TTL). Since the data rarely changes and serving outdated data for up to 24 hours is acceptable, caching API responses significantly reduces the load on the Aurora database. This directly translates to lower database costs and improved performance for users as they receive cached responses instead of hitting the database for every request. It requires minimal changes to the existing architecture, primarily configuration within API Gateway.\n\n**Why option 1 is incorrect:**\nSwitching to an Application Load Balancer (ALB) doesn't directly address the database load issue. An ALB primarily distributes traffic across multiple compute instances (like EC2 or containers). While it can improve application availability and scalability, it doesn't reduce the number of requests hitting the Aurora database. It also requires significant architectural changes.\n\n**Why option 2 is incorrect:**\nAdding Amazon Aurora Read Replicas can help distribute the read load across multiple database instances, potentially improving read performance. However, it doesn't directly reduce the overall number of read operations and therefore doesn't significantly reduce costs. Read replicas also introduce additional management overhead and costs associated with the replicated instances. The question emphasizes minimizing changes, and adding read replicas is a more significant change than enabling API Gateway caching.\n\n**Why option 3 is incorrect:**\nEnabling AWS Lambda In-Memory caching (using Lambda's execution environment) can improve performance for subsequent invocations of the same Lambda function, but it's not a reliable solution for caching data across multiple requests or for longer durations. Lambda functions can be invoked on different containers, and the in-memory cache is not shared across these containers. Also, Lambda functions can be scaled out, and the cache is not shared across these instances. This approach is not suitable for caching data for up to 24 hours and won't significantly reduce the load on the Aurora database.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy. As a Solutions Architect, which of the following would you identify as the correct description for the given policy?",
    "options": [
      {
        "id": 0,
        "text": "It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket",
        "correct": false
      },
      {
        "id": 1,
        "text": "It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket",
        "correct": true
      },
      {
        "id": 2,
        "text": "It ensures Amazon EC2 instances that have inherited a security group can access the bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because a bucket policy can use the `NotIpAddress` condition to explicitly deny access from a specific IP address within a larger CIDR block that's otherwise allowed by an `IpAddress` condition. This allows for fine-grained control over access based on IP addresses. The policy would first allow access from the entire CIDR range and then specifically deny access from a single IP within that range. This creates an exception to the broader CIDR-based access.\n\n**Why option 0 is incorrect:**\nis incorrect because it doesn't account for the 'NotIpAddress' condition, which is crucial for understanding the policy's effect. The policy doesn't simply authorize an IP address and a CIDR; it likely authorizes a CIDR *except* for a specific IP address.\n\n**Why option 2 is incorrect:**\nis incorrect because the question focuses on IP address restrictions, not security group inheritance for EC2 instances. While EC2 instances can access S3, the policy described is specifically about IP-based access control, not EC2 instance roles or security groups.\n\n**Why option 3 is incorrect:**\nis incorrect because S3 buckets do not expose external IPs. S3 is a service accessed via DNS names, not directly through IP addresses. The policy controls which IP addresses can access the bucket, not the other way around.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection. What do you recommend? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Egress Only Internet Gateway as a backup connection",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Site-to-Site VPN as a backup connection",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Direct Connect connection as a primary connection",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Site-to-Site VPN as a primary connection",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Direct Connect connection as a backup connection",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2
    ],
    "explanation": "**Why option 1 is correct:**\nUse AWS Site-to-Site VPN as a backup connection) is correct because AWS Site-to-Site VPN provides an encrypted connection over the public internet. It can be configured as a backup connection to Direct Connect. Option 2 (Use AWS Direct Connect connection as a primary connection) is correct because AWS Direct Connect provides a dedicated, private network connection between the on-premises data center and AWS. This fulfills the requirement for a dedicated private connection.\n\n**Why option 2 is correct:**\nUse AWS Site-to-Site VPN as a backup connection) is correct because AWS Site-to-Site VPN provides an encrypted connection over the public internet. It can be configured as a backup connection to Direct Connect. Option 2 (Use AWS Direct Connect connection as a primary connection) is correct because AWS Direct Connect provides a dedicated, private network connection between the on-premises data center and AWS. This fulfills the requirement for a dedicated private connection.\n\n**Why option 0 is incorrect:**\nUse Egress Only Internet Gateway as a backup connection) is incorrect because an Egress Only Internet Gateway is used to allow instances in a private subnet to initiate outbound traffic to the internet, but prevents the internet from initiating a connection with the instances. It doesn't establish a connection between the on-premises data center and AWS.\n\n**Why option 3 is incorrect:**\nUse AWS Site-to-Site VPN as a primary connection) is incorrect because the question specifically states the need for a dedicated private connection as the primary connection, which Site-to-Site VPN does not provide. It uses the public internet.\n\n**Why option 4 is incorrect:**\nUse AWS Direct Connect connection as a backup connection) is incorrect because the question specifies Direct Connect as the primary connection and a backup connection in case of Direct Connect failure. Using Direct Connect as a backup doesn't meet the primary connection requirement.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "What does this AWS CloudFormation snippet do? (Select three)",
    "options": [
      {
        "id": 0,
        "text": "It lets traffic flow from one IP on port 22",
        "correct": true
      },
      {
        "id": 1,
        "text": "It configures a security group's outbound rules",
        "correct": false
      },
      {
        "id": 2,
        "text": "It configures a security group's inbound rules",
        "correct": true
      },
      {
        "id": 3,
        "text": "It configures the inbound rules of a network access control list (network ACL)",
        "correct": false
      },
      {
        "id": 4,
        "text": "It only allows the IP 0.0.0.0 to reach HTTP",
        "correct": false
      },
      {
        "id": 5,
        "text": "It allows any IP to pass through on the HTTP port",
        "correct": true
      },
      {
        "id": 6,
        "text": "It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      2,
      5
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0, 2, and 5 are correct. The CloudFormation snippet, if it defines an ingress rule for a security group allowing traffic from 0.0.0.0/0 on port 22, effectively allows traffic from any IP address on port 22 (SSH). This means it lets traffic flow from any IP on port 22. Option 2 is correct because the snippet configures the inbound rules of a security group. Option 5 is correct because 0.0.0.0/0 represents any IP address, so allowing traffic from 0.0.0.0/0 on port 80 (HTTP) allows any IP to pass through on the HTTP port.\n\n**Why option 2 is correct:**\nOptions 0, 2, and 5 are correct. The CloudFormation snippet, if it defines an ingress rule for a security group allowing traffic from 0.0.0.0/0 on port 22, effectively allows traffic from any IP address on port 22 (SSH). This means it lets traffic flow from any IP on port 22. Option 2 is correct because the snippet configures the inbound rules of a security group. Option 5 is correct because 0.0.0.0/0 represents any IP address, so allowing traffic from 0.0.0.0/0 on port 80 (HTTP) allows any IP to pass through on the HTTP port.\n\n**Why option 5 is correct:**\nOptions 0, 2, and 5 are correct. The CloudFormation snippet, if it defines an ingress rule for a security group allowing traffic from 0.0.0.0/0 on port 22, effectively allows traffic from any IP address on port 22 (SSH). This means it lets traffic flow from any IP on port 22. Option 2 is correct because the snippet configures the inbound rules of a security group. Option 5 is correct because 0.0.0.0/0 represents any IP address, so allowing traffic from 0.0.0.0/0 on port 80 (HTTP) allows any IP to pass through on the HTTP port.\n\n**Why option 1 is incorrect:**\nis incorrect because the snippet configures *inbound* rules, not outbound rules. Outbound rules control traffic leaving the resources associated with the security group, while inbound rules control traffic entering.\n\n**Why option 3 is incorrect:**\nis incorrect because the snippet configures a *security group*, not a Network Access Control List (NACL). Security groups operate at the instance level, while NACLs operate at the subnet level.\n\n**Why option 4 is incorrect:**\nis incorrect because it misinterprets the effect of allowing traffic from 0.0.0.0/0. 0.0.0.0/0 represents *any* IP address, not just the IP 0.0.0.0.\n\n**Why option 6 is incorrect:**\nis incorrect because it suggests a restriction based on a specific IP address (192.168.1.1), which is not implied by allowing traffic from 0.0.0.0/0. Allowing traffic from 0.0.0.0/0 means allowing traffic from *any* IP address.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "An e-commerce analytics company is preparing to archive several years of transaction records and customer analytics reports in Amazon S3 for long-term storage. To meet compliance requirements, the archived data must be encrypted at rest. Additionally, the solution must be cost-effective and ensure that key rotation occurs automatically every 12 months to comply with the companys internal data governance policy. Which solution will meet these requirements with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 buckets default encryption to use the customer managed key. Migrate the data to the S3 bucket",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow",
        "correct": false
      },
      {
        "id": 2,
        "text": "Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages AWS KMS with a customer-managed key (CMK) and automatic key rotation. Using a CMK allows for control over the encryption keys and meets the compliance requirement of encryption at rest. Enabling automatic key rotation in KMS ensures that the keys are rotated every 12 months as required by the company's internal data governance policy. Configuring the S3 bucket's default encryption to use the CMK simplifies the encryption process for all objects stored in the bucket. This solution provides a balance between security, compliance, cost-effectiveness, and minimal operational overhead, as KMS handles the key management and rotation automatically.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS CloudHSM for key generation and client-side encryption introduces significant operational overhead. CloudHSM requires managing the HSM cluster, which involves tasks like patching, scaling, and ensuring high availability. Client-side encryption also adds complexity to the application and requires managing the encryption process within the application code. Rotating keys annually using an on-premises key management workflow would be cumbersome and error-prone. This option is not cost-effective or operationally efficient compared to using KMS.\n\n**Why option 2 is incorrect:**\nis incorrect because importing key material into KMS and then rotating it is more complex than using KMS-generated keys with automatic rotation. While it does address the encryption and rotation requirements, the initial step of encrypting data locally adds unnecessary complexity and overhead. Also, managing the initial encryption process outside of AWS services increases the risk of errors and inconsistencies. It's less operationally efficient than using KMS to generate and manage the keys directly.\n\n**Why option 3 is incorrect:**\nis incorrect because while SSE-S3 provides encryption at rest and is the simplest option, it does not allow for automatic key rotation that meets the specific 12-month requirement. S3 manages the keys, and the rotation schedule is not configurable. Therefore, it does not fulfill the compliance requirement of rotating keys every 12 months.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in theus-west-1Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in theus-east-1Region. The on-premises data center does not allow the use of AWS Snowball. As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console",
        "correct": false
      },
      {
        "id": 1,
        "text": "Copy data from the source bucket to the destination bucket using the aws S3 sync command",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Snowball Edge device to copy the data from one Region to another Region",
        "correct": false
      },
      {
        "id": 3,
        "text": "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration",
        "correct": true
      }
    ],
    "correctAnswers": [
      1,
      4
    ],
    "explanation": "**Why option 1 is correct:**\nCopy data from the source bucket to the destination bucket using the aws S3 sync command) is correct because the `aws s3 sync` command is a viable method for copying large amounts of data between S3 buckets, even across regions. It's a command-line tool that can handle large datasets and provides features like retries and parallel uploads, making it suitable for a 1 PB transfer. Option 4 (Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration) is also correct. S3 Batch Replication is designed for copying existing objects. While primarily intended for ongoing replication, it can be used for a one-time copy by setting it up, running the replication, and then deleting the configuration. This is a more managed approach than using `aws s3 sync` directly.\n\n**Why option 4 is correct:**\nCopy data from the source bucket to the destination bucket using the aws S3 sync command) is correct because the `aws s3 sync` command is a viable method for copying large amounts of data between S3 buckets, even across regions. It's a command-line tool that can handle large datasets and provides features like retries and parallel uploads, making it suitable for a 1 PB transfer. Option 4 (Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration) is also correct. S3 Batch Replication is designed for copying existing objects. While primarily intended for ongoing replication, it can be used for a one-time copy by setting it up, running the replication, and then deleting the configuration. This is a more managed approach than using `aws s3 sync` directly.\n\n**Why option 0 is incorrect:**\nSet up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console) is incorrect because S3 Transfer Acceleration is designed to accelerate uploads to S3, not S3 to S3 transfers. It optimizes the path from the client to the S3 bucket. While it could potentially improve the speed of the `aws s3 sync` command, it's not a direct solution for copying between buckets. Also, the S3 console is not the primary way to use S3TA for bucket-to-bucket copies.\n\n**Why option 2 is incorrect:**\nUse AWS Snowball Edge device to copy the data from one Region to another Region) is incorrect because the question explicitly states that the on-premises data center does not allow the use of AWS Snowball. This constraint eliminates Snowball as a viable option.\n\n**Why option 3 is incorrect:**\nCopy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console) is incorrect because the S3 console is not practical for transferring 1 PB of data. The console is suitable for small file transfers and management tasks, but it lacks the robustness and efficiency required for such a large dataset. It would be extremely slow and prone to errors.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options. Which of the following solutions can be used to improve the performance of the workload?",
    "options": [
      {
        "id": 0,
        "text": "Select an Elastic Inference accelerator while launching Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Select the appropriate capacity reservation while launching Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 2,
        "text": "Select dedicated instance tenancy while launching Amazon EC2 instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Select a cluster placement group while launching Amazon EC2 instances",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nselecting a cluster placement group, is the correct answer. Cluster placement groups are designed for applications that require low network latency, high network throughput, and tightly coupled node-to-node communication. They place instances close together within a single Availability Zone, enabling high-bandwidth, low-latency networking. This is ideal for HPC workloads where nodes need to communicate frequently and quickly.\n\n**Why option 0 is incorrect:**\nselecting an Elastic Inference accelerator, is incorrect. Elastic Inference is designed to accelerate deep learning inference workloads, not general HPC workloads. While it can improve performance for specific machine learning tasks, it doesn't address the core requirements of low-latency, high-throughput networking for tightly coupled node-to-node communication in a general HPC environment.\n\n**Why option 1 is incorrect:**\nselecting the appropriate capacity reservation, is incorrect. Capacity reservations ensure that you have the required EC2 instance capacity available when you need it. While important for availability and preventing capacity-related launch failures, capacity reservations do not directly improve network performance or address the specific requirements of low latency and high throughput for tightly coupled node-to-node communication. It only guarantees resource availability, not performance optimization.\n\n**Why option 2 is incorrect:**\nselecting dedicated instance tenancy, is incorrect. Dedicated instance tenancy ensures that your EC2 instances run on hardware dedicated to a single customer. While this can provide some isolation and potentially improve performance in certain scenarios, it doesn't directly address the need for low-latency, high-throughput networking for tightly coupled node-to-node communication. Dedicated instances primarily focus on security and compliance requirements rather than network optimization for HPC workloads.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 54,
    "text": "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance. Which of the following solutions will you recommend to address the given use-case?",
    "options": [
      {
        "id": 0,
        "text": "Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure",
        "correct": true
      },
      {
        "id": 1,
        "text": "Schedule manual backups using Redis append-only file (AOF)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure",
        "correct": false
      },
      {
        "id": 3,
        "text": "Schedule daily automatic backups at a time when you expect low resource utilization for your cluster",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nopting for a Multi-AZ configuration with automatic failover, is the best solution. Multi-AZ provides high availability by replicating the Redis cluster across multiple Availability Zones. In case of a failure in the primary AZ, ElastiCache automatically fails over to a standby replica in another AZ, minimizing downtime. This automatic failover ensures minimal interruption to the application. While some data loss is possible during the failover window, it is significantly less than other options. It also maintains good application performance as the failover is automatic and fast.\n\n**Why option 1 is incorrect:**\nscheduling manual backups using Redis AOF, is not ideal for disaster recovery with minimal downtime. Manual backups are time-consuming and require manual intervention to restore. The recovery time objective (RTO) would be high. While AOF provides persistence, relying solely on manual backups doesn't address the need for automatic failover and minimal downtime.\n\n**Why option 2 is incorrect:**\nadding read replicas across multiple Availability Zones (AZs), improves read performance and availability but doesn't provide automatic failover for the primary node. If the primary node fails, the application would still experience downtime while a new primary is promoted or created. Read replicas are primarily for scaling read operations, not for automatic disaster recovery.\n\n**Why option 3 is incorrect:**\nscheduling daily automatic backups, is a good practice for data durability but doesn't address the need for minimal downtime during a failure. Backups are useful for restoring data after a disaster, but they don't provide automatic failover or minimize the interruption to the application. The recovery process from a backup would take a significant amount of time, violating the minimal downtime requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub. As a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks",
        "correct": true
      },
      {
        "id": 2,
        "text": "Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks",
        "correct": false
      },
      {
        "id": 3,
        "text": "Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Transit Gateway, is the correct answer. AWS Transit Gateway simplifies network architecture by acting as a central hub for connecting multiple VPCs and on-premises networks. It reduces the complexity of managing multiple VPC peering connections or a Transit VPC solution. Transit Gateway offers features like route tables, route propagation, and security policies, enabling centralized control and visibility over network traffic. It significantly reduces operational overhead compared to other options, especially as the number of VPCs and on-premises connections grows. It avoids the need to manage routing appliances in a Transit VPC or the complexity of a full or partial mesh of VPC peering connections.\n\n**Why option 0 is incorrect:**\nusing a Transit VPC solution, is incorrect. While a Transit VPC can connect multiple VPCs and on-premises networks, it requires managing and maintaining virtual appliances (e.g., routers or firewalls) within the Transit VPC. This adds operational overhead, including patching, scaling, and troubleshooting the appliances. Transit VPC solutions are more complex to set up and manage compared to AWS Transit Gateway.\n\n**Why option 2 is incorrect:**\nusing a fully meshed VPC peering, is incorrect. A fully meshed VPC peering requires creating a direct peering connection between every pair of VPCs. This approach becomes unmanageable and complex as the number of VPCs increases. The number of peering connections grows quadratically (n*(n-1)/2), leading to significant operational overhead in managing routes and security groups. It doesn't directly address the on-premises connectivity requirement without additional solutions.\n\n**Why option 3 is incorrect:**\nusing a partially meshed VPC peering, is incorrect. A partially meshed VPC peering reduces the number of connections compared to a full mesh, but it still requires managing multiple peering connections and routing configurations. It doesn't provide a centralized hub for managing connectivity and lacks the scalability and manageability of AWS Transit Gateway. It also doesn't directly address the on-premises connectivity requirement without additional solutions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 56,
    "text": "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company. As a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowball",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use multi-part upload feature of Amazon S3",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Direct Connect to provide extra bandwidth",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Versioning",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe multi-part upload feature of Amazon S3 is designed specifically for uploading large files. It allows you to break down a single large object into smaller parts, which can be uploaded independently and in parallel. This offers several advantages:\n\n*   **Improved Reliability:** If one part fails to upload, only that part needs to be re-uploaded, rather than the entire file.\n*   **Faster Uploads:** Parallel uploads can significantly reduce the overall upload time, especially over networks with high latency or limited bandwidth.\n*   **Resumability:** If an upload is interrupted, you can resume it from where it left off, without losing progress.\n*   **Handles Large Files:** S3 Multi-part upload is designed to handle files larger than 5GB, up to 5TB.\n\n**Why option 0 is incorrect:**\nAWS Snowball is designed for transferring large amounts of data (terabytes to petabytes) when network connectivity is limited or unreliable. While it could be used, it's overkill for a single 1 TB file upload, especially if the partner company has a reasonable internet connection. Snowball involves shipping physical devices, which introduces significant delays and logistical complexity compared to a network-based solution like multi-part upload.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nAmazon S3 Versioning is a feature that keeps multiple versions of an object in the same bucket. It's useful for data protection and recovery, but it doesn't directly address the challenge of uploading large files efficiently. Versioning would be an additional feature to consider *after* addressing the upload mechanism, but it's not the primary solution for the problem presented.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 57,
    "text": "A media company operates a video rendering pipeline on Amazon EKS, where containerized jobs are scheduled using Kubernetes deployments. The application experiences bursty traffic patterns, particularly during peak streaming hours. The platform uses the Kubernetes Horizontal Pod Autoscaler (HPA) to scale pods based on CPU utilization. A solutions architect observes that the total number of EC2 worker nodes remains constant during traffic spikes, even when all nodes are at maximum resource utilization. The company needs a solution that enables automatic scaling of the underlying compute infrastructure when pod demand exceeds cluster capacity. Which solution should the architect implement to resolve this issue with the least operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because the Kubernetes Cluster Autoscaler is specifically designed to automatically scale the number of nodes in an EKS cluster based on the resource requirements of pending pods. It integrates with the EC2 Auto Scaling group, allowing it to launch new nodes when pods cannot be scheduled due to insufficient resources and terminate nodes when they are underutilized. This solution directly addresses the problem of the worker nodes not scaling and minimizes operational overhead by automating the scaling process.\n\n**Why option 0 is incorrect:**\nis incorrect because while AWS Fargate eliminates the need to manage EC2 instances, it might not be the most cost-effective or performant solution for all video rendering workloads. Migrating the entire pipeline to Fargate would involve significant architectural changes and potentially require refactoring the application. It also doesn't directly address the existing problem of the current EC2-based EKS cluster not scaling. Furthermore, Fargate might introduce limitations on resource configurations compared to EC2 instances.\n\n**Why option 2 is incorrect:**\nis incorrect because while using EC2 Auto Scaling with custom CloudWatch alarms is a valid approach for scaling EC2 instances, it's not the ideal solution for scaling EKS worker nodes based on pod demand. This approach requires manually configuring CloudWatch alarms and scaling policies based on cluster-wide CPU and memory usage, which can be complex and less responsive to individual pod resource requests. It also doesn't directly consider the Kubernetes scheduler's perspective on pod placement and resource availability. The Cluster Autoscaler is a more Kubernetes-native and automated solution.\n\n**Why option 3 is incorrect:**\nis incorrect because implementing an AWS Lambda function to manually trigger node scaling is a complex and operationally heavy solution. It requires writing and maintaining custom code, configuring IAM permissions, and setting up a scheduled event. This approach is also less responsive than the Cluster Autoscaler, as it only checks for unschedulable pods every 10 minutes. The manual intervention and delayed scaling make it a less desirable solution compared to the automated Cluster Autoscaler.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 58,
    "text": "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest. Which of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon RDS to use SSL for data in transit",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use IAM authentication to access the database instead of the database user's access credentials",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\n'Configure Amazon RDS to use SSL for data in transit,' is the correct answer. SSL (Secure Sockets Layer) or its successor TLS (Transport Layer Security) encrypts the communication channel between the client (EC2 instance) and the database server. This ensures that data transmitted over the network is protected from eavesdropping and tampering. RDS supports SSL/TLS encryption for data in transit, and enabling it is a standard practice for securing database connections. This directly addresses the requirement of securing data-in-transit.\n\n**Why option 1 is incorrect:**\n'Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database,' is incorrect. While blocking SSH access is generally a good security practice, it doesn't directly address the requirement of securing data-in-transit between the web application and the database. SSH is typically used for administrative access, not for the application's database connections. The application likely uses a different port (e.g., 5432 for PostgreSQL) for database communication. Blocking SSH would not encrypt the data being transferred by the application.\n\n**Why option 2 is incorrect:**\n'Use IAM authentication to access the database instead of the database user's access credentials,' is incorrect. IAM authentication enhances security by using IAM roles and policies to control database access. While it improves authentication and authorization, it doesn't inherently encrypt the data being transmitted between the application and the database. IAM authentication addresses *who* can access the database, not *how* the data is protected during transit. SSL/TLS is still needed to encrypt the data-in-transit even with IAM authentication.\n\n**Why option 3 is incorrect:**\n'Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database,' is incorrect. Similar to option 1, blocking SSH access is a good security practice, but it doesn't directly address the requirement of securing data-in-transit between the web application and the database. Security groups control network traffic based on IP addresses and ports. Blocking SSH would not encrypt the data being transferred by the application.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS. As a Solutions Architect, which of the following solutions will you suggest? (Select two)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EMR for serverless orchestration of the containerized services",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon SageMaker for serverless orchestration of the containerized services",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      2
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they leverage AWS Fargate for serverless container orchestration. Amazon EKS (Elastic Kubernetes Service) is a managed Kubernetes service that allows you to run Kubernetes without managing the control plane. When used with Fargate, you don't need to manage the worker nodes either. Amazon ECS (Elastic Container Service) is AWS's own container orchestration service. When used with Fargate, ECS also provides a serverless container execution environment. Both EKS and ECS with Fargate abstract away the underlying infrastructure management, allowing the company to focus on deploying and managing the containerized APIs.\n\n**Why option 2 is correct:**\nThese are correct because they leverage AWS Fargate for serverless container orchestration. Amazon EKS (Elastic Kubernetes Service) is a managed Kubernetes service that allows you to run Kubernetes without managing the control plane. When used with Fargate, you don't need to manage the worker nodes either. Amazon ECS (Elastic Container Service) is AWS's own container orchestration service. When used with Fargate, ECS also provides a serverless container execution environment. Both EKS and ECS with Fargate abstract away the underlying infrastructure management, allowing the company to focus on deploying and managing the containerized APIs.\n\n**Why option 0 is incorrect:**\nAmazon EMR (Elastic MapReduce) is designed for big data processing and analytics using frameworks like Hadoop and Spark. It is not designed for container orchestration or API hosting, and it does not offer a serverless orchestration option for containers.\n\n**Why option 3 is incorrect:**\nAmazon SageMaker is a machine learning service for building, training, and deploying machine learning models. It is not designed for general-purpose container orchestration or API hosting.\n\n**Why option 4 is incorrect:**\nWhile Amazon ECS can be used with EC2 instances, this option explicitly states that the company is looking for a *serverless* orchestration solution. Using EC2 instances requires managing the underlying infrastructure, which contradicts the serverless requirement.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 60,
    "text": "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often. As a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon CloudFront distribution in front of your website",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Auto Scaling Group",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Amazon Route 53 Multi Value record",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the website on Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing an Auto Scaling Group, is the correct answer. Auto Scaling Groups allow you to automatically adjust the number of EC2 instances based on demand. This is crucial for handling the anticipated 10x increase in traffic. By configuring scaling policies based on metrics like CPU utilization or network traffic, the Auto Scaling Group can dynamically add or remove instances to match the load, ensuring the website remains responsive and available during the flash sale. This directly addresses the requirement of scaling the infrastructure to handle the surge in traffic.\n\n**Why option 0 is incorrect:**\nusing an Amazon CloudFront distribution in front of your website, is incorrect because while CloudFront is excellent for caching static content and reducing latency, it's less effective for highly dynamic content that changes frequently. CloudFront can cache dynamic content, but the cache invalidation process might not be fast enough to keep up with the frequent changes, potentially serving stale content to users. While CloudFront can help with distribution and some load reduction, it doesn't address the core need for dynamic scaling of the underlying infrastructure to handle the increased processing demands of a dynamic website.\n\n**Why option 2 is incorrect:**\nusing an Amazon Route 53 Multi Value record, is incorrect. Route 53 Multi Value answer routing allows you to configure Route 53 to return multiple healthy records, which can help distribute traffic across multiple resources. However, it doesn't automatically scale the underlying infrastructure. It simply distributes traffic to existing resources. If those resources are overloaded, the website will still experience performance issues. It's a traffic distribution mechanism, not a scaling solution.\n\n**Why option 3 is incorrect:**\ndeploying the website on Amazon S3, is incorrect. Amazon S3 is designed for storing static content like images, videos, and HTML files. It is not suitable for hosting dynamic websites that require server-side processing or database interactions. While S3 can serve static parts of a website, the dynamic elements would still need to be hosted elsewhere, and S3 alone wouldn't address the scaling requirements for the dynamic parts of the application.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A streaming media company operates a high-traffic content delivery platform on AWS. The application backend is deployed on Amazon EC2 instances within an Auto Scaling group across multiple Availability Zones in a VPC. The team has observed that workloads follow predictable usage patterns, such as higher viewership on weekends and in the evenings, along with occasional real-time spikes due to viral content. To reduce cost and improve responsiveness, the team wants an automated scaling approach that can forecast future demand using historical usage patterns, scale in advance based on those predictions, and react quickly to unplanned usage surges in real time. Which scaling strategy should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it combines predictive scaling and target tracking scaling policies. Predictive scaling analyzes historical data to forecast future demand and proactively adjusts the Auto Scaling group's capacity. Target tracking scaling policies dynamically adjust the capacity based on a chosen metric (e.g., CPU utilization, network traffic) to maintain a target value. This combination allows the system to scale in advance for predictable patterns and react quickly to unexpected surges.\n\n**Why option 0 is incorrect:**\nis incorrect because step scaling policies based on CPU utilization only react to current load. While they can handle real-time spikes, they don't address the requirement of forecasting and scaling in advance for predictable patterns. Step scaling is reactive, not proactive.\n\n**Why option 2 is incorrect:**\nis incorrect because scheduled scaling actions only address predictable patterns. They don't react to real-time spikes and require manual adjustments, which contradicts the requirement for an automated scaling approach. Manual adjustments are also prone to human error and may not be as responsive as automated solutions.\n\n**Why option 3 is incorrect:**\nis incorrect because simple scaling policies with longer cooldown periods are not suitable for handling real-time spikes. The longer cooldown period will delay the scaling response, potentially leading to performance degradation during sudden traffic increases. Also, relying solely on network throughput might not be the most accurate indicator of application load.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 62,
    "text": "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication. As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
    "options": [
      {
        "id": 0,
        "text": "Use Elastic Load Balancing (ELB) for effective decoupling of system architecture",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge to decouple the system architecture",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon EventBridge is the best choice for decoupling the system architecture in this scenario. EventBridge is a serverless event bus that allows applications to publish and subscribe to events. It enables loosely coupled architectures where applications can react to events without needing to know the details of the event source or destination. EventBridge also supports filtering events based on content, routing events to different targets, and transforming events before they are delivered. EventBridge integrates well with various AWS services and third-party applications, making it suitable for both in-house and external integrations. The key benefit is that the SaaS application can emit events, and other applications can subscribe to these events without direct dependencies.\n\n**Why option 0 is incorrect:**\nElastic Load Balancing (ELB) is primarily used for distributing incoming traffic across multiple instances of an application. While it provides some level of decoupling by abstracting the underlying instances, it doesn't address the asynchronous communication requirement between different applications. ELB is more focused on load distribution and high availability for a single application, not inter-application communication.\n\n**Why option 1 is incorrect:**\nAmazon Simple Queue Service (SQS) can be used for asynchronous communication, but it's more suitable for point-to-point communication between applications. In this scenario, the SaaS application needs to send updates to multiple in-house and third-party applications. Using SQS would require the SaaS application to send a message to a separate queue for each application, which is less efficient and more complex than using EventBridge. SQS also lacks the event filtering and routing capabilities of EventBridge.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "A research organization is running a high-performance computing (HPC) workload using Amazon EC2 instances that are distributed across multiple Availability Zones (AZs) within a single AWS Region. The workload requires access to a shared file system with the lowest possible latency for frequent reads and writes.The team decides to use Amazon Elastic File System (Amazon EFS) for its scalability and simplicity. To ensure optimal performance and reduce network latency, the solution architect must design the architecture so that each EC2 instance can access the file system with the least possible delay. Which of the following is the most appropriate solution to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because creating EFS mount targets in each AZ and mounting the EFS file system to EC2 instances within the *same* AZ minimizes network latency. EFS is designed to be accessed from instances within the same AZ as its mount target. Accessing EFS across AZs introduces latency due to inter-AZ network traffic. By ensuring each EC2 instance accesses an EFS mount target within its own AZ, the solution minimizes the network distance and therefore the latency for file system operations. This approach leverages EFS's distributed architecture to provide optimal performance for the HPC workload.\n\n**Why option 0 is incorrect:**\nis incorrect because Mountpoint for Amazon S3 is designed for accessing S3 objects, not for providing a low-latency shared file system suitable for HPC workloads. S3 is object storage, not a file system, and Mountpoint for Amazon S3 introduces overhead that would significantly increase latency compared to EFS. Also, S3 is eventually consistent, which is not suitable for HPC workloads requiring strong consistency.\n\n**Why option 1 is incorrect:**\nis incorrect because creating a single EFS mount target in one AZ and allowing all EC2 instances in other AZs to access it introduces significant cross-AZ network latency. This defeats the purpose of distributing the EC2 instances across multiple AZs for high availability and performance. The cross-AZ traffic will negatively impact the overall performance of the HPC workload, violating the primary requirement of minimizing latency.\n\n**Why option 2 is incorrect:**\nis incorrect because using EC2 instances as access points for other instances adds an unnecessary layer of complexity and potential bottlenecks. It also introduces a single point of failure if the EC2 instance acting as the access point fails. EFS is designed to be directly accessed by EC2 instances, making this approach redundant and less efficient.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A global insurance company is modernizing its infrastructure by migrating multiple line-of-business applications from its on-premises data centers to AWS. These applications will be deployed across several AWS accounts, all governed under a centralized AWS Organizations structure. The company manages all user identities, groups, and access policies within its on-premises Microsoft Active Directory and wants to continue doing so. The goal is to enable seamless single sign-in across all AWS accounts without duplicating user identity stores or manually provisioning accounts. Which solution best meets these requirements in the most operationally efficient manner?",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages AWS IAM Identity Center (successor to AWS SSO) and AWS Directory Service for Microsoft Active Directory (Enterprise Edition) to achieve seamless SSO and centralized identity management. IAM Identity Center provides a central place to manage access to multiple AWS accounts. By integrating it with AWS Directory Service for Microsoft Active Directory (Enterprise Edition), which is a managed AD service, and establishing a two-way trust relationship with the on-premises AD, users can authenticate using their existing AD credentials. This eliminates the need to create separate IAM users in each AWS account or manage a separate identity store. The two-way trust allows users in the on-premises AD to access AWS resources and vice-versa, if needed. This approach is operationally efficient because it automates user provisioning and deprovisioning, and it centralizes access management.\n\n**Why option 0 is incorrect:**\nis incorrect because manually creating user accounts and groups within IAM Identity Center and managing synchronization with on-premises AD using custom PowerShell scripts is not operationally efficient. It requires significant manual effort and increases the risk of errors and inconsistencies. While IAM Identity Center is a good choice, the manual synchronization defeats the purpose of seamless integration with the existing AD.\n\n**Why option 1 is incorrect:**\nis incorrect because deploying an OpenLDAP server on Amazon EC2 and syncing it with the on-premises AD is a complex and less secure solution compared to using AWS Directory Service for Microsoft Active Directory (Enterprise Edition). It requires managing the EC2 instance, the OpenLDAP server, and the synchronization process. Furthermore, integrating it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider adds unnecessary complexity. AWS Directory Service for Microsoft Active Directory (Enterprise Edition) is a managed service that simplifies the integration with on-premises AD and provides a more secure and reliable solution. This option also introduces a single point of failure with the EC2 instance hosting OpenLDAP.\n\n**Why option 3 is incorrect:**\nis incorrect because while Amazon Cognito can be used for identity management, it's primarily designed for customer-facing applications and not for internal enterprise users managed in Active Directory. Creating a custom OpenID Connect (OIDC) federation with the on-premises Active Directory and using Cognito identity pools to assign IAM roles is more complex and less efficient than using AWS IAM Identity Center with AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Cognito is not the best fit for managing access to multiple AWS accounts for internal users.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 65,
    "text": "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem. What can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB Streams",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB DAX",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon DynamoDB Global Tables",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB. It sits in front of DynamoDB and caches frequently accessed items. This significantly reduces the load on DynamoDB, especially for hot partitions. DAX is designed to be transparent to the application, requiring minimal code changes. It is a fully managed, highly available, and scalable in-memory cache that can improve read performance by an order of magnitude. By caching the data of the popular athletes, DAX can effectively alleviate the hot partition problem without requiring significant application refactoring.\n\n**Why option 0 is incorrect:**\nAmazon DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. It is used for data replication, auditing, and triggering other actions based on data changes. It does not address the hot partition problem related to high read volume. Streams are useful for event-driven architectures but not for caching.\n\n**Why option 2 is incorrect:**\nAmazon DynamoDB Global Tables provide multi-region, active-active replication for DynamoDB tables. While this can improve availability and reduce latency for geographically distributed users, it does not directly address the hot partition problem within a single region. Replicating the entire table doesn't solve the problem of a single partition being overloaded with read requests. It's more about disaster recovery and global distribution, not caching.\n\n**Why option 3 is incorrect:**\nAmazon ElastiCache is a fully managed, in-memory data store and caching service. While ElastiCache *could* be used to cache data from DynamoDB, it would require significantly more application refactoring than DAX. The application would need to be modified to explicitly check ElastiCache before querying DynamoDB, and to update the cache when data changes. DAX is specifically designed to be a transparent cache for DynamoDB, minimizing code changes.",
    "domain": "Design High-Performing Architectures"
  }
]