[
  {
    "id": 1,
    "text": "A company runs an application on Amazon EC2 instances. The company needs to implement a \ndisaster recovery (DR) solution for the application. The DR solution needs to have a recovery \ntime objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible \nAWS resources during normal operations. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a",
        "correct": true
      },
      {
        "id": 2,
        "text": "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by creating AMIs as backups and copying them to a secondary region. During normal operations, no EC2 instances are running in the secondary region, minimizing resource usage and cost. In a disaster scenario, the AMIs can be used to quickly launch new EC2 instances in the secondary region. The RTO is met because launching instances from AMIs is relatively fast, and the AMIs are pre-copied to the secondary region, avoiding delays associated with data transfer during a disaster.\n\n**Why option 0 is incorrect:**\nWhile creating AMIs is a good starting point for disaster recovery, simply copying them to a different S3 bucket within the same region does not provide disaster recovery capabilities. If the entire AWS region becomes unavailable, the AMIs in the S3 bucket will also be unavailable, making it impossible to recover the application. This option fails to meet the basic requirement of having a DR solution in a separate geographical location.\n\n**Why option 2 is incorrect:**\nLaunching EC2 instances in a secondary region and keeping them running increases the cost significantly during normal operations. This contradicts the requirement of using the fewest possible AWS resources during normal operations. While this approach would likely meet the RTO requirement, it is not the most operationally efficient solution.\n\n**Why option 3 is incorrect:**\nLaunching EC2 instances in a secondary Availability Zone (AZ) does not provide a disaster recovery solution. Availability Zones are geographically close to each other within the same region. A regional disaster could affect multiple or all AZs within that region, rendering the DR solution ineffective. Disaster recovery requires replicating resources to a different AWS region.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company hosts a multiplayer gaming application on AWS. The company wants the application \nto read data with sub-millisecond latency and run one-time queries on historical data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement for sub-millisecond latency by utilizing DynamoDB, a NoSQL database designed for high-performance applications. DynamoDB Accelerator (DAX) further enhances read performance by providing an in-memory cache, significantly reducing latency for frequently accessed data. DynamoDB also supports querying historical data, although it might require techniques like Global Secondary Indexes or exporting data to other services for more complex analytical queries. The fully managed nature of DynamoDB and DAX minimizes operational overhead, as AWS handles tasks such as scaling, patching, and backups.\n\n**Why option 0 is incorrect:**\nWhile Amazon RDS can provide good performance, it's generally not optimized for sub-millisecond latency like DynamoDB with DAX. Exporting data using a custom script adds operational overhead and complexity, as it requires development, maintenance, and scheduling. It also doesn't address the need for low latency access to frequently used data.\n\n**Why option 1 is incorrect:**\nStoring data directly in Amazon S3 is suitable for large volumes of data and archival purposes, but it's not designed for sub-millisecond latency reads. S3 is an object storage service, not a database, and querying data directly in S3 typically involves using services like Athena or Redshift Spectrum, which are not optimized for the extremely low latency required by the application. While S3 Lifecycle policies help manage storage costs, they don't improve read performance.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "A company has a regional subscription-based streaming service that runs in a single AWS \nRegion. The architecture consists of web servers and application servers on Amazon EC2 \ninstances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The \narchitecture includes an Amazon Aurora database cluster that extends across multiple Availability \nZones. \n \nThe company wants to expand globally and to ensure that its application has minimal downtime.",
    "options": [
      {
        "id": 0,
        "text": "Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the web tier and the applicatin tier to a second Region. Create an Aurora PostSQL",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement of global expansion and minimal downtime by extending the Auto Scaling groups to deploy instances in multiple AWS Regions. By having instances in multiple regions, the application can continue to serve users even if one region experiences an outage. This approach also allows for serving users from the closest region, reducing latency and improving performance. This is a cost-effective way to achieve high availability and global reach without completely duplicating the entire infrastructure.\n\n**Why option 1 is incorrect:**\nThis option proposes deploying the web and application tiers to a second region and adding an Aurora PostgreSQL instance. While deploying the tiers to a second region contributes to global expansion, simply adding an Aurora PostgreSQL instance in the second region does not address the data replication and consistency challenges required for a globally distributed application with minimal downtime. It doesn't specify how data will be synchronized between the original Aurora cluster and the new instance, potentially leading to data inconsistencies and increased downtime during failover.\n\n**Why option 2 is incorrect:**\nThis option suggests deploying the web and application tiers to a second region and creating an Aurora PostgreSQL instance. Similar to option 1, this approach doesn't address the critical aspect of data replication and consistency between the two Aurora instances. Creating a separate Aurora PostgreSQL instance in the second region without a proper data synchronization mechanism would lead to data inconsistencies and potential downtime during failover or switching between regions. It also doesn't leverage Aurora's global database capabilities.\n\n**Why option 3 is incorrect:**\nThis option proposes deploying the web and application tiers to a second region and using an Amazon Aurora global database. While using an Aurora Global Database is a good approach for global expansion and disaster recovery, it's not the most appropriate solution for *minimal* downtime in all scenarios. Aurora Global Database provides fast cross-region disaster recovery, but it typically involves a short recovery time objective (RTO) which, while small, isn't zero. Also, Aurora Global Database is more expensive than simply extending the Auto Scaling groups to multiple regions. Extending the Auto Scaling groups is a more cost-effective and simpler solution for achieving high availability and global reach, especially if the primary goal is minimal downtime and not necessarily disaster recovery with a specific RTO.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. \nThe company does not want to use the default domain name for the distribution. Instead, the \ncompany wants to use a different domain name for the distribution. \n \nWhich solution will deploy the certificate with icurring any additional costs?",
    "options": [
      {
        "id": 0,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": true
      },
      {
        "id": 2,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-east-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements because ACM certificates issued by Amazon are free when used with AWS services like CloudFront. Furthermore, ACM certificates used with CloudFront must be requested in the us-east-1 region (North Virginia) regardless of where the CloudFront distribution itself is located. Requesting a public certificate fulfills the need for SSL/TLS encryption and allows the use of a custom domain name.\n\n**Why option 0 is incorrect:**\nPrivate certificates are typically used for internal resources and are not directly trusted by external clients accessing the CloudFront distribution. While you can import a private certificate into ACM and use it with CloudFront, it requires additional configuration and might not be the most straightforward approach. More importantly, using a private certificate in this context would likely involve additional steps and costs associated with establishing trust with external clients.\n\n**Why option 2 is incorrect:**\nWhile requesting a public certificate is correct, the region is incorrect. ACM certificates for CloudFront must be requested in us-east-1.\n\n**Why option 3 is incorrect:**\nWhile requesting a public certificate is correct, the region is incorrect. ACM certificates for CloudFront must be requested in us-east-1.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A solutions architect is designing a company’s disaster recovery (DR) architecture. The company \nhas a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled \nbackup. The DR design needs to include multiple AWS Regions. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the schedule backup of the MySQL database in an Amazon S3 bucket that is configured for",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by leveraging Amazon Aurora Global Database. Aurora Global Database is designed for disaster recovery, providing fast, cross-region replication with typical replication lag of less than one second. It allows for a read-only replica in a secondary region, which can be promoted to a read-write instance in case of a regional outage. This minimizes data loss and recovery time. Furthermore, Aurora handles the replication and failover process, significantly reducing the operational overhead compared to managing replication manually or with other solutions.\n\n**Why option 0 is incorrect:**\nMigrating the MySQL database to multiple EC2 instances and configuring a standby instance involves significant operational overhead. It requires setting up and managing replication between the instances, monitoring the health of each instance, and handling failover manually. This approach is complex and error-prone, making it less desirable than a managed solution.\n\n**Why option 1 is incorrect:**\nMigrating to Amazon RDS with a Multi-AZ deployment provides high availability within a single region, but it does not directly address the multi-region disaster recovery requirement. While Multi-AZ provides resilience against instance or availability zone failures, it does not protect against regional outages. Setting up cross-region replication with RDS requires additional configuration and management, increasing operational overhead compared to Aurora Global Database. Also, read replicas are not designed for DR failover and require promotion, which adds complexity.\n\n**Why option 3 is incorrect:**\nStoring scheduled backups in S3 configured for cross-region replication (CRR) addresses the data backup aspect of DR, but it doesn't provide a readily available, operational database in the secondary region. Restoring from backups can take a significant amount of time, resulting in a longer recovery time objective (RTO) and recovery point objective (RPO) compared to Aurora Global Database. It also requires additional steps to launch and configure a new database instance in the secondary region, increasing operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A rapidly growing global ecommerce company is hosting its web application on AWS. The web \napplication includes static content and dynamic content. The website stores online transaction \nprocessing (OLTP) data in an Amazon RDS database. The website's users are experiencing slow \npage loads. \n \nWhich combination of actions should a solutions architect take to resolve this issue? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Redshift cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon CloudFront distribution",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the dynamic web content in Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a read replica for the RDS DB instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure a Multi-AZ deployment for the RDS DB instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the slow page load issue by caching static content closer to the users. CloudFront is a content delivery network (CDN) that distributes content from origin servers to edge locations around the world. By caching static assets like images, CSS, and JavaScript files at edge locations, CloudFront reduces latency and improves page load times for users regardless of their geographic location. This is particularly effective for a global e-commerce company with users distributed worldwide.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon Redshift is a data warehouse service designed for analytical workloads (OLAP), not for improving the performance of a web application serving dynamic and static content. The question mentions OLTP data in RDS, which is a different use case than what Redshift is designed for. Introducing Redshift would not directly address the slow page load issue.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while S3 is suitable for hosting static content, hosting dynamic web content in S3 is not a standard practice. Dynamic content typically requires server-side processing and logic, which S3 does not provide. Moving dynamic content to S3 would likely break the application and not solve the slow page load issue. Furthermore, even if it were possible, it wouldn't address the global distribution aspect as effectively as a CDN.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while creating a read replica for the RDS database can help offload read traffic and potentially improve database performance, it primarily benefits the dynamic content generation aspect of the application. It doesn't directly address the delivery of static content, which is a significant contributor to page load times, especially for a global audience. The primary bottleneck described in the question is slow page loads, which suggests a need for content caching and distribution, not just database read scaling.\n\n**Why option 4 is incorrect:**\nThis is incorrect because configuring a Multi-AZ deployment for the RDS DB instance enhances the availability and durability of the database but does not directly address the slow page load issue. Multi-AZ primarily provides failover capabilities in case of an outage, improving the application's resilience but not its performance in terms of page load times. While important for production environments, it's not the most relevant solution for the specific problem described.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A solutions architect wants all new users to have specific complexity requirements and mandatory \nrotation periods tor IAM user passwords. \n \nWhat should the solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Set an overall password policy for the entire AWS account.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set a password policy for each IAM user in the AWS account.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use third-party vendor software to set password requirements.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct approach because IAM password policies are account-wide. Setting a password policy at the account level ensures that all IAM users created within that account adhere to the specified complexity requirements and rotation periods. This provides a centralized and consistent way to manage password security.\n\n**Why option 1 is incorrect:**\nThis is incorrect because IAM policies are set at the account level, not individually for each user. Setting password policies individually would be extremely cumbersome and difficult to manage, especially in environments with many users. It also doesn't align with AWS best practices for security management.\n\n**Why option 2 is incorrect:**\nThis is incorrect because using third-party vendor software is unnecessary. AWS provides native functionality through IAM password policies to meet the requirements. Introducing third-party software adds complexity, cost, and potential security risks without providing a significant benefit.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while CloudWatch Events can trigger actions based on AWS API calls, it's not the appropriate mechanism for setting password policies. CloudWatch Events would be overly complex and inefficient for this task. IAM password policies are the direct and intended way to manage password requirements.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company wants to deploy a new public web application on AWS. The application includes a \nweb server tier that uses Amazon EC2 instances. The application also includes a database tier \nthat uses an Amazon RDS for MySQL DB instance. \n \nThe application must be secure and accessible for global customers that have dynamic IP \naddresses. \n \nHow should a solutions architect configure the security groups to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group tor the web servers to allow inbound traffic on port 443 from",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the security group for the web servers to allow inbound traffic on port 443 from the IP",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the security group for the web servers to allow inbound traffic on port 443 from the IP",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the web servers to allow inbound traffic on port 443 from",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A company is planning to migrate a commercial off-the-shelf application from is on-premises data \ncenter to AWS. The software has a software licensing model using sockets and cores with \npredictable capacity and uptime requirements. The company wants to use its existing licenses, \nwhich were purchased earlier this year. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n229 \n \nWhich Amazon EC2 pricing option is the MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Dedicated Reserved Hosts",
        "correct": true
      },
      {
        "id": 1,
        "text": "Dedicated On-Demand Hosts",
        "correct": false
      },
      {
        "id": 2,
        "text": "Dedicated Reserved Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Dedicated On-Demand Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "An ecommerce company is experiencing an increase in user traffic. The company's store is \ndeployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a \nseparate database tier. As traffic increases, the company notices that the architecture is causing \nsignificant delays in sending timely marketing and order confirmation email to users. The \ncompany wants to reduce the time it spends resolving complex email delivery issues and \nminimize operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a separate application tier using EC2 instances dedicated to email processing.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the web instance to send email through Amazon Simple Notification Service (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a separate application tier using EC2 instances dedicated to email processing. Place the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon \nEC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database \ntier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier \nrequires access to the database to retrieve product information. \n \nThe web application is not working as intended. The web application reports that it cannot \nconnect to the database. The database is confirmed to be up and running. All configurations for \nthe network ACLs, security groups, and route tables are still in their default states. \n \nWhat should a solutions architect recommend to fix the application?",
    "options": [
      {
        "id": 0,
        "text": "Add an explicit rule to the private subnet's network ACL to allow traffic from the web tier's EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a route in the VPC route table to allow traffic between the web tier’s EC2 instances and the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the web tier's EC2 instances and the database tier's RDS instance into two separate",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an inbound rule to the security group of the database tier's RDS instance to allow traffic from",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company is running a multi-tier ecommerce web application in the AWS Cloud. The application \nruns on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon \nRDS is configured with the latest generation DB instance with 2,000 GB of storage in a General \nPurpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database \nperformance affects the application during periods of high demand. \n \nA database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the \napplication performance always degrades when the number of read and write IOPS is higher than \n20,000. \n \nWhat should a solutions architect do to improve the application performance?",
    "options": [
      {
        "id": 0,
        "text": "Replace the volume with a magnetic volume.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the number of IOPS on the gp3 volume.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the volume with a Provisioned IOPS SSD (Io2) volume.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 13,
    "text": "A company is deploying a new application on Amazon EC2 instances. The application writes data \nto Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all \ndata that is written to the EBS volumes is encrypted at rest. \n \nWhich solution will meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "An image-hosting company stores its objects in Amazon S3 buckets. The company wants to \navoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the \nentire AWS account need to remain private. \n \nWhich solution will meal these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Trusted Advisor to find publicly accessible S3 Dockets. Configure email notifications In",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 0 is incorrect:**\nis incorrect because while RDS for Oracle read replicas provide read scalability, they do not allow for customization of the underlying operating system or database environment. The question specifically states that the company requires customization capabilities.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A financial company hosts a web application on AWS. The application uses an Amazon API \nGateway Regional API endpoint to give users the ability to retrieve current stock prices. The \ncompany's security team has noticed an increase in the number of API requests. The security \nteam is concerned that HTTP flood attacks might take the application offline. \nA solutions architect must design a solution to protect the application from this type of attack. \n \nWhich solution meats these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 0 is incorrect:**\nis incorrect because granting maximum privileges violates the principle of least privilege. This can lead to accidental or malicious misuse of resources and increase the attack surface.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A payment processing company records all voice communication with its customers and stores \nthe audio files in an Amazon S3 bucket. The company needs to capture the text from the audio \nfiles. The company must remove from the text any personally identifiable information (Pll) that \nbelongs to customers. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function",
        "correct": false
      },
      {
        "id": 1,
        "text": "When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon Transcribe transcription job with Pll redaction turned on. When an audio file",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Connect contact flow that ingests the audio files with transcription turned on.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 18,
    "text": "A company is migrating its on-premises workload to the AWS Cloud. The company already uses \nseveral Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution \nthat automatically starts and stops the EC2 instances and D6 instances outside of business \nhours. The solution must minimize cost and infrastructure maintenance. \n \nWhich solution will meet these requirement?",
    "options": [
      {
        "id": 0,
        "text": "Scale the EC2 instances by using elastic resize Scale the DB instances to zero outside of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that will start and stop the EC2 instances and DB instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 19,
    "text": "A company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The \ninstances run in an Auto Scaling group behind an Application Load Balancer (ALB). All \necommerce data is stored in an Amazon RDS for ManaDB Multi-AZ DB instance. The company \nwants to optimize customer session management during transactions. The application must store \nsession data durably. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Turn on the sticky sessions feature (session affinity) on the ALB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an Amazon DynamoDB table to store customer session information",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Cognito user pool to manage user session information",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon ElastiCache for Redis cluster to store customer session information",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Systems Manager Application Manager in the application to manage user session",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records \nfor analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be \nup to 10 GB in size. Based on the number of sales events, the job can take up to an hour to \ncomplete. The CPU and memory usage of the job are constant and are known in advance. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n234 \nA solutions architect needs to minimize the amount of operational effort that is needed for the job \nto run. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) duster with an Amazon EC2 launch",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect must migrate a Windows Internet Information Services (IIS) web application \nto AWS. The application currently relies on a file share hosted in the user's on-premises network-\nattached storage (NAS). The solutions architect has proposed migrating the MS web servers to \nAmazon EC2 instances in multiple Availability Zones that are connected to the storage solution, \nand configuring an Elastic Load Balancer attached to the instances. \nWhich replacement to the on-premises file share is MOST resilient and durable?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the file share to Amazon RDS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the file share to AWS Storage Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the file share to Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the file share to Amazon Elastic File System (Amazon EFS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company wants to restrict access to the content of one of its man web applications and to \nprotect the content by using authorization techniques available on AWS. The company wants to \nimplement a serverless architecture end an authentication solution for fewer tian 100 users. The \nsolution needs to integrate with the main web application and serve web content globally. The \nsolution must also scale as to company's user base grows while providing lowest login latency \npossible.",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Cognito tor authentication. Use Lambda#Edge tor authorization. Use Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Directory Service for Microsoft Active Directory tor authentication. Use AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Usa Amazon Cognito for authentication. Use AWS Lambda tor authorization. Use Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "An ecommerce company is building a distributed application that involves several serverless \nfunctions and AWS services to complete order-processing tasks. These tasks require manual \napprovals as part of the workflow. A solutions architect needs to design an architecture for the \norder-processing application. The solution must be able to combine multiple AWS Lambda \nfunctions into responsive serverless applications. The solution also must orchestrate data and \nservices that run on Amazon EC2 instances, containers, or on-premises servers. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Step Functions to build the application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Integrate all the application components in an AWS Glue job",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to build the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Lambda functions and Amazon EventBridge (Amazon CloudWatch Events) events to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based \napplication for users around the world. The application is hosted on redundant servers in the \ncompany's on-premises data centers in the United States. Asia, and Europe. The company's \ncompliance requirements state that the application must be hosted on premises. The company \nwants to improve the performance and availability of the application. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "A Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company runs an application on Amazon EC2 Linux instances across multiple Availability \nZones. The application needs a storage layer that is highly available and Portable Operating \nSystem Interface (POSIX)-compliant. The storage layer must provide maximum data durability \nand must be shareable across the EC2 instances. The data in the storage layer will be accessed \nfrequently for the first 30 days and will be accessed infrequently after that time. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a Lifecycle",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a Lifecycle",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 26,
    "text": "A company wants to migrate its 1 PB on-premises image repository to AWS. The images will be \nused by a serverless web application. Images stored in the repository are rarely accessed, but \nthey must be immediately available Additionally, the images must be encrypted at rest and \nprotected from accidental deletion. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement client-side encryption and store the images in an Amazon S3 Glacier vault. Set a vault",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the images in an Amazon S3 bucket in the S3 Standard-Infrequent Access (S3 Standard-",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the images in an Amazon FSx for Windows File Server file share. Configure the Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the images in an Amazon Elastic File System (Amazon EFS) file share in the Infrequent",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company runs an application that receives data from thousands of geographically dispersed \nremote devices that use UDP. The application processes the data immediately and sends a \nmessage back to the device if necessary. No data is stored. \n \nThe company needs a solution that minimizes latency for the data transmission from the devices. \nThe solution also must provide rapid failover to another AWS Region. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Global Accelerator Create an Application Load Balancer (ALB) in each of the two",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 28,
    "text": "An ecommerce company is running a multi-tier application on AWS. The front-end and backend \ntiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend \ntier communicates with the RDS instance. There are frequent calls to return identical datasets \nfrom the database that are causing performance slowdowns. \n \nWhich action should be taken to improve the performance of the backend?",
    "options": [
      {
        "id": 0,
        "text": "Implement Amazon SNS to store the database calls.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement Amazon ElasticCache to cache the large database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement an RDS for MySQL read replica to cache database calls.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement Amazon Kinesis Data Firehose to stream the calls to the database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "A hospital is designing a new application that gathers symptoms from patients. The hospital has \ndecided to use Amazon Simple Queue Service (Amazon SOS) and Amazon Simple Notification \nService (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure \ndesign Data must be encrypted at test and in transit. Only authorized personnel of the hospital \nshould be able to access the data. \n \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Turn on server-side encryption on the SQS components. Update tie default key policy to restrict",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on server-side encryption on the SNS components by using an AWS Key Management",
        "correct": true
      },
      {
        "id": 2,
        "text": "Turn on encryption on the SNS components. Update the default key policy to restrict key usage to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on server-side encryption on the SOS components by using an AWS Key Management",
        "correct": false
      },
      {
        "id": 4,
        "text": "Turn on server-side encryption on the SOS components by using an AWS Key Management",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A solutions architect is creating a new VPC design. There are two public subnets for the load \nbalancer, two private subnets for web servers, and two private subnets for MySQL. The web \nservers use only HTTPS. The solutions architect has already created a security group for the load \nbalancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the \nleast access required to still be able to perform its tasks. \n \nWhich additional configuration strategy should the solutions architect use to meet these \nrequirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a security group for the web servers and allow port 443 from the load balancer. Create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a network ACL for the web servers and allow port 443 from the load balancer. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company wants to use Amazon S3 for the secondary copy of its on-premises dataset. The \ncompany would rarely need to access this copy. The storage solution's cost should be minimal. \n \nWhich storage solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "S3 Standard",
        "correct": false
      },
      {
        "id": 1,
        "text": "S3 Intelligent-Tiering",
        "correct": false
      },
      {
        "id": 2,
        "text": "S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": true
      },
      {
        "id": 3,
        "text": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A solutions architect is designing a two-tiered architecture that includes a public subnet and a \ndatabase subnet. The web servers in the public subnet must be open to the internet on port 443. \nThe Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the \nweb servers on port 3306. \n \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a security group for the DB instance. Add a rule to allow traffic from the public subnet",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a security group for the web servers in the public subnet. Add a rule to allow traffic from",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a security group for the DB instance. Add a rule to allow traffic from the web servers'",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company has an application that collects data from loT sensors on automobiles. The data is \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n240 \nstreamed and stored in Amazon S3 through Amazon Kinesis Date Firehose. The data produces \ntrillions of S3 objects each year. Each morning, the company uses the data from the previous 30 \ndays to retrain a suite of machine learning (ML) models. \n \nFour times each year, the company uses the data from the previous 12 months to perform \nanalysis and train other ML models. The data must be available with minimal delay for up to 1 \nyear. After 1 year, the data must be retained for archival purposes. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "A company recently deployed a new auditing system to centralize information about operating \nsystem versions patching and installed software for Amazon EC2 instances. A solutions architect \nmust ensure all instances provisioned through EC2 Auto Scaling groups successfully send \nreports to the auditing system as soon as they are launched and terminated. \n \nWhich solution achieves these goals MOST efficiently?",
    "options": [
      {
        "id": 0,
        "text": "Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run a custom script on the instance operating system to send data to the audit system. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to \nthe database come from serverless applications. Application traffic to the database changes \nsignificantly at random intervals. At limes of high demand, users report that their applications \nexperience database connection rejection errors. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n241 \n \nWhich solution will resolve this issue with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy Amazon ElastCache for Memcached between the users' application and the DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Multi-AZ for the DB instance. Configure the users' application to switch between the DB",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A solutions architect is designing the architecture for a software demonstration environment. The \nenvironment will run on Amazon EC2 instances in an Auto Scaling group behind an Application \nLoad Balancer (ALB). The system will experience significant increases in traffic during working \nhours but Is not required to operate on weekends. \n \nWhich combination of actions should the solutions architect take to ensure that the system can \nscale to meet demand? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Auto Scaling to adjust the ALB capacity based on request rate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company has deployed a serverless application that invokes an AWS Lambda function when \nnew documents are uploaded to an Amazon S3 bucket. The application uses the Lambda \nfunction to process the documents. After a recent marketing campaign, the company noticed that \nthe application did not process many of the documents. \n \nWhat should a solutions architect do to improve the architecture of this application?",
    "options": [
      {
        "id": 0,
        "text": "Set the Lambda function's runtime timeout value to 15 minutes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 bucket replication policy. Stage the documents m the S3 bucket for later",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an additional Lambda function Load balance the processing of the documents across the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Queue Service (Amazon SOS) queue. Send the requests to the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 \nand needs the required permissions to perform the task. The developer already has an IAM user \nwith valid IAM credentials required for Amazon S3. \n \nWhat should a solutions architect do to grant the permissions?",
    "options": [
      {
        "id": 0,
        "text": "Add required IAM permissions in the resource policy of the Lambda function.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a signed request using the existing IAM credentials in the Lambda function",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user and use the existing IAM credentials in the Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM execution role with the required permissions and attach the IAM rote to the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the company’s VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the company’s CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company has a large dataset for its online advertising business stored in an Amazon RDS for \nMySQL DB instance in a single Availability Zone. The company wants business reporting queries \nto run without impacting the write operations to the production DB instance. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy RDS read replicas to process the business reporting queries.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Scale up the DB instance to a larger instance type to handle write operations and queries.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the OB distance in multiple Availability Zones to process the business reporting queries.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ). Multi-AZ uses synchronous replication and spans at least two Availability Zones (AZs).\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 40,
    "text": "A meteorological startup company has a custom web application to sell weather data to its users \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n243 \nonline. The company uses Amazon DynamoDB to store is data and wants to build a new service \nthat sends an alert to the managers of four Internal teams every time a new weather event is \nrecorded. The company does not want true new service to affect the performance of the current \napplication. \n \nWhat should a solutions architect do to meet these requirement with the LEAST amount of \noperational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use DynamoDB transactions to write new event data to the table. Configure the transactions to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Have the current application publish a message to four Amazon Simple Notification Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Amazon DynamoDB Streams on the table. Use triggers to write to a mingle Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Add a custom attribute to each record to flag new items. Write a cron job that scans the table",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company is developing a real-time multiplayer game that uses UDP for communications \nbetween the client and servers. In an Auto Scaling group Spikes in demand are anticipated during \nthe day, so the game server platform must adapt accordingly. Developers want to store gamer \nscores and other non-relational data in a database solution that will scale without intervention. \n \nWhich solution should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Route 53 for traffic distribution and Amazon Aurora Serverless for data storage.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a Network Load Balancer for traffic distribution and Amazon Aurora Global Database for data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Application Load Balancer for traffic distribution and Amazon DynamoDB global tables for",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host \na digital media streaming application. The EKS cluster will use a managed node group that is \nbacked by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must \nencrypt all data at rest by using a customer managed key that is stored in AWS Key Management \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n244 \nService (AWS KMS). \n \nWhich combination of actions will meet this requirement with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use a Kubernetes plugin that uses the customer managed key to perform data encryption.",
        "correct": false
      },
      {
        "id": 1,
        "text": "After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable EBS encryption by default in the AWS Region where the EKS cluster will be created.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create the EKS cluster. Create an IAM role that has a policy that grants permission to the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company has a web application with sporadic usage patterns. There is heavy usage at the \nbeginning of each month moderate usage at the start of each week and unpredictable usage \nduring the week. The application consists of a web server and a MySQL database server running \ninside the data center. The company would like to move the application to the AWS Cloud and \nneeds to select a cost-effective database platform that will not require database modifications. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon RDS for MySQL",
        "correct": false
      },
      {
        "id": 2,
        "text": "MySQL-compatible Amazon Aurora Serverless",
        "correct": true
      },
      {
        "id": 3,
        "text": "MySQL deployed on Amazon EC2 in an Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company uses a payment processing system that requires messages for a particular payment \nID to be received in the same order that they were sent. Otherwise, the payments might be \nprocessed incorrectly. \n \nWhich actions should a solutions architect take to meet this requirement? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Write the messages to an Amazon DynamoDB table with the payment ID as the partition key",
        "correct": false
      },
      {
        "id": 1,
        "text": "Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because there is no built-in configuration in the S3 console to require additional confirmation for deletion. While custom solutions could be built, this is not a standard or readily available feature and does not provide the same level of protection as MFA Delete or Versioning.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "An IAM user made several configuration changes to AWS resources in their company's account \nduring a production deployment last week. A solutions architect learned that a couple of security \ngroup rules are not configured as desired. The solutions architect wants to confirm which IAM \nuser was responsible for making changes. \n \nWhich service should the solutions architect use to find the desired information?",
    "options": [
      {
        "id": 0,
        "text": "Amazon GuardDuty",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Inspector",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS CloudTrail",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Config",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 0 is incorrect:**\nis incorrect because partition placement groups are primarily used for large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. While they provide fault tolerance by distributing instances across partitions, they do not offer the low latency and high throughput benefits of cluster placement groups, which are more critical for HPC applications.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company runs a public three-Tier web application in a VPC. The application runs on Amazon \nEC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets \nneed to communicate with a license server over the internet. The company needs a managed \nsolution that minimizes operational maintenance. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision a NAT instance in a public subnet. Modify each private subnets route table with a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision a NAT instance in a private subnet. Modify each private subnet's route table with a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n246 \nA company needs to transfer 600 TB of data from its on-premises network-attached storage \n(NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is \nsensitive and must be encrypted in transit. The company's internet connection can support an \nupload speed of 100 Mbps. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 multi-part upload functionality to transfer the fees over HTTPS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPN connection between the on-premises NAS system and the nearest AWS Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company needs a backup strategy for its three-tier stateless web application. The web \napplication runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling \npolicy that is configured to respond to scaling events. The database tier runs on Amazon RDS for \nPostgreSQL. The web application does not require temporary local storage on the EC2 instances. \nThe company’s recovery point objective (RPO) is 2 hours. \n \nThe backup strategy must maximize scalability and optimize resource utilization for this \nenvironment. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable",
        "correct": true
      },
      {
        "id": 3,
        "text": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 49,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n247 \nA company needs to ingest and handle large amounts of streaming data that its application \ngenerates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis \nData Streams, which is configured with default settings. Every other day, the application \nconsumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) \nprocessing. The company observes that Amazon S3 is not receiving all the data that the \napplication sends to Kinesis Data Streams. \n \nWhat should a solutions architect do to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Update the Kinesis Data Streams default settings by modifying the data retention period.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Update the application to use the Kinesis Producer Library (KPL) lo send the data to Kinesis Data",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the number of Kinesis shards lo handle the throughput of me data that is sent to Kinesis",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 instances with Amazon EFS mount points, is incorrect because EFS is a network file system designed for shared access and scalability, not for high random I/O performance. EFS would introduce latency and be significantly more expensive than instance store. Furthermore, the application already handles data replication, making the shared storage aspect of EFS unnecessary. EFS is also not ideal for high random I/O workloads.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 \ninstances runs several 1-hour tasks on a schedule. These tasks were written by different teams \nand have no common programming language. The company is concerned about performance \nand scalability while these tasks run on a single instance. A solutions architect needs to \nimplement a solution to resolve these concerns. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge",
        "correct": true
      },
      {
        "id": 1,
        "text": "Convert the EC2 instance to a container. Use AWS App Runner to create the container on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 1 is incorrect:**\nis incorrect because while it creates IAM policies for DynamoDB and S3, attaching them to the EC2 instance profile grants those permissions to *all* Pods running on those nodes. Kubernetes RBAC controls access *within* the cluster, not to AWS resources. This approach doesn't provide the fine-grained control needed to restrict UI Pods to DynamoDB and data-processing Pods to S3. All pods running on the node would inherit the permissions of the instance profile.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "A company wants to migrate an Oracle database to AWS. The database consists of a single table \nthat contains millions of geographic information systems (GIS) images that are high resolution \nand are identified by a geographic code. When a natural disaster occurs tens of thousands of \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n248 \nimages get updated every few minutes. Each geographic code has a single image or row that is \nassociated with it. The company wants a solution that is highly available and scalable during such \nevents. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the images and geographic codes in a database table. Use Oracle running on an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the images in Amazon S3 buckets Store geographic codes and image S3 URLs in a",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 52,
    "text": "A company has implemented a self-managed DNS service on AWS. The solution consists of the \nfollowing: \n \n- Amazon EC2 instances in different AWS Regions \n- Endpoints of a standard accelerator in AWS Global Accelerator \n \nThe company wants to protect the solution against DDoS attacks. \n \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis can handle streaming data, it adds unnecessary complexity and cost compared to using CloudWatch metric filters directly on CloudTrail logs. Kinesis is better suited for high-volume, real-time data processing, which is not explicitly required in this scenario. The latency introduced by Kinesis and Lambda might also be higher than using CloudWatch metric filters and alarms directly.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The \ncompany uses Amazon EC2 Windows Server instances behind an Application Load Balancer to \nhost its dynamic application. The company needs a highly available storage solution for the \napplication. The application consists of static files and dynamic server-side code. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n249",
    "options": [
      {
        "id": 0,
        "text": "Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows",
        "correct": false
      },
      {
        "id": 4,
        "text": "Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company hosts a frontend application that uses an Amazon API Gateway API backend that is \nintegrated with AWS Lambda. When the API receives requests, the Lambda function loads many \nlibraries. Then the Lambda function connects to an Amazon RDS database, processes the data, \nand returns the data to the frontend application. The company wants to ensure that response \nlatency is as low as possible for all its users with the fewest number of changes to the company's \noperations. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Establish a connection between the frontend application and the database to make queries faster",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure provisioned concurrency for the Lambda function that handles the requests.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase the size of the database to increase the number of connections Lambda can establish at",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 56,
    "text": "A company is building a game system that needs to send unique events to separate leaderboard, \nmatchmaking, and authentication services concurrently. The company needs an AWS event-\ndriven system that guarantees the order of the events. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EventBridge event bus",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Simple Notification Service (Amazon SNS) FIFO topics",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Simple Notification Service (Amazon SNS) standard topics",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Simple Queue Service (Amazon SQS) FIFO queues",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A new employee has joined a company as a deployment engineer. The deployment engineer will \nbe using AWS CloudFormation templates to create multiple AWS resources. A solutions architect \nwants the deployment engineer to perform job activities while following the principle of least \nprivilege. \n \nWhich combination of actions should the solutions architect take to accomplish this goal? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n251",
    "options": [
      {
        "id": 0,
        "text": "Have the deployment engineer use AWS account root user credentials for performing AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has an",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create an IAM role for the deployment engineer to explicitly define the permissions specific to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nMicrosoft SQL Server on AWS is a database service and does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). While SQL Server can store data, it doesn't provide file storage or DFS compatibility.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company is implementing a shared storage solution for a gaming application that is hosted in \nthe AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution \nmust be fully managed. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS DataSync task that shares the data as a mountable file system. Mount the file",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company has a business system that generates hundreds of reports each day. The business \nsystem saves the reports to a network share in CSV format. The company needs to store this \ndata in the AWS Cloud in near-real time for analysis. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 File Gateway. Update the business system to use a new network share",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing AWS Site-to-Site VPN to establish a connection between the data center and AWS Cloud, is incorrect. While Site-to-Site VPN provides encryption, it relies on the public internet, which can introduce latency and is not a dedicated connection. It also may not provide the high throughput required. It's a less expensive and simpler solution than Direct Connect, but it doesn't meet the low-latency and high-throughput requirements.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 \nbuckets and is accessed with varying frequency. The company does not know access patterns for \nall the data. The company needs to implement a solution for each S3 bucket to optimize the cost \nof S3 usage. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the S3 storage class analysis tool to determine the correct tier for each object in the S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A solutions architect needs to allow team members to access Amazon S3 buckets in two different \nAWS accounts: a development account and a production account. The team currently has access \nto S3 buckets in the development account by using unique IAM users that are assigned to an IAM \ngroup that has appropriate permissions in the account. \n \nThe solutions architect has created an IAM role in the production account. The role has a policy \nthat grants access to an S3 bucket in the production account. \n \nWhich solution will meet these requirements while complying with the principle of least privilege?",
    "options": [
      {
        "id": 0,
        "text": "Attach the Administrator Access policy to the development account users.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add the development account as a principal in the trust policy of the role in the production",
        "correct": true
      },
      {
        "id": 2,
        "text": "Turn off the S3 Block Public Access feature on the S3 bucket in the production account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a user in the production account with unique credentials for each team member.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 \nworkloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that \nprevents any resources from being created in any other Region. A security policy requires the \ncompany to encrypt all data at rest. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n253 \nAn audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) \nvolumes for EC2 instances without encrypting the volumes. The company wants any new EC2 \ninstances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS \nvolumes. The company wants a solution that will have minimal effect on employees who create \nEBS volumes. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "In the Amazon EC2 console, select the EBS encryption account attribute and define a default",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM permission boundary. Attach the permission boundary to the root organizational",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Update the IAM policies for each account to deny the ec2:CreateVolume action when the",
        "correct": false
      },
      {
        "id": 4,
        "text": "In the Organizations management account, specify the Default EBS volume encryption setting.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nOption 4: Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3. While VPN connections provide secure communication, they often introduce overhead and latency, potentially slowing down upload speeds compared to using the public internet with S3 Transfer Acceleration. Also, setting up and managing multiple VPN connections across different regions can be complex and costly. It's not the most cost-effective solution for improving S3 upload speeds.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming \ndatabase administrative tasks for production database workloads. The company wants to ensure \nthat its database is highly available and will provide automatic failover support in most scenarios \nin less than 40 seconds. The company wants to offload reads off of the primary instance and \nkeep costs as low as possible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.",
    "domain": "Design Cost-Optimized Architectures"
  }
]