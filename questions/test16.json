[
  {
    "id": 1,
    "text": "A company runs an application on Amazon EC2 instances. The company needs to implement a \ndisaster recovery (DR) solution for the application. The DR solution needs to have a recovery \ntime objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible \nAWS resources during normal operations. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a",
        "correct": true
      },
      {
        "id": 2,
        "text": "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOption B would be the most operationally efficient solution for implementing a DR solution for the application, meeting the requirement of an RTO of less than 4 hours and using the fewest possible AWS resources during normal operations. By creating Amazon Machine Images (AMIs) to back up the EC2 instances and copying them to a secondary AWS Region, the company can ensure that they have a reliable backup in the event of a disaster. By using AWS CloudFormation to automate infrastructure deployment in the secondary Region, the company can minimize the amount of time and effort required to set up the DR solution.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company hosts a multiplayer gaming application on AWS. The company wants the application \nto read data with sub-millisecond latency and run one-time queries on historical data. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nDynamoDB supports some of the world's largest scale applications by providing consistent, single-digit millisecond response times at any scale. You can build applications with virtually unlimited throughput and storage. https://aws.amazon.com/dynamodb/dax/?nc1=h_ls\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 3,
    "text": "A company has a regional subscription-based streaming service that runs in a single AWS \nRegion. The architecture consists of web servers and application servers on Amazon EC2 \ninstances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The \narchitecture includes an Amazon Aurora database cluster that extends across multiple Availability \nZones. \n \nThe company wants to expand globally and to ensure that its application has minimal downtime.",
    "options": [
      {
        "id": 0,
        "text": "Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the web tier and the applicatin tier to a second Region. Create an Aurora PostSQL",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 4,
    "text": "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. \nThe company does not want to use the default domain name for the distribution. Instead, the \ncompany wants to use a different domain name for the distribution. \n \nWhich solution will deploy the certificate with icurring any additional costs?",
    "options": [
      {
        "id": 0,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": false
      },
      {
        "id": 1,
        "text": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-",
        "correct": true
      },
      {
        "id": 2,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-east-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Request an Amazon issued public certificate from AWS Certificate Manager (ACU) in the us-",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A solutions architect is designing a company’s disaster recovery (DR) architecture. The company \nhas a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled \nbackup. The DR design needs to include multiple AWS Regions. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the schedule backup of the MySQL database in an Amazon S3 bucket that is configured for",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 6,
    "text": "A rapidly growing global ecommerce company is hosting its web application on AWS. The web \napplication includes static content and dynamic content. The website stores online transaction \nprocessing (OLTP) data in an Amazon RDS database. The website's users are experiencing slow \npage loads. \n \nWhich combination of actions should a solutions architect take to resolve this issue? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Redshift cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon CloudFront distribution",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the dynamic web content in Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a read replica for the RDS DB instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure a Multi-AZ deployment for the RDS DB instance",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nTo resolve the issue of slow page loads for a rapidly growing e-commerce website hosted on AWS, a solutions architect can take the following two actions: 1. Set up an Amazon CloudFront distribution 2. Create a read replica for the RDS DB instance Configuring an Amazon Redshift cluster is not relevant to this issue since Redshift is a data warehousing service and is typically used for the analytical processing of large amounts of data. Hosting the dynamic web content in Amazon S3 may not necessarily improve performance since S3 is an object storage service, not a web application server. While S3 can be used to host static Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A solutions architect wants all new users to have specific complexity requirements and mandatory \nrotation periods tor IAM user passwords. \n \nWhat should the solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Set an overall password policy for the entire AWS account.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set a password policy for each IAM user in the AWS account.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use third-party vendor software to set password requirements.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTo accomplish this, the solutions architect should set an overall password policy for the entire AWS account. This policy will apply to all IAM users in the account, including new users.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company wants to deploy a new public web application on AWS. The application includes a \nweb server tier that uses Amazon EC2 instances. The application also includes a database tier \nthat uses an Amazon RDS for MySQL DB instance. \n \nThe application must be secure and accessible for global customers that have dynamic IP \naddresses. \n \nHow should a solutions architect configure the security groups to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure the security group tor the web servers to allow inbound traffic on port 443 from",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the security group for the web servers to allow inbound traffic on port 443 from the IP",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the security group for the web servers to allow inbound traffic on port 443 from the IP",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the security group for the web servers to allow inbound traffic on port 443 from",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A company is planning to migrate a commercial off-the-shelf application from is on-premises data \ncenter to AWS. The software has a software licensing model using sockets and cores with \npredictable capacity and uptime requirements. The company wants to use its existing licenses, \nwhich were purchased earlier this year. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n229 \n \nWhich Amazon EC2 pricing option is the MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Dedicated Reserved Hosts",
        "correct": true
      },
      {
        "id": 1,
        "text": "Dedicated On-Demand Hosts",
        "correct": false
      },
      {
        "id": 2,
        "text": "Dedicated Reserved Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Dedicated On-Demand Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nDedicated Host Reservations provide a billing discount compared to running On-Demand Dedicated Hosts. Reservations are available in three payment options. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 10,
    "text": "An ecommerce company is experiencing an increase in user traffic. The company's store is \ndeployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a \nseparate database tier. As traffic increases, the company notices that the architecture is causing \nsignificant delays in sending timely marketing and order confirmation email to users. The \ncompany wants to reduce the time it spends resolving complex email delivery issues and \nminimize operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a separate application tier using EC2 instances dedicated to email processing.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure the web instance to send email through Amazon Simple Notification Service (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a separate application tier using EC2 instances dedicated to email processing. Place the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon SES is a cost-effective and scalable email service that enables businesses to send and receive email using their own email addresses and domains. Configuring the web instance to send email through Amazon SES is a simple and effective solution that can reduce the time spent resolving complex email delivery issues and minimize operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon \nEC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database \ntier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier \nrequires access to the database to retrieve product information. \n \nThe web application is not working as intended. The web application reports that it cannot \nconnect to the database. The database is confirmed to be up and running. All configurations for \nthe network ACLs, security groups, and route tables are still in their default states. \n \nWhat should a solutions architect recommend to fix the application?",
    "options": [
      {
        "id": 0,
        "text": "Add an explicit rule to the private subnet's network ACL to allow traffic from the web tier's EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a route in the VPC route table to allow traffic between the web tier’s EC2 instances and the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the web tier's EC2 instances and the database tier's RDS instance into two separate",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an inbound rule to the security group of the database tier's RDS instance to allow traffic from",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nBy default, all inbound traffic to an RDS instance is blocked. Therefore, an inbound rule needs to be added to the security group of the RDS instance to allow traffic from the security group of the web tier's EC2 instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "A company is running a multi-tier ecommerce web application in the AWS Cloud. The application \nruns on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon \nRDS is configured with the latest generation DB instance with 2,000 GB of storage in a General \nPurpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database \nperformance affects the application during periods of high demand. \n \nA database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the \napplication performance always degrades when the number of read and write IOPS is higher than \n20,000. \n \nWhat should a solutions architect do to improve the application performance?",
    "options": [
      {
        "id": 0,
        "text": "Replace the volume with a magnetic volume.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the number of IOPS on the gp3 volume.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replace the volume with a Provisioned IOPS SSD (Io2) volume.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo improve the application performance, you can replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes. This will increase the number of IOPS available to the database and improve performance. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 13,
    "text": "A company is deploying a new application on Amazon EC2 instances. The application writes data \nto Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all \ndata that is written to the EBS volumes is encrypted at rest. \n \nWhich solution will meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nWhen you create an EBS volume, you can specify whether to encrypt the volume. If you choose to encrypt the volume, all data written to the volume is automatically encrypted at rest using AWS-managed keys. You can also use customer-managed keys (CMKs) stored in AWS KMS to encrypt and protect your EBS volumes. You can create encrypted EBS volumes and attach them to EC2 instances to ensure that all data written to the volumes is encrypted at rest.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "An image-hosting company stores its objects in Amazon S3 buckets. The company wants to \navoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the \nentire AWS account need to remain private. \n \nWhich solution will meal these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Trusted Advisor to find publicly accessible S3 Dockets. Configure email notifications In",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public- access.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A financial company hosts a web application on AWS. The application uses an Amazon API \nGateway Regional API endpoint to give users the ability to retrieve current stock prices. The \ncompany's security team has noticed an increase in the number of API requests. The security \nteam is concerned that HTTP flood attacks might take the application offline. \nA solutions architect must design a solution to protect the application from this type of attack. \n \nWhich solution meats these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nA rate-based rule in AWS WAF allows the security team to configure thresholds that trigger rate- based rules, which enable AWS WAF to track the rate of requests for a specified time period and then block them automatically when the threshold is exceeded. This provides the ability to prevent HTTP flood attacks with minimal operational overhead. https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "A payment processing company records all voice communication with its customers and stores \nthe audio files in an Amazon S3 bucket. The company needs to capture the text from the audio \nfiles. The company must remove from the text any personally identifiable information (Pll) that \nbelongs to customers. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function",
        "correct": false
      },
      {
        "id": 1,
        "text": "When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon Transcribe transcription job with Pll redaction turned on. When an audio file",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Connect contact flow that ingests the audio files with transcription turned on.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 18,
    "text": "A company is migrating its on-premises workload to the AWS Cloud. The company already uses \nseveral Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution \nthat automatically starts and stops the EC2 instances and D6 instances outside of business \nhours. The solution must minimize cost and infrastructure maintenance. \n \nWhich solution will meet these requirement?",
    "options": [
      {
        "id": 0,
        "text": "Scale the EC2 instances by using elastic resize Scale the DB instances to zero outside of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that will start and stop the EC2 instances and DB instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThe most efficient solution for automatically starting and stopping EC2 instances and DB instances on a schedule while minimizing cost and infrastructure maintenance is to create an AWS Lambda function and configure Amazon EventBridge to invoke the function on a schedule.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 19,
    "text": "A company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The \ninstances run in an Auto Scaling group behind an Application Load Balancer (ALB). All \necommerce data is stored in an Amazon RDS for ManaDB Multi-AZ DB instance. The company \nwants to optimize customer session management during transactions. The application must store \nsession data durably. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Turn on the sticky sessions feature (session affinity) on the ALB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an Amazon DynamoDB table to store customer session information",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Cognito user pool to manage user session information",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Amazon ElastiCache for Redis cluster to store customer session information",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Systems Manager Application Manager in the application to manage user session",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/caching/session-management/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records \nfor analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be \nup to 10 GB in size. Based on the number of sales events, the job can take up to an hour to \ncomplete. The CPU and memory usage of the job are constant and are known in advance. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n234 \nA solutions architect needs to minimize the amount of operational effort that is needed for the job \nto run. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) duster with an Amazon EC2 launch",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 21,
    "text": "A solutions architect must migrate a Windows Internet Information Services (IIS) web application \nto AWS. The application currently relies on a file share hosted in the user's on-premises network-\nattached storage (NAS). The solutions architect has proposed migrating the MS web servers to \nAmazon EC2 instances in multiple Availability Zones that are connected to the storage solution, \nand configuring an Elastic Load Balancer attached to the instances. \nWhich replacement to the on-premises file share is MOST resilient and durable?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the file share to Amazon RDS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the file share to AWS Storage Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the file share to Amazon FSx for Windows File Server",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the file share to Amazon Elastic File System (Amazon EFS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon FSx makes it easy and cost effective to launch, run, and scale feature-rich, high- performance file systems in the cloud.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company wants to restrict access to the content of one of its man web applications and to \nprotect the content by using authorization techniques available on AWS. The company wants to \nimplement a serverless architecture end an authentication solution for fewer tian 100 users. The \nsolution needs to integrate with the main web application and serve web content globally. The \nsolution must also scale as to company's user base grows while providing lowest login latency \npossible.",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Cognito tor authentication. Use Lambda#Edge tor authorization. Use Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Directory Service for Microsoft Active Directory tor authentication. Use AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Usa Amazon Cognito for authentication. Use AWS Lambda tor authorization. Use Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 23,
    "text": "An ecommerce company is building a distributed application that involves several serverless \nfunctions and AWS services to complete order-processing tasks. These tasks require manual \napprovals as part of the workflow. A solutions architect needs to design an architecture for the \norder-processing application. The solution must be able to combine multiple AWS Lambda \nfunctions into responsive serverless applications. The solution also must orchestrate data and \nservices that run on Amazon EC2 instances, containers, or on-premises servers. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Step Functions to build the application.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Integrate all the application components in an AWS Glue job",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to build the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Lambda functions and Amazon EventBridge (Amazon CloudWatch Events) events to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Step Functions is a fully managed service that makes it easy to build applications by coordinating the components of distributed applications and microservices using visual workflows. With Step Functions, you can combine multiple AWS Lambda functions into responsive serverless applications and orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Step Functions also allows for manual approvals as part of the workflow. This solution meets all the requirements with the least operational overhead.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based \napplication for users around the world. The application is hosted on redundant servers in the \ncompany's on-premises data centers in the United States. Asia, and Europe. The company's \ncompliance requirements state that the application must be hosted on premises. The company \nwants to improve the performance and availability of the application. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "A Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nQ: How is AWS Global Accelerator different from Amazon CloudFront? A: AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company runs an application on Amazon EC2 Linux instances across multiple Availability \nZones. The application needs a storage layer that is highly available and Portable Operating \nSystem Interface (POSIX)-compliant. The storage layer must provide maximum data durability \nand must be shareable across the EC2 instances. The data in the storage layer will be accessed \nfrequently for the first 30 days and will be accessed infrequently after that time. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a Lifecycle",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a Lifecycle",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/efs/features/infrequent-access/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 26,
    "text": "A company wants to migrate its 1 PB on-premises image repository to AWS. The images will be \nused by a serverless web application. Images stored in the repository are rarely accessed, but \nthey must be immediately available Additionally, the images must be encrypted at rest and \nprotected from accidental deletion. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Implement client-side encryption and store the images in an Amazon S3 Glacier vault. Set a vault",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the images in an Amazon S3 bucket in the S3 Standard-Infrequent Access (S3 Standard-",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the images in an Amazon FSx for Windows File Server file share. Configure the Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the images in an Amazon Elastic File System (Amazon EFS) file share in the Infrequent",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company runs an application that receives data from thousands of geographically dispersed \nremote devices that use UDP. The application processes the data immediately and sends a \nmessage back to the device if necessary. No data is stored. \n \nThe company needs a solution that minimizes latency for the data transmission from the devices. \nThe solution also must provide rapid failover to another AWS Region. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Global Accelerator Create an Application Load Balancer (ALB) in each of the two",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nGeographically dispersed (related to UDP) - Global Accelerator - multiple entrances worldwide to the AWS network to provide better transfer rates. UDP - NLB (Network Load Balancer).\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 28,
    "text": "An ecommerce company is running a multi-tier application on AWS. The front-end and backend \ntiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend \ntier communicates with the RDS instance. There are frequent calls to return identical datasets \nfrom the database that are causing performance slowdowns. \n \nWhich action should be taken to improve the performance of the backend?",
    "options": [
      {
        "id": 0,
        "text": "Implement Amazon SNS to store the database calls.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Implement Amazon ElasticCache to cache the large database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement an RDS for MySQL read replica to cache database calls.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement Amazon Kinesis Data Firehose to stream the calls to the database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nKey term is identical datasets from the database it means caching can solve this issue by cached in frequently used dataset from DB.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 29,
    "text": "A hospital is designing a new application that gathers symptoms from patients. The hospital has \ndecided to use Amazon Simple Queue Service (Amazon SOS) and Amazon Simple Notification \nService (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure \ndesign Data must be encrypted at test and in transit. Only authorized personnel of the hospital \nshould be able to access the data. \n \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Turn on server-side encryption on the SQS components. Update tie default key policy to restrict",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on server-side encryption on the SNS components by using an AWS Key Management",
        "correct": true
      },
      {
        "id": 2,
        "text": "Turn on encryption on the SNS components. Update the default key policy to restrict key usage to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on server-side encryption on the SOS components by using an AWS Key Management",
        "correct": false
      },
      {
        "id": 4,
        "text": "Turn on server-side encryption on the SOS components by using an AWS Key Management",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nFor a customer managed KMS key, you must configure the key policy to add permissions for each queue producer and consumer. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-key- management.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A solutions architect is creating a new VPC design. There are two public subnets for the load \nbalancer, two private subnets for web servers, and two private subnets for MySQL. The web \nservers use only HTTPS. The solutions architect has already created a security group for the load \nbalancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the \nleast access required to still be able to perform its tasks. \n \nWhich additional configuration strategy should the solutions architect use to meet these \nrequirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a security group for the web servers and allow port 443 from the load balancer. Create a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a network ACL for the web servers and allow port 443 from the load balancer. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nLoad balancer is public facing accepting all traffic coming towards the VPC (0.0.0.0/0). The web server needs to trust traffic originating from the ALB. The DB will only trust traffic originating from the Web server on port 3306 for Mysql.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company wants to use Amazon S3 for the secondary copy of its on-premises dataset. The \ncompany would rarely need to access this copy. The storage solution's cost should be minimal. \n \nWhich storage solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "S3 Standard",
        "correct": false
      },
      {
        "id": 1,
        "text": "S3 Intelligent-Tiering",
        "correct": false
      },
      {
        "id": 2,
        "text": "S3 Standard-Infrequent Access (S3 Standard-IA)",
        "correct": true
      },
      {
        "id": 3,
        "text": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 32,
    "text": "A solutions architect is designing a two-tiered architecture that includes a public subnet and a \ndatabase subnet. The web servers in the public subnet must be open to the internet on port 443. \nThe Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the \nweb servers on port 3306. \n \nWhich combination of steps should the solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a security group for the DB instance. Add a rule to allow traffic from the public subnet",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a security group for the web servers in the public subnet. Add a rule to allow traffic from",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a security group for the DB instance. Add a rule to allow traffic from the web servers'",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company has an application that collects data from loT sensors on automobiles. The data is \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n240 \nstreamed and stored in Amazon S3 through Amazon Kinesis Date Firehose. The data produces \ntrillions of S3 objects each year. Each morning, the company uses the data from the previous 30 \ndays to retrain a suite of machine learning (ML) models. \n \nFour times each year, the company uses the data from the previous 12 months to perform \nanalysis and train other ML models. The data must be available with minimal delay for up to 1 \nyear. After 1 year, the data must be retained for archival purposes. \n \nWhich storage solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "A company recently deployed a new auditing system to centralize information about operating \nsystem versions patching and installed software for Amazon EC2 instances. A solutions architect \nmust ensure all instances provisioned through EC2 Auto Scaling groups successfully send \nreports to the auditing system as soon as they are launched and terminated. \n \nWhich solution achieves these goals MOST efficiently?",
    "options": [
      {
        "id": 0,
        "text": "Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send",
        "correct": false
      },
      {
        "id": 3,
        "text": "Run a custom script on the instance operating system to send data to the audit system. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon EC2 Auto Scaling offers the ability to add lifecycle hooks to your Auto Scaling groups. These hooks let you create solutions that are aware of events in the Auto Scaling instance lifecycle, and then perform a custom action on instances when the corresponding lifecycle event occurs. https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to \nthe database come from serverless applications. Application traffic to the database changes \nsignificantly at random intervals. At limes of high demand, users report that their applications \nexperience database connection rejection errors. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n241 \n \nWhich solution will resolve this issue with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy Amazon ElastCache for Memcached between the users' application and the DB instance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Multi-AZ for the DB instance. Configure the users' application to switch between the DB",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nMany applications, including those built on modern serverless architectures, can have a large number of open connections to the database server and may open and close database connections at a high rate, exhausting database memory and compute resources. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. https://aws.amazon.com/pt/rds/proxy/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A solutions architect is designing the architecture for a software demonstration environment. The \nenvironment will run on Amazon EC2 instances in an Auto Scaling group behind an Application \nLoad Balancer (ALB). The system will experience significant increases in traffic during working \nhours but Is not required to operate on weekends. \n \nWhich combination of actions should the solutions architect take to ensure that the system can \nscale to meet demand? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Auto Scaling to adjust the ALB capacity based on request rate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company has deployed a serverless application that invokes an AWS Lambda function when \nnew documents are uploaded to an Amazon S3 bucket. The application uses the Lambda \nfunction to process the documents. After a recent marketing campaign, the company noticed that \nthe application did not process many of the documents. \n \nWhat should a solutions architect do to improve the architecture of this application?",
    "options": [
      {
        "id": 0,
        "text": "Set the Lambda function's runtime timeout value to 15 minutes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an S3 bucket replication policy. Stage the documents m the S3 bucket for later",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an additional Lambda function Load balance the processing of the documents across the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Queue Service (Amazon SOS) queue. Send the requests to the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo improve the architecture of this application, the best solution would be to use Amazon Simple Queue Service (Amazon SQS) to buffer the requests and decouple the S3 bucket from the Lambda function. This will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available. This will ensure that the documents are not lost and can be processed at a later time if the Lambda function is not available. By using Amazon SQS, the architecture is decoupled and the Lambda function can process the documents in a scalable and fault-tolerant manner.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 38,
    "text": "A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 \nand needs the required permissions to perform the task. The developer already has an IAM user \nwith valid IAM credentials required for Amazon S3. \n \nWhat should a solutions architect do to grant the permissions?",
    "options": [
      {
        "id": 0,
        "text": "Add required IAM permissions in the resource policy of the Lambda function.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a signed request using the existing IAM credentials in the Lambda function",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user and use the existing IAM credentials in the Lambda function.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM execution role with the required permissions and attach the IAM rote to the",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo grant the necessary permissions to an AWS Lambda function to upload files to Amazon S3, a solutions architect should create an IAM execution role with the required permissions and attach the IAM role to the Lambda function. This approach follows the principle of least privilege and ensures that the Lambda function can only access the resources it needs to perform its specific task.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company has a large dataset for its online advertising business stored in an Amazon RDS for \nMySQL DB instance in a single Availability Zone. The company wants business reporting queries \nto run without impacting the write operations to the production DB instance. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy RDS read replicas to process the business reporting queries.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Scale up the DB instance to a larger instance type to handle write operations and queries.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the OB distance in multiple Availability Zones to process the business reporting queries.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 40,
    "text": "A meteorological startup company has a custom web application to sell weather data to its users \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n243 \nonline. The company uses Amazon DynamoDB to store is data and wants to build a new service \nthat sends an alert to the managers of four Internal teams every time a new weather event is \nrecorded. The company does not want true new service to affect the performance of the current \napplication. \n \nWhat should a solutions architect do to meet these requirement with the LEAST amount of \noperational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use DynamoDB transactions to write new event data to the table. Configure the transactions to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Have the current application publish a message to four Amazon Simple Notification Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Amazon DynamoDB Streams on the table. Use triggers to write to a mingle Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Add a custom attribute to each record to flag new items. Write a cron job that scans the table",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe best solution to meet these requirements with the least amount of operational overhead is to enable Amazon DynamoDB Streams on the table and use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe. This solution requires minimal configuration and infrastructure setup, and Amazon DynamoDB Streams provide a low-latency way to capture changes to the DynamoDB table. The triggers automatically capture the changes and publish them to the SNS topic, which notifies the internal teams.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company is developing a real-time multiplayer game that uses UDP for communications \nbetween the client and servers. In an Auto Scaling group Spikes in demand are anticipated during \nthe day, so the game server platform must adapt accordingly. Developers want to store gamer \nscores and other non-relational data in a database solution that will scale without intervention. \n \nWhich solution should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Route 53 for traffic distribution and Amazon Aurora Serverless for data storage.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a Network Load Balancer for traffic distribution and Amazon Aurora Global Database for data",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Application Load Balancer for traffic distribution and Amazon DynamoDB global tables for",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nA Network Load Balancer can handle UDP traffic, and Amazon DynamoDB on-demand can provide automatic scaling without intervention.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host \na digital media streaming application. The EKS cluster will use a managed node group that is \nbacked by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must \nencrypt all data at rest by using a customer managed key that is stored in AWS Key Management \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n244 \nService (AWS KMS). \n \nWhich combination of actions will meet this requirement with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use a Kubernetes plugin that uses the customer managed key to perform data encryption.",
        "correct": false
      },
      {
        "id": 1,
        "text": "After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable EBS encryption by default in the AWS Region where the EKS cluster will be created.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create the EKS cluster. Create an IAM role that has a policy that grants permission to the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company has a web application with sporadic usage patterns. There is heavy usage at the \nbeginning of each month moderate usage at the start of each week and unpredictable usage \nduring the week. The application consists of a web server and a MySQL database server running \ninside the data center. The company would like to move the application to the AWS Cloud and \nneeds to select a cost-effective database platform that will not require database modifications. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon DynamoDB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon RDS for MySQL",
        "correct": false
      },
      {
        "id": 2,
        "text": "MySQL-compatible Amazon Aurora Serverless",
        "correct": true
      },
      {
        "id": 3,
        "text": "MySQL deployed on Amazon EC2 in an Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon RDS for MySQL is a fully-managed relational database service that makes it easy to set up, operate, and scale MySQL deployments in the cloud. Amazon Aurora Serverless is an on- demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible edition), where the database will automatically start up, shut down, and scale capacity up or down based on your application’s needs. It is a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company uses a payment processing system that requires messages for a particular payment \nID to be received in the same order that they were sent. Otherwise, the payments might be \nprocessed incorrectly. \n \nWhich actions should a solutions architect take to meet this requirement? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Write the messages to an Amazon DynamoDB table with the payment ID as the partition key",
        "correct": false
      },
      {
        "id": 1,
        "text": "Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "An IAM user made several configuration changes to AWS resources in their company's account \nduring a production deployment last week. A solutions architect learned that a couple of security \ngroup rules are not configured as desired. The solutions architect wants to confirm which IAM \nuser was responsible for making changes. \n \nWhich service should the solutions architect use to find the desired information?",
    "options": [
      {
        "id": 0,
        "text": "Amazon GuardDuty",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Inspector",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS CloudTrail",
        "correct": true
      },
      {
        "id": 3,
        "text": "AWS Config",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe best option is to use AWS CloudTrail to find the desired information. AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of AWS account activities. CloudTrail can be used to log all changes made to resources in an AWS account, including changes made by IAM users, EC2 instances, AWS management console, and other AWS services. By using CloudTrail, the solutions architect can identify the IAM user who made the configuration changes to the security group rules.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company runs a public three-Tier web application in a VPC. The application runs on Amazon \nEC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets \nneed to communicate with a license server over the internet. The company needs a managed \nsolution that minimizes operational maintenance. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Provision a NAT instance in a public subnet. Modify each private subnets route table with a",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision a NAT instance in a private subnet. Modify each private subnet's route table with a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAs the company needs a managed solution that minimizes operational maintenance - NAT Gateway is a public subnet is the answer.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n246 \nA company needs to transfer 600 TB of data from its on-premises network-attached storage \n(NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is \nsensitive and must be encrypted in transit. The company's internet connection can support an \nupload speed of 100 Mbps. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 multi-part upload functionality to transfer the fees over HTTPS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a VPN connection between the on-premises NAS system and the nearest AWS Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe best option is to use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices and use the devices to transfer the data to Amazon S3. Snowball Edge is a petabyte-scale data transfer device that can help transfer large amounts of data securely and quickly. Using Snowball Edge can be the most cost-effective solution for transferring large amounts of data over long distances and can help meet the requirement of transferring 600 TB of data within two weeks.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company needs a backup strategy for its three-tier stateless web application. The web \napplication runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling \npolicy that is configured to respond to scaling events. The database tier runs on Amazon RDS for \nPostgreSQL. The web application does not require temporary local storage on the EC2 instances. \nThe company’s recovery point objective (RPO) is 2 hours. \n \nThe backup strategy must maximize scalability and optimize resource utilization for this \nenvironment. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable",
        "correct": true
      },
      {
        "id": 3,
        "text": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe web application does not require temporary local storage on the EC2 instances => No EBS snapshot is required, retaining the latest AMI is enough.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 49,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n247 \nA company needs to ingest and handle large amounts of streaming data that its application \ngenerates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis \nData Streams, which is configured with default settings. Every other day, the application \nconsumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) \nprocessing. The company observes that Amazon S3 is not receiving all the data that the \napplication sends to Kinesis Data Streams. \n \nWhat should a solutions architect do to resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Update the Kinesis Data Streams default settings by modifying the data retention period.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Update the application to use the Kinesis Producer Library (KPL) lo send the data to Kinesis Data",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the number of Kinesis shards lo handle the throughput of me data that is sent to Kinesis",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html The\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 \ninstances runs several 1-hour tasks on a schedule. These tasks were written by different teams \nand have no common programming language. The company is concerned about performance \nand scalability while these tasks run on a single instance. A solutions architect needs to \nimplement a solution to resolve these concerns. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge",
        "correct": true
      },
      {
        "id": 1,
        "text": "Convert the EC2 instance to a container. Use AWS App Runner to create the container on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nLambda functions are short lived; the Lambda max timeout is 900 seconds (15 minutes). This can be difficult to manage and can cause issues in production applications. We'll take a look at AWS Lambda timeout limits, timeout errors, monitoring timeout errors, and how to apply best practices to handle them effectively.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "A company wants to migrate an Oracle database to AWS. The database consists of a single table \nthat contains millions of geographic information systems (GIS) images that are high resolution \nand are identified by a geographic code. When a natural disaster occurs tens of thousands of \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n248 \nimages get updated every few minutes. Each geographic code has a single image or row that is \nassociated with it. The company wants a solution that is highly available and scalable during such \nevents. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store the images and geographic codes in a database table. Use Oracle running on an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the images in Amazon S3 buckets Store geographic codes and image S3 URLs in a",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 52,
    "text": "A company has implemented a self-managed DNS service on AWS. The solution consists of the \nfollowing: \n \n- Amazon EC2 instances in different AWS Regions \n- Endpoints of a standard accelerator in AWS Global Accelerator \n \nThe company wants to protect the solution against DDoS attacks. \n \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Shield is a managed service that provides protection against Distributed Denial of Service (DDoS) attacks for applications running on AWS. AWS Shield Standard is automatically enabled to all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service. AWS Shield Advanced provides additional protections against more sophisticated and larger attacks for your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The \ncompany uses Amazon EC2 Windows Server instances behind an Application Load Balancer to \nhost its dynamic application. The company needs a highly available storage solution for the \napplication. The application consists of static files and dynamic server-side code. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.) \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n249",
    "options": [
      {
        "id": 0,
        "text": "Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows",
        "correct": false
      },
      {
        "id": 4,
        "text": "Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://www.techtarget.com/searchaws/tip/Amazon-FSx-vs-EFS-Compare-the-AWS-file-services FSx is built for high performance and submillisecond latency using solid-state drive storage volumes. This design enables users to select storage capacity and latency independently. Thus, even a subterabyte file system can have 256 Mbps or higher throughput and support volumes up to 64 TB.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company hosts a frontend application that uses an Amazon API Gateway API backend that is \nintegrated with AWS Lambda. When the API receives requests, the Lambda function loads many \nlibraries. Then the Lambda function connects to an Amazon RDS database, processes the data, \nand returns the data to the frontend application. The company wants to ensure that response \nlatency is as low as possible for all its users with the fewest number of changes to the company's \noperations. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Establish a connection between the frontend application and the database to make queries faster",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure provisioned concurrency for the Lambda function that handles the requests.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Increase the size of the database to increase the number of connections Lambda can establish at",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nConfigure provisioned concurrency for the Lambda function that handles the requests. Provisioned concurrency allows you to set the amount of compute resources that are available to the Lambda function, so that it can handle more requests at once and reduce latency. Caching the results of the queries in Amazon S3 could also help to reduce latency, but it would not be as effective as setting up provisioned concurrency. Increasing the size of the database would not help to reduce latency, as this would not increase the number of connections the Lambda function could establish, and establishing a direct connection between the frontend application and the database would bypass the API, which would not be the best solution either.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 56,
    "text": "A company is building a game system that needs to send unique events to separate leaderboard, \nmatchmaking, and authentication services concurrently. The company needs an AWS event-\ndriven system that guarantees the order of the events. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EventBridge event bus",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Simple Notification Service (Amazon SNS) FIFO topics",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Simple Notification Service (Amazon SNS) standard topics",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Simple Queue Service (Amazon SQS) FIFO queues",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A new employee has joined a company as a deployment engineer. The deployment engineer will \nbe using AWS CloudFormation templates to create multiple AWS resources. A solutions architect \nwants the deployment engineer to perform job activities while following the principle of least \nprivilege. \n \nWhich combination of actions should the solutions architect take to accomplish this goal? \n(Choose two.) \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n251",
    "options": [
      {
        "id": 0,
        "text": "Have the deployment engineer use AWS account root user credentials for performing AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has an",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create an IAM role for the deployment engineer to explicitly define the permissions specific to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 59,
    "text": "A company is implementing a shared storage solution for a gaming application that is hosted in \nthe AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution \nmust be fully managed. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS DataSync task that shares the data as a mountable file system. Mount the file",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company has a business system that generates hundreds of reports each day. The business \nsystem saves the reports to a network share in CSV format. The company needs to store this \ndata in the AWS Cloud in near-real time for analysis. \n \nWhich solution will meet these requirements with the LEAST administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 File Gateway. Update the business system to use a new network share",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/storagegateway/file/?nc1=h_ls Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 \nbuckets and is accessed with varying frequency. The company does not know access patterns for \nall the data. The company needs to implement a solution for each S3 bucket to optimize the cost \nof S3 usage. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use the S3 storage class analysis tool to determine the correct tier for each object in the S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://aws.amazon.com/s3/storage-classes/intelligent-tiering/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A solutions architect needs to allow team members to access Amazon S3 buckets in two different \nAWS accounts: a development account and a production account. The team currently has access \nto S3 buckets in the development account by using unique IAM users that are assigned to an IAM \ngroup that has appropriate permissions in the account. \n \nThe solutions architect has created an IAM role in the production account. The role has a policy \nthat grants access to an S3 bucket in the production account. \n \nWhich solution will meet these requirements while complying with the principle of least privilege?",
    "options": [
      {
        "id": 0,
        "text": "Attach the Administrator Access policy to the development account users.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add the development account as a principal in the trust policy of the role in the production",
        "correct": true
      },
      {
        "id": 2,
        "text": "Turn off the S3 Block Public Access feature on the S3 bucket in the production account.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a user in the production account with unique credentials for each team member.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy adding the development account as a principal in the trust policy of the IAM role in the production account, you are allowing users from the development account to assume the role in the production account. This allows the team members to access the S3 bucket in the production account without granting them unnecessary privileges.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 \nworkloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that \nprevents any resources from being created in any other Region. A security policy requires the \ncompany to encrypt all data at rest. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n253 \nAn audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) \nvolumes for EC2 instances without encrypting the volumes. The company wants any new EC2 \ninstances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS \nvolumes. The company wants a solution that will have minimal effect on employees who create \nEBS volumes. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "In the Amazon EC2 console, select the EBS encryption account attribute and define a default",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM permission boundary. Attach the permission boundary to the root organizational",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Update the IAM policies for each account to deny the ec2:CreateVolume action when the",
        "correct": false
      },
      {
        "id": 4,
        "text": "In the Organizations management account, specify the Default EBS volume encryption setting.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nSCP that denies the ec2:CreateVolume action when the ec2:Encrypted condition equals false. This will prevent users and service accounts in member accounts from creating unencrypted EBS volumes in the ap-southeast-2 Region.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming \ndatabase administrative tasks for production database workloads. The company wants to ensure \nthat its database is highly available and will provide automatic failover support in most scenarios \nin less than 40 seconds. The company wants to offload reads off of the primary instance and \nkeep costs as low as possible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.ht ml\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  }
]