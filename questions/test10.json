[
  {
    "id": 0,
    "text": "A company has a data ingestion workflow that includes the following components: \n \n- An Amazon Simple Notation Service (Amazon SNS) topic that receives \nnotifications about new data deliveries. \n- An AWS Lambda function that processes and stores the data \n \nThe ingestion workflow occasionally fails because of network connectivity issues.  \nWhen tenure occurs the corresponding data is not ingested unless the company manually reruns \nthe job. \n \nWhat should a solutions architect do to ensure that all notifications are eventually processed?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Lambda function for deployment across multiple Availability Zones",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify me Lambda functions configuration to increase the CPU and memory allocations tor the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the SNS topic's retry strategy to increase both the number of retries and the wait time",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on failure",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by introducing a queue (SQS) between the SNS topic and the Lambda function. When SNS publishes a message, it's placed in the SQS queue. The Lambda function then consumes messages from the queue. If the Lambda function fails to process a message due to a network issue, the message remains in the queue (or is returned to the queue if configured as such) and can be retried later. SQS provides built-in retry mechanisms and dead-letter queues for handling messages that cannot be processed after a certain number of attempts, ensuring no data is lost. This decouples the SNS notification from the Lambda processing, making the system more resilient to transient failures.\n\n**Why option 0 is incorrect:**\nConfiguring the Lambda function for deployment across multiple Availability Zones increases the availability of the Lambda function itself, protecting against AZ-level failures. However, it does not address the underlying issue of network connectivity problems preventing the Lambda function from processing the SNS notifications in the first place. The notification may still be lost before the Lambda function even attempts to process it.\n\n**Why option 1 is incorrect:**\nIncreasing the CPU and memory allocations for the Lambda function might help with performance issues or timeouts if they are the cause of the failures. However, the problem is specifically stated to be network connectivity issues. Increasing resources will not resolve intermittent network connectivity problems. This is a red herring.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company has a service that produces event data. The company wants to use AWS to process \nthe event data as it is received.  \nThe data is written in a specific order that must be maintained throughout processing.  \nThe company wants to implement a solution that minimizes operational overhead. \nHow should a solutions architect accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because SQS FIFO (First-In-First-Out) queues are designed to guarantee that messages are processed in the exact order they are sent. This directly addresses the requirement of maintaining the order of event data. SQS is a fully managed service, which minimizes operational overhead for the company. Standard queues do not guarantee order.\n\n**Why option 1 is incorrect:**\nThis is incorrect because SNS is a publish/subscribe service primarily used for broadcasting messages to multiple subscribers. It does not inherently guarantee message ordering, and it's not designed for processing data in a specific sequence. While SNS can trigger other services, it doesn't directly address the ordering requirement.\n\n**Why option 2 is incorrect:**\nThis is incorrect because SQS standard queues provide best-effort ordering, meaning messages might not always be delivered in the exact order they were sent. This violates the requirement of maintaining the order of event data. While SQS standard queues offer high throughput, they are not suitable when strict ordering is crucial.\n\n**Why option 3 is incorrect:**\nThis is incorrect because SNS is a publish/subscribe service primarily used for broadcasting messages to multiple subscribers. It does not inherently guarantee message ordering, and it's not designed for processing data in a specific sequence. While SNS can trigger other services, it doesn't directly address the ordering requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company is migrating an application from on-premises servers to Amazon EC2 instances. As \npart of the migration design requirements, a solutions architect must implement infrastructure \nmetric alarms. The company does not need to take action if CPU utilization increases to more \nthan 50% for a short burst of time. However, if the CPU utilization increases to more than 50% \nand read IOPS on the disk are high at the same time, the company needs to act as soon as \npossible. The solutions architect also must reduce false alarms. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon CloudWatch composite alarms where possible.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create single Amazon CloudWatch metric alarms with multiple metric thresholds where",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because CloudWatch composite alarms allow you to combine multiple metric alarms into a single alarm. This addresses the requirement to trigger an action only when both CPU utilization and disk read IOPS are high. By creating individual metric alarms for CPU utilization and disk read IOPS, and then combining them with a composite alarm using a logical AND condition, the company can ensure that an action is only triggered when both conditions are met. This reduces false alarms compared to triggering an alarm based on a single metric.\n\n**Why option 1 is incorrect:**\nThis is incorrect because CloudWatch dashboards are primarily for visualization and do not provide a mechanism for correlating multiple metrics to trigger an alarm. While dashboards are useful for monitoring, they don't automate the process of reacting to combined metric conditions. Humans would still need to manually observe the dashboard and react, which doesn't meet the 'act as soon as possible' requirement.\n\n**Why option 2 is incorrect:**\nThis is incorrect because CloudWatch Synthetics canaries are used to monitor the availability and performance of web applications and APIs from an end-user perspective. They are not designed for monitoring infrastructure metrics like CPU utilization and disk IOPS. While canaries could potentially indirectly detect issues related to high CPU and IOPS, they are not the appropriate tool for directly monitoring these metrics and correlating them to trigger alarms.\n\n**Why option 3 is incorrect:**\nThis is incorrect because single CloudWatch metric alarms can only monitor a single metric at a time. While you can set multiple thresholds for a single metric, you cannot correlate multiple metrics (CPU utilization AND disk read IOPS) within a single metric alarm. This option would not allow the company to trigger an action only when both conditions are met, leading to either missed alerts or false alarms.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "A company wants to migrate its on-premises data center to AWS. According to the company's \ncompliance requirements, the company can use only the ap-northeast-3 Region. Company \nadministrators are not permitted to connect VPCs to the internet. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Control Tower to implement data residency guardrails to deny internet access and",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use rules in AWS WAF to prevent internet access.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Config to activate managed rules to detect and alert for internet gateways and to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Control Tower allows you to implement guardrails that enforce compliance policies across your AWS environment. Specifically, you can use Control Tower to set up a data residency guardrail that restricts resource creation to the ap-northeast-3 Region. Additionally, Control Tower can enforce a guardrail that denies internet access by preventing the creation of internet gateways or restricting route table configurations.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS WAF is designed to protect web applications from common web exploits and does not directly control VPC-level internet access. While WAF can inspect traffic, it cannot prevent the creation of internet gateways or modify route tables to block internet access for the entire VPC.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while AWS Organizations SCPs can restrict actions within an organization, they cannot directly prevent VPCs from connecting to the internet. SCPs can limit the creation of internet gateways, but they do not inherently prevent existing VPCs from being configured with internet access through route tables. They also don't enforce region restrictions.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while network ACLs can deny outbound traffic to 0.0.0.0/0, this solution is not scalable or centrally managed. It requires manual configuration for each VPC and does not prevent the creation of internet gateways. Furthermore, it does not address the region restriction requirement.\n\n**Why option 4 is incorrect:**\nThis is incorrect because while AWS Config can detect and alert on the presence of internet gateways, it does not prevent their creation or automatically remediate the issue. It provides visibility but does not enforce the compliance requirement of preventing internet access.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company uses a three-tier web application to provide training to new employees. The \napplication is accessed for only 12 hours every day. The company is using an Amazon RDS for \nMySQL DB instance to store information and wants to minimize costs. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an IAM policy for AWS Systems Manager Session Manager.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create AWS Lambda functions to start and stop the DB instance.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of minimizing costs by stopping the RDS instance when it is not in use. AWS Lambda functions can be scheduled to start the RDS instance before the application is accessed and stop the RDS instance after the application is no longer needed. This will significantly reduce the cost of running the RDS instance, as you only pay for the time it is running. The Lambda functions can be triggered by CloudWatch Events (now EventBridge).\n\n**Why option 0 is incorrect:**\nIAM policies for Systems Manager Session Manager are used to control access to EC2 instances and other resources through the Systems Manager service. While Systems Manager can be used for various tasks, including automation, this option does not directly address the requirement of minimizing RDS costs by stopping and starting the instance. It focuses on access control, not cost optimization.\n\n**Why option 1 is incorrect:**\nElastiCache is a caching service that improves application performance by storing frequently accessed data in memory. While caching can reduce the load on the database, it does not directly address the requirement of minimizing costs by stopping the RDS instance when it is not in use. ElastiCache would still incur costs even when the application is not being accessed. Also, the question states that the application is accessed for only 12 hours every day, suggesting that the database is not heavily loaded and caching may not be the primary concern. Launching an EC2 instance is not directly related to minimizing the cost of the RDS instance. It doesn't address the core requirement of stopping and starting the database based on usage. An EC2 instance could potentially host a script to start/stop the RDS instance, but Lambda is a more cost-effective and serverless approach for this specific task.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 5,
    "text": "A company sells ringtones created from clips of popular songs. The files containing the ringtones \nare stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of \nfiles, but downloads are infrequent for ringtones older than 90 days. The company needs to save \nmoney on storage while keeping the most accessed files readily available for its users. \n \nWhich action should the company take to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using an S3 Lifecycle policy to automatically transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days. Since downloads are infrequent for older files, moving them to S3 Standard-IA significantly reduces storage costs. S3 Lifecycle policies automate this process, ensuring that the transition happens without manual intervention. S3 Standard-IA is suitable because the files are larger than 128KB, avoiding the minimum storage charge for smaller objects. This is the most direct and cost-effective way to manage the storage tiers based on access frequency.\n\n**Why option 0 is incorrect:**\nWhile S3 Standard-IA is a good choice for infrequently accessed data, configuring it as the initial storage tier doesn't address the requirement of keeping frequently accessed files readily available. All files, even the frequently accessed ones, would be stored in S3 Standard-IA from the beginning, which is not optimal. The problem statement indicates that recent files are accessed more frequently.\n\n**Why option 1 is incorrect:**\nS3 Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. While it could eventually move older files to a less expensive tier, it incurs a small monthly monitoring and automation charge per object. For millions of files, this charge could be significant and make it less cost-effective than a simple lifecycle policy. Also, the question is looking for the *most* cost-effective solution, and a lifecycle policy is generally cheaper when the access pattern is predictable (infrequent access after 90 days).\n\n**Why option 2 is incorrect:**\nS3 Inventory provides a scheduled CSV file output of your objects and their metadata. While it can be used to identify objects for lifecycle management, it doesn't directly move the objects to S3 Standard-IA. You would still need to implement a separate mechanism (like a script or another lifecycle policy) to move the files based on the inventory data. This adds complexity and cost compared to directly using an S3 Lifecycle policy.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company needs to save the results from a medical trial to an Amazon S3 repository. The \nrepository must allow a few scientists to add new files and must restrict all other users to read-\nonly access. No users can have the ability to modify or delete any files in the repository. The \ncompany must keep every file in the repository for a minimum of 1 year after its creation date. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n39",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Object Lock in governance mode with a legal hold of 1 year",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use S3 Object Lock in compliance mode with a retention period of 365 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket Use an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the S3 bucket to invoke an AWS Lambda function every tune an object is added",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of preventing modification or deletion of objects and ensuring a minimum retention period. S3 Object Lock in compliance mode prevents any user, including the root user, from deleting or modifying an object during the retention period. Setting the retention period to 365 days (1 year) ensures that the data is kept for the required duration. IAM policies can then be used to control who can upload new objects and who can only read existing objects.\n\n**Why option 0 is incorrect:**\nWhile S3 Object Lock in governance mode also prevents deletion or modification, it can be overridden by users with specific IAM permissions. The question specifies that *no* users should be able to modify or delete files, making compliance mode the more appropriate choice. Legal hold only prevents deletion until the hold is removed, it doesn't enforce a minimum retention period.\n\n**Why option 2 is incorrect:**\nUsing IAM roles alone can restrict users from deleting or changing objects, but it doesn't provide the immutability guarantee required to prevent accidental or malicious deletion or modification. IAM policies can be modified or deleted, potentially circumventing the protection. S3 Object Lock provides a stronger, more reliable mechanism for data retention and immutability.\n\n**Why option 3 is incorrect:**\nConfiguring an S3 bucket to invoke an AWS Lambda function every time an object is added doesn't directly address the requirements of access control or data retention. While a Lambda function could potentially be used to enforce some access control rules or trigger actions related to retention, it would be a more complex and less reliable solution than using S3 Object Lock and IAM policies directly. It also adds unnecessary operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A large media company hosts a web application on AWS. The company wants to start caching \nconfidential media files so that users around the world will have reliable access to the files. The \ncontent is stored in Amazon S3 buckets. The company must deliver the content quickly, \nregardless of where the requests originate geographically. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to connect the S3 buckets to the web application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Global Accelerator to connect the S3 buckets to the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using CloudFront, a content delivery network (CDN), to cache the media files at edge locations around the world. This ensures that users receive the content quickly, regardless of their geographic location. CloudFront integrates seamlessly with S3 as an origin, making it easy to serve content stored in S3. CloudFront also supports features for securing content, such as signed URLs and signed cookies, which are important for confidential media files.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because AWS DataSync is primarily used for data transfer between on-premises storage and AWS storage services, or between AWS storage services. It does not provide caching or content delivery capabilities to end users. It's not designed for serving content to users globally with low latency.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because AWS Global Accelerator improves the performance of TCP and UDP traffic by routing user traffic to the optimal AWS endpoint. While it can improve performance, it does not provide caching capabilities. It also doesn't directly serve content from S3. It would need to be used in conjunction with another service like CloudFront to meet the caching requirement. Global Accelerator is better suited for applications that need to be highly available and resilient to regional failures.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company produces batch data that comes from different databases. The company also \nproduces live stream data from network sensors and application APIs. The company needs to \nconsolidate all the data into one place for business analytics. The company needs to process the \nincoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-\ntime queries and import the data into a business intelligence tool to show key performance \nindicators (KPIs). \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Athena foe one-time queries.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Analytics for one-time queries.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create custom AWS Lambda functions to move the individual records from me databases to an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Glue extract transform, and toad (ETL) job to convert the data into JSON format.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon Athena allows you to run SQL queries directly against data stored in Amazon S3 without the need to manage any infrastructure. It's serverless, pay-per-query, and ideal for one-time or ad-hoc queries as required by the scenario. It eliminates the operational overhead of setting up and managing a database or data warehouse for these queries.\n\n**Why option 4 is correct:**\nThis is correct because AWS Lake Formation simplifies the process of setting up, securing, and managing data lakes. Using blueprints automates the discovery and ingestion of data from various sources into the data lake, reducing the manual effort and operational overhead associated with building a data lake from scratch. Blueprints can automatically crawl data sources, create tables in the AWS Glue Data Catalog, and move data into the appropriate S3 buckets, streamlining the data ingestion process.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon Kinesis Data Analytics is designed for real-time data processing and continuous analytics on streaming data. While it can process streaming data, it's not the best choice for one-time queries on data already staged in S3. Athena is more suitable for this purpose.\n\n**Why option 2 is incorrect:**\nThis is incorrect because creating custom AWS Lambda functions to move individual records from databases to S3 would introduce significant operational overhead. It would require writing, deploying, and maintaining complex code for each database source. AWS Glue provides a more managed and scalable solution for ETL processes.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while AWS Glue is a valid ETL tool, converting the data into JSON format is not explicitly required by the problem statement and might not be the most efficient format for all data types. Lake Formation blueprints can handle various data formats, and Athena can query data in different formats as well. Focusing solely on JSON conversion adds unnecessary complexity.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A gaming company has a web application that displays scores. The application runs on Amazon \nEC2 instances behind an Application Load Balancer. The application stores data in an Amazon \nRDS for MySQL database. Users are starting to experience long delays and interruptions that are \ncaused by database read performance. The company wants to improve the user experience while \nminimizing changes to the application's architecture. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache in front of the database.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use RDS Proxy between the application and the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application from EC2 instances to AWS Lambda.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon ElastiCache acts as an in-memory cache that sits in front of the database. By caching frequently accessed data, ElastiCache reduces the load on the RDS database, thereby improving read performance and reducing latency for users. This approach requires minimal changes to the application architecture, as the application can be configured to check the cache before querying the database.\n\n**Why option 1 is incorrect:**\nThis is incorrect because RDS Proxy primarily addresses connection management and pooling, which helps with write-heavy workloads or applications with many concurrent connections. While it can offer some performance benefits, it doesn't directly address read performance bottlenecks in the same way as caching. The problem is specifically stated as read performance issues.\n\n**Why option 2 is incorrect:**\nThis is incorrect because migrating the application to AWS Lambda would involve a significant architectural change, requiring rewriting the application to be serverless and potentially impacting existing functionality. This contradicts the requirement to minimize changes to the application's architecture. Furthermore, Lambda doesn't inherently solve the database read performance issue.\n\n**Why option 3 is incorrect:**\nThis is incorrect because migrating the database to Amazon DynamoDB would involve a significant architectural change, requiring rewriting the data access layer of the application to interact with a NoSQL database. This contradicts the requirement to minimize changes to the application's architecture. Additionally, while DynamoDB can offer high performance, it requires a different data modeling approach and might not be suitable for all types of data or queries.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A business's backup data totals 700 terabytes (TB) and is kept in network attached storage \n(NAS) at its data center. This backup data must be available in the event of occasional regulatory \ninquiries and preserved for a period of seven years. The organization has chosen to relocate its \nbackup data from its on-premises data center to Amazon Web Services (AWS). Within one \nmonth, the migration must be completed. The company's public internet connection provides 500 \nMbps of dedicated capacity for data transport. \n \nWhat should a solutions architect do to ensure that data is migrated and stored at the LOWEST \npossible cost?",
    "options": [
      {
        "id": 0,
        "text": "Order AWS Snowball devices to transfer the data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy a VPN connection between the data center and Amazon VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-effective solution for transferring a large amount of data (700 TB) given the limited bandwidth (500 Mbps) and the one-month deadline. Transferring 700 TB over a 500 Mbps connection would take significantly longer than one month. AWS Snowball devices are designed for transferring large datasets offline, and their cost is generally lower than the cost of maintaining a dedicated high-bandwidth connection like Direct Connect for a one-time data transfer. Storing the data in S3 Glacier or S3 Glacier Deep Archive after the transfer would be the most cost-effective storage option for long-term archival with infrequent access.\n\n**Why option 1 is incorrect:**\nWhile a VPN connection would allow data transfer, the limited 500 Mbps bandwidth would make transferring 700 TB within one month impractical. Furthermore, maintaining a VPN connection incurs ongoing costs, making it less cost-effective than using Snowball for a one-time migration.\n\n**Why option 2 is incorrect:**\nProvisioning a 500 Mbps AWS Direct Connect connection would be expensive, especially for a one-time data migration. The cost of Direct Connect includes port fees, data transfer fees, and potentially cross-connect fees. It's also an overkill, as the existing 500 Mbps internet connection is already available. Snowball is a more cost-effective option for transferring large amounts of data when bandwidth is limited.\n\n**Why option 3 is incorrect:**\nAWS DataSync is designed for ongoing data synchronization, not a one-time bulk migration of this scale. While it can handle large datasets, transferring 700 TB over a 500 Mbps connection using DataSync would still take a very long time and incur data transfer costs. The cost of running DataSync agents and the time required for the transfer make it less cost-effective than using Snowball for this scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 11,
    "text": "A company wants to direct its users to a backup static error page if the company's primary \nwebsite is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The \ndomain is pointing to an Application Load Balancer (ALB). The company needs a solution that \nminimizes changes and infrastructure overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the Route 53 records to use a latency routing policy.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Route 53 active-passive failover configuration.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the Route 53 records to use a multivalue answer routing policy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by configuring Route 53 to monitor the health of the ALB. If the ALB becomes unhealthy (e.g., unresponsive), Route 53 will automatically switch the DNS record to point to the backup static error page. This approach minimizes changes because it leverages Route 53's built-in health checks and failover capabilities. It also minimizes infrastructure overhead because it doesn't require additional servers or complex configurations beyond setting up the health check and the backup page's DNS record.\n\n**Why option 0 is incorrect:**\nLatency routing policy directs traffic to the resource with the lowest latency. While it can improve performance, it doesn't provide automatic failover to a backup page in case of primary website unavailability. It focuses on optimizing user experience based on latency, not ensuring high availability with a failover mechanism.\n\n**Why option 2 is incorrect:**\nActive-active configuration distributes traffic across multiple resources simultaneously. While this improves availability, it doesn't directly address the requirement of displaying a backup static error page when the primary website is completely unavailable. It assumes both resources are always serving the same content, not a backup page. Setting up an EC2 instance adds unnecessary infrastructure overhead, which the question specifically aims to avoid.\n\n**Why option 3 is incorrect:**\nMultivalue answer routing policy returns multiple IP addresses in response to a DNS query. While this can improve availability by distributing traffic across multiple resources, it doesn't guarantee that users will be directed to a backup page if the primary website is unavailable. It relies on the client to choose a working IP address, and if all addresses are unavailable, the user will still experience an error. It also doesn't provide a health check mechanism to automatically switch to a backup page.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 12,
    "text": "A corporation has recruited a new cloud engineer who should not have access to the \nCompanyConfidential Amazon S3 bucket. The cloud engineer must have read and write \npermissions on an S3 bucket named AdminTools. \n \nWhich IAM policy will satisfy these criteria? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n42",
    "options": [
      {
        "id": 0,
        "text": "B.",
        "correct": true
      },
      {
        "id": 2,
        "text": "D.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis policy correctly addresses the requirements. It grants the necessary 's3:GetObject', 's3:PutObject', 's3:ListBucket' permissions on the 'AdminTools' bucket, allowing the engineer to read, write, and list the bucket's contents. Crucially, it includes an explicit 'Deny' statement for all S3 actions ('s3:*') on the 'CompanyConfidential' bucket. This explicit denial ensures that even if other policies were to grant access to the 'CompanyConfidential' bucket, this policy would override them due to the explicit deny taking precedence.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A new employee has joined a company as a deployment engineer. The deployment engineer will \nbe using AWS CloudFormation templates to create multiple AWS resources.  \nA solutions architect wants the deployment engineer to perform job activities while following the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n44 \nprinciple of least privilege. \n \nWhich steps should the solutions architect do in conjunction to reach this goal? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Have the deployment engineer use AWS account roof user credentials for performing AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM User for the deployment engineer and add the IAM user to a group that has",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create an IAM role for the deployment engineer to explicitly define the permissions specific to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because creating a new IAM user and adding them to a group with specific permissions is a fundamental way to implement least privilege. By carefully defining the permissions granted to the group, the deployment engineer can only access the resources and actions required for their CloudFormation deployments. This avoids granting broad, unnecessary access that could lead to security vulnerabilities.\n\n**Why option 4 is correct:**\nThis is correct because creating an IAM role allows for temporary and specific permissions to be granted. The deployment engineer can assume this role to perform specific actions related to CloudFormation deployments. This further restricts the engineer's access to only the necessary permissions for the duration of the role's session, enhancing security and adhering to the principle of least privilege.\n\n**Why option 0 is incorrect:**\nThis is incorrect because using the root user credentials is a major security risk. The root user has unrestricted access to all AWS resources and services within the account. Sharing or using root user credentials violates the principle of least privilege and can lead to accidental or malicious damage to the AWS environment.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while creating an IAM user and adding them to a group is a step in the right direction, the question states that the group should have 'unrestricted access' to AWS resources. This directly contradicts the principle of least privilege, as the engineer would have more permissions than necessary.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company runs a high performance computing (HPC) workload on AWS. The workload required \nlow-latency network performance and high network throughput with tightly coupled node-to-node \ncommunication. The Amazon EC2 instances are properly sized for compute and storage \ncapacity, and are launched using default options. \n \nWhat should a solutions architect propose to improve the performance of the workload?",
    "options": [
      {
        "id": 0,
        "text": "Choose a cluster placement group while launching Amazon EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Choose dedicated instance tenancy while launching Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Choose an Elastic Inference accelerator while launching Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Choose the required capacity reservation while launching Amazon EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because cluster placement groups are specifically designed to provide low-latency, high-throughput network connectivity between instances within the group. They achieve this by placing instances close to each other within the same Availability Zone, minimizing network latency and maximizing bandwidth for inter-instance communication, which is critical for tightly coupled HPC workloads.\n\n**Why option 1 is incorrect:**\nThis is incorrect because dedicated instance tenancy provides hardware isolation at the host level, but it does not directly address the low-latency, high-throughput network requirements of the HPC workload. While it might offer some performance benefits due to reduced resource contention, it's not the primary solution for optimizing network performance for tightly coupled communication.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Elastic Inference accelerators are used to accelerate deep learning inference workloads, not general-purpose HPC workloads. They are designed to offload the computational burden of inference from the CPU or GPU, but they do not improve network latency or throughput between instances.\n\n**Why option 3 is incorrect:**\nThis is incorrect because capacity reservations ensure that you have sufficient EC2 capacity available when you need it. While important for availability and preventing capacity constraints, it doesn't directly improve the network performance or latency between instances, which is the primary concern for the HPC workload.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 15,
    "text": "A company wants to use the AWS Cloud to make an existing application highly available and \nresilient. The current version of the application resides in the company's data center. The \napplication recently experienced data loss after a database server crashed because of an \nunexpected power outage. The company needs a solution that avoids any single points of failure. \nThe solution must give the application the ability to scale to meet user demand. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirements by distributing application servers across multiple Availability Zones within a region. Using an Auto Scaling group ensures that the application can automatically scale up or down based on demand, maintaining performance and availability. Placing instances across multiple Availability Zones eliminates a single point of failure, as the application can continue to operate even if one Availability Zone experiences an outage. The database component, while not explicitly mentioned in this option, is implicitly understood to also be deployed in a highly available configuration (e.g., using RDS Multi-AZ) to fully address the data loss concern.\n\n**Why option 1 is incorrect:**\nDeploying application servers in a single Availability Zone does not address the requirement of avoiding single points of failure. If that Availability Zone experiences an outage, the entire application will become unavailable. While Auto Scaling provides scalability, it doesn't provide high availability in this scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not specify the database component. While deploying application servers across multiple Availability Zones with Auto Scaling is a good start, the question mentions data loss due to a database server crash. A complete solution must also address the database's high availability and resilience. Without a highly available database solution, the application remains vulnerable to data loss and downtime.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not specify the database component. While deploying application servers across multiple Availability Zones with Auto Scaling is a good start, the question mentions data loss due to a database server crash. A complete solution must also address the database's high availability and resilience. Without a highly available database solution, the application remains vulnerable to data loss and downtime.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto \nScaling group in the AWS Cloud. The application will transmit data by using UDP packets. The \ncompany wants to ensure that the application can scale out and in as traffic increases and \ndecreases. What should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Attach a Network Load Balancer to the Auto Scaling group",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an Application Load Balancer to the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Network Load Balancers (NLBs) are designed to handle UDP traffic efficiently. NLBs operate at Layer 4 of the OSI model and can forward UDP packets directly to the EC2 instances in the Auto Scaling group. They also provide high throughput and low latency, which are crucial for gaming applications. NLBs are also capable of handling the dynamic IP addresses of instances launched by the Auto Scaling group, ensuring traffic is routed correctly as the application scales out and in.\n\n**Why option 1 is incorrect:**\nApplication Load Balancers (ALBs) operate at Layer 7 of the OSI model and primarily handle HTTP/HTTPS traffic. They do not support UDP traffic, making them unsuitable for this gaming application.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 with a weighted policy is primarily used for DNS-based routing and does not provide the real-time traffic distribution and health checks necessary for a dynamically scaling application. While it can distribute traffic across multiple endpoints, it doesn't offer the same level of responsiveness and granularity as a load balancer for handling traffic fluctuations within an Auto Scaling group.\n\n**Why option 3 is incorrect:**\nUsing a NAT instance for port forwarding is not a scalable or highly available solution. A single NAT instance can become a bottleneck and a single point of failure. It also requires manual configuration and maintenance, which is not ideal for an Auto Scaling environment where instances are dynamically launched and terminated.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A solutions architect is designing a customer-facing application for a company. The application's \ndatabase will have a clearly defined access pattern throughout the year and will have a variable \nnumber of reads and writes that depend on the time of year. The company must retain audit \nrecords for the database for 7 days. The recovery point objective (RPO) must be less than 5 \nhours. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB with auto scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Redshift. Configure concurrency scaling.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon RDS with Provisioned IOPS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Aurora MySQL with auto scaling.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement for variable read/write capacity by utilizing concurrency scaling. Amazon Redshift's concurrency scaling automatically adds query processing power when needed to handle spikes in user activity. Redshift also supports audit logging, which can be configured to retain logs for the required 7 days. Furthermore, Redshift snapshots can be used to meet the RPO requirement of less than 5 hours.\n\n**Why option 0 is incorrect:**\nWhile DynamoDB with auto scaling can handle variable read/write capacity, it's primarily a NoSQL database and might not be suitable if the application requires complex SQL queries or relational database features. Also, meeting the audit requirements and RPO might require additional configuration and management compared to Redshift.\n\n**Why option 2 is incorrect:**\nAmazon RDS with Provisioned IOPS can handle variable read/write capacity to some extent, but it might not scale as efficiently as Redshift's concurrency scaling for handling large spikes in user activity. While RDS supports auditing and backups for RPO, the concurrency scaling feature of Redshift makes it a better fit for the variable workload described in the question.\n\n**Why option 3 is incorrect:**\nAmazon Aurora MySQL with auto scaling can handle variable read/write capacity. However, Redshift is designed for analytical workloads and provides built-in features like concurrency scaling that are more suitable for handling large spikes in user activity and complex queries, especially when combined with the audit and RPO requirements.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company hosts a two-tier application on Amazon EC2 instances and Amazon RDS. The \napplication's demand varies based on the time of day. The load is minimal after work hours and \non weekends. The EC2 instances run in an EC2 Auto Scaling group that is configured with a \nminimum of two instances and a maximum of five instances. The application must be available at \nall times, but the company is concerned about overall cost. \n \nWhich solution meets the availability requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use all EC2 Spot Instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase EC2 Instance Savings Plans to cover five EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase two EC2 Reserved Instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase EC2 Instance Savings Plans to cover two EC2 instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by providing a cost-effective way to cover the base load of two EC2 instances. Since the Auto Scaling group always maintains at least two instances for availability, purchasing EC2 Instance Savings Plans for these two instances guarantees a significant discount compared to On-Demand pricing. This optimizes cost during periods of low demand without compromising availability. The remaining instances, which are scaled up during peak demand, can be covered by On-Demand pricing or potentially Spot Instances (although Spot Instances are not guaranteed). Savings Plans are a good balance between cost savings and commitment.\n\n**Why option 0 is incorrect:**\nUsing only EC2 Spot Instances is risky for a system that requires constant availability. Spot Instances can be terminated with a short notice if the Spot price exceeds the bid price, potentially leading to application downtime. While Spot Instances can be cost-effective, they are not suitable for the minimum two instances required for constant availability.\n\n**Why option 1 is incorrect:**\nPurchasing EC2 Instance Savings Plans to cover five EC2 instances is not the most cost-effective solution. The Auto Scaling group only requires a minimum of two instances. Paying for Savings Plans for all five instances, even during periods of low demand when only two instances are running, would result in unnecessary costs. This option doesn't optimize for the variable demand pattern.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 19,
    "text": "A company has an ecommerce checkout workflow that writes an order to a database and calls a \nservice to process the payment. Users are experiencing timeouts during the checkout process.  \nWhen users resubmit the checkout form, multiple unique orders are created for the same desired \ntransaction. \n \nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?",
    "options": [
      {
        "id": 0,
        "text": "Configure the web application to send an order message to Amazon Kinesis Data Firehose.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the order in the database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the order in the database.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by implementing an idempotency key. Storing the order in the database allows for the implementation of a check to see if an order with the same unique identifier (e.g., order ID, user ID + timestamp) already exists. If an order with the same identifier is found, the system can return the existing order instead of creating a new one. This ensures that even if the user resubmits the checkout form due to a timeout, only one order is created in the database.\n\n**Why option 0 is incorrect:**\nSending an order message to Amazon Kinesis Data Firehose is primarily for data streaming and analytics, not for preventing duplicate order creation. While Firehose can be used for data ingestion and processing, it doesn't inherently provide idempotency or prevent duplicate records from being created in the first place. It would require additional logic downstream to handle duplicates, making it a less direct and efficient solution than storing the order in a database with idempotency checks.\n\n**Why option 1 is incorrect:**\nCreating a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged is not the correct approach to prevent duplicate order creation. CloudTrail logs API calls made to AWS services; it doesn't directly interact with the application's checkout workflow or database. While CloudTrail can be useful for auditing and monitoring, it's not designed to prevent duplicate transactions. Triggering a Lambda function from CloudTrail logs would be a reactive approach and would not prevent the initial creation of the duplicate order. Furthermore, relying on CloudTrail logs introduces latency and complexity that are unnecessary for solving this problem.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A company is planning to build a high performance computing (HPC) workload as a service \nsolution that Is hosted on AWS.  \nA group of 16 AmazonEC2Ltnux Instances requires the lowest possible latency for node-to-node \ncommunication.  \nThe instances also need a shared block device volume for high-performing storage. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a cluster placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use a cluster placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a partition placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a spread placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses both requirements. Cluster placement groups are designed to minimize latency between instances within the group, which is crucial for HPC workloads. EBS Multi-Attach allows a single EBS volume to be attached to multiple instances simultaneously, providing a shared block storage solution. Provisioned IOPS SSD EBS volumes offer high performance, suitable for demanding workloads.\n\n**Why option 1 is incorrect:**\nWhile a cluster placement group correctly addresses the low latency requirement, Amazon EFS is a network file system and generally does not provide the same level of performance as a block storage solution like EBS, especially for HPC workloads requiring high IOPS and low latency. EFS is also not a block device volume.\n\n**Why option 2 is incorrect:**\nPartition placement groups are designed to distribute instances across different partitions to reduce the risk of correlated failures. They do not focus on minimizing latency between instances. Also, Amazon EFS is a network file system and generally does not provide the same level of performance as a block storage solution like EBS, especially for HPC workloads requiring high IOPS and low latency. EFS is also not a block device volume.\n\n**Why option 3 is incorrect:**\nSpread placement groups are designed to distribute instances across distinct underlying hardware to maximize availability. They do not focus on minimizing latency between instances. While EBS Multi-Attach can provide shared block storage, the spread placement group does not meet the low latency requirement for node-to-node communication.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "A company has an event-driven application that invokes AWS Lambda functions up to 800 times \neach minute with varying runtimes.  \nThe Lambda functions access data that is stored in an Amazon Aurora MySQL OB cluster.  \nThe company is noticing connection timeouts as user activity increases The database shows no \nsigns of being overloaded. CPU, memory, and disk access metrics are all low.  \nWhich solution will resolve this issue with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Adjust the size of the Aurora MySQL nodes to handle more connections.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up Amazon ElastiCache tor Redls to cache commonly read items from the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Aurora Replica as a reader node.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ROS Proxy to create a proxy.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the problem of connection exhaustion by pooling and multiplexing database connections. RDS Proxy sits between the Lambda functions and the Aurora database, reducing the number of direct connections to the database. It maintains a pool of connections and reuses them across multiple Lambda invocations. This significantly reduces the load on the database's connection management and prevents connection timeouts, especially with bursty traffic from Lambda. It also provides connection failover capabilities and security enhancements, all with minimal operational overhead since it's a managed service.\n\n**Why option 0 is incorrect:**\nIncreasing the size of the Aurora MySQL nodes might increase the maximum number of connections the database can handle, but it's an expensive solution and doesn't directly address the underlying problem of connection management. It also requires more operational overhead to manage the larger instance. The problem isn't CPU, memory, or disk I/O, so scaling up the instance size is not the most efficient solution.\n\n**Why option 1 is incorrect:**\nWhile caching commonly read items can reduce the load on the database, it doesn't directly address the connection timeout issue. The problem is not slow queries or high database load in general, but rather the inability to establish connections. Caching would only help if the connection timeouts were caused by slow queries consuming all available connections, but the question states the database is not overloaded. It also adds operational overhead to manage the cache.\n\n**Why option 2 is incorrect:**\nAdding an Aurora Replica as a reader node will offload read traffic from the primary instance, but it doesn't solve the connection timeout issue. Lambda functions still need to establish connections to the database, and the replica won't reduce the number of connections required. The problem is the sheer number of connections being opened, not the read load on the primary instance. While it might improve read performance, it doesn't address the root cause of the connection timeouts.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is building a containerized application on premises and decides to move the \napplication to AWS.  \nThe application will have thousands of users soon after li is deployed.  \nThe company Is unsure how to manage the deployment of containers at scale. The company \nneeds to deploy the containerized application in a highly available architecture that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store container images In an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store container images in a repository that runs on an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirements effectively. Storing container images in Amazon ECR provides a secure and scalable repository. Using Amazon ECS with the Fargate launch type eliminates the need to manage the underlying EC2 instances, minimizing operational overhead. Fargate provides built-in high availability and handles the scaling of the infrastructure. Target tracking allows ECS to automatically scale the number of tasks based on a specified metric (e.g., CPU utilization), ensuring the application can handle the expected load and scale as needed.\n\n**Why option 1 is incorrect:**\nWhile storing container images in ECR and using target tracking for scaling are good practices, using the EC2 launch type for ECS introduces operational overhead. The company would be responsible for managing the EC2 instances, including patching, scaling, and ensuring high availability. This contradicts the requirement to minimize operational overhead.\n\n**Why option 2 is incorrect:**\nStoring container images on an EC2 instance introduces significant operational overhead and is not a highly available or scalable solution. Managing a container registry on EC2 requires manual configuration, patching, and scaling, which is contrary to the requirement of minimizing operational overhead. It also lacks the built-in redundancy and availability of a managed service like ECR.\n\n**Why option 3 is incorrect:**\nCreating an AMI with the container image is not a standard or efficient way to deploy containerized applications. AMIs are typically used for deploying virtual machines, not containers. This approach does not leverage the benefits of containerization, such as portability and scalability, and it adds unnecessary complexity to the deployment process. It also doesn't address the requirement for high availability or automatic scaling.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A company's application Is having performance issues. The application staleful and needs to \ncomplete m-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation \nto deploy infrastructure and used the M5 EC2 Instance family. As traffic increased, the application \nperformance degraded. Users are reporting delays when the users attempt to access the \napplication.  \nWhich solution will resolve these issues in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Replace the EC2 Instances with T3 EC2 instances that run in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the performance issues by upgrading to R5 instances, which are memory-optimized, and by implementing comprehensive monitoring. R5 instances provide more memory per vCPU than M5 instances, which is crucial for memory-intensive applications. Using the CloudWatch agent to generate custom application latency metrics provides detailed insights into application performance, enabling proactive capacity planning and troubleshooting. This approach provides the necessary resources and monitoring for optimal performance and operational efficiency.\n\n**Why option 0 is incorrect:**\nT3 instances are burstable instances and are not suitable for memory-intensive workloads. They are designed for applications with low to moderate CPU utilization and may not provide consistent performance under heavy load. Replacing M5 with T3 would likely exacerbate the performance issues. Also, while an Auto Scaling group would help with scaling, the underlying instance type is not appropriate.\n\n**Why option 1 is incorrect:**\nWhile modifying the CloudFormation template to run the EC2 instances in an Auto Scaling group would improve scalability and availability, it doesn't address the underlying memory constraints causing the performance degradation. The application is memory-intensive, and simply scaling the existing M5 instances might not be sufficient to resolve the performance issues. The root cause is the lack of sufficient memory, not the lack of scaling capabilities.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "An ecommerce company has an order-processing application that uses Amazon API Gateway \nand an AWS Lambda function.  \nThe application stores data in an Amazon Aurora PostgreSQL database.  \nDuring a recent sales event, a sudden surge in customer orders occurred.  \nSome customers experienced timeouts and the application did not process the orders of those \ncustomers.  \nA solutions architect determined that the CPU utilization and memory utilization were high on the \ndatabase because of a large number of open connections.  \nThe solutions architect needs to prevent the timeout errors while making the least possible \nchanges to the application. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure provisioned concurrency for the Lambda function.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS Proxy to create a proxy for the database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a read replica for the database in a different AWS Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the problem of excessive database connections. Amazon RDS Proxy sits between the application and the database, pooling and sharing database connections. This reduces the number of direct connections to the database, lowering CPU and memory utilization and preventing timeouts caused by connection exhaustion. It requires minimal changes to the application, as it primarily involves updating the database connection string to point to the RDS Proxy endpoint.\n\n**Why option 0 is incorrect:**\nConfiguring provisioned concurrency for the Lambda function will increase the number of Lambda instances available to handle requests. While this might help with the initial surge of requests, it will likely exacerbate the database connection problem. More Lambda instances will lead to even more database connections, further stressing the database and increasing the likelihood of timeouts. It doesn't address the root cause of the problem, which is the database connection limit.\n\n**Why option 2 is incorrect:**\nCreating a read replica in a different AWS Region will not solve the problem of high CPU and memory utilization on the primary database due to a large number of open connections. Read replicas are primarily used for offloading read traffic, but the order-processing application likely involves write operations to the database. The write operations will still be directed to the primary database, which will continue to experience high utilization and timeouts. This solution also involves more significant infrastructure changes than necessary.\n\n**Why option 3 is incorrect:**\nMigrating the data from Aurora PostgreSQL to Amazon DynamoDB is a significant change to the application architecture. While DynamoDB is a highly scalable NoSQL database, it requires substantial code modifications to adapt the application to the new database. This option violates the requirement of making the least possible changes to the application. Furthermore, the migration process itself can be complex and time-consuming, and it might not be the most appropriate solution for all types of data and queries.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company runs a global web application on Amazon EC2 instances behind an Application Load \nBalancer. \nThe application stores data in Amazon Aurora.  \nThe company needs to create a disaster recovery solution and can tolerate up to 30 minutes of \ndowntime and potential data loss.  \nThe solution does not need to handle the load when the primary infrastructure is healthy. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n50 \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application with the required infrastructure elements in place.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Host a scaled-down deployment of the application in a second AWS Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replicate the primary infrastructure in a second AWS Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Back up data with AWS Backup.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by having all the necessary infrastructure components (EC2, ALB, Aurora) pre-configured in a secondary region. This allows for a faster failover because the infrastructure is already in place. While it doesn't explicitly mention data replication, Aurora provides options for cross-region read replicas that can be promoted to a write instance in case of a disaster. This approach balances cost-effectiveness with the RTO and RPO requirements, as it avoids the cost of running a scaled-down or replicated environment continuously.\n\n**Why option 1 is incorrect:**\nWhile a scaled-down deployment in a second region offers faster failover than option 3, it incurs higher costs because resources are constantly running. The question states the solution does not need to handle the load when the primary infrastructure is healthy, making this option less cost-effective.\n\n**Why option 2 is incorrect:**\nReplicating the primary infrastructure in a second AWS Region is the most expensive option. It involves running a full copy of the application and database in another region, which is unnecessary given the requirement that the DR solution does not need to handle the load when the primary infrastructure is healthy. This approach is overkill for the stated RTO and RPO.\n\n**Why option 3 is incorrect:**\nBacking up data with AWS Backup is a good practice, but it doesn't address the RTO requirement of 30 minutes. Restoring from backups can take longer than 30 minutes, and it doesn't include the time needed to provision the infrastructure. While backups are essential for data recovery, they are insufficient as a standalone DR solution in this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company wants to measure the effectiveness of its recent marketing campaigns.  \nThe company performs batch processing on csv files of sales data and stores the results in an \nAmazon S3 bucket once every hour.  \nThe S3 bipetabytes of objects. The company runs one-time queries in Amazon Athena to \ndetermine which products are most popular on a particular date for a particular region Queries \nsometimes fail or take longer than expected to finish.  \nWhich actions should a solutions architect take to improve the query performance and reliability? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Reduce the S3 object sizes to less than 126 MB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Partition the data by date and region in Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the files as large, single objects in Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Kinosis Data Analytics to run the Queries as pan of the batch processing operation",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use an AWS Glue extract, transform, and load (ETL) process to convert the csv files into Apache Parquet format.",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because storing data as large, single objects in S3 is generally more efficient for Athena. Athena works best with larger files as it reduces the overhead of processing numerous small files. While there's a balance to be struck to avoid excessively large files that might hinder parallel processing, the question implies the current file sizes are causing issues, suggesting they are too small. By consolidating smaller files into larger objects, Athena can process the data more efficiently, improving query performance and reducing the number of S3 requests.\n\n**Why option 4 is correct:**\nThis is correct because converting the CSV files to Apache Parquet format significantly improves Athena query performance. Parquet is a columnar storage format, which allows Athena to read only the necessary columns for a query, reducing the amount of data scanned. This leads to faster query execution and lower costs. Additionally, Parquet supports compression, further reducing storage costs and improving I/O performance. Using AWS Glue ETL to perform this conversion is a suitable approach.\n\n**Why option 0 is incorrect:**\nThis is incorrect because reducing S3 object sizes further would likely worsen the performance of Athena queries. Athena performs better with fewer, larger files than with many small files, as it reduces the overhead of processing each individual file. Reducing object sizes to less than 126 MB would increase the number of files and the associated overhead.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while partitioning data by date and region is a good practice for optimizing Athena queries, it doesn't directly address the immediate performance and reliability issues described in the scenario. Partitioning helps Athena to scan only the relevant data based on the query's WHERE clause, but it doesn't solve the underlying problem of inefficient data format (CSV) and potentially too many small files. While beneficial, it's not the most impactful solution compared to converting to Parquet and optimizing file sizes.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 27,
    "text": "A company is running several business applications in three separate VPCs within the us-east-1 \nRegion. The applications must be able to communicate between VPCs. The applications also \nmust be able to consistently send hundreds of gigabytes of data each day to a latency-sensitive \napplication that runs in a single on- premises data center. \nA solutions architect needs to design a network connectivity solution that maximizes cost-\neffectiveness. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure three AWS Site-to-Site VPN connections from the data center to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch a third-party virtual network appliance in each VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up three AWS Direct Connect connections from the data center to a Direct Connect",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up one AWS Direct Connect connection from the data center to AWS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by providing a dedicated, private network connection between the on-premises data center and AWS. A single Direct Connect connection can be used to access multiple VPCs through the use of Virtual Private Gateways (VGWs) and Direct Connect Gateways (DXGWs). This is more cost-effective than multiple Site-to-Site VPN connections or multiple Direct Connect connections. It also provides lower latency and higher bandwidth compared to VPN, which is crucial for the latency-sensitive application and the large data transfer volume. Using a Direct Connect Gateway allows the single Direct Connect connection to be shared across multiple VPCs in different AWS accounts or regions, further enhancing cost-effectiveness and scalability.\n\n**Why option 0 is incorrect:**\nWhile Site-to-Site VPN connections can provide connectivity between the data center and AWS, using three separate VPN connections would be less cost-effective than a single Direct Connect connection, especially considering the high bandwidth requirements. VPN connections also have higher latency compared to Direct Connect, which is not suitable for the latency-sensitive application. The overhead of managing and maintaining three separate VPN connections would also increase operational complexity.\n\n**Why option 1 is incorrect:**\nLaunching a third-party virtual network appliance in each VPC would primarily address inter-VPC communication but would not directly solve the connectivity to the on-premises data center. It would also add significant cost and complexity by requiring the management and maintenance of three separate virtual appliances. This option doesn't address the high bandwidth and low latency requirements for communication with the on-premises data center. Furthermore, it doesn't provide a cost-effective solution for connecting to the on-premises data center.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "An online photo application lets users upload photos and perform image editing operations. The \napplication offers two classes of service free and paid Photos submitted by paid users are \nprocessed before those submitted by free users Photos are uploaded to Amazon S3 and the job \ninformation is sent to Amazon SQS. \nWhich configuration should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use one SQS FIFO queue.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use two SQS FIFO queues: one for paid and one for free.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use two SQS standard queues one for paid and one for free.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use one SQS standard queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement of prioritizing paid users. By using two separate SQS standard queues, one for paid users and one for free users, the application can consume messages from the paid queue first. This ensures that paid users' image processing jobs are processed before free users' jobs. Standard queues provide high throughput and are suitable for this scenario where strict ordering within each class of service is not explicitly required, only prioritization between the two classes.\n\n**Why option 0 is incorrect:**\nUsing a single SQS FIFO queue would not allow for prioritization of paid users. FIFO queues guarantee order of messages, but do not inherently provide a mechanism to prioritize certain messages over others. All messages would be processed in the order they were received, regardless of whether they were submitted by paid or free users.\n\n**Why option 1 is incorrect:**\nUsing two SQS FIFO queues (one for paid and one for free) does not directly address the prioritization requirement in the most efficient manner. While it separates the messages, the application would still need to actively manage which queue to consume from first. Furthermore, FIFO queues have lower throughput compared to standard queues, which might become a bottleneck if the application experiences high traffic. The problem requires prioritization between two classes of users, not strict ordering within each class, making standard queues a better fit.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 29,
    "text": "A company hosts its product information webpages on AWS. The existing solution uses multiple \nAmazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. The \nwebsite also uses a custom DNS name and communicates with HTTPS only using a dedicated \nSSL certificate. The company is planning a new product launch and wants to be sure that users \nfrom around the world have the best possible experience on the new website. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Redesign the application to use Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "Redesign the application to use AWS Elastic Beanstalk",
        "correct": false
      },
      {
        "id": 2,
        "text": "Redesign the application to use a Network Load Balancer.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Redesign the application to use Amazon S3 static website hosting",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon CloudFront is a content delivery network (CDN) service that caches content at edge locations around the world. By caching the website's content closer to users, CloudFront reduces latency and improves website loading times, providing a better user experience, especially for users geographically distant from the origin server. CloudFront also integrates seamlessly with AWS services like ALB and supports custom DNS names and HTTPS using SSL certificates. It also provides DDoS protection and can handle spikes in traffic during a product launch.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Elastic Beanstalk is a platform-as-a-service (PaaS) that simplifies the deployment and management of web applications. While it can be used to deploy the application, it doesn't inherently address the requirement of providing the best possible experience for users around the world. It doesn't provide content caching at edge locations like CloudFront, so it won't significantly reduce latency for geographically distant users. It's more about simplifying deployment than optimizing global performance.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a Network Load Balancer (NLB) is primarily used for TCP and UDP traffic and is designed for high performance and low latency. While it can handle high traffic volumes, it doesn't provide content caching or distribution like a CDN. It won't improve the user experience for geographically dispersed users as effectively as CloudFront. The existing ALB already provides load balancing at the application layer. Switching to an NLB doesn't address the global performance requirement.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon S3 static website hosting is suitable for serving static content like HTML, CSS, and JavaScript files. While it's cost-effective, it doesn't provide the dynamic content delivery capabilities required for a full-fledged web application. The question mentions product information webpages, which likely involve some dynamic content or interaction. Furthermore, S3 alone doesn't provide the global content distribution and caching benefits of a CDN like CloudFront, which is crucial for optimizing the user experience for a global audience.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company has 150 TB of archived image data stored on-premises that needs to be moved to the \nAWS Cloud within the next month. \nThe company's current network connection allows up to 100 Mbps uploads for this purpose \nduring the night only. \nWhat is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowmobile to ship the data to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order multiple AWS Snowball devices to ship the data to AWS.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Amazon S3 Transfer Acceleration and securely upload the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by providing a physical data transfer mechanism that bypasses the network bandwidth limitations. Given the 150 TB data volume and the limited 100 Mbps upload speed available only during the night, transferring the data over the network within one month is highly unlikely and would be very time consuming. Using multiple Snowball devices allows for parallel data transfer, significantly reducing the overall migration time compared to relying solely on the network. Snowball is also more cost-effective than Snowmobile for this data volume.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Snowmobile is designed for much larger data volumes (petabytes) and is significantly more expensive than Snowball. For 150 TB, using Snowmobile would be an overkill and not cost-effective.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because Amazon S3 Transfer Acceleration optimizes network transfers to S3 but is still limited by the available bandwidth. With only 100 Mbps available during the night, transferring 150 TB within a month would be extremely difficult, even with Transfer Acceleration. The network bandwidth is the bottleneck, and Transfer Acceleration cannot overcome this limitation. It also adds to the cost.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because creating an S3 VPC endpoint and establishing a VPN connection does not address the fundamental bandwidth limitation. While a VPN provides a secure connection, it doesn't increase the available bandwidth. The 100 Mbps constraint remains, making it impractical to transfer 150 TB within the specified timeframe. This option also adds complexity and cost without solving the core problem.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "A company hosts its web application on AWS using seven Amazon EC2 instances. The company \nrequires that the IP addresses of all healthy EC2 instances be returned in response to DNS \nqueries. Which policy should be used to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Simple routing policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Latency routing policy",
        "correct": false
      },
      {
        "id": 2,
        "text": "Multivalue routing policy",
        "correct": true
      },
      {
        "id": 3,
        "text": "Geolocation routing policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Multivalue answer routing policy allows you to configure Route 53 to return multiple values, such as IP addresses for your web servers, in response to DNS queries. Route 53 returns up to eight healthy records for each query. It also supports health checks, ensuring that only the IP addresses of healthy instances are returned. This directly addresses the requirement of returning IP addresses of all healthy EC2 instances.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Simple routing policy returns a single resource record set. While you can specify multiple values in a single record (e.g., multiple IP addresses in an A record), Route 53 will return *all* of those IPs regardless of their health. It doesn't provide health checking capabilities to return only healthy instances.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Latency routing policy routes traffic to the resource that provides the lowest latency to the user. While it considers health checks, it's designed to route traffic to the *best* single instance based on latency, not to return the IP addresses of *all* healthy instances. It aims for optimal performance for a single connection, not providing a list of all healthy IPs.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "A company wants to use AWS Systems Manager to manage a fleet of Amazon EC2 instances. \nAccording to the company's security requirements, no EC2 instances can have internet access. A \nsolutions architect needs to design network connectivity from the EC2 instances to Systems \nManager while fulfilling this security obligation. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the EC2 instances into a private subnet with no route to the internet.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an interface VPC endpoint for Systems Manager.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy a NAT gateway into a public subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an internet gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating a private connection between the EC2 instances and the Systems Manager service. An interface VPC endpoint for Systems Manager allows traffic to reach Systems Manager without traversing the internet. This ensures that the EC2 instances remain isolated within the private subnet and adhere to the security requirement of no internet access. The VPC endpoint provides a secure and private connection, leveraging the AWS network infrastructure.\n\n**Why option 0 is incorrect:**\nWhile deploying EC2 instances into a private subnet with no route to the internet is a good starting point for security, it doesn't, by itself, establish connectivity to Systems Manager. Without a mechanism to reach Systems Manager, the instances cannot be managed. This option only addresses part of the problem, the 'no internet access' part, but not the 'manage EC2 instances using Systems Manager' part.\n\n**Why option 2 is incorrect:**\nDeploying a NAT gateway into a public subnet provides outbound internet access for instances in the private subnet. This directly violates the security requirement of no internet access for the EC2 instances. While a NAT Gateway allows instances in the private subnet to initiate outbound traffic to the internet, it's not a secure or recommended solution when the requirement is to avoid internet access altogether.\n\n**Why option 3 is incorrect:**\nDeploying an internet gateway provides direct internet access to the EC2 instances, which is in direct contradiction to the security requirement of no internet access. An internet gateway enables instances to communicate directly with the public internet, making it an unsuitable solution for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company needs to build a reporting solution on AWS. The solution must support SQL queries \nthat data analysts run on the data. \nThe data analysts will run lower than 10 total queries each day. The company generates 3 GB of \nnew data daily in an on-premises relational database. This data needs to be transferred to AWS \nto perform reporting tasks. \nWhat should a solutions architect recommend to meet these requirements at the LOWEST cost?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Kinesis Data Firehose delivery stream to deliver the data into an Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Export a daily copy of the data from the on-premises database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of transferring data from the on-premises database to AWS for reporting. Exporting a daily copy of the data is a simple and cost-effective method for a small data volume of 3 GB. This eliminates the need for continuous replication services like DMS, which incur ongoing costs. The exported data can then be loaded into a suitable AWS service for querying, such as S3 and Athena, which are cost-effective for low query frequency.\n\n**Why option 0 is incorrect:**\nUsing AWS Database Migration Service (DMS) for continuous replication is an overkill and more expensive than necessary for only 3 GB of data per day and less than 10 queries. DMS is designed for continuous data replication and migration, which is not required in this scenario. The ongoing costs associated with DMS, including the replication instance and storage, would be higher than a simple daily export.\n\n**Why option 1 is incorrect:**\nAmazon Kinesis Data Firehose is designed for real-time data streaming and ingestion. It's not cost-effective for a small, daily batch of 3 GB of data. Firehose incurs costs based on the amount of data ingested and transformed, and it's optimized for high-velocity data streams, which are not present in this scenario. Furthermore, it doesn't directly support SQL queries on the raw data without additional processing and storage.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "A company wants to monitor its AWS costs for financial review. The cloud operations team is \ndesigning an architecture in the AWS Organizations management account to query AWS Cost \nand Usage Reports for all member accounts. \nThe team must run this query once a month and provide a detailed analysis of the bill. \nWhich solution is the MOST scalable and cost-effective way to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3. Use Amazon Athena for analysis.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3. Use Amazon Redshift for analysis.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis. Use Amazon QuickSight for analysis.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by enabling Cost and Usage Reports (CUR) for each member account and delivering them to a centralized Amazon S3 bucket. Amazon Redshift is then used for analysis. This approach is scalable because CUR can handle large volumes of data. Redshift is suitable for complex analytical queries on large datasets, and since the analysis is only performed monthly, Redshift can be scaled down or even paused when not in use, making it cost-effective. Centralizing the reports in S3 simplifies access and management for the cloud operations team.\n\n**Why option 0 is incorrect:**\nWhile Kinesis can handle streaming data, it's not the most appropriate choice for monthly cost analysis. Kinesis is designed for real-time data processing, which is unnecessary in this scenario. Using EMR for monthly analysis would likely be more expensive than using Redshift, especially considering the infrequent nature of the analysis. EMR clusters require more overhead and configuration compared to Redshift's managed service.\n\n**Why option 1 is incorrect:**\nEnabling Cost and Usage Reports in the management account will only provide consolidated billing data, not detailed cost information for each member account. Athena is a good option for querying data in S3, but it is not as performant as Redshift for complex analytical queries on large datasets. While Athena is cost-effective for ad-hoc queries, Redshift is often more efficient for recurring, complex analyses like the monthly billing review.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company collects data for temperature, humidity, and atmospheric pressure in cities across \nmultiple continents. The average volume of data that the company collects from each site daily is \n500 GB. Each site has a high-speed Internet connection. \nThe company wants to aggregate the data from all these global sites as quickly as possible in a \nsingle Amazon S3 bucket. The solution must minimize operational complexity. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Turn on S3 Transfer Acceleration on the destination S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Upload the data from each site to an S3 bucket in the closest Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the data from each site to an Amazon EC2 instance in the closest Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because S3 Transfer Acceleration utilizes Amazon CloudFront's globally distributed edge locations to optimize data transfer to S3. Data is routed to the nearest edge location, which then uses optimized network paths to upload the data to the destination S3 bucket. This significantly improves upload speeds, especially for geographically dispersed locations, while requiring minimal configuration and operational overhead. It directly addresses the requirements of fast aggregation and minimal operational complexity.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while uploading to the closest region might seem intuitive, it adds operational complexity. It would require setting up and managing multiple S3 buckets in different regions. Then, a separate process would be needed to consolidate the data from these regional buckets into a single destination bucket. This increases management overhead and doesn't necessarily guarantee the fastest transfer to the final destination. The question specifically asks for a solution that minimizes operational complexity.\n\n**Why option 2 is incorrect:**\nThis is incorrect because using AWS Snowball Edge is not suitable for daily data transfers when a high-speed internet connection is available. Snowball Edge is designed for situations where network bandwidth is limited or unreliable. Transferring 500GB daily via Snowball Edge would involve significant logistical overhead (shipping devices, manual data transfer, etc.), making it operationally complex and slow compared to using the existing high-speed internet connections.\n\n**Why option 3 is incorrect:**\nThis is incorrect because using EC2 instances as intermediaries adds significant operational overhead. It would require provisioning and managing EC2 instances in multiple regions, configuring them to receive data, and then transferring that data to the destination S3 bucket. This adds complexity in terms of instance management, security, and data transfer orchestration. Furthermore, it doesn't inherently guarantee faster transfer speeds compared to S3 Transfer Acceleration, and it's more expensive and complex to manage.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A company needs the ability to analyze the log files of its proprietary application. The logs are \nstored in JSON format in an Amazon S3 bucket Queries will be simple and will run on-demand. \nA solutions architect needs to perform the analysis with minimal changes to the existing \narchitecture. \nWhat should the solutions architect do to meet these requirements with the LEAST amount of \noperational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch Logs to store the logs",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Athena directly with Amazon S3 to run the queries as needed",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue to catalog the logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution directly addresses the requirement of querying JSON logs stored in S3 with minimal operational overhead. Amazon Athena is a serverless query service that allows you to analyze data directly in S3 using standard SQL. It requires minimal setup and no data warehousing, making it ideal for on-demand, simple queries on existing data in S3.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because loading data into Amazon Redshift involves significant operational overhead, including managing a Redshift cluster, defining schemas, and performing ETL processes. This adds unnecessary complexity and cost for simple, on-demand queries. It also requires data migration, which the question aims to avoid.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because while CloudWatch Logs is a good service for collecting and monitoring logs, it's not the best choice for analyzing existing logs stored in S3. It would require migrating the logs from S3 to CloudWatch Logs, adding operational overhead and not directly addressing the requirement of analyzing logs already in S3. Also, CloudWatch Logs Insights is better suited for operational monitoring than ad-hoc analysis of JSON data.\n\n**Why option 3 is incorrect:**\nThis option is incomplete. While AWS Glue can catalog the logs, it doesn't provide a direct querying capability. Glue is primarily used for ETL and metadata management. You would still need another service to actually query the data after it's cataloged. Athena uses the Glue catalog, but Athena alone provides the complete solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company uses AWS Organizations to manage multiple AWS accounts for different \ndepartments. The management account has an Amazon S3 bucket that contains project reports. \nThe company wants to limit access to this S3 bucket to only users of accounts within the \norganization in AWS Organizations. \nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an organizational unit (OU) for each department.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization,",
        "correct": false
      },
      {
        "id": 3,
        "text": "Tag each user that needs access to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution directly addresses the requirement by using the `aws:PrincipalOrgID` global condition key in the S3 bucket policy. This condition key allows you to specify that only principals (users, roles) belonging to a specific AWS Organization ID are allowed to access the S3 bucket. This approach minimizes operational overhead because it's a simple and direct configuration within the bucket policy, avoiding the need for complex IAM role setups, tagging, or monitoring solutions. It leverages the built-in AWS Organizations integration with IAM policies.\n\n**Why option 1 is incorrect:**\nCreating OUs for each department, while useful for organizational structure and applying policies at the OU level, doesn't directly restrict access to the S3 bucket based on organization membership. You would still need to configure IAM roles and policies for each OU, which increases operational overhead compared to using the `aws:PrincipalOrgID` condition key. This approach is more about managing permissions within departments rather than restricting access based on the organization as a whole.\n\n**Why option 2 is incorrect:**\nUsing AWS CloudTrail to monitor `CreateAccount` and `InviteAccountToOrganization` events is relevant for auditing and tracking changes to the organization structure, but it doesn't directly address the requirement of restricting access to the S3 bucket. CloudTrail provides visibility into account creation and invitation activities, but it doesn't enforce access control policies on S3 buckets. This option is about monitoring, not access control.\n\n**Why option 3 is incorrect:**\nTagging users that need access to the S3 bucket is a valid approach for managing permissions, but it requires more operational overhead than using the `aws:PrincipalOrgID` condition key. You would need to ensure that all users are correctly tagged and that the S3 bucket policy is updated whenever users are added or removed. This approach is more complex and error-prone compared to the direct organization-based access control provided by `aws:PrincipalOrgID`.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "An application runs on an Amazon EC2 instance in a VPC. The application processes logs that \nare stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without \nconnectivity to the internet. \nWhich solution will provide private network connectivity to Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint to the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an instance profile on Amazon EC2 to allow S3 access.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon API Gateway API with a private link to access the S3 endpoint.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by creating a gateway VPC endpoint for S3. Gateway endpoints allow instances in a VPC to access S3 using AWS's private network, without traversing the internet. This provides secure and private connectivity.\n\n**Why option 1 is incorrect:**\nThis option involves streaming logs to CloudWatch Logs and then exporting them to S3. While it achieves the goal of getting the logs to S3, it doesn't directly address the requirement of private network connectivity for the EC2 instance's initial access to S3. The EC2 instance would still need a way to send the logs to CloudWatch, which might involve internet access or other network configurations. Also, this adds unnecessary complexity.\n\n**Why option 2 is incorrect:**\nAn instance profile grants the EC2 instance permissions to access S3, but it doesn't provide private network connectivity. The instance still needs a network path to reach S3, and without a VPC endpoint, that path would likely involve the internet. The instance profile only handles authorization, not networking.\n\n**Why option 3 is incorrect:**\nWhile API Gateway with a private link can provide private connectivity to services, it's an overkill solution for simply accessing S3 from an EC2 instance within the same VPC. A gateway VPC endpoint is a much simpler and more cost-effective solution for this specific scenario. API Gateway is more suitable when you need to expose S3 data through an API with additional features like authentication, request transformation, and rate limiting, which are not required in this case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is hosting a web application on AWS using a single Amazon EC2 instance that stores \nuser-uploaded documents in an Amazon EBS volume. For better scalability and availability, the \ncompany duplicated the architecture and created a second EC2 instance and EBS volume in \nanother Availability Zone, placing both behind an Application Load Balancer. After completing this \nchange, users reported that, each time they refreshed the website, they could see one subset of \ntheir documents or the other, but never all of the documents at the same time. \nWhat should a solutions architect propose to ensure users see all of their documents at once?",
    "options": [
      {
        "id": 0,
        "text": "Copy the data so both EBS volumes contain all the documents.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Application Load Balancer to direct a user to the server with the documents",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the data from both EBS volumes to Amazon EFS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the Application Load Balancer to send the request to both servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon EFS (Elastic File System) provides a shared file system that can be mounted by multiple EC2 instances simultaneously. By copying the data from both EBS volumes to EFS, both EC2 instances will have access to the same set of documents, ensuring that users see all of their documents regardless of which instance handles their request. EFS is designed for this type of shared storage scenario and provides the necessary consistency and availability.\n\n**Why option 0 is incorrect:**\nThis is incorrect because simply copying the data so both EBS volumes contain all the documents is a one-time solution and does not address the ongoing data synchronization issue. Any new documents uploaded to one instance will not be automatically replicated to the other, leading to the same problem in the future. Furthermore, managing data consistency between two EBS volumes manually is complex and error-prone.\n\n**Why option 1 is incorrect:**\nThis is incorrect because the Application Load Balancer is designed to distribute traffic based on factors like load and availability, not based on the data stored on the backend instances. Trying to configure the ALB to direct a user to a specific server based on the documents they need would be complex, inefficient, and would violate the principle of load balancing. It would also introduce significant latency and potential single points of failure.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company uses NFS to store large video files in on-premises network attached storage. Each \nvideo file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer \ngrowing. The company decides to migrate the video files to Amazon S3. The company must \nmigrate the video files as soon as possible while using the least possible network bandwidth. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Snowball Edge job.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy an S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an AWS Direct Connect connection between the on-premises network and AWS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by physically shipping the data to AWS. Given the 70 TB data volume and the constraint of minimizing network bandwidth, using AWS Snowball Edge is the most efficient way to transfer the data. It avoids consuming on-premises network bandwidth during the migration process and allows for a faster transfer compared to transferring over a network connection.\n\n**Why option 0 is incorrect:**\nCreating an S3 bucket is a necessary first step, but it doesn't address the data migration itself. It only provides a destination for the data. It doesn't solve the problem of minimizing network bandwidth usage during the transfer of 70 TB of data.\n\n**Why option 2 is incorrect:**\nS3 File Gateway is designed for hybrid cloud storage, allowing on-premises applications to access S3 as a file share. While it can be used to migrate data, it relies on the existing network connection to transfer the data. Given the large data volume (70 TB) and the requirement to minimize network bandwidth usage, S3 File Gateway is not the optimal solution. It would take a significant amount of time and consume considerable network bandwidth.\n\n**Why option 3 is incorrect:**\nSetting up an AWS Direct Connect connection provides a dedicated network connection between the on-premises network and AWS. While it offers more bandwidth than a standard internet connection, it's still a network-based solution. Given the 70 TB data volume and the requirement to minimize network bandwidth usage during the initial migration, AWS Snowball Edge is a faster and more efficient solution. Direct Connect is more suitable for ongoing, smaller data transfers after the initial migration.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company has an application that ingests incoming messages. Dozens of other applications and \nmicroservices then quickly consume these messages. The number of messages varies drastically \nand sometimes increases suddenly to 100,000 each second. The company wants to decouple the \nsolution and increase scalability. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Persist the messages to Amazon Kinesis Data Analytics.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write the messages to Amazon Kinesis Data Streams with a single shard.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using Amazon SNS, a highly scalable publish/subscribe messaging service. SNS allows the ingestion application to publish messages to a topic, and multiple applications and microservices can subscribe to that topic to receive the messages. This decouples the ingestion application from the consumers, as it doesn't need to know about them directly. SNS handles the distribution of messages to all subscribers, and it can automatically scale to handle the high message volumes described in the scenario. The publish/subscribe pattern is ideal for distributing messages to multiple consumers.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon Kinesis Data Analytics is primarily used for processing streaming data in real-time. While it can ingest data, it's not the best choice for simply distributing messages to multiple consumers. It's more suited for complex data transformations and analysis, which isn't the primary requirement here. Furthermore, persisting all messages to Kinesis Data Analytics before consumption adds unnecessary latency and complexity.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while Auto Scaling can help scale the ingestion application, it doesn't address the decoupling requirement. The EC2 instances would still need to know about all the consumers and how to distribute the messages to them. This creates a tight coupling between the ingestion application and the consumers, making it difficult to scale and maintain the system. Scaling the ingestion application alone doesn't solve the problem of distributing messages to many consumers efficiently.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a single shard in Kinesis Data Streams would quickly become a bottleneck when the message rate increases to 100,000 messages per second. Kinesis Data Streams requires proper shard allocation to handle high throughput, and a single shard would not be sufficient. While Kinesis Data Streams provides decoupling, it requires more configuration and management of shards to handle the variable message volumes effectively. SNS is a simpler and more scalable solution for this particular scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company is migrating a distributed application to AWS. The application serves variable \nworkloads. The legacy platform consists of a primary server that coordinates jobs across multiple \ncompute nodes. The company wants to modernize the application with a solution that maximizes \nresiliency and scalability. \nHow should a solutions architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement the primary server and the compute nodes with Amazon EC2 instances that are",
        "correct": false
      },
      {
        "id": 3,
        "text": "implement the primary server and the compute nodes with Amazon EC2 instances that are",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by decoupling the primary server from the compute nodes using an SQS queue. The primary server can enqueue jobs into the SQS queue, and the compute nodes can independently dequeue and process these jobs. This decoupling allows the compute nodes to scale independently based on the workload in the queue. If a compute node fails, other nodes can continue processing jobs from the queue, enhancing resiliency. SQS also provides built-in features for handling message retries and dead-letter queues, further improving reliability.\n\n**Why option 0 is incorrect:**\nThis option is incomplete. While using SQS is a good starting point, simply configuring an SQS queue as a destination without specifying what is being sent to the queue or how the compute nodes will interact with it doesn't provide a complete solution. The primary server needs to enqueue jobs, and the compute nodes need to dequeue them for processing. This option lacks that crucial detail.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A company is running an SMB file server in its data center. The file server stores large files that \nare accessed frequently for the first few days after the files are created. After 7 days the files are \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n60 \nrarely accessed. \n \nThe total data size is increasing and is close to the company's total storage capacity. A solutions \narchitect must increase the company's available storage space without losing low-latency access \nto the most recently accessed files. The solutions architect must also provide file lifecycle \nmanagement to avoid future storage issues. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 File Gateway to extend the company's storage space.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon FSx for Windows File Server file system to extend the company's storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Install a utility on each user's computer to access Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing a hybrid cloud storage solution. Amazon S3 File Gateway allows the company to seamlessly extend its on-premises storage to Amazon S3. The most recently accessed files are cached locally for low-latency access, while older, less frequently accessed files are stored in S3, reducing the storage burden on the on-premises file server. File Gateway also supports lifecycle policies to automatically transition data to different S3 storage classes based on age, providing lifecycle management.\n\n**Why option 0 is incorrect:**\nUsing AWS DataSync to copy data older than 7 days to AWS would require a separate process to manage the data and wouldn't provide seamless access to the archived files. Users would need to know where the files are located (on-premises or in S3) and potentially use different tools to access them. It doesn't provide a transparent extension of the existing file server.\n\n**Why option 2 is incorrect:**\nCreating an Amazon FSx for Windows File Server file system would provide a fully managed Windows file server in AWS, but it wouldn't directly address the need to extend the existing on-premises file server. It would require migrating data and potentially changing user workflows. It doesn't directly integrate with the existing on-premises SMB server.\n\n**Why option 3 is incorrect:**\nInstalling a utility on each user's computer to access Amazon S3 would be a complex and inefficient solution. It would require significant configuration and training for users. It also wouldn't provide a seamless extension of the existing file server and wouldn't address the need for low-latency access to recent files. Users would need to manage file access and storage locations manually.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company is building an ecommerce web application on AWS. The application sends \ninformation about new orders to an Amazon API Gateway REST API to process. The company \nwants to ensure that orders are processed in the order that they are received. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an API Gateway integration to publish a message to an Amazon Simple Notification",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an API Gateway authorizer to block any requests while the application processes an order.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of ordered processing by leveraging Amazon Simple Queue Service (SQS). SQS FIFO (First-In-First-Out) queues guarantee that messages are delivered and processed in the exact order they are sent. Integrating API Gateway with SQS allows the application to reliably enqueue order information, ensuring that the processing backend consumes and processes orders in the correct sequence. This approach also decouples the API layer from the order processing logic, improving resilience and scalability.\n\n**Why option 0 is incorrect:**\nUsing Amazon Simple Notification Service (SNS) is not suitable for ordered processing. SNS is a publish/subscribe service designed for broadcasting messages to multiple subscribers. It does not guarantee message order or provide a queuing mechanism for reliable, ordered processing. While SNS can be used to notify multiple systems about a new order, it doesn't address the core requirement of processing orders in the order they were received.\n\n**Why option 2 is incorrect:**\nUsing an API Gateway authorizer to block requests while processing an order is an inefficient and impractical solution. This approach would serialize all requests, severely limiting the application's throughput and availability. It would create a single point of failure and would not scale effectively. API Gateway authorizers are primarily designed for authentication and authorization, not for managing order processing concurrency.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora \ndatabase. The EC2 instances connect to the database by using user names and passwords that \nare stored locally in a file. The company wants to minimize the operational overhead of credential \nmanagement. \nWhat should a solutions architect do to accomplish this goal? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n61",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Systems Manager Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket lo store objects that are encrypted with an AWS Key.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume or each EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Secrets Manager is specifically designed to manage secrets, including database credentials. It allows you to store, rotate, and retrieve secrets securely. The EC2 instances can retrieve the credentials programmatically from Secrets Manager, eliminating the need to store them locally and automating the credential management process. Secrets Manager also provides auditing and versioning capabilities, enhancing security and compliance.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while AWS Systems Manager Parameter Store can store secrets, it's primarily designed for configuration data and not specifically optimized for managing database credentials with features like automatic rotation and auditing that Secrets Manager provides. Using Parameter Store for database credentials requires more manual configuration and doesn't offer the same level of security and management capabilities as Secrets Manager.\n\n**Why option 2 is incorrect:**\nThis is incorrect because storing encrypted objects in S3 would require the EC2 instances to have the necessary permissions to access and decrypt the objects. This adds complexity to the solution and doesn't directly address the credential management issue. The EC2 instances would still need to manage the decryption key, which could become another secret management problem. This solution is not as streamlined or secure as using Secrets Manager.\n\n**Why option 3 is incorrect:**\nThis is incorrect because encrypting the EBS volume only protects the data at rest on the volume. It does not address the problem of managing database credentials. The credentials would still need to be stored somewhere on the EC2 instance, even if the volume is encrypted, and the operational overhead of managing those credentials would remain.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 46,
    "text": "A global company hosts its web application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The web application has static data and dynamic data. The company \nstores its static data in an Amazon S3 bucket. The company wants to improve performance and \nreduce latency for the static data and dynamic data. The company is using its own domain name \nregistered with Amazon Route 53. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution that has the ALB as an origin.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution that has the ALB as an origin.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most effective solution because it leverages CloudFront's caching capabilities for both static and dynamic content. By configuring both the S3 bucket and the ALB as origins, CloudFront can cache static content directly from S3, reducing the load on the ALB and improving performance. For dynamic content, CloudFront can cache the responses from the ALB, further reducing latency and improving the user experience. This approach provides a comprehensive solution for optimizing the delivery of both types of content globally.\n\n**Why option 1 is incorrect:**\nThis option only addresses the dynamic content served by the ALB. While it would improve performance for dynamic content, it completely ignores the static content stored in S3. The static content would not be cached by CloudFront, leading to suboptimal performance for those assets.\n\n**Why option 2 is incorrect:**\nThis option only addresses the static content stored in S3. While it would improve performance for static content, it completely ignores the dynamic content served by the ALB. The dynamic content would not be cached by CloudFront, leading to suboptimal performance for those requests.\n\n**Why option 3 is incorrect:**\nThis option is identical to option 1 and therefore incorrect for the same reasons. It only addresses the dynamic content served by the ALB and ignores the static content in S3.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance \nactivities, the company needs to rotate the credentials tor its Amazon ROS tor MySQL databases \nacross multiple AWS Regions. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the credentials as secrets in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the credentials as secrets in AWS Systems Manager by creating a secure string",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Secrets Manager is designed specifically for managing secrets, including database credentials. It offers built-in rotation capabilities for RDS databases, including MySQL, which significantly reduces operational overhead. Secrets Manager can also replicate secrets across multiple AWS Regions, fulfilling the multi-region requirement. The automated rotation feature minimizes manual intervention and reduces the risk of human error during the maintenance window.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while AWS Systems Manager Parameter Store (Secure Strings) can store secrets, it doesn't offer built-in rotation capabilities for RDS databases. Implementing rotation with Parameter Store would require custom scripting and automation, increasing operational overhead compared to Secrets Manager. Also, while Parameter Store supports hierarchical paths, replicating secrets across regions requires additional configuration and management, making it less efficient than Secrets Manager's built-in replication.\n\n**Why option 2 is incorrect:**\nThis is incorrect because storing credentials directly in an S3 bucket, even with server-side encryption, is not a secure or recommended practice for managing database credentials. It lacks built-in rotation capabilities and requires significant custom development for secure access control, encryption key management, and rotation. This approach introduces a high operational overhead and security risks.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while AWS KMS is essential for encrypting data, it doesn't directly provide a secret management or rotation service. Using KMS to encrypt credentials would require custom code to handle the encryption, decryption, storage, and rotation of the encrypted secrets. This approach would be more complex and have higher operational overhead compared to using AWS Secrets Manager, which is specifically designed for this purpose. The 'multi-' prefix is misleading and doesn't change the fundamental issue that KMS alone isn't a secret management solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company is planning to run a group of Amazon EC2 instances that connect to an Amazon \nAurora database. The company has built an AWS CloudFormation template to deploy the EC2 \ninstances and the Aurora DB cluster. The company wants to allow the instances to authenticate \nto the database in a secure way. The company does not want to maintain static database \ncredentials. \nWhich solution meets these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a database user with a user name and password. Add parameters for the database user name and password to the CloudFormation template. Pass the parameters to the EC2 instances when the instances are launched.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a database user with a user name and password. Store the user name and password in AWS Systems Manager Parameter Store. Configure the EC2 instances to retrieve the database credentials from Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the DB cluster to use IAM database authentication. Create a database user to use with IAM authentication. Associate a role with the EC2 instances to allow applications on the instances to access the database.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the DB cluster to use IAM database authentication with an IAM user. Create a database user that has a name that matches the IAM user. Associate the IAM user with the EC2 instances to allow applications on the instances to access the database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by leveraging IAM database authentication, which eliminates the need to manage database usernames and passwords directly on the EC2 instances. By associating an IAM role with the EC2 instances, the applications running on those instances can obtain temporary credentials to authenticate to the Aurora database. This approach is more secure than storing credentials and reduces operational overhead because AWS manages the credential rotation and lifecycle.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it involves managing database usernames and passwords within the CloudFormation template and passing them to the EC2 instances. This approach is less secure due to the risk of exposing the credentials and requires manual management of the credentials, increasing operational overhead.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because while it avoids hardcoding credentials directly in the CloudFormation template, it still requires managing database usernames and passwords. Storing the credentials in Systems Manager Parameter Store is an improvement over hardcoding, but it still necessitates creating and managing the credentials themselves, adding to the operational burden. IAM database authentication provides a more streamlined and secure alternative.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A solutions architect is designing a shared storage solution for a web application that is deployed \nacross multiple Availability Zones. The web application runs on \nAmazon EC2 instances that are in an Auto Scaling group. The company plans to make frequent \nchanges to the content. The solution must have strong consistency in returning the new content \nas soon as the changes occur. \nWhich solutions meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block storage that is mounted to the individual EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the individual EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a shared Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon S3 bucket to store the web content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Storage Gateway Volume Gateway in iSCSI mode provides block-level access to storage. When configured with cached volumes, it stores frequently accessed data locally for low-latency access, while asynchronously backing up the entire volume to S3. This allows multiple EC2 instances to mount the same volume and have near real-time access to changes. The iSCSI protocol ensures strong consistency, and the cached volumes provide performance benefits. The Volume Gateway can be deployed across multiple AZs for high availability.\n\n**Why option 1 is correct:**\nThis is correct because Amazon EFS is a fully managed, scalable, elastic file system that is designed to be shared across multiple EC2 instances simultaneously. It provides strong consistency for file operations within a region, ensuring that all instances see the latest changes immediately. EFS is inherently multi-AZ, providing high availability and durability. Mounting the EFS file system on each EC2 instance in the Auto Scaling group allows for easy sharing of web content and immediate propagation of changes.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a single EBS volume cannot be attached to multiple EC2 instances simultaneously. While EBS Multi-Attach exists, it's limited to specific instance types and operating systems, and is not designed for general-purpose shared file system use cases. It also doesn't inherently provide a shared file system structure.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS DataSync is designed for data transfer and synchronization between on-premises storage and AWS, or between AWS storage services. It's not suitable for providing real-time shared storage with strong consistency for a web application. DataSync operates on a scheduled or on-demand basis, not continuously in a way that would guarantee immediate consistency for frequent content changes.\n\n**Why option 4 is incorrect:**\nThis is incorrect because while Amazon S3 is excellent for storing static web content, it's not designed for frequent content changes requiring strong consistency across multiple EC2 instances. S3 offers eventual consistency for PUTS and DELETES in all regions, meaning there might be a delay before changes are visible to all instances. Using S3 directly would require complex caching strategies and invalidation mechanisms to ensure consistency, which adds significant overhead and complexity.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company that operates a web application on premises is preparing to launch a newer version of \nthe application on AWS. The company needs to route requests to either the AWS-hosted or the \non-premises-hosted application based on the URL query string. The on-premises application is \nnot available from the internet, and a VPN connection is established between Amazon VPC and \nthe company's data center. The company wants to use an Application Load Balancer (ALB) for \nthis launch. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use two ALBs: one for on-premises and one for the AWS resource.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use two ALBs: one for on-premises and one for the AWS resource.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use one ALB with two target groups: one for the AWS resource and one for on premises.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use one ALB with two AWS Auto Scaling groups: one for the AWS resource and one for on",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution correctly addresses the requirements by utilizing a single ALB with two target groups. One target group is for the AWS-hosted application instances, and the other target group is for the on-premises application. The ALB can be configured with listener rules to inspect the URL query string and forward requests to the appropriate target group based on the query string's value. Since the on-premises application is not directly accessible from the internet, the target group for the on-premises application would need to be configured to route traffic through the VPN connection. This approach minimizes complexity and cost by using a single ALB.\n\n**Why option 0 is incorrect:**\nUsing two ALBs would be unnecessarily complex and expensive. It would require managing two separate load balancers, which increases operational overhead. Also, it doesn't directly address the requirement of routing based on the URL query string. You would need an additional mechanism to direct traffic to the correct ALB based on the query string, adding further complexity.\n\n**Why option 1 is incorrect:**\nUsing two ALBs would be unnecessarily complex and expensive. It would require managing two separate load balancers, which increases operational overhead. Also, it doesn't directly address the requirement of routing based on the URL query string. You would need an additional mechanism to direct traffic to the correct ALB based on the query string, adding further complexity. This is a duplicate of option 0.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company wants to move from many standalone AWS accounts to a consolidated, multi-account \narchitecture. The company plans to create many new AWS accounts for different business units. \nThe company needs to authenticate access to these AWS accounts by using a centralized \ncorporate directory service \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Select TWO )",
    "options": [
      {
        "id": 0,
        "text": "Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Amazon Cognito identity pool. Configure AWS Single Sign-On to accept Amazon Cognito authentication.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS Single Sign-On to AWS Directory Service.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up AWS Single Sign-On (AWS SSO) in the organization. Configure AWS SSO and integrate it with the company's corporate directory service.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because creating a new organization in AWS Organizations with all features turned on is the foundational step for managing multiple AWS accounts centrally. Enabling all features allows for the use of Service Control Policies (SCPs) and other advanced management capabilities, which are crucial for governing the new AWS accounts effectively.\n\n**Why option 4 is correct:**\nThis is correct because setting up AWS Single Sign-On (AWS SSO) in the organization and integrating it with the company's corporate directory service provides a centralized authentication mechanism. AWS SSO allows users to authenticate using their existing corporate credentials and then access multiple AWS accounts and applications with a single sign-on. This directly addresses the requirement for centralized authentication.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while Amazon Cognito can handle authentication, it's primarily designed for user authentication in web and mobile applications, not for managing access to multiple AWS accounts within an organization. Configuring AWS Single Sign-On to accept Amazon Cognito authentication would add unnecessary complexity and is not the ideal solution for integrating with a corporate directory service for AWS account access.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while SCPs are useful for managing AWS accounts within an organization, adding AWS Single Sign-On to AWS Directory Service is not the correct approach. AWS SSO is designed to integrate with existing directory services (like AWS Directory Service or Active Directory) directly, not to be added as a component within them. The correct approach is to integrate AWS SSO with the corporate directory service.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "An entertainment company is using Amazon DynamoDB to store media metadata. \nThe application is read intensive and experiencing delays. \nThe company does not have staff to handle additional operational overhead and needs to \nimprove the performance efficiency of DynamoDB without reconfiguring the application. \nWhat should a solutions architect recommend to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache for Redis",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB Accelerate (DAX)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Replicate data by using DynamoDB global tables",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Memcached with Auto Discovery enabled",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement for improved read performance in DynamoDB without application changes. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It sits in front of DynamoDB tables and delivers microsecond latency for read-intensive workloads. Because it's DynamoDB-aware, it requires minimal configuration changes to the application, meeting the 'no reconfiguration' requirement. It's also fully managed, addressing the 'no additional operational overhead' requirement.\n\n**Why option 0 is incorrect:**\nWhile ElastiCache for Redis is a caching solution, it's not specifically designed for DynamoDB. Integrating it would require significant application changes to manage the cache invalidation and data consistency between DynamoDB and Redis, violating the 'no reconfiguring the application' requirement. It also introduces additional operational overhead for managing the Redis cluster.\n\n**Why option 2 is incorrect:**\nDynamoDB global tables are designed for multi-region replication and disaster recovery, not primarily for improving read performance within a single region. While global tables can improve read latency for users in different geographical locations, they don't directly address the performance issue within the current application's region. Furthermore, setting up and managing global tables adds operational overhead.\n\n**Why option 3 is incorrect:**\nSimilar to Redis, ElastiCache for Memcached is a caching solution, but it's not specifically designed for DynamoDB. Integrating it would require application changes to manage the cache, violating the 'no reconfiguring the application' requirement. Although Auto Discovery simplifies cluster management, it still introduces operational overhead that the company wants to avoid.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company has an application that provides marketing services to stores. The services are based \non previous purchases by store customers. The stores upload transaction data to the company \nthrough SFTP, and the data is processed and analyzed to generate new marketing offers. Some \nof the files can exceed 200 GB in size. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n66 \nRecently, the company discovered that some of the stores have uploaded files that contain \npersonally identifiable information (PII) that should not have been included. The company wants \nadministrators to be alerted if PII is shared again. The company also wants to automate \nremediation. \n \nWhat should a solutions architect do to meet these requirements with the LEAST development \neffort?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the objects that contain PII.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements effectively. Amazon S3 provides a secure and scalable storage solution for the large files. Amazon Macie is designed specifically for discovering and protecting sensitive data, including PII, in S3. It offers pre-built data identifiers and can be customized. Using Amazon SNS for notifications allows administrators to be alerted immediately when PII is detected, fulfilling the alerting requirement. The manual removal of objects, while not fully automated, is a reasonable trade-off for minimizing development effort, as full automation might require more complex workflows and custom code.\n\n**Why option 0 is incorrect:**\nAmazon Inspector is primarily designed for vulnerability management and security assessments of EC2 instances and container images, not for scanning data within S3 objects for PII. While Inspector can perform some file integrity monitoring, it's not its primary function, and it's not well-suited for PII detection. Also, triggering an S3 Lifecycle policy to remove objects based on PII detection would require custom integration with Inspector, adding complexity and development effort. Inspector is not the right tool for this job.\n\n**Why option 2 is incorrect:**\nImplementing custom scanning algorithms in a Lambda function would require significant development effort to create and maintain accurate PII detection logic. This approach also needs to handle large files efficiently, which can be challenging with Lambda's execution time and memory limitations. While SNS notifications are appropriate, the overall solution is not the least development effort option. Furthermore, relying solely on notifications without any automated remediation leaves the PII exposed for a longer period.\n\n**Why option 3 is incorrect:**\nImplementing custom scanning algorithms in a Lambda function suffers from the same drawbacks as option 2: significant development effort and challenges with large file handling. While using Amazon SES for notifications is a viable alternative to SNS, it doesn't fundamentally change the complexity of the solution. Triggering an S3 Lifecycle policy to remove objects after PII detection would require custom integration with the Lambda function, adding to the development effort. The combination of custom code, Lambda limitations, and complex integration makes this option less desirable than using Macie.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a \nspecific AWS Region for an upcoming event that will last 1 week. \n \nWhat should the company do to guarantee the EC2 capacity?",
    "options": [
      {
        "id": 0,
        "text": "Purchase Reserved instances that specify the Region needed",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an On Demand Capacity Reservation that specifies the Region needed",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase Reserved instances that specify the Region and three Availability Zones needed",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution directly addresses the requirement of guaranteed EC2 capacity in three specific Availability Zones within a region. On-Demand Capacity Reservations allow you to reserve compute capacity for your EC2 instances in a specific Availability Zone for any duration. This ensures that the capacity will be available when you need it, which is crucial for the upcoming event.\n\n**Why option 0 is incorrect:**\nPurchasing Reserved Instances that specify only the Region does not guarantee capacity in specific Availability Zones. While Reserved Instances provide cost savings, they do not reserve actual capacity. The instances can still be launched in any AZ within the region, and there's no guarantee that capacity will be available in the desired AZs.\n\n**Why option 1 is incorrect:**\nCreating an On-Demand Capacity Reservation that specifies only the Region is insufficient. Capacity Reservations need to be created at the Availability Zone level to guarantee capacity in those specific zones. Specifying only the region does not guarantee capacity in the three specific AZs mentioned in the question.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company's website uses an Amazon EC2 instance store for its catalog of items. The company \nwants to make sure that the catalog is highly available and that the catalog is stored in a durable \nlocation. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n67 \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the catalog to Amazon ElastiCache for Redis.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a larger EC2 instance with a larger instance store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by providing a durable and highly available storage solution. Amazon EFS is a network file system that can be mounted on multiple EC2 instances simultaneously, providing high availability. Data stored in EFS is replicated across multiple Availability Zones, ensuring durability.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because ElastiCache is a caching service, not a durable storage solution for a catalog. While it can improve performance, it's not suitable for storing the entire catalog due to its volatile nature and cost considerations for large datasets.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because increasing the size of the instance store does not address the fundamental problem of its ephemeral nature. Data is still lost if the instance fails, and it doesn't provide high availability. Instance store data is tied to the lifecycle of the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because S3 Glacier Deep Archive is designed for long-term archival storage of infrequently accessed data. It's not suitable for a catalog that needs to be readily available for website access due to its retrieval times and cost structure for frequent access.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 \nyear of the call, but users access the files infrequently after 1 year. The company wants to \noptimize its solution by giving users the ability to query and retrieve files that are less than 1-year-\nold as quickly as possible. A delay in retrieving older files is acceptable. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store individual files with tags in Amazon S3 Glacier Instant Retrieval.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store individual files in Amazon S3 Intelligent-Tiering.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store individual files with tags in Amazon S3 Standard storage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store individual files in Amazon S3 Standard storage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution automatically moves data to the most cost-effective storage tier based on access patterns. Frequent access data is stored in a tier optimized for fast retrieval, while infrequently accessed data is moved to a cheaper tier. This aligns perfectly with the requirement of fast access for recent files and acceptable delays for older files, all while optimizing costs.\n\n**Why option 0 is incorrect:**\nS3 Glacier Instant Retrieval is designed for archiving data with infrequent access and immediate retrieval needs. While it provides immediate retrieval, it's generally more expensive than S3 Intelligent-Tiering for data that is frequently accessed within the first year. Also, using tags for individual files doesn't address the automatic tiering requirement; it would require manual management or custom scripting to move files between storage classes based on age.\n\n**Why option 2 is incorrect:**\nS3 Standard is designed for frequently accessed data. While it provides fast retrieval, it's more expensive than other storage classes for data that is infrequently accessed. This solution doesn't address the cost optimization requirement for older files.\n\n**Why option 3 is incorrect:**\nS3 Standard is designed for frequently accessed data. While it provides fast retrieval, it's more expensive than other storage classes for data that is infrequently accessed. This solution doesn't address the cost optimization requirement for older files.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 57,
    "text": "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The \nworkload is powered by third-party software. The company needs to patch the third-party \nsoftware on all EC2 instances as quickly as possible to remediate a critical security vulnerability. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n68",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to apply the patch to all EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Run Command to run a custom command that applies the patch to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution allows for immediate execution of a custom script or command across a fleet of EC2 instances. Run Command provides the flexibility to define the exact steps needed to apply the third-party software patch. It's a direct and efficient way to address the critical vulnerability quickly without needing to configure more complex systems like Patch Manager or maintenance windows.\n\n**Why option 0 is incorrect:**\nWhile Lambda can execute code, it's not the most efficient or direct method for patching EC2 instances. Lambda functions are typically triggered by events and are designed for short-lived tasks. Using Lambda to connect to and patch 1,000 EC2 instances would require complex orchestration and could introduce latency and scaling challenges. It's not the primary use case for Lambda and would be an unnecessarily complex solution.\n\n**Why option 1 is incorrect:**\nPatch Manager is designed for managing operating system and application patches provided by AWS or third-party vendors. While it can automate patching, it might not be suitable for applying a custom patch for third-party software, especially if the patch isn't available through standard patch repositories. It also requires more configuration and setup than Run Command for a one-time, urgent patching scenario.\n\n**Why option 2 is incorrect:**\nMaintenance windows are designed for scheduled maintenance activities. While they can be used to apply patches, they are not ideal for addressing a critical security vulnerability that needs to be remediated as quickly as possible. Scheduling a maintenance window introduces a delay, which is unacceptable in this scenario. The urgency of the situation necessitates a more immediate solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A company is developing an application that provides order shipping statistics for retrieval by a \nREST API. The company wants to extract the shipping statistics, organize the data into an easy-\nto-read HTML format, and send the report to several email addresses at the same time every \nmorning. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to send the data to Amazon Kinesis Data Firehose.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nThis option correctly addresses the requirement of sending the formatted data via email. Amazon SES is a reliable and scalable email service that can be used to send emails to multiple recipients. It can handle the email sending part of the solution efficiently.\n\n**Why option 3 is correct:**\nThis option provides the scheduling and data retrieval mechanism. Amazon EventBridge allows scheduling events, in this case, to trigger a Lambda function at a specific time every morning. The Lambda function can then query the application's REST API to extract the required shipping statistics. This addresses the scheduling and data retrieval aspects of the problem.\n\n**Why option 0 is incorrect:**\nKinesis Data Firehose is designed for streaming data to destinations like S3, Redshift, or Elasticsearch. It's not suitable for querying an API and formatting data into HTML for email. It's an overkill for this scenario and doesn't directly address the email sending requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nStoring the application data in S3 and using SNS for email notifications is not the correct approach. The question specifies retrieving data from a REST API, not from data stored in S3. Furthermore, SNS is primarily for notifications and doesn't provide the necessary formatting capabilities to convert data into an easy-to-read HTML format. While SNS can send emails, it's not designed for complex formatting and reporting like SES.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company wants to migrate its on-premises application to AWS. The application produces output \nfiles that vary in size from tens of gigabytes to hundreds of terabytes The application data must \nbe stored in a standard file system structure. The company wants a solution that scales \nautomatically, is highly available, and requires minimum operational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon EFS for storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon S3 for storage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement for a standard file system structure by using Amazon EFS. EFS provides a fully managed, scalable, and highly available NFS file system. Running the application on EC2 instances in a Multi-AZ Auto Scaling group ensures high availability and automatic scaling based on demand. This minimizes operational overhead as AWS manages the underlying infrastructure.\n\n**Why option 0 is incorrect:**\nWhile using Amazon ECS for containerization provides scalability, Amazon S3 is object storage and does not provide a standard file system structure. The application requires a file system, making S3 unsuitable for this requirement.\n\n**Why option 1 is incorrect:**\nWhile Amazon EKS provides container orchestration and Amazon EFS provides a file system, using EKS introduces more operational overhead compared to using EC2 instances in an Auto Scaling group for this specific scenario. The question emphasizes minimizing operational overhead, and EKS requires more management and configuration.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company needs to store its accounting records in Amazon S3. The records must be \nimmediately accessible for 1 year and then must be archived for an additional 9 years. No one at \nthe company, including administrative users and root users, can be able to delete the records \nduring the entire 10-year period. The records must be stored with maximum resiliency. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the records in S3 Glacier for the entire 10-year period.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the records by using S3 Intelligent-Tiering.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by initially storing the records in S3 Standard, which provides immediate accessibility and high durability for the first year. An S3 Lifecycle policy can then automatically transition the records to S3 Glacier Deep Archive after one year, fulfilling the long-term archival requirement. To prevent deletion, S3 Object Lock in Governance mode can be enabled on the bucket. Governance mode allows administrative users to bypass retention settings under specific conditions, but this can be controlled with IAM policies. Alternatively, S3 Object Lock in Compliance mode can be used, which provides the strongest level of immutability, preventing any user, including the root user, from deleting the objects during the retention period. This combination of lifecycle policies and object lock ensures both the storage tiering and immutability requirements are met.\n\n**Why option 0 is incorrect:**\nStoring the records in S3 Glacier for the entire 10-year period would meet the immutability requirement if Object Lock is enabled. However, it would not meet the immediate accessibility requirement for the first year. Glacier is designed for infrequent access and has retrieval times that are not suitable for immediate access.\n\n**Why option 1 is incorrect:**\nS3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it optimizes storage costs, it does not inherently provide immutability. Object Lock would still need to be configured separately to prevent deletion. Also, it doesn't guarantee the data will be in a readily accessible tier for the first year and then moved to an archive tier for the remaining nine years, as the access patterns may not align with the required transition.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A company runs multiple Windows workloads on AWS. The company's employees use Windows \nfile shares that are hosted on two Amazon EC2 instances. The file shares synchronize data \nbetween themselves and maintain duplicate copies. The company wants a highly available and \ndurable storage solution that preserves how users currently access the files. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate all the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon S3 File Gateway.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ",
        "correct": true
      },
      {
        "id": 3,
        "text": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by providing a fully managed, highly available, and durable Windows file server. Amazon FSx for Windows File Server is built on Windows Server and supports the SMB protocol, allowing users to continue accessing files using their existing methods. Multi-AZ deployment ensures high availability by automatically failing over to a standby file server in a different Availability Zone in case of an outage. FSx for Windows File Server also provides durable storage with automatic backups and replication.\n\n**Why option 0 is incorrect:**\nMigrating all data to Amazon S3 would require significant changes to how users access the files. S3 is an object storage service, not a file system, and it doesn't natively support Windows file shares. Users would need to use different tools and methods to access the data, which would disrupt their workflow and require retraining. While S3 is highly durable and scalable, it doesn't meet the requirement of preserving the current file access methods.\n\n**Why option 1 is incorrect:**\nAmazon S3 File Gateway provides on-premises applications with access to data stored in Amazon S3. While it can provide access to S3 data, it doesn't directly replace the existing Windows file shares or provide a native Windows file server environment. It would require changes to how users access the files and wouldn't be as seamless as using a native Windows file server solution. It also adds complexity to the architecture without directly addressing the need for a highly available and durable Windows file share.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A solutions architect is developing a multiple-subnet VPC architecture. The solution will consist of \nsix subnets in two Availability Zones. The subnets are defined as public, private and dedicated for \ndatabases. Only the Amazon EC2 instances running in the private subnets should be able to \naccess a database. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a now route table that excludes the route to the public subnets' CIDR blocks.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a security group that denies ingress from the security group used by instances in the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a security group that allows ingress from the security group used by instances in the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a new peering connection between the public subnets and the private subnets.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by creating a security group for the database instances and configuring it to allow ingress traffic only from the security group associated with the EC2 instances in the private subnets. This ensures that only instances within the private subnets can connect to the database, effectively isolating it from the public subnets.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because route tables control the routing of traffic between subnets and to the internet gateway or other network destinations. While route tables are essential for network configuration, they don't provide the granular control needed to restrict access based on the source of the connection at the instance level. Excluding routes to public subnets' CIDR blocks in a new route table would prevent database access from those subnets, but it wouldn't allow access from the private subnets, violating the requirement.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because denying ingress from the security group used by instances in the public subnets would prevent instances in the public subnets from accessing the database, which is a desired outcome. However, it would also prevent instances in the *private* subnets from accessing the database, which violates the requirement that only private subnets should be able to access the database. The correct approach is to *allow* ingress from the private subnet's security group, not deny it from the public subnet's security group.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company has registered its domain name with Amazon Route 53. The company uses Amazon \nAPI Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. \nThird-party services consume the APIs securely. The company wants to design its API Gateway \nURL with the company's domain name and corresponding certificate so that the third-party \nservices can use HTTPS. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution correctly addresses all requirements. Creating a Regional API Gateway endpoint allows associating it with a custom domain name. Importing the certificate into ACM in the same region (ca-central-1) is essential for HTTPS. Attaching the certificate to the API Gateway endpoint enables secure communication. Configuring Route 53 to route traffic to the API Gateway endpoint using a custom domain name makes the API accessible via the company's domain.\n\n**Why option 0 is incorrect:**\nUsing stage variables in API Gateway to overwrite the default URL is not the correct approach for associating a custom domain name and certificate. While stage variables can modify the URL, they don't handle certificate management or DNS routing. This option also doesn't correctly address the HTTPS requirement.\n\n**Why option 1 is incorrect:**\nWhile creating Route 53 DNS records and pointing them to the API Gateway endpoint is necessary, importing the certificate into ACM in the us-east-1 Region is incorrect. ACM certificates must be in the same region as the API Gateway when using Regional API Gateway endpoints. The certificate must be in ca-central-1.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company is running a popular social media website. The website gives users the ability to \nupload images to share with other users. The company wants to make sure that the images do \nnot contain inappropriate content. The company needs a solution that minimizes development \neffort. What should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Comprehend to detect inappropriate content.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Rekognition to detect inappropriate content.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon SageMaker to detect inappropriate content.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct choice because Amazon Rekognition provides pre-trained models specifically for image analysis, including the detection of explicit or suggestive content. It offers features like moderation labels that can identify various types of inappropriate content, such as nudity, violence, or hate symbols. Using Rekognition minimizes development effort as it is a managed service with a simple API, allowing the company to quickly integrate content moderation into their image upload workflow without needing to build and train their own machine learning models.\n\n**Why option 0 is incorrect:**\nAmazon Comprehend is a natural language processing (NLP) service used for analyzing text. It is not designed for image analysis or content moderation in images. Therefore, it is not suitable for detecting inappropriate content in images.\n\n**Why option 2 is incorrect:**\nAmazon SageMaker is a machine learning platform that allows you to build, train, and deploy custom machine learning models. While it could be used to build a custom inappropriate content detection model, it would require significantly more development effort than using a pre-trained service like Amazon Rekognition. The question specifically asks for a solution that minimizes development effort.\n\n**Why option 3 is incorrect:**\nAWS Fargate is a compute engine for running containers without managing servers. While Fargate could be used to deploy a custom machine learning model for content moderation, this approach would require significant development effort to build, train, and containerize the model. This contradicts the requirement to minimize development effort. Furthermore, it doesn't provide the content moderation functionality itself; it only provides the infrastructure to run it.",
    "domain": "Design Resilient Architectures"
  }
]