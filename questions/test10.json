[
  {
    "id": 0,
    "text": "A company has a data ingestion workflow that includes the following components: \n \n- An Amazon Simple Notation Service (Amazon SNS) topic that receives \nnotifications about new data deliveries. \n- An AWS Lambda function that processes and stores the data \n \nThe ingestion workflow occasionally fails because of network connectivity issues.  \nWhen tenure occurs the corresponding data is not ingested unless the company manually reruns \nthe job. \n \nWhat should a solutions architect do to ensure that all notifications are eventually processed?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Lambda function for deployment across multiple Availability Zones",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify me Lambda functions configuration to increase the CPU and memory allocations tor the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the SNS topic's retry strategy to increase both the number of retries and the wait time",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on failure",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nConfiguring an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination for the SNS topic and modifying the Lambda function to process messages in the queue ensures that all notifications are eventually processed. When the Lambda function fails due to network connectivity issues, SNS can route failed messages to the SQS dead-letter queue (DLQ). SQS provides durable message storage, ensuring messages are not lost even if processing fails. The Lambda function can then be configured to poll the SQS queue and retry processing messages until they succeed. This decouples the message delivery from processing, providing resilience against transient failures. SQS automatically retries message delivery and maintains messages until they are successfully processed and deleted.\n\n**Why option 0 is incorrect:**\nThe option that says configure the Lambda function for deployment across multiple Availability Zones is incorrect because Lambda functions are automatically deployed across multiple AZs by AWS - you cannot configure this manually. Additionally, deploying across multiple AZs doesn't solve the problem of network connectivity issues causing message processing failures. The issue is that when Lambda fails, messages are lost unless there's a mechanism to retry them.\n\n**Why option 1 is incorrect:**\nThe option that says modify the Lambda function configuration to increase CPU and memory allocations is incorrect because increasing CPU and memory doesn't address network connectivity issues. Network connectivity problems are external to the Lambda function's resources and won't be resolved by allocating more compute resources. The function may still fail due to network issues regardless of its CPU/memory configuration.\n\n**Why option 2 is incorrect:**\nThe option that says configure the SNS topic's retry strategy to increase both the number of retries and the wait time between retries is incorrect because while SNS can retry failed deliveries, if the Lambda function continues to fail due to persistent network issues, retries will eventually exhaust and messages may be lost. SNS retries have limits, and without a durable storage mechanism like SQS, messages can be lost if all retries fail. Additionally, SNS doesn't have configurable retry strategies in the way described - it has default retry policies.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company has a service that produces event data. The company wants to use AWS to process \nthe event data as it is received.  \nThe data is written in a specific order that must be maintained throughout processing.  \nThe company wants to implement a solution that minimizes operational overhead. \nHow should a solutions architect accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon Inspector is not designed to monitor malicious activity on data stored in Amazon S3. GuardDuty is the correct service for threat detection in S3. Also, while GuardDuty provides some security findings related to EC2, Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company is migrating an application from on-premises servers to Amazon EC2 instances. As \npart of the migration design requirements, a solutions architect must implement infrastructure \nmetric alarms. The company does not need to take action if CPU utilization increases to more \nthan 50% for a short burst of time. However, if the CPU utilization increases to more than 50% \nand read IOPS on the disk are high at the same time, the company needs to act as soon as \npossible. The solutions architect also must reduce false alarms. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon CloudWatch composite alarms where possible.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create single Amazon CloudWatch metric alarms with multiple metric thresholds where",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "A company wants to migrate its on-premises data center to AWS. According to the company's \ncompliance requirements, the company can use only the ap-northeast-3 Region. Company \nadministrators are not permitted to connect VPCs to the internet. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Control Tower to implement data residency guardrails to deny internet access and",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use rules in AWS WAF to prevent internet access.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Config to activate managed rules to detect and alert for internet gateways and to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.\n\n**Why option 3 is incorrect:**\nusing Server-Side Encryption with customer-provided keys (SSE-C), is incorrect because it requires the startup to manage the encryption keys. The question explicitly states the startup does not want to manage the encryption keys. With SSE-C, S3 encrypts the data using the key provided by the customer, but the customer is responsible for managing and protecting that key.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company uses a three-tier web application to provide training to new employees. The \napplication is accessed for only 12 hours every day. The company is using an Amazon RDS for \nMySQL DB instance to store information and wants to minimize costs. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an IAM policy for AWS Systems Manager Session Manager.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create AWS Lambda functions to start and stop the DB instance.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company sells ringtones created from clips of popular songs. The files containing the ringtones \nare stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of \nfiles, but downloads are infrequent for ringtones older than 90 days. The company needs to save \nmoney on storage while keeping the most accessed files readily available for its users. \n \nWhich action should the company take to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company needs to save the results from a medical trial to an Amazon S3 repository. The \nrepository must allow a few scientists to add new files and must restrict all other users to read-\nonly access. No users can have the ability to modify or delete any files in the repository. The \ncompany must keep every file in the repository for a minimum of 1 year after its creation date. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n39",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Object Lock in governance mode with a legal hold of 1 year",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use S3 Object Lock in compliance mode with a retention period of 365 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket Use an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the S3 bucket to invoke an AWS Lambda function every tune an object is added",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A large media company hosts a web application on AWS. The company wants to start caching \nconfidential media files so that users around the world will have reliable access to the files. The \ncontent is stored in Amazon S3 buckets. The company must deliver the content quickly, \nregardless of where the requests originate geographically. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to connect the S3 buckets to the web application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Global Accelerator to connect the S3 buckets to the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company produces batch data that comes from different databases. The company also \nproduces live stream data from network sensors and application APIs. The company needs to \nconsolidate all the data into one place for business analytics. The company needs to process the \nincoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-\ntime queries and import the data into a business intelligence tool to show key performance \nindicators (KPIs). \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Athena foe one-time queries.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Analytics for one-time queries.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create custom AWS Lambda functions to move the individual records from me databases to an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Glue extract transform, and toad (ETL) job to convert the data into JSON format.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 4 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 1 is incorrect:**\nAWS Glue is a fully managed extract, transform, and load (ETL) service. While Glue can process data, it is not a storage solution and does not directly address the need for high-performance, parallel storage for 'hot data' or cost-effective storage for 'cold data'. It is primarily used for data cataloging and transformation.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A gaming company has a web application that displays scores. The application runs on Amazon \nEC2 instances behind an Application Load Balancer. The application stores data in an Amazon \nRDS for MySQL database. Users are starting to experience long delays and interruptions that are \ncaused by database read performance. The company wants to improve the user experience while \nminimizing changes to the application's architecture. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache in front of the database.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use RDS Proxy between the application and the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application from EC2 instances to AWS Lambda.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A business's backup data totals 700 terabytes (TB) and is kept in network attached storage \n(NAS) at its data center. This backup data must be available in the event of occasional regulatory \ninquiries and preserved for a period of seven years. The organization has chosen to relocate its \nbackup data from its on-premises data center to Amazon Web Services (AWS). Within one \nmonth, the migration must be completed. The company's public internet connection provides 500 \nMbps of dedicated capacity for data transport. \n \nWhat should a solutions architect do to ensure that data is migrated and stored at the LOWEST \npossible cost?",
    "options": [
      {
        "id": 0,
        "text": "Order AWS Snowball devices to transfer the data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy a VPN connection between the data center and Amazon VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 1 is incorrect:**\nis incorrect because manually creating IAM roles in each member account and instructing developers to assume roles is a highly manual and error-prone process. It doesn't provide centralized management or easy integration with the existing Active Directory. It also increases the operational burden and makes it difficult to maintain consistent access control policies across accounts. Control Tower helps with account provisioning but doesn't solve the identity management problem directly.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 11,
    "text": "A company wants to direct its users to a backup static error page if the company's primary \nwebsite is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The \ndomain is pointing to an Application Load Balancer (ALB). The company needs a solution that \nminimizes changes and infrastructure overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the Route 53 records to use a latency routing policy.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Route 53 active-passive failover configuration.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the Route 53 records to use a multivalue answer routing policy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 12,
    "text": "A corporation has recruited a new cloud engineer who should not have access to the \nCompanyConfidential Amazon S3 bucket. The cloud engineer must have read and write \npermissions on an S3 bucket named AdminTools. \n \nWhich IAM policy will satisfy these criteria? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n42",
    "options": [
      {
        "id": 0,
        "text": "B.",
        "correct": true
      },
      {
        "id": 2,
        "text": "D.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 2 is incorrect:**\nleveraging Amazon QuickSight with Amazon Redshift, is incorrect because QuickSight is a business intelligence service for visualizing data, and Redshift is a data warehouse for storing and analyzing large datasets. While these services are useful for the analytics platform itself, they don't address the real-time data ingestion and processing requirements. They are downstream components and don't provide the API endpoint or real-time processing capabilities needed.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A new employee has joined a company as a deployment engineer. The deployment engineer will \nbe using AWS CloudFormation templates to create multiple AWS resources.  \nA solutions architect wants the deployment engineer to perform job activities while following the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n44 \nprinciple of least privilege. \n \nWhich steps should the solutions architect do in conjunction to reach this goal? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Have the deployment engineer use AWS account roof user credentials for performing AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM User for the deployment engineer and add the IAM user to a group that has",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create an IAM role for the deployment engineer to explicitly define the permissions specific to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 4 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 0 is incorrect:**\nusing Amazon Athena SQL queries, is incorrect because while Athena can query Parquet files and perform transformations, it requires writing SQL code. The question explicitly asks for a code-free interface. Storing queries in Glue Data Catalog and sharing them through Athena's query editor does not provide the visual workflow and data lineage capabilities that DataBrew offers. It also doesn't inherently provide data profiling.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company runs a high performance computing (HPC) workload on AWS. The workload required \nlow-latency network performance and high network throughput with tightly coupled node-to-node \ncommunication. The Amazon EC2 instances are properly sized for compute and storage \ncapacity, and are launched using default options. \n \nWhat should a solutions architect propose to improve the performance of the workload?",
    "options": [
      {
        "id": 0,
        "text": "Choose a cluster placement group while launching Amazon EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Choose dedicated instance tenancy while launching Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Choose an Elastic Inference accelerator while launching Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Choose the required capacity reservation while launching Amazon EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 15,
    "text": "A company wants to use the AWS Cloud to make an existing application highly available and \nresilient. The current version of the application resides in the company's data center. The \napplication recently experienced data loss after a database server crashed because of an \nunexpected power outage. The company needs a solution that avoids any single points of failure. \nThe solution must give the application the ability to scale to meet user demand. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto \nScaling group in the AWS Cloud. The application will transmit data by using UDP packets. The \ncompany wants to ensure that the application can scale out and in as traffic increases and \ndecreases. What should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Attach a Network Load Balancer to the Auto Scaling group",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an Application Load Balancer to the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A solutions architect is designing a customer-facing application for a company. The application's \ndatabase will have a clearly defined access pattern throughout the year and will have a variable \nnumber of reads and writes that depend on the time of year. The company must retain audit \nrecords for the database for 7 days. The recovery point objective (RPO) must be less than 5 \nhours. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB with auto scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Redshift. Configure concurrency scaling.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon RDS with Provisioned IOPS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Aurora MySQL with auto scaling.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.\n\n**Why option 3 is incorrect:**\nis incorrect because removing full database access for *all* IAM users is too restrictive and would likely prevent developers from performing necessary tasks. Developers need some level of access to DynamoDB to build and test features. The goal is to provide the *least privilege* necessary, not to completely deny access. A more granular approach is required.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company hosts a two-tier application on Amazon EC2 instances and Amazon RDS. The \napplication's demand varies based on the time of day. The load is minimal after work hours and \non weekends. The EC2 instances run in an EC2 Auto Scaling group that is configured with a \nminimum of two instances and a maximum of five instances. The application must be available at \nall times, but the company is concerned about overall cost. \n \nWhich solution meets the availability requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use all EC2 Spot Instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase EC2 Instance Savings Plans to cover five EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase two EC2 Reserved Instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase EC2 Instance Savings Plans to cover two EC2 instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 19,
    "text": "A company has an ecommerce checkout workflow that writes an order to a database and calls a \nservice to process the payment. Users are experiencing timeouts during the checkout process.  \nWhen users resubmit the checkout form, multiple unique orders are created for the same desired \ntransaction. \n \nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?",
    "options": [
      {
        "id": 0,
        "text": "Configure the web application to send an order message to Amazon Kinesis Data Firehose.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the order in the database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the order in the database.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon Lookout for Vision is designed for image analysis, not text analysis. It's not suitable for extracting entities from text files. Furthermore, using API Gateway to push updates to the frontend in real-time is unnecessary and adds complexity and cost, as the frontend can query DynamoDB directly.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A company is planning to build a high performance computing (HPC) workload as a service \nsolution that Is hosted on AWS.  \nA group of 16 AmazonEC2Ltnux Instances requires the lowest possible latency for node-to-node \ncommunication.  \nThe instances also need a shared block device volume for high-performing storage. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a cluster placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use a cluster placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a partition placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a spread placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate, is incorrect because it requires 500 operations per second (1000 messages / 2 messages per batch = 500 operations). While this is still within SQS FIFO limits, a batch size of 4 (Option 0) is more efficient as it reduces the number of operations required to achieve the desired throughput, leading to lower costs and potentially better performance.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "A company has an event-driven application that invokes AWS Lambda functions up to 800 times \neach minute with varying runtimes.  \nThe Lambda functions access data that is stored in an Amazon Aurora MySQL OB cluster.  \nThe company is noticing connection timeouts as user activity increases The database shows no \nsigns of being overloaded. CPU, memory, and disk access metrics are all low.  \nWhich solution will resolve this issue with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Adjust the size of the Aurora MySQL nodes to handle more connections.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up Amazon ElastiCache tor Redls to cache commonly read items from the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Aurora Replica as a reader node.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ROS Proxy to create a proxy.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is building a containerized application on premises and decides to move the \napplication to AWS.  \nThe application will have thousands of users soon after li is deployed.  \nThe company Is unsure how to manage the deployment of containers at scale. The company \nneeds to deploy the containerized application in a highly available architecture that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store container images In an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store container images in a repository that runs on an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A company's application Is having performance issues. The application staleful and needs to \ncomplete m-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation \nto deploy infrastructure and used the M5 EC2 Instance family. As traffic increased, the application \nperformance degraded. Users are reporting delays when the users attempt to access the \napplication.  \nWhich solution will resolve these issues in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Replace the EC2 Instances with T3 EC2 instances that run in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 0 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability by replacing failed instances, it doesn't provide content-based routing. An Elastic IP (EIP) is associated with an instance, not an Auto Scaling group. EIPs are generally discouraged for instances behind a load balancer because the load balancer handles the public endpoint and distribution of traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "An ecommerce company has an order-processing application that uses Amazon API Gateway \nand an AWS Lambda function.  \nThe application stores data in an Amazon Aurora PostgreSQL database.  \nDuring a recent sales event, a sudden surge in customer orders occurred.  \nSome customers experienced timeouts and the application did not process the orders of those \ncustomers.  \nA solutions architect determined that the CPU utilization and memory utilization were high on the \ndatabase because of a large number of open connections.  \nThe solutions architect needs to prevent the timeout errors while making the least possible \nchanges to the application. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure provisioned concurrency for the Lambda function.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS Proxy to create a proxy for the database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a read replica for the database in a different AWS Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.\n\n**Why option 3 is incorrect:**\nis incorrect because weighted routing policy is used to distribute traffic across multiple resources based on assigned weights. It's typically used for A/B testing or gradual deployments, not for geographic restrictions.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company runs a global web application on Amazon EC2 instances behind an Application Load \nBalancer. \nThe application stores data in Amazon Aurora.  \nThe company needs to create a disaster recovery solution and can tolerate up to 30 minutes of \ndowntime and potential data loss.  \nThe solution does not need to handle the load when the primary infrastructure is healthy. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n50 \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application with the required infrastructure elements in place.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Host a scaled-down deployment of the application in a second AWS Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replicate the primary infrastructure in a second AWS Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Back up data with AWS Backup.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.\n\n**Why option 3 is incorrect:**\nis incorrect because while it acknowledges that EFS is a regional service, copying the spreadsheet to EFS file systems in other regions introduces significant operational overhead for synchronization and version control. It doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would be working on different copies, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company wants to measure the effectiveness of its recent marketing campaigns.  \nThe company performs batch processing on csv files of sales data and stores the results in an \nAmazon S3 bucket once every hour.  \nThe S3 bipetabytes of objects. The company runs one-time queries in Amazon Athena to \ndetermine which products are most popular on a particular date for a particular region Queries \nsometimes fail or take longer than expected to finish.  \nWhich actions should a solutions architect take to improve the query performance and reliability? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Reduce the S3 object sizes to less than 126 MB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Partition the data by date and region in Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the files as large, single objects in Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Kinosis Data Analytics to run the Queries as pan of the batch processing operation",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use an AWS Glue extract, transform, and load (ETL) process to convert the csv files into Apache Parquet format.",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 4 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 0 is incorrect:**\nis incorrect because while Amazon EFS provides NFS compatibility and lifecycle management, it's generally more expensive than using S3 for large-scale data storage, especially when considering infrequently accessed data. EFS One Zone-IA is cheaper than standard EFS, but S3 with lifecycle policies to Glacier or Deep Archive will be more cost-effective for long-term archival of infrequently accessed data. Also, migrating all data to EFS upfront might not be the most cost-effective approach if a significant portion of the data is rarely accessed.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 27,
    "text": "A company is running several business applications in three separate VPCs within the us-east-1 \nRegion. The applications must be able to communicate between VPCs. The applications also \nmust be able to consistently send hundreds of gigabytes of data each day to a latency-sensitive \napplication that runs in a single on- premises data center. \nA solutions architect needs to design a network connectivity solution that maximizes cost-\neffectiveness. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure three AWS Site-to-Site VPN connections from the data center to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch a third-party virtual network appliance in each VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up three AWS Direct Connect connections from the data center to a Direct Connect",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up one AWS Direct Connect connection from the data center to AWS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 0 is incorrect:**\nis incorrect because SNS is a fully managed service and the engineering team does not provision or manage the underlying servers. SNS automatically scales to handle message traffic. The problem lies in the Lambda function's ability to process the messages delivered by SNS, not SNS itself.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "An online photo application lets users upload photos and perform image editing operations. The \napplication offers two classes of service free and paid Photos submitted by paid users are \nprocessed before those submitted by free users Photos are uploaded to Amazon S3 and the job \ninformation is sent to Amazon SQS. \nWhich configuration should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use one SQS FIFO queue.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use two SQS FIFO queues: one for paid and one for free.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use two SQS standard queues one for paid and one for free.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use one SQS standard queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 29,
    "text": "A company hosts its product information webpages on AWS. The existing solution uses multiple \nAmazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. The \nwebsite also uses a custom DNS name and communicates with HTTPS only using a dedicated \nSSL certificate. The company is planning a new product launch and wants to be sure that users \nfrom around the world have the best possible experience on the new website. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Redesign the application to use Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "Redesign the application to use AWS Elastic Beanstalk",
        "correct": false
      },
      {
        "id": 2,
        "text": "Redesign the application to use a Network Load Balancer.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Redesign the application to use Amazon S3 static website hosting",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company has 150 TB of archived image data stored on-premises that needs to be moved to the \nAWS Cloud within the next month. \nThe company's current network connection allows up to 100 Mbps uploads for this purpose \nduring the night only. \nWhat is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowmobile to ship the data to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order multiple AWS Snowball devices to ship the data to AWS.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Amazon S3 Transfer Acceleration and securely upload the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 0 is incorrect:**\nHost-based routing routes traffic based on the hostname in the HTTP host header. While useful for routing to different applications based on domain names (e.g., www.example.com vs. api.example.com), it doesn't address the requirement of routing based on the URL path within the same domain.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "A company hosts its web application on AWS using seven Amazon EC2 instances. The company \nrequires that the IP addresses of all healthy EC2 instances be returned in response to DNS \nqueries. Which policy should be used to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Simple routing policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Latency routing policy",
        "correct": false
      },
      {
        "id": 2,
        "text": "Multivalue routing policy",
        "correct": true
      },
      {
        "id": 3,
        "text": "Geolocation routing policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "A company wants to use AWS Systems Manager to manage a fleet of Amazon EC2 instances. \nAccording to the company's security requirements, no EC2 instances can have internet access. A \nsolutions architect needs to design network connectivity from the EC2 instances to Systems \nManager while fulfilling this security obligation. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the EC2 instances into a private subnet with no route to the internet.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an interface VPC endpoint for Systems Manager.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy a NAT gateway into a public subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an internet gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 0 is incorrect:**\nusing AWS Lambda, is incorrect because Lambda functions have a maximum execution time limit (currently 15 minutes). The workflow takes 60 minutes, exceeding this limit. While it's possible to chain Lambda functions, it adds complexity and might not be the most efficient or cost-effective approach for a long-running process like this.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company needs to build a reporting solution on AWS. The solution must support SQL queries \nthat data analysts run on the data. \nThe data analysts will run lower than 10 total queries each day. The company generates 3 GB of \nnew data daily in an on-premises relational database. This data needs to be transferred to AWS \nto perform reporting tasks. \nWhat should a solutions architect recommend to meet these requirements at the LOWEST cost?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Kinesis Data Firehose delivery stream to deliver the data into an Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Export a daily copy of the data from the on-premises database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 0 is incorrect:**\nOption 0: Deploy an Amazon API Gateway with throttling and usage plans to slow down incoming purchase requests during peak times and maintain application stability. While API Gateway can help with rate limiting, it doesn't address the underlying scalability issue. Throttling requests will prevent the system from being overwhelmed, but it will also result in lost sales and a poor user experience. The goal is to handle the increased load, not to reduce it.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "A company wants to monitor its AWS costs for financial review. The cloud operations team is \ndesigning an architecture in the AWS Organizations management account to query AWS Cost \nand Usage Reports for all member accounts. \nThe team must run this query once a month and provide a detailed analysis of the bill. \nWhich solution is the MOST scalable and cost-effective way to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3. Use Amazon Athena for analysis.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3. Use Amazon Redshift for analysis.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis. Use Amazon QuickSight for analysis.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 3 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company collects data for temperature, humidity, and atmospheric pressure in cities across \nmultiple continents. The average volume of data that the company collects from each site daily is \n500 GB. Each site has a high-speed Internet connection. \nThe company wants to aggregate the data from all these global sites as quickly as possible in a \nsingle Amazon S3 bucket. The solution must minimize operational complexity. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Turn on S3 Transfer Acceleration on the destination S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Upload the data from each site to an S3 bucket in the closest Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the data from each site to an Amazon EC2 instance in the closest Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A company needs the ability to analyze the log files of its proprietary application. The logs are \nstored in JSON format in an Amazon S3 bucket Queries will be simple and will run on-demand. \nA solutions architect needs to perform the analysis with minimal changes to the existing \narchitecture. \nWhat should the solutions architect do to meet these requirements with the LEAST amount of \noperational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch Logs to store the logs",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Athena directly with Amazon S3 to run the queries as needed",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue to catalog the logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 0 is incorrect:**\nis incorrect because it involves creating a new AMI and launching a new instance. This is a more time-consuming and resource-intensive process than simply putting the existing instance into Standby or suspending the ReplaceUnhealthy process. Manual scaling is also less efficient than leveraging the built-in features of Auto Scaling.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company uses AWS Organizations to manage multiple AWS accounts for different \ndepartments. The management account has an Amazon S3 bucket that contains project reports. \nThe company wants to limit access to this S3 bucket to only users of accounts within the \norganization in AWS Organizations. \nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an organizational unit (OU) for each department.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization,",
        "correct": false
      },
      {
        "id": 3,
        "text": "Tag each user that needs access to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.\n\n**Why option 3 is incorrect:**\nis incorrect because it oversimplifies the pricing. While ECS itself doesn't have a direct hourly charge, the resources used by ECS (EC2 instances, Fargate vCPU/memory) are charged. The pricing depends on the launch type used.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "An application runs on an Amazon EC2 instance in a VPC. The application processes logs that \nare stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without \nconnectivity to the internet. \nWhich solution will provide private network connectivity to Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint to the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an instance profile on Amazon EC2 to allow S3 access.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon API Gateway API with a private link to access the S3 endpoint.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is hosting a web application on AWS using a single Amazon EC2 instance that stores \nuser-uploaded documents in an Amazon EBS volume. For better scalability and availability, the \ncompany duplicated the architecture and created a second EC2 instance and EBS volume in \nanother Availability Zone, placing both behind an Application Load Balancer. After completing this \nchange, users reported that, each time they refreshed the website, they could see one subset of \ntheir documents or the other, but never all of the documents at the same time. \nWhat should a solutions architect propose to ensure users see all of their documents at once?",
    "options": [
      {
        "id": 0,
        "text": "Copy the data so both EBS volumes contain all the documents.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Application Load Balancer to direct a user to the server with the documents",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the data from both EBS volumes to Amazon EFS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the Application Load Balancer to send the request to both servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company uses NFS to store large video files in on-premises network attached storage. Each \nvideo file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer \ngrowing. The company decides to migrate the video files to Amazon S3. The company must \nmigrate the video files as soon as possible while using the least possible network bandwidth. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Snowball Edge job.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy an S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an AWS Direct Connect connection between the on-premises network and AWS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 0 is incorrect:**\nis incorrect because while it leverages serverless components (AWS Glue and Amazon Athena), Athena's ML capabilities are not as robust or performant as Redshift ML for the described use case, especially with large-scale data. Athena ML is more suited for ad-hoc analysis and smaller datasets. Furthermore, querying data directly from S3 for ML can be slower than using a dedicated data warehouse like Redshift, which is optimized for analytical workloads.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company has an application that ingests incoming messages. Dozens of other applications and \nmicroservices then quickly consume these messages. The number of messages varies drastically \nand sometimes increases suddenly to 100,000 each second. The company wants to decouple the \nsolution and increase scalability. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Persist the messages to Amazon Kinesis Data Analytics.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write the messages to Amazon Kinesis Data Streams with a single shard.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 0 is incorrect:**\nis incorrect because using a Route 53 Resolver inbound endpoint would allow on-premises systems to resolve DNS records within the VPC, which is the opposite of what the question requires. The application in AWS needs to resolve on-premises DNS records, not the other way around. Enabling recursive DNS resolution in the VPC is not sufficient on its own to resolve on-premises records.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company is migrating a distributed application to AWS. The application serves variable \nworkloads. The legacy platform consists of a primary server that coordinates jobs across multiple \ncompute nodes. The company wants to modernize the application with a solution that maximizes \nresiliency and scalability. \nHow should a solutions architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement the primary server and the compute nodes with Amazon EC2 instances that are",
        "correct": false
      },
      {
        "id": 3,
        "text": "implement the primary server and the compute nodes with Amazon EC2 instances that are",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A company is running an SMB file server in its data center. The file server stores large files that \nare accessed frequently for the first few days after the files are created. After 7 days the files are \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n60 \nrarely accessed. \n \nThe total data size is increasing and is close to the company's total storage capacity. A solutions \narchitect must increase the company's available storage space without losing low-latency access \nto the most recently accessed files. The solutions architect must also provide file lifecycle \nmanagement to avoid future storage issues. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 File Gateway to extend the company's storage space.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon FSx for Windows File Server file system to extend the company's storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Install a utility on each user's computer to access Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company is building an ecommerce web application on AWS. The application sends \ninformation about new orders to an Amazon API Gateway REST API to process. The company \nwants to ensure that orders are processed in the order that they are received. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an API Gateway integration to publish a message to an Amazon Simple Notification",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an API Gateway authorizer to block any requests while the application processes an order.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora \ndatabase. The EC2 instances connect to the database by using user names and passwords that \nare stored locally in a file. The company wants to minimize the operational overhead of credential \nmanagement. \nWhat should a solutions architect do to accomplish this goal? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n61",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Systems Manager Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket lo store objects that are encrypted with an AWS Key.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume or each EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 1 is incorrect:**\nis incorrect because spread placement groups are designed to reduce correlated failures by placing instances on distinct underlying hardware. While this improves availability, it does not optimize for network performance. Spread placement groups are suitable for applications where instance failure is a major concern, but they are not the best choice for HPC workloads that require low latency and high throughput.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 46,
    "text": "A global company hosts its web application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The web application has static data and dynamic data. The company \nstores its static data in an Amazon S3 bucket. The company wants to improve performance and \nreduce latency for the static data and dynamic data. The company is using its own domain name \nregistered with Amazon Route 53. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution that has the ALB as an origin.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution that has the ALB as an origin.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance \nactivities, the company needs to rotate the credentials tor its Amazon ROS tor MySQL databases \nacross multiple AWS Regions. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the credentials as secrets in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the credentials as secrets in AWS Systems Manager by creating a secure string",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.\n\n**Why option 3 is incorrect:**\nAmazon S3 Intelligent-Tiering automatically moves data between frequent, infrequent, and archive access tiers based on access patterns. While it could potentially be a good fit, the question explicitly states the data is accessed only twice a year. Therefore, S3 Standard-IA provides a more predictable and likely lower cost solution, as Intelligent-Tiering has monitoring and transition costs that may not be offset by the infrequent access savings in this specific scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company is planning to run a group of Amazon EC2 instances that connect to an Amazon \nAurora database. The company has built an AWS CloudFormation template to deploy the EC2 \ninstances and the Aurora DB cluster. The company wants to allow the instances to authenticate \nto the database in a secure way. The company does not want to maintain static database \ncredentials. \nWhich solution meets these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a database user with a user name and password. Add parameters for the database user name and password to the CloudFormation template. Pass the parameters to the EC2 instances when the instances are launched.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a database user with a user name and password. Store the user name and password in AWS Systems Manager Parameter Store. Configure the EC2 instances to retrieve the database credentials from Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the DB cluster to use IAM database authentication. Create a database user to use with IAM authentication. Associate a role with the EC2 instances to allow applications on the instances to access the database.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the DB cluster to use IAM database authentication with an IAM user. Create a database user that has a name that matches the IAM user. Associate the IAM user with the EC2 instances to allow applications on the instances to access the database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A solutions architect is designing a shared storage solution for a web application that is deployed \nacross multiple Availability Zones. The web application runs on \nAmazon EC2 instances that are in an Auto Scaling group. The company plans to make frequent \nchanges to the content. The solution must have strong consistency in returning the new content \nas soon as the changes occur. \nWhich solutions meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block storage that is mounted to the individual EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the individual EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a shared Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon S3 bucket to store the web content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 1 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company that operates a web application on premises is preparing to launch a newer version of \nthe application on AWS. The company needs to route requests to either the AWS-hosted or the \non-premises-hosted application based on the URL query string. The on-premises application is \nnot available from the internet, and a VPN connection is established between Amazon VPC and \nthe company's data center. The company wants to use an Application Load Balancer (ALB) for \nthis launch. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use two ALBs: one for on-premises and one for the AWS resource.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use two ALBs: one for on-premises and one for the AWS resource.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use one ALB with two target groups: one for the AWS resource and one for on premises.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use one ALB with two AWS Auto Scaling groups: one for the AWS resource and one for on",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 1 is incorrect:**\nis incorrect because while it creates IAM policies for DynamoDB and S3, attaching them to the EC2 instance profile grants those permissions to *all* Pods running on those nodes. Kubernetes RBAC controls access *within* the cluster, not to AWS resources. This approach doesn't provide the fine-grained control needed to restrict UI Pods to DynamoDB and data-processing Pods to S3. All pods running on the node would inherit the permissions of the instance profile.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company wants to move from many standalone AWS accounts to a consolidated, multi-account \narchitecture. The company plans to create many new AWS accounts for different business units. \nThe company needs to authenticate access to these AWS accounts by using a centralized \ncorporate directory service \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Select TWO )",
    "options": [
      {
        "id": 0,
        "text": "Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Amazon Cognito identity pool. Configure AWS Single Sign-On to accept Amazon Cognito authentication.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS Single Sign-On to AWS Directory Service.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up AWS Single Sign-On (AWS SSO) in the organization. Configure AWS SSO and integrate it with the company's corporate directory service.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 4 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 1 is incorrect:**\nstoring the data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect for similar reasons as S3 Standard-IA. While it offers even lower storage costs, it also has higher retrieval costs and a 30-day minimum storage duration charge. More importantly, S3 One Zone-IA stores data in a single Availability Zone, which makes it less durable than S3 Standard. While durability isn't explicitly mentioned as a primary concern, it's generally a good practice to use S3 Standard unless there's a very specific reason to use a lower-durability option. The frequent access pattern and the 30-day minimum storage duration make this option less cost-effective. Also, the loss of an AZ could impact the analytics pipeline.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "An entertainment company is using Amazon DynamoDB to store media metadata. \nThe application is read intensive and experiencing delays. \nThe company does not have staff to handle additional operational overhead and needs to \nimprove the performance efficiency of DynamoDB without reconfiguring the application. \nWhat should a solutions architect recommend to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache for Redis",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB Accelerate (DAX)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Replicate data by using DynamoDB global tables",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Memcached with Auto Discovery enabled",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company has an application that provides marketing services to stores. The services are based \non previous purchases by store customers. The stores upload transaction data to the company \nthrough SFTP, and the data is processed and analyzed to generate new marketing offers. Some \nof the files can exceed 200 GB in size. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n66 \nRecently, the company discovered that some of the stores have uploaded files that contain \npersonally identifiable information (PII) that should not have been included. The company wants \nadministrators to be alerted if PII is shared again. The company also wants to automate \nremediation. \n \nWhat should a solutions architect do to meet these requirements with the LEAST development \neffort?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the objects that contain PII.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 0 is incorrect:**\nis incorrect because simple scaling policies react to alarms based on thresholds. While you *could* use simple scaling, it requires manual configuration of the scaling adjustment (how many instances to add or remove) and doesn't automatically adjust to maintain a target value. It's less sophisticated and requires more manual intervention than target tracking.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a \nspecific AWS Region for an upcoming event that will last 1 week. \n \nWhat should the company do to guarantee the EC2 capacity?",
    "options": [
      {
        "id": 0,
        "text": "Purchase Reserved instances that specify the Region needed",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an On Demand Capacity Reservation that specifies the Region needed",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase Reserved instances that specify the Region and three Availability Zones needed",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because invoking a second Lambda function via API Gateway adds unnecessary complexity and latency. It also introduces additional costs associated with API Gateway usage and Lambda invocations. This approach is less efficient than directly accessing the EFS file system.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company's website uses an Amazon EC2 instance store for its catalog of items. The company \nwants to make sure that the catalog is highly available and that the catalog is stored in a durable \nlocation. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n67 \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the catalog to Amazon ElastiCache for Redis.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a larger EC2 instance with a larger instance store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 2 is incorrect:**\nmigrating the website to Amazon S3 and using S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia, is incorrect. While S3 CRR can replicate data to Asia, it doesn't address the dynamic nature of the website. S3 is primarily for static content. Migrating the entire website to S3, including the dynamic backend, would be a significant undertaking and would not meet the requirement for a quick implementation. Furthermore, the question states the backend must remain in the United States.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 \nyear of the call, but users access the files infrequently after 1 year. The company wants to \noptimize its solution by giving users the ability to query and retrieve files that are less than 1-year-\nold as quickly as possible. A delay in retrieving older files is acceptable. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store individual files with tags in Amazon S3 Glacier Instant Retrieval.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store individual files in Amazon S3 Intelligent-Tiering.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store individual files with tags in Amazon S3 Standard storage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store individual files in Amazon S3 Standard storage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon Route 53 is a DNS service, but it doesn't inherently provide the global traffic management and fast failover capabilities required for this scenario. While Route 53 can be configured for failover using health checks and routing policies, it relies on DNS propagation, which can take longer than Global Accelerator's failover time. The question specifies the use of a custom DNS service, so Route 53 is not the primary solution for global traffic management and failover.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 57,
    "text": "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The \nworkload is powered by third-party software. The company needs to patch the third-party \nsoftware on all EC2 instances as quickly as possible to remediate a critical security vulnerability. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n68",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to apply the patch to all EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Run Command to run a custom command that applies the patch to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it uses Amazon Kinesis Data Streams to ensure ordered processing of score updates. Kinesis Data Streams is designed for real-time streaming data and guarantees record order within a shard. AWS Lambda provides a serverless compute environment to process the updates, minimizing management overhead. Amazon DynamoDB is a highly available and scalable NoSQL database, suitable for storing the processed updates. The combination of these services addresses all requirements: order, scalability, high availability, and minimal management.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SNS is a publish/subscribe messaging service that does not guarantee message order. While Lambda can process the updates, SNS is not the right choice for ordered processing. Also, running a SQL database on an EC2 instance increases management overhead compared to a managed database service like DynamoDB or RDS.\n\n**Why option 1 is incorrect:**\nis incorrect because while Kinesis Data Streams provides ordered processing, using a fleet of EC2 instances with Auto Scaling to process the data increases management overhead. Lambda is a more suitable serverless option. While DynamoDB is a good choice for the database, the EC2 instance processing is not optimal for minimizing management overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A company is developing an application that provides order shipping statistics for retrieval by a \nREST API. The company wants to extract the shipping statistics, organize the data into an easy-\nto-read HTML format, and send the report to several email addresses at the same time every \nmorning. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to send the data to Amazon Kinesis Data Firehose.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 3 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company wants to migrate its on-premises application to AWS. The application produces output \nfiles that vary in size from tens of gigabytes to hundreds of terabytes The application data must \nbe stored in a standard file system structure. The company wants a solution that scales \nautomatically, is highly available, and requires minimum operational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon EFS for storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon S3 for storage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company needs to store its accounting records in Amazon S3. The records must be \nimmediately accessible for 1 year and then must be archived for an additional 9 years. No one at \nthe company, including administrative users and root users, can be able to delete the records \nduring the entire 10-year period. The records must be stored with maximum resiliency. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the records in S3 Glacier for the entire 10-year period.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the records by using S3 Intelligent-Tiering.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing AWS Direct Connect plus a VPN, is the correct answer. Direct Connect provides a dedicated, low-latency, high-throughput connection, fulfilling those requirements. However, Direct Connect, by itself, does *not* provide encryption. Adding a VPN on top of Direct Connect addresses the encryption requirement. While Direct Connect itself is a dedicated connection, the VPN adds a layer of security, ensuring the data transmitted is encrypted. This combination satisfies all the stated requirements: dedicated connection, encryption, low latency, and high throughput. The operational overhead is also acceptable as stated in the question.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing AWS Transit Gateway to establish a connection between the data center and AWS Cloud, is incorrect. Transit Gateway is primarily used to connect multiple VPCs and on-premises networks. While it can be used to connect a data center to AWS, it doesn't inherently provide a dedicated, low-latency, high-throughput connection like Direct Connect. Also, Transit Gateway itself doesn't provide encryption; it would need to be configured separately, and it's not the primary solution for a dedicated, encrypted connection from a single data center.\n\n**Why option 3 is incorrect:**\nusing AWS Direct Connect to establish a connection between the data center and AWS Cloud, is incorrect. Direct Connect provides a dedicated, low-latency, high-throughput connection, but it does *not* inherently provide encryption. The question explicitly requires an encrypted connection, making Direct Connect alone insufficient.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A company runs multiple Windows workloads on AWS. The company's employees use Windows \nfile shares that are hosted on two Amazon EC2 instances. The file shares synchronize data \nbetween themselves and maintain duplicate copies. The company wants a highly available and \ndurable storage solution that preserves how users currently access the files. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate all the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon S3 File Gateway.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ",
        "correct": true
      },
      {
        "id": 3,
        "text": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 3 is incorrect:**\nis incorrect because the AMI is also present in Region B, as it was copied from Region A.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A solutions architect is developing a multiple-subnet VPC architecture. The solution will consist of \nsix subnets in two Availability Zones. The subnets are defined as public, private and dedicated for \ndatabases. Only the Amazon EC2 instances running in the private subnets should be able to \naccess a database. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a now route table that excludes the route to the public subnets' CIDR blocks.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a security group that denies ingress from the security group used by instances in the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a security group that allows ingress from the security group used by instances in the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a new peering connection between the public subnets and the private subnets.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company has registered its domain name with Amazon Route 53. The company uses Amazon \nAPI Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. \nThird-party services consume the APIs securely. The company wants to design its API Gateway \nURL with the company's domain name and corresponding certificate so that the third-party \nservices can use HTTPS. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company is running a popular social media website. The website gives users the ability to \nupload images to share with other users. The company wants to make sure that the images do \nnot contain inappropriate content. The company needs a solution that minimizes development \neffort. What should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Comprehend to detect inappropriate content.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Rekognition to detect inappropriate content.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon SageMaker to detect inappropriate content.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it explicitly grants the `s3:DeleteObject` action on all objects within the specified S3 bucket (`arn:aws:s3:::example-bucket/*`). This directly addresses the requirement of allowing the development team to delete objects. The resource ARN is correctly formatted to apply the permission to all objects within the bucket. This option adheres to the principle of least privilege by granting only the necessary permission.\n\n**Why option 0 is incorrect:**\nis incorrect because it grants `s3:*`, which is overly permissive. It grants all S3 actions on the objects within the bucket, violating the principle of least privilege. While it would allow deleting objects, it also grants many other unnecessary permissions.\n\n**Why option 2 is incorrect:**\nis incorrect because `s3:*Object` is not a valid IAM action. IAM actions are specific and well-defined. This wildcard usage is not supported and would not grant the desired permission.\n\n**Why option 3 is incorrect:**\nis incorrect because the resource ARN `arn:aws:s3:::example-bucket*` is not a valid format for specifying objects within a bucket. It would likely be interpreted as applying to a bucket named 'example-bucket*' rather than all objects within 'example-bucket'. The correct format to apply to all objects within the bucket is `arn:aws:s3:::example-bucket/*`.",
    "domain": "Design Resilient Architectures"
  }
]