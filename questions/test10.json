[
  {
    "id": 0,
    "text": "A company has a data ingestion workflow that includes the following components: \n \n- An Amazon Simple Notation Service (Amazon SNS) topic that receives \nnotifications about new data deliveries. \n- An AWS Lambda function that processes and stores the data \n \nThe ingestion workflow occasionally fails because of network connectivity issues.  \nWhen tenure occurs the corresponding data is not ingested unless the company manually reruns \nthe job. \n \nWhat should a solutions architect do to ensure that all notifications are eventually processed?",
    "options": [
      {
        "id": 0,
        "text": "Configure the Lambda function for deployment across multiple Availability Zones",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify me Lambda functions configuration to increase the CPU and memory allocations tor the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the SNS topic's retry strategy to increase both the number of retries and the wait time",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on failure",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nConfiguring an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination for the SNS topic and modifying the Lambda function to process messages in the queue ensures that all notifications are eventually processed. When the Lambda function fails due to network connectivity issues, SNS can route failed messages to the SQS dead-letter queue (DLQ). SQS provides durable message storage, ensuring messages are not lost even if processing fails. The Lambda function can then be configured to poll the SQS queue and retry processing messages until they succeed. This decouples the message delivery from processing, providing resilience against transient failures. SQS automatically retries message delivery and maintains messages until they are successfully processed and deleted.\n\n**Why option 0 is incorrect:**\nThe option that says configure the Lambda function for deployment across multiple Availability Zones is incorrect because Lambda functions are automatically deployed across multiple AZs by AWS - you cannot configure this manually. Additionally, deploying across multiple AZs doesn't solve the problem of network connectivity issues causing message processing failures. The issue is that when Lambda fails, messages are lost unless there's a mechanism to retry them.\n\n**Why option 1 is incorrect:**\nThe option that says modify the Lambda function configuration to increase CPU and memory allocations is incorrect because increasing CPU and memory doesn't address network connectivity issues. Network connectivity problems are external to the Lambda function's resources and won't be resolved by allocating more compute resources. The function may still fail due to network issues regardless of its CPU/memory configuration.\n\n**Why option 2 is incorrect:**\nThe option that says configure the SNS topic's retry strategy to increase both the number of retries and the wait time between retries is incorrect because while SNS can retry failed deliveries, if the Lambda function continues to fail due to persistent network issues, retries will eventually exhaust and messages may be lost. SNS retries have limits, and without a durable storage mechanism like SQS, messages can be lost if all retries fail. Additionally, SNS doesn't have configurable retry strategies in the way described - it has default retry policies.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company has a service that produces event data. The company wants to use AWS to process \nthe event data as it is received.  \nThe data is written in a specific order that must be maintained throughout processing.  \nThe company wants to implement a solution that minimizes operational overhead. \nHow should a solutions architect accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCreating an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages and setting up an AWS Lambda function to process messages from the queue is the solution that minimizes operational overhead while maintaining message order. FIFO queues guarantee that messages are processed exactly once and in the exact order they are sent, which is essential when data must be written and processed in a specific order. FIFO queues are fully managed, requiring no infrastructure provisioning or management. Lambda functions can be configured as event sources for SQS queues, automatically triggering when messages arrive, which minimizes operational overhead. This serverless architecture scales automatically and requires minimal configuration.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon SNS topic to deliver notifications containing payloads and configure a Lambda function as a subscriber is incorrect because SNS is a pub/sub messaging service that delivers messages immediately to all subscribers, but it doesn't guarantee message ordering. SNS messages can arrive out of order, and if a subscriber is unavailable, messages may be lost. SNS doesn't provide the FIFO ordering guarantee required when data must be written and processed in a specific order.\n\n**Why option 2 is incorrect:**\nThe option that says create an Amazon SQS standard queue to hold messages and set up a Lambda function to process messages independently is incorrect because standard SQS queues provide at-least-once delivery but don't guarantee message ordering. Messages in a standard queue may be delivered out of order, which violates the requirement that data must be written and processed in a specific order. Standard queues are designed for high throughput but don't maintain strict ordering.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon SNS topic to deliver notifications and configure an SQS queue as a subscriber is incorrect because while this creates a queue, SNS doesn't guarantee message ordering. When SNS delivers messages to an SQS queue, the messages may arrive out of order. Additionally, if you need FIFO ordering, you would need a FIFO SQS queue, but SNS cannot deliver directly to FIFO queues in a way that maintains strict ordering. The direct SQS FIFO queue approach is simpler and more operationally efficient.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company is migrating an application from on-premises servers to Amazon EC2 instances. As \npart of the migration design requirements, a solutions architect must implement infrastructure \nmetric alarms. The company does not need to take action if CPU utilization increases to more \nthan 50% for a short burst of time. However, if the CPU utilization increases to more than 50% \nand read IOPS on the disk are high at the same time, the company needs to act as soon as \npossible. The solutions architect also must reduce false alarms. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create Amazon CloudWatch composite alarms where possible.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create single Amazon CloudWatch metric alarms with multiple metric thresholds where",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCreating Amazon CloudWatch composite alarms is the correct solution for monitoring multiple conditions simultaneously and reducing false alarms. Composite alarms evaluate the states of multiple underlying metric alarms and only trigger when all specified conditions are met. In this case, you can create two metric alarms: one for CPU utilization > 50% and another for high read IOPS. Then create a composite alarm that only goes into ALARM state when BOTH underlying alarms are in ALARM state simultaneously. This ensures that alerts are only sent when CPU is high AND disk IOPS are high at the same time, not when CPU spikes briefly alone. Composite alarms reduce false alarms by requiring multiple conditions to be true before alerting, which is exactly what's needed here.\n\n**Why option 1 is incorrect:**\nThe option that says create Amazon CloudWatch dashboards to visualize metrics and react to issues quickly is incorrect because dashboards are visualization tools that require manual monitoring and reaction. Dashboards don't automatically alert when conditions are met - they just display metrics. The requirement is to \"act as soon as possible\" when specific conditions occur, which requires automated alarms, not manual dashboard monitoring. Dashboards don't reduce false alarms or provide automated alerting.\n\n**Why option 2 is incorrect:**\nThe option that says create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm is incorrect because Synthetics canaries are designed for end-to-end monitoring of application availability and performance from a user's perspective, not for monitoring infrastructure metrics like CPU utilization and disk IOPS. Canaries run synthetic transactions to test application functionality, but they don't monitor EC2 instance-level metrics. This approach doesn't address the requirement to monitor CPU and IOPS metrics simultaneously.\n\n**Why option 3 is incorrect:**\nThe option that says create single Amazon CloudWatch metric alarms with multiple metric thresholds is incorrect because a single metric alarm can only monitor one metric at a time. You cannot create a single alarm that monitors both CPU utilization and read IOPS simultaneously. To monitor multiple metrics together, you need either multiple alarms with a composite alarm, or separate alarms that don't provide the \"both conditions must be true\" logic needed to reduce false alarms.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "A company wants to migrate its on-premises data center to AWS. According to the company's \ncompliance requirements, the company can use only the ap-northeast-3 Region. Company \nadministrators are not permitted to connect VPCs to the internet. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Control Tower to implement data residency guardrails to deny internet access and",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use rules in AWS WAF to prevent internet access.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Config to activate managed rules to detect and alert for internet gateways and to",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3 is correct. AWS Control Tower provides guardrails that can enforce organizational policies at scale. The \"Region Deny\" guardrail can restrict resource creation to only the ap-northeast-3 Region, ensuring compliance with data residency requirements. Additionally, Control Tower can implement guardrails that prevent VPCs from having internet gateways attached, effectively denying internet access. Control Tower integrates with AWS Organizations and provides centralized governance, making it the most comprehensive solution for enforcing both the region restriction and the no-internet-access requirement across all accounts.\n\n**Why option 1 is incorrect:**\nThe option that says use rules in AWS WAF to prevent internet access is incorrect because AWS WAF (Web Application Firewall) is designed to protect web applications from common web exploits and attacks. WAF operates at the application layer (Layer 7) and filters HTTP/HTTPS traffic, but it cannot prevent VPCs from connecting to the internet. WAF doesn't control network-level access or prevent internet gateway attachments. Additionally, denying access to all AWS Regions except ap-northeast-3 in AWS account settings is not a standard feature - region restrictions require service control policies or Control Tower guardrails.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS Organizations to configure service control policies (SCPs) that prevent VPCs from gaining internet access is partially correct but incomplete. While SCPs can restrict certain actions, SCPs cannot directly prevent VPCs from having internet gateways attached or prevent internet connectivity. SCPs work by denying IAM permissions, but internet gateway attachment and routing are network-level configurations that SCPs cannot directly control. However, SCPs can be used to deny access to regions other than ap-northeast-3, but this alone doesn't fully meet the requirement to prevent internet access.\n\n**Why option 3 is incorrect:**\nThe option that says create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0 is incorrect because network ACLs (NACLs) are stateless and operate at the subnet level, not the VPC level. Creating NACL rules to deny outbound traffic would prevent legitimate internal communication and would need to be configured for each subnet in each VPC, which is operationally complex. Additionally, NACLs don't prevent internet gateways from being attached to VPCs - they only filter traffic. The IAM policy approach to prevent region usage is also not the most effective method compared to SCPs or Control Tower guardrails.\n\n**Why option 4 is incorrect:**\nThe option that says use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3 is incorrect because AWS Config is a compliance and auditing service that detects and reports on configuration changes, but it does not prevent actions from occurring. Config can alert when internet gateways are attached or when resources are created in other regions, but it cannot prevent these actions. The requirement is to prevent VPCs from connecting to the internet, not just to detect and alert when they do. Config is reactive, not preventive.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company uses a three-tier web application to provide training to new employees. The \napplication is accessed for only 12 hours every day. The company is using an Amazon RDS for \nMySQL DB instance to store information and wants to minimize costs. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an IAM policy for AWS Systems Manager Session Manager.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Launch an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create AWS Lambda functions to start and stop the DB instance.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nCreating AWS Lambda functions to start and stop the DB instance, along with Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions, is the most cost-effective solution. Since the application is only accessed for 12 hours per day, the RDS instance can be stopped during the 12 hours when it's not needed, eliminating compute costs. Lambda functions can be created to start the RDS instance before the 12-hour access window and stop it after. EventBridge scheduled rules can trigger these Lambda functions on a daily schedule (e.g., start at 8 AM, stop at 8 PM). This approach minimizes costs by only paying for RDS compute resources during the 12 hours of actual usage, while storage costs continue at a lower rate. Lambda functions are serverless and incur minimal costs, making this solution highly cost-effective.\n\n**Why option 0 is incorrect:**\nThe option that says configure an IAM policy for AWS Systems Manager Session Manager, create an IAM role for the policy, update the trust relationship of the role, and set up automatic start and stop for the DB instance is incorrect because Systems Manager Session Manager is used for secure shell access to EC2 instances, not for managing RDS instances. Session Manager doesn't have built-in capabilities to start and stop RDS instances on a schedule. While you could potentially use Systems Manager Automation documents, this approach is more complex and less efficient than using Lambda with EventBridge. The IAM policy and role configuration described doesn't directly enable scheduled start/stop functionality.\n\n**Why option 1 is incorrect:**\nThe option that says create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from the cache when the DB instance is stopped, and invalidate the cache after the DB instance is started is incorrect because ElastiCache doesn't solve the cost minimization requirement - it adds additional costs for the cache cluster. While caching can improve performance, the requirement is specifically to minimize costs for the RDS instance. Additionally, if the DB instance is stopped, users cannot access data from cache if the cache needs to be populated from the database. Cache invalidation after DB restart doesn't address the core requirement of minimizing RDS costs through scheduled start/stop.\n\n**Why option 2 is incorrect:**\nThe option that says launch an Amazon EC2 instance, create an IAM role that grants access to Amazon RDS, attach the role to the EC2 instance, and configure a cron job to start and stop the EC2 instance on the desired schedule is incorrect because this approach adds unnecessary infrastructure costs. Running an EC2 instance 24/7 just to manage RDS start/stop schedules defeats the purpose of cost minimization. The EC2 instance itself incurs costs even when idle. Additionally, the cron job would start and stop the EC2 instance, not the RDS instance. A serverless Lambda-based approach eliminates the need for a persistent EC2 instance and is more cost-effective.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A company sells ringtones created from clips of popular songs. The files containing the ringtones \nare stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of \nfiles, but downloads are infrequent for ringtones older than 90 days. The company needs to save \nmoney on storage while keeping the most accessed files readily available for its users. \n \nWhich action should the company take to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nImplementing an S3 Lifecycle policy that moves objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days is the most cost-effective solution. S3 Lifecycle policies automatically transition objects between storage classes based on age, without requiring manual intervention or additional infrastructure. Since files older than 90 days have infrequent downloads, moving them to S3 Standard-IA reduces storage costs by approximately 50% while maintaining immediate access when needed. Files less than 90 days old remain in S3 Standard, keeping the most accessed files readily available with low latency. Lifecycle policies are free to configure and execute automatically, making this the most cost-effective approach that meets both the cost savings requirement and the need to keep frequently accessed files readily available.\n\n**Why option 0 is incorrect:**\nThe option that says configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects is incorrect because S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. If all files start in Standard-IA, the company will pay higher storage costs and retrieval fees for files that are frequently accessed (those less than 90 days old). Standard-IA has a minimum storage duration charge of 30 days and charges per-GB retrieval fees, which would be expensive for frequently accessed files. The requirement states that downloads are infrequent only for files older than 90 days, so newer files should remain in Standard storage.\n\n**Why option 1 is incorrect:**\nThe option that says move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days is incorrect because S3 Intelligent-Tiering automatically moves objects between access tiers based on access patterns, not based on age. Intelligent-Tiering monitors access patterns and moves objects that haven't been accessed for 30+ days to the Infrequent Access tier. However, you cannot configure Intelligent-Tiering to move objects after a specific number of days (like 90 days) - it works based on access patterns. Additionally, Intelligent-Tiering charges a small monitoring and automation fee per object, which could add up with millions of files, making it less cost-effective than a simple Lifecycle policy.\n\n**Why option 2 is incorrect:**\nThe option that says configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days is incorrect because S3 Inventory is a reporting and analytics tool that generates reports about objects and their metadata. Inventory reports are stored in S3 buckets and can be used for compliance, operational, and cost optimization purposes, but Inventory itself does not move objects between storage classes. You would need a separate process (like Lambda functions or manual scripts) to read the inventory reports and move objects, which adds complexity and operational overhead compared to the automated Lifecycle policy approach.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company needs to save the results from a medical trial to an Amazon S3 repository. The \nrepository must allow a few scientists to add new files and must restrict all other users to read-\nonly access. No users can have the ability to modify or delete any files in the repository. The \ncompany must keep every file in the repository for a minimum of 1 year after its creation date. \n \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n39",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Object Lock in governance mode with a legal hold of 1 year",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use S3 Object Lock in compliance mode with a retention period of 365 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket Use an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure the S3 bucket to invoke an AWS Lambda function every tune an object is added",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing S3 Object Lock in compliance mode with a retention period of 365 days is the correct solution. S3 Object Lock prevents objects from being deleted or overwritten for a fixed amount of time or indefinitely. Compliance mode is the most restrictive mode - even the root user cannot delete or overwrite objects during the retention period, ensuring that files are kept for the minimum 1 year requirement. This provides immutable storage that meets compliance requirements for medical trial data. Combined with IAM policies that grant write access only to scientists and read-only access to others, this solution ensures that files cannot be modified or deleted by anyone during the retention period, while still allowing authorized scientists to add new files.\n\n**Why option 0 is incorrect:**\nThe option that says use S3 Object Lock in governance mode with a legal hold of 1 year is incorrect because governance mode allows users with special permissions (like `s3:BypassGovernanceMode`) to delete or overwrite objects during the retention period, which violates the requirement that no users can modify or delete files. Legal hold is a separate feature that prevents deletion until the legal hold is removed, but it doesn't have a fixed retention period - it must be manually removed. The requirement specifies a minimum 1 year retention period, which is better handled by compliance mode with a fixed retention period.\n\n**Why option 2 is incorrect:**\nThe option that says use an IAM role to restrict all users from deleting or changing objects in the S3 bucket is incorrect because IAM policies alone cannot prevent object deletion or modification if a user has the necessary permissions, and they don't provide immutable storage guarantees. Even with restrictive IAM policies, there's a risk that permissions could be changed, or that users with administrative access could delete objects. Additionally, IAM policies don't enforce time-based retention requirements - they can't guarantee that files are kept for a minimum of 1 year. S3 Object Lock provides a stronger, immutable storage mechanism that cannot be bypassed.\n\n**Why option 3 is incorrect:**\nThe option that says configure the S3 bucket to invoke an AWS Lambda function every time an object is added, and use the Lambda function to set a tag on the object with a timestamp is incorrect because S3 object tags are metadata that can be modified or deleted, and they don't prevent object deletion or modification. Tags are useful for organization and lifecycle policies, but they don't provide immutable storage or prevent users from deleting objects. Even if you use tags to track creation dates, there's no mechanism to enforce the 1-year retention requirement or prevent deletion based on tags alone.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A large media company hosts a web application on AWS. The company wants to start caching \nconfidential media files so that users around the world will have reliable access to the files. The \ncontent is stored in Amazon S3 buckets. The company must deliver the content quickly, \nregardless of where the requests originate geographically. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to connect the S3 buckets to the web application.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS Global Accelerator to connect the S3 buckets to the web application.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nDeploying Amazon CloudFront to connect the S3 buckets to CloudFront edge servers is the correct solution for delivering confidential media files quickly to users worldwide. CloudFront is a Content Delivery Network (CDN) that caches content at edge locations around the world, reducing latency by serving content from locations closest to users. CloudFront can be configured with Origin Access Identity (OAI) or Origin Access Control (OAC) to securely access private S3 buckets, ensuring that confidential media files are protected. CloudFront caches files at edge locations, so subsequent requests for the same content are served from the cache, providing fast and reliable access regardless of geographic location. This solution meets both the caching requirement and the global delivery requirement.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DataSync to connect the S3 buckets to the web application is incorrect because AWS DataSync is a data transfer service designed for moving large amounts of data between on-premises storage and AWS, or between AWS services. DataSync is not designed for serving content to end users or providing global content delivery. It doesn't provide caching capabilities or reduce latency for user requests. DataSync is used for one-time or scheduled data migrations, not for real-time content delivery to users worldwide.\n\n**Why option 1 is incorrect:**\nThe option that says deploy AWS Global Accelerator to connect the S3 buckets to the web application is incorrect because AWS Global Accelerator is a networking service that improves application availability and performance by routing traffic over AWS's global network infrastructure to application endpoints. However, Global Accelerator doesn't cache content - it proxies requests and connects to the application every time for the response. For media files stored in S3, Global Accelerator would route requests to S3, but without caching, each request would still need to travel to the S3 bucket's region, which doesn't provide the fast, cached delivery that CloudFront offers. Global Accelerator is better suited for dynamic applications, not static media file delivery.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application is incorrect because SQS is a message queuing service designed for decoupling and scaling microservices, distributed systems, and serverless applications. SQS doesn't provide content delivery, caching, or CDN capabilities. It's used for asynchronous message processing, not for serving media files to end users. SQS cannot deliver cached content to users around the world or reduce latency for media file access.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 8,
    "text": "A company produces batch data that comes from different databases. The company also \nproduces live stream data from network sensors and application APIs. The company needs to \nconsolidate all the data into one place for business analytics. The company needs to process the \nincoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-\ntime queries and import the data into a business intelligence tool to show key performance \nindicators (KPIs). \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Athena foe one-time queries.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon Kinesis Data Analytics for one-time queries.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create custom AWS Lambda functions to move the individual records from me databases to an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Glue extract transform, and toad (ETL) job to convert the data into JSON format.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nUsing Amazon Athena for one-time queries is correct because Athena is a serverless interactive query service that allows you to analyze data directly in S3 using standard SQL. Since the data will be staged in different S3 buckets, Athena can query this data without requiring any infrastructure setup or data loading into a database. Athena charges only for the queries you run and the data scanned, making it ideal for one-time queries with minimal operational overhead. Teams can run ad-hoc queries on the consolidated data in S3 without managing servers or clusters.\n\n**Why option 4 is correct:**\nUsing blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake, and using AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format is correct. Lake Formation provides blueprints that help identify and catalog data sources, making it easier to set up a data lake. AWS Glue is a serverless ETL service that can crawl various data sources (databases, APIs, streams), extract and transform the data, and load it into S3 in optimized formats like Parquet. This approach consolidates batch data from databases and live stream data from sensors/APIs into S3 with minimal operational overhead, as Glue is fully managed and doesn't require infrastructure provisioning.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Kinesis Data Analytics for one-time queries is incorrect because Kinesis Data Analytics is designed for real-time stream processing and continuous queries on streaming data, not for one-time ad-hoc queries on batch data. Kinesis Data Analytics processes data in real-time as it flows through streams, which doesn't match the requirement for one-time queries on consolidated data. Additionally, Kinesis Data Analytics requires setting up and managing Kinesis streams, which adds operational overhead compared to serverless solutions like Athena.\n\n**Why option 2 is incorrect:**\nThe option that says create custom AWS Lambda functions to move individual records from the databases to an Amazon Redshift cluster is incorrect because this approach requires significant operational overhead. You would need to write, deploy, and maintain custom Lambda functions for each data source. Additionally, Redshift is a data warehouse that requires cluster provisioning, management, and scaling, which adds operational complexity. The requirement is to stage data in S3 buckets, not load it into Redshift, and the approach should minimize operational overhead.\n\n**Why option 3 is incorrect:**\nThe option that says use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format and load the data into multiple Amazon OpenSearch Service clusters is incorrect because while Glue can handle the ETL part, loading data into multiple OpenSearch clusters adds unnecessary operational overhead. OpenSearch is designed for search and analytics workloads, not for general business intelligence queries. The requirement is to stage data in S3 buckets for one-time queries, not to load it into search clusters. Additionally, managing multiple OpenSearch clusters increases operational complexity and costs.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 9,
    "text": "A gaming company has a web application that displays scores. The application runs on Amazon \nEC2 instances behind an Application Load Balancer. The application stores data in an Amazon \nRDS for MySQL database. Users are starting to experience long delays and interruptions that are \ncaused by database read performance. The company wants to improve the user experience while \nminimizing changes to the application's architecture. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache in front of the database.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use RDS Proxy between the application and the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application from EC2 instances to AWS Lambda.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing Amazon ElastiCache in front of the database is the correct solution to improve database read performance while minimizing changes to the application architecture. ElastiCache provides an in-memory caching layer (using Redis or Memcached) that stores frequently accessed data, allowing the application to retrieve data from the fast cache instead of querying the slower RDS MySQL database for every request. This significantly reduces database load and improves response times. The application only needs minor code changes to check the cache first before querying the database, and to update the cache when data changes. This approach minimizes architectural changes while dramatically improving read performance for frequently accessed data like game scores.\n\n**Why option 1 is incorrect:**\nThe option that says use RDS Proxy between the application and the database is incorrect because RDS Proxy is primarily designed for connection pooling and managing database connections, not for improving read performance through caching. RDS Proxy helps reduce the number of database connections and improves connection management, but it doesn't cache query results or reduce database load for read operations. While RDS Proxy can help with connection-related performance issues, it doesn't address the core problem of slow database read performance that requires caching frequently accessed data.\n\n**Why option 2 is incorrect:**\nThe option that says migrate the application from EC2 instances to AWS Lambda is incorrect because migrating to Lambda would require significant architectural changes, which violates the requirement to minimize changes. Additionally, Lambda migration alone doesn't solve the database read performance issue - the application would still need to query the same RDS database, potentially experiencing the same delays. Lambda is better suited for event-driven workloads, and migrating a web application from EC2 to Lambda would require rewriting significant portions of the application code and changing how the application handles requests.\n\n**Why option 3 is incorrect:**\nThe option that says migrate the database from Amazon RDS for MySQL to Amazon DynamoDB is incorrect because this would require massive architectural changes, rewriting the application's data access layer, and potentially redesigning the data model to fit DynamoDB's NoSQL structure. This violates the requirement to minimize changes to the application's architecture. Additionally, DynamoDB migration is a complex process that could introduce compatibility issues, data migration challenges, and require extensive application code changes. The requirement is to improve read performance with minimal changes, not to migrate to a completely different database system.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 10,
    "text": "A business's backup data totals 700 terabytes (TB) and is kept in network attached storage \n(NAS) at its data center. This backup data must be available in the event of occasional regulatory \ninquiries and preserved for a period of seven years. The organization has chosen to relocate its \nbackup data from its on-premises data center to Amazon Web Services (AWS). Within one \nmonth, the migration must be completed. The company's public internet connection provides 500 \nMbps of dedicated capacity for data transport. \n \nWhat should a solutions architect do to ensure that data is migrated and stored at the LOWEST \npossible cost?",
    "options": [
      {
        "id": 0,
        "text": "Order AWS Snowball devices to transfer the data.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy a VPN connection between the data center and Amazon VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOrdering AWS Snowball devices to transfer the data and using a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive is the most cost-effective solution. With 700 TB of data and only 500 Mbps internet capacity, transferring over the internet would take approximately 130 days (700 TB / 500 Mbps), which exceeds the 1-month requirement. AWS Snowball devices can transfer up to 80 TB per device, so approximately 9 Snowball devices would be needed. Snowball provides fast, secure, offline data transfer that bypasses internet bandwidth limitations. After the data is transferred to S3, a lifecycle policy can automatically transition it to S3 Glacier Deep Archive, which is the lowest-cost storage class at $0.00099 per GB/month, perfect for long-term archival storage with occasional regulatory inquiries. This solution meets both the migration timeline and the lowest cost storage requirement.\n\n**Why option 1 is incorrect:**\nThe option that says deploy a VPN connection between the data center and Amazon VPC and use the AWS CLI to copy the data from on-premises to Amazon S3 Glacier is incorrect because a VPN connection doesn't increase bandwidth - it still uses the same 500 Mbps internet connection. Transferring 700 TB over 500 Mbps would take approximately 130 days, far exceeding the 1-month requirement. Additionally, VPN adds encryption overhead and doesn't improve transfer speeds. While S3 Glacier is cost-effective, the transfer method doesn't meet the timeline requirement.\n\n**Why option 2 is incorrect:**\nThe option that says provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3, then use a lifecycle policy to transition to S3 Glacier Deep Archive is incorrect because Direct Connect at 500 Mbps would still take approximately 130 days to transfer 700 TB, exceeding the 1-month timeline. Additionally, Direct Connect requires physical installation and setup time, which could delay the migration start. While Direct Connect provides dedicated network connectivity, it doesn't solve the bandwidth limitation problem for such a large dataset within the required timeframe.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS DataSync to transfer the data and deploy a DataSync agent on-premises, then use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier is incorrect because DataSync still relies on the available network bandwidth. With 500 Mbps capacity, DataSync would take approximately 130 days to transfer 700 TB, which exceeds the 1-month requirement. While DataSync can optimize transfers and handle network interruptions, it cannot overcome the fundamental bandwidth limitation. Additionally, DataSync cannot directly write to S3 Glacier - it writes to S3 Standard, and then a lifecycle policy would need to transition to Glacier, adding an extra step.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 11,
    "text": "A company wants to direct its users to a backup static error page if the company's primary \nwebsite is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The \ndomain is pointing to an Application Load Balancer (ALB). The company needs a solution that \nminimizes changes and infrastructure overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the Route 53 records to use a latency routing policy.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a Route 53 active-passive failover configuration.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the Route 53 records to use a multivalue answer routing policy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nSetting up a Route 53 active-passive failover configuration is the correct solution that minimizes changes and infrastructure overhead. Route 53 health checks can monitor the health of the Application Load Balancer (ALB). When the ALB is healthy, Route 53 routes traffic to it. When the health check detects that the ALB is unavailable, Route 53 automatically fails over to a secondary record that points to a static error page hosted in an S3 bucket (configured as a website endpoint). This solution requires minimal changes - you only need to create a health check for the ALB, create a secondary record pointing to the S3 bucket, and configure failover routing. No changes are needed to the ALB or application infrastructure. The static error page in S3 provides a cost-effective, highly available backup with minimal operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says update the Route 53 records to use a latency routing policy and add a static error page hosted in an S3 bucket to the records so that traffic is sent to the most responsive endpoints is incorrect because latency routing policy routes traffic based on the lowest latency to endpoints, not based on availability. If the primary ALB is unavailable, latency routing won't automatically fail over to the S3 bucket - it will still try to route to the ALB if it appears to have lower latency. Latency routing doesn't provide automatic failover based on health checks, which is what's needed to redirect users to a backup error page when the primary site is unavailable.\n\n**Why option 2 is incorrect:**\nThe option that says set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts the error page is incorrect because active-active configurations distribute traffic across multiple endpoints, which doesn't meet the requirement to direct users to a backup error page only when the primary site is unavailable. Additionally, running an EC2 instance to host a static error page adds unnecessary infrastructure overhead and costs compared to hosting the error page in S3. The requirement is to minimize infrastructure overhead, and S3 static website hosting is more cost-effective than an EC2 instance.\n\n**Why option 3 is incorrect:**\nThe option that says update the Route 53 records to use a multivalue answer routing policy is incorrect because multivalue answer routing returns multiple healthy IP addresses in response to DNS queries, allowing clients to choose which IP to connect to. However, multivalue routing doesn't provide automatic failover to a backup error page when the primary site is unavailable. It's designed for distributing traffic across multiple healthy endpoints, not for failover scenarios. Multivalue routing also doesn't integrate with health checks in a way that automatically redirects to a backup page when the primary endpoint fails.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 12,
    "text": "A corporation has recruited a new cloud engineer who should not have access to the \nCompanyConfidential Amazon S3 bucket. The cloud engineer must have read and write \npermissions on an S3 bucket named AdminTools. \n \nWhich IAM policy will satisfy these criteria? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n42",
    "options": [
      {
        "id": 0,
        "text": "B.",
        "correct": true
      },
      {
        "id": 2,
        "text": "D.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe IAM policy that explicitly denies access to the CompanyConfidential S3 bucket and allows read and write permissions on the AdminTools bucket satisfies the requirements. IAM policies use explicit deny statements that take precedence over allow statements, ensuring that even if the user gains permissions through other means, they cannot access the CompanyConfidential bucket. The policy should include `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, and `s3:ListBucket` permissions for the AdminTools bucket to provide read and write access. The explicit deny for CompanyConfidential ensures compliance with the principle of least privilege by preventing access to sensitive data while allowing necessary access to AdminTools.\n\n**Why option 2 is incorrect:**\nThe other IAM policy option (D) likely doesn't include an explicit deny statement for the CompanyConfidential bucket, or it grants broader permissions than necessary. Without an explicit deny, there's a risk that the cloud engineer could gain access to CompanyConfidential through other IAM policies, groups, or roles. The requirement is clear that the engineer must not have access to CompanyConfidential, which requires an explicit deny statement in the IAM policy to ensure this restriction is enforced regardless of other permissions.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 13,
    "text": "A new employee has joined a company as a deployment engineer. The deployment engineer will \nbe using AWS CloudFormation templates to create multiple AWS resources.  \nA solutions architect wants the deployment engineer to perform job activities while following the \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n44 \nprinciple of least privilege. \n \nWhich steps should the solutions architect do in conjunction to reach this goal? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Have the deployment engineer use AWS account roof user credentials for performing AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new IAM User for the deployment engineer and add the IAM user to a group that has",
        "correct": true
      },
      {
        "id": 4,
        "text": "Create an IAM role for the deployment engineer to explicitly define the permissions specific to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3,
      4
    ],
    "explanation": "**Why option 3 is correct:**\nCreating a new IAM user for the deployment engineer and adding the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only follows the principle of least privilege. This approach grants only the specific permissions needed for CloudFormation operations, preventing the engineer from performing actions beyond their job requirements. By using IAM groups and policies, permissions can be managed centrally and consistently. The policy should include permissions for CloudFormation actions (like `cloudformation:CreateStack`, `cloudformation:UpdateStack`, `cloudformation:DeleteStack`) and any necessary permissions for the resources that CloudFormation will create, but nothing more.\n\n**Why option 4 is correct:**\nCreating an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launching stacks using that IAM role is the best practice for least privilege. IAM roles are preferred over IAM users for programmatic access because they don't require long-term credentials and can be assumed when needed. The role should have permissions scoped specifically to the CloudFormation operations and the resources that will be created. When launching stacks, the engineer can specify this role using the `--role-arn` parameter, ensuring that CloudFormation uses the role's permissions rather than the engineer's user permissions. This provides better security and auditability.\n\n**Why option 0 is incorrect:**\nThe option that says have the deployment engineer use AWS account root user credentials for performing AWS CloudFormation stack operations is incorrect because using root user credentials violates the principle of least privilege entirely. The root user has full access to all AWS services and resources, which is far beyond what a deployment engineer needs. Root user credentials should never be shared or used for day-to-day operations. Using root credentials creates a significant security risk and makes it impossible to audit who performed which actions, as root actions cannot be attributed to specific individuals.\n\n**Why option 1 is incorrect:**\nThe option that says create a new IAM user for the deployment engineer and add the IAM user to a group that has the PowerUsers IAM policy attached is incorrect because the PowerUsers policy grants broad permissions to most AWS services, excluding IAM and billing management. This violates the principle of least privilege by granting far more permissions than necessary for CloudFormation operations. A deployment engineer only needs permissions to create, update, and delete CloudFormation stacks and the resources those stacks create, not broad access to all AWS services.\n\n**Why option 2 is incorrect:**\nThe option that says create a new IAM user for the deployment engineer and add the IAM user to a group that has the AdministratorAccess IAM policy attached is incorrect because AdministratorAccess grants full permissions to all AWS services and resources, which completely violates the principle of least privilege. A deployment engineer should only have permissions necessary for CloudFormation operations, not full administrative access. Granting AdministratorAccess defeats the purpose of following least privilege principles and creates significant security risks.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company runs a high performance computing (HPC) workload on AWS. The workload required \nlow-latency network performance and high network throughput with tightly coupled node-to-node \ncommunication. The Amazon EC2 instances are properly sized for compute and storage \ncapacity, and are launched using default options. \n \nWhat should a solutions architect propose to improve the performance of the workload?",
    "options": [
      {
        "id": 0,
        "text": "Choose a cluster placement group while launching Amazon EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Choose dedicated instance tenancy while launching Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Choose an Elastic Inference accelerator while launching Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Choose the required capacity reservation while launching Amazon EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nChoosing a cluster placement group while launching Amazon EC2 instances is the correct solution for HPC workloads requiring low-latency network performance and high network throughput with tightly coupled node-to-node communication. Cluster placement groups place instances within a single Availability Zone in close physical proximity, enabling instances to communicate over a high-bandwidth, low-latency network. This is ideal for applications that require tightly coupled node-to-node communication, such as HPC workloads, where network performance is critical. Cluster placement groups provide up to 10 Gbps network performance between instances, significantly better than the default network performance of EC2 instances launched without placement groups.\n\n**Why option 1 is incorrect:**\nThe option that says choose dedicated instance tenancy while launching Amazon EC2 instances is incorrect because dedicated tenancy ensures that instances run on hardware dedicated to a single AWS account, but it doesn't improve network performance or latency. Dedicated tenancy is used for compliance and isolation requirements, not for performance optimization. The network performance characteristics remain the same regardless of tenancy. For HPC workloads requiring low-latency, high-throughput communication, cluster placement groups are necessary, not dedicated tenancy.\n\n**Why option 2 is incorrect:**\nThe option that says choose an Elastic Inference accelerator while launching Amazon EC2 instances is incorrect because Elastic Inference accelerators are designed to attach GPU inference acceleration to EC2 instances for machine learning workloads, not for improving network performance. Elastic Inference doesn't affect network latency or throughput between instances. HPC workloads with tightly coupled node-to-node communication require optimized network placement, not GPU acceleration.\n\n**Why option 3 is incorrect:**\nThe option that says choose the required capacity reservation while launching Amazon EC2 instances is incorrect because capacity reservations guarantee EC2 capacity in a specific Availability Zone for a specified duration, but they don't improve network performance. Capacity reservations ensure capacity availability but don't affect network latency or throughput. For HPC workloads, the key requirement is low-latency, high-throughput network communication between instances, which is achieved through cluster placement groups, not capacity reservations.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 15,
    "text": "A company wants to use the AWS Cloud to make an existing application highly available and \nresilient. The current version of the application resides in the company's data center. The \napplication recently experienced data loss after a database server crashed because of an \nunexpected power outage. The company needs a solution that avoids any single points of failure. \nThe solution must give the application the ability to scale to meet user demand. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nDeploying the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones and using an Amazon RDS DB instance in a Multi-AZ configuration is the correct solution. Auto Scaling groups across multiple Availability Zones ensure that if one AZ fails, instances in other AZs continue serving traffic, eliminating single points of failure at the application layer. Auto Scaling also provides the ability to scale out and in based on demand. RDS Multi-AZ configuration automatically replicates the database synchronously to a standby instance in a different Availability Zone. If the primary database fails due to an outage, RDS automatically fails over to the standby, preventing data loss and ensuring high availability. This addresses the previous data loss issue from the power outage by providing automatic failover and data replication.\n\n**Why option 1 is incorrect:**\nThe option that says deploy the application servers using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone, deploy the database on an EC2 instance, and enable EC2 Auto Recovery is incorrect because deploying everything in a single Availability Zone creates a single point of failure. If that AZ experiences an outage (like the power outage mentioned), both the application and database would be unavailable. EC2 Auto Recovery restarts failed instances but doesn't prevent data loss or provide automatic failover. Additionally, managing a database on EC2 requires significant operational overhead compared to managed RDS, and EC2 Auto Recovery doesn't provide the same level of data protection as RDS Multi-AZ.\n\n**Why option 2 is incorrect:**\nThe option that says deploy the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones, use an Amazon RDS DB instance with a read replica in a single Availability Zone, and promote the read replica to replace the primary DB instance if the primary fails is incorrect because this approach requires manual intervention to promote the read replica, which doesn't meet the requirement for automatic failover and high availability. Read replicas are asynchronous and may have replication lag, potentially causing data loss. Additionally, having the read replica in the same AZ as the primary doesn't protect against AZ-level failures. RDS Multi-AZ provides automatic, synchronous replication and failover without manual intervention.\n\n**Why option 3 is incorrect:**\nThe option that says deploy the application servers using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones, deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones, and use Amazon EBS Multi-Attach to create shared storage between the instances is incorrect because EBS Multi-Attach is designed for specific use cases like clustered applications that require shared block storage, but it doesn't provide automatic failover or data replication. Managing database replication and failover on EC2 requires significant operational overhead and custom configuration, which is error-prone and doesn't provide the same level of reliability as RDS Multi-AZ. This approach also doesn't address the data loss concern as effectively as RDS Multi-AZ's synchronous replication.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto \nScaling group in the AWS Cloud. The application will transmit data by using UDP packets. The \ncompany wants to ensure that the application can scale out and in as traffic increases and \ndecreases. What should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Attach a Network Load Balancer to the Auto Scaling group",
        "correct": true
      },
      {
        "id": 1,
        "text": "Attach an Application Load Balancer to the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAttaching a Network Load Balancer to the Auto Scaling group is the correct solution. Network Load Balancers (NLBs) operate at Layer 4 and support both TCP and UDP protocols, making them ideal for gaming applications that transmit data using UDP packets. NLBs provide ultra-low latency and can handle millions of requests per second, which is essential for gaming workloads. When attached to an Auto Scaling group, the NLB automatically distributes traffic across healthy instances as the group scales out and in based on traffic patterns. This ensures that the gaming application can handle variable traffic loads while maintaining low latency for UDP packet transmission. NLBs also provide health checks to ensure traffic is only routed to healthy instances.\n\n**Why option 1 is incorrect:**\nThe option that says attach an Application Load Balancer to the Auto Scaling group is incorrect because Application Load Balancers (ALBs) operate at Layer 7 (HTTP/HTTPS) and are designed for HTTP/HTTPS traffic, not UDP packets. Gaming applications that use UDP require Layer 4 (transport layer) load balancing. ALBs cannot handle UDP traffic effectively and are not suitable for gaming applications that rely on UDP for real-time communication. ALBs are optimized for HTTP/HTTPS workloads and provide features like content-based routing that aren't relevant for UDP gaming traffic.\n\n**Why option 2 is incorrect:**\nThe option that says deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately is incorrect because Route 53 is a DNS service that routes traffic at the DNS level, not a load balancer. Weighted routing distributes traffic based on weights assigned to multiple resources, but it doesn't provide the same level of health checking, automatic failover, and connection-level load balancing that a Network Load Balancer provides. Route 53 weighted routing is better suited for blue/green deployments or gradual traffic shifting, not for real-time load balancing of gaming traffic. DNS-based routing also introduces additional latency and doesn't provide the same level of connection persistence needed for gaming applications.\n\n**Why option 3 is incorrect:**\nThe option that says deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group is incorrect because NAT instances are designed for outbound internet connectivity for private subnet instances, not for load balancing incoming traffic. NAT instances are single points of failure, don't scale automatically, and require manual configuration and management. They don't provide health checking or automatic traffic distribution across multiple instances. This approach doesn't meet the requirement for the application to scale out and in automatically, and NAT instances become a bottleneck for high-throughput gaming traffic.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A solutions architect is designing a customer-facing application for a company. The application's \ndatabase will have a clearly defined access pattern throughout the year and will have a variable \nnumber of reads and writes that depend on the time of year. The company must retain audit \nrecords for the database for 7 days. The recovery point objective (RPO) must be less than 5 \nhours. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon DynamoDB with auto scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Redshift. Configure concurrency scaling.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon RDS with Provisioned IOPS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Aurora MySQL with auto scaling.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon Redshift, configuring concurrency scaling, activating audit logging, and performing database snapshots every 4 hours meets all the requirements. Redshift is a data warehouse designed for analytics workloads with clearly defined access patterns. Concurrency scaling automatically adds additional cluster capacity to handle variable read and write workloads based on demand, which addresses the requirement for variable reads and writes depending on the time of year. Redshift audit logging can be configured to retain audit records for 7 days. Performing database snapshots every 4 hours ensures that the Recovery Point Objective (RPO) is less than 5 hours, as snapshots capture the point-in-time state of the database. Redshift snapshots are incremental and stored in S3, providing durable backup capabilities.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon DynamoDB with auto scaling and use on-demand backups and Amazon DynamoDB Streams is incorrect because DynamoDB is a NoSQL database designed for high-performance, low-latency applications, not for analytics workloads with clearly defined access patterns. While DynamoDB auto scaling can handle variable workloads, DynamoDB Streams are designed for real-time data processing and change data capture, not for meeting RPO requirements. DynamoDB on-demand backups don't provide the same level of point-in-time recovery granularity needed for a 5-hour RPO requirement. Additionally, DynamoDB audit logging capabilities are more limited compared to Redshift.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon RDS with Provisioned IOPS, activate the database auditing parameter, and perform database snapshots every 5 hours is incorrect because performing snapshots every 5 hours doesn't meet the RPO requirement of less than 5 hours. If a failure occurs just before a snapshot, you could lose up to 5 hours of data, which exceeds the RPO requirement. Additionally, RDS Provisioned IOPS is designed for consistent I/O performance but doesn't automatically scale to handle variable workloads like Redshift concurrency scaling does. RDS is better suited for transactional workloads, not analytics workloads with clearly defined access patterns.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Aurora MySQL with auto scaling and activate the database auditing parameter is incorrect because Aurora auto scaling adjusts the number of Aurora Replicas based on workload, but it doesn't address the RPO requirement. Aurora provides continuous backup to S3, but without explicit snapshot scheduling, you cannot guarantee that the RPO will be less than 5 hours. Aurora is designed for transactional database workloads, not analytics workloads. While Aurora has audit logging capabilities, the solution doesn't specify how to meet the 7-day audit retention requirement or the RPO requirement of less than 5 hours.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company hosts a two-tier application on Amazon EC2 instances and Amazon RDS. The \napplication's demand varies based on the time of day. The load is minimal after work hours and \non weekends. The EC2 instances run in an EC2 Auto Scaling group that is configured with a \nminimum of two instances and a maximum of five instances. The application must be available at \nall times, but the company is concerned about overall cost. \n \nWhich solution meets the availability requirement MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use all EC2 Spot Instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase EC2 Instance Savings Plans to cover five EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase two EC2 Reserved Instances.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Purchase EC2 Instance Savings Plans to cover two EC2 instances.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nPurchasing EC2 Instance Savings Plans to cover two EC2 instances is the most cost-effective solution that meets the availability requirement. Savings Plans provide significant discounts (up to 72% compared to On-Demand pricing) for a 1- or 3-year commitment. By purchasing Savings Plans for the minimum capacity (2 instances), the company ensures that the base load is covered at a discounted rate. When demand increases and Auto Scaling adds more instances (up to 5), those additional instances will be charged at On-Demand rates, but only when needed. Since the load is minimal after work hours and on weekends, most of the time the application will run with just 2 instances at the discounted Savings Plan rate. This approach balances cost optimization with availability, as the application remains available at all times while minimizing costs during low-demand periods.\n\n**Why option 0 is incorrect:**\nThe option that says use all EC2 Spot Instances is incorrect because Spot Instances can be interrupted by AWS with a 2-minute notice when AWS needs the capacity back. While Spot Instances offer significant cost savings (up to 90% discount), they don't meet the availability requirement of being available at all times. Spot Instances are suitable for fault-tolerant, flexible workloads that can handle interruptions, but not for applications that must be available at all times. The requirement explicitly states the application must be available at all times, which Spot Instances cannot guarantee.\n\n**Why option 1 is incorrect:**\nThe option that says purchase EC2 Instance Savings Plans to cover five EC2 instances is incorrect because this approach purchases Savings Plans for the maximum capacity, which means paying for 5 instances even when only 2 are running. Since the load is minimal after work hours and on weekends, most of the time the application will only need 2 instances. Purchasing Savings Plans for 5 instances means paying for unused capacity during low-demand periods, which is not cost-effective. The requirement is to meet availability MOST cost-effectively, and purchasing for maximum capacity doesn't optimize costs.\n\n**Why option 2 is incorrect:**\nThe option that says purchase two EC2 Reserved Instances is incorrect because Reserved Instances provide discounts but are less flexible than Savings Plans. Reserved Instances are tied to specific instance types, sizes, and Availability Zones, which can limit flexibility. Savings Plans provide the same discount level but offer more flexibility - they apply to any instance type, size, or AZ within the instance family. Additionally, Reserved Instances require more upfront planning and commitment to specific configurations, while Savings Plans provide more operational flexibility for Auto Scaling groups that may use different instance types or sizes.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 19,
    "text": "A company has an ecommerce checkout workflow that writes an order to a database and calls a \nservice to process the payment. Users are experiencing timeouts during the checkout process.  \nWhen users resubmit the checkout form, multiple unique orders are created for the same desired \ntransaction. \n \nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?",
    "options": [
      {
        "id": 0,
        "text": "Configure the web application to send an order message to Amazon Kinesis Data Firehose.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the order in the database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store the order in the database.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nStoring the order in the database, sending a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue, setting the payment service to retrieve the message and process the order, and deleting the message from the queue is the correct solution. This approach decouples order creation from payment processing, making the workflow more resilient to timeouts. When a user submits a checkout form, the order is immediately stored in the database with a unique order number. The order number is then sent to an SQS FIFO queue. FIFO queues guarantee exactly-once processing and prevent duplicate messages, which prevents multiple orders from being created for the same transaction. If the payment service times out or fails, the message remains in the queue and can be retried without creating duplicate orders. The message is only deleted after successful processing, ensuring idempotency.\n\n**Why option 0 is incorrect:**\nThe option that says configure the web application to send an order message to Amazon Kinesis Data Firehose and set the payment service to retrieve the message from Kinesis Data Firehose and process the order is incorrect because Kinesis Data Firehose is designed for streaming data to destinations like S3, Redshift, or Elasticsearch for analytics purposes, not for transactional message processing. Firehose doesn't provide message queuing capabilities, retry mechanisms, or exactly-once processing guarantees that are needed to prevent duplicate orders. Firehose is optimized for high-throughput data ingestion, not for ensuring transactional integrity in ecommerce workflows.\n\n**Why option 1 is incorrect:**\nThe option that says create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request, and use Lambda to query the database, call the payment service, and pass in the order information is incorrect because CloudTrail is an auditing and logging service that records API calls, not a real-time event processing system. CloudTrail logs are typically delivered with delays (5-15 minutes), which doesn't meet the requirement for immediate order processing. Additionally, this approach doesn't prevent duplicate orders - if a user resubmits the form, CloudTrail would log multiple requests, potentially creating multiple orders. CloudTrail is not designed for transactional workflows.\n\n**Why option 2 is incorrect:**\nThe option that says store the order in the database, send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS), set the payment service to poll Amazon SNS, retrieve the message, and process the order is incorrect because SNS is a pub/sub messaging service that delivers messages to multiple subscribers immediately, but it doesn't provide message queuing, exactly-once processing, or retry capabilities. SNS doesn't guarantee that messages are processed exactly once - if the payment service fails to process a message, it's lost. Additionally, SNS doesn't support polling - subscribers receive messages via HTTP/HTTPS endpoints or SQS queues. SNS is designed for fan-out messaging to multiple subscribers, not for ensuring transactional integrity in ecommerce workflows.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 20,
    "text": "A company is planning to build a high performance computing (HPC) workload as a service \nsolution that Is hosted on AWS.  \nA group of 16 AmazonEC2Ltnux Instances requires the lowest possible latency for node-to-node \ncommunication.  \nThe instances also need a shared block device volume for high-performing storage. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use a cluster placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use a cluster placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a partition placement group. Create shared file systems across the instances by using Amazon Elastic File System (Amazon EFS).",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a spread placement group. Attach a single Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume to all the instances by using Amazon EBS Multi-Attach.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing a cluster placement group and attaching a single Provisioned IOPS SSD EBS volume to all instances using Amazon EBS Multi-Attach is the correct solution. Cluster placement groups place instances within a single Availability Zone in close physical proximity, providing the lowest possible latency for node-to-node communication (up to 10 Gbps network performance). EBS Multi-Attach allows a single Provisioned IOPS SSD (io1/io2) volume to be attached to multiple EC2 instances simultaneously, providing shared block storage with high performance. Provisioned IOPS volumes can deliver up to 64,000 IOPS and 1,000 MB/s throughput, making them ideal for high-performance HPC workloads that require low-latency shared storage. This combination provides both the network performance and shared storage requirements.\n\n**Why option 1 is incorrect:**\nThe option that says use a cluster placement group and create shared file systems across the instances using Amazon Elastic File System (Amazon EFS) is incorrect because while cluster placement groups provide low-latency networking, EFS is a network file system that introduces network latency for file operations. EFS is designed for shared file storage across multiple AZs and is optimized for throughput, not low-latency block-level access. HPC workloads that require high-performing shared block device volumes need block-level storage with low latency, not network file systems. EFS also has performance limitations compared to Provisioned IOPS EBS volumes for high-performance workloads.\n\n**Why option 2 is incorrect:**\nThe option that says use a partition placement group and create shared file systems across the instances using Amazon EFS is incorrect because partition placement groups are designed for large distributed workloads like Hadoop or HDFS, where instances are placed into logical partitions. Partition placement groups don't provide the same level of low-latency, high-throughput networking as cluster placement groups. Additionally, EFS introduces network latency and doesn't provide the high-performance block-level storage that HPC workloads require. Partition placement groups are optimized for fault isolation, not for low-latency node-to-node communication.\n\n**Why option 3 is incorrect:**\nThe option that says use a spread placement group and attach a single Provisioned IOPS SSD EBS volume using EBS Multi-Attach is incorrect because spread placement groups place instances on distinct underlying hardware to minimize the risk of simultaneous failures. While this provides high availability, spread placement groups don't provide the low-latency, high-throughput networking that cluster placement groups offer. Spread placement groups are designed for applications that need high availability, not for HPC workloads requiring the lowest possible latency for node-to-node communication. The network performance in spread placement groups is the same as default EC2 networking, not the enhanced performance of cluster placement groups.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 21,
    "text": "A company has an event-driven application that invokes AWS Lambda functions up to 800 times \neach minute with varying runtimes.  \nThe Lambda functions access data that is stored in an Amazon Aurora MySQL OB cluster.  \nThe company is noticing connection timeouts as user activity increases The database shows no \nsigns of being overloaded. CPU, memory, and disk access metrics are all low.  \nWhich solution will resolve this issue with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Adjust the size of the Aurora MySQL nodes to handle more connections.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up Amazon ElastiCache tor Redls to cache commonly read items from the database.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Aurora Replica as a reader node.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ROS Proxy to create a proxy.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUsing Amazon RDS Proxy to create a proxy, setting the DB cluster as the target database, and configuring the Lambda functions to connect to the proxy rather than directly to the DB cluster is the correct solution with the least operational overhead. RDS Proxy is a fully managed database proxy service that pools and shares database connections, reducing connection overhead and improving scalability. When Lambda functions connect directly to Aurora, each function invocation can create a new database connection, leading to connection exhaustion and timeouts. RDS Proxy maintains a pool of database connections and reuses them across Lambda invocations, dramatically reducing the number of connections to the database. Since the database shows no signs of being overloaded (low CPU, memory, disk), the issue is connection management, not database capacity. RDS Proxy requires minimal configuration and no code changes to the Lambda functions - just update the connection string.\n\n**Why option 0 is incorrect:**\nThe option that says adjust the size of the Aurora MySQL nodes to handle more connections and configure retry logic in the Lambda functions is incorrect because scaling up the database doesn't address the root cause - connection exhaustion from too many concurrent connections. Larger instances can handle more connections, but with 800 Lambda invocations per minute, the connection pool will still be exhausted. Additionally, retry logic doesn't solve connection timeout issues - it just retries failed connections, which will continue to fail if connections are exhausted. This approach adds operational overhead through manual scaling and code changes, and doesn't solve the underlying connection pooling problem.\n\n**Why option 1 is incorrect:**\nThe option that says set up Amazon ElastiCache for Redis to cache commonly read items and configure Lambda functions to connect to ElastiCache for reads is incorrect because caching read data doesn't solve connection timeout issues. The problem is that Lambda functions are creating too many database connections, not that they're reading too much data. Even if reads are cached, Lambda functions still need to connect to the database for writes and cache misses, so connection exhaustion will persist. Additionally, this approach requires significant code changes to implement caching logic and doesn't address the connection management issue.\n\n**Why option 2 is incorrect:**\nThe option that says add an Aurora Replica as a reader node and configure Lambda functions to connect to the reader endpoint is incorrect because adding a read replica doesn't solve connection timeout issues - it just distributes read traffic. Lambda functions still need to connect to the primary database for writes, and connection exhaustion will occur on both the primary and replica. Additionally, this approach requires code changes to implement read/write splitting logic and adds operational overhead through managing an additional database instance. The database metrics show it's not overloaded, so adding capacity doesn't address the connection pooling problem.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "A company is building a containerized application on premises and decides to move the \napplication to AWS.  \nThe application will have thousands of users soon after li is deployed.  \nThe company Is unsure how to manage the deployment of containers at scale. The company \nneeds to deploy the containerized application in a highly available architecture that minimizes \noperational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store container images In an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store container images in a repository that runs on an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nStoring container images in an Amazon Elastic Container Registry (Amazon ECR) repository, using an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type, and using target tracking to scale automatically based on demand is the correct solution. ECR provides a fully managed container registry for storing and managing Docker images. Fargate is a serverless compute engine for containers that eliminates the need to provision, configure, or manage EC2 instances. With Fargate, you simply define your container requirements (CPU and memory), and AWS handles the infrastructure. Fargate automatically distributes containers across multiple Availability Zones for high availability. Target tracking scaling policies automatically adjust the number of tasks based on CloudWatch metrics, ensuring the application scales to meet demand with minimal operational overhead. This solution minimizes operational complexity while providing high availability.\n\n**Why option 1 is incorrect:**\nThe option that says store container images in ECR and use ECS with the EC2 launch type with target tracking is incorrect because the EC2 launch type requires you to provision, configure, and manage EC2 instances, which increases operational overhead. With EC2 launch type, you need to choose instance types, manage cluster capacity, handle instance scaling, and optimize cluster packing. This doesn't minimize operational overhead compared to Fargate, which is fully serverless. While ECS with EC2 can be highly available, it requires more management than Fargate.\n\n**Why option 2 is incorrect:**\nThe option that says store container images in a repository on an EC2 instance, run containers on EC2 instances across multiple AZs, monitor CPU utilization in CloudWatch, and launch new instances as needed is incorrect because this approach requires significant operational overhead. You need to manage the container registry server, configure EC2 instances, set up monitoring, and manually or semi-automatically scale instances. This doesn't leverage managed AWS services like ECR and ECS, which provide better operational efficiency. Running a container registry on EC2 adds unnecessary complexity and management overhead.\n\n**Why option 3 is incorrect:**\nThe option that says create an AMI containing the container image, launch EC2 instances in an Auto Scaling group across multiple AZs, and use CloudWatch alarms to scale is incorrect because this approach treats containers as static AMIs, which defeats the purpose of containerization. Containers should be deployed dynamically from container images, not baked into AMIs. This approach requires rebuilding AMIs for every container update, which is inefficient and increases operational overhead. Additionally, you lose the benefits of container orchestration platforms like ECS, which provide better container management, service discovery, and load balancing.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A company's application Is having performance issues. The application staleful and needs to \ncomplete m-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation \nto deploy infrastructure and used the M5 EC2 Instance family. As traffic increased, the application \nperformance degraded. Users are reporting delays when the users attempt to access the \napplication.  \nWhich solution will resolve these issues in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Replace the EC2 Instances with T3 EC2 instances that run in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nModifying the CloudFormation templates to replace M5 instances with R5 instances and deploying the Amazon CloudWatch agent on EC2 instances to generate custom application latency metrics is the most operationally efficient solution. The application is stateful and needs to complete in-memory tasks, indicating it's memory-intensive. R5 instances are memory-optimized (up to 768 GB RAM) compared to M5 general-purpose instances, which will improve performance for memory-intensive workloads. EC2 instances don't provide memory metrics to CloudWatch by default - the CloudWatch agent must be installed to collect memory and custom application metrics. This allows for proper monitoring and capacity planning. Since the infrastructure is managed via CloudFormation, modifying templates is the most operationally efficient approach, ensuring infrastructure-as-code practices are maintained.\n\n**Why option 0 is incorrect:**\nThe option that says replace EC2 instances with T3 instances that run in an Auto Scaling group is incorrect because T3 instances are burstable performance instances designed for workloads with variable CPU usage, not memory-intensive stateful applications. T3 instances have limited memory compared to M5 or R5 instances and use CPU credits that can be exhausted under sustained load, causing performance degradation. The application is stateful and memory-intensive, so it needs memory-optimized instances, not burstable instances. Additionally, this approach doesn't address the need for proper monitoring.\n\n**Why option 1 is incorrect:**\nThe option that says modify CloudFormation templates to run EC2 instances in an Auto Scaling group and manually increase capacity when necessary is incorrect because while Auto Scaling helps with availability, it doesn't solve the performance issue. The problem is that M5 instances don't have enough memory for the memory-intensive workload. Simply adding more M5 instances doesn't address the root cause - each instance still has insufficient memory. Additionally, manual capacity management increases operational overhead and doesn't scale efficiently. The application needs memory-optimized instances, not just more instances.\n\n**Why option 2 is incorrect:**\nThe option that says modify CloudFormation templates to replace instances with R5 instances and use CloudWatch built-in EC2 memory metrics is incorrect because EC2 instances don't provide memory metrics to CloudWatch by default. Built-in EC2 metrics only include CPU utilization, network I/O, disk I/O, and status checks - not memory utilization. To monitor memory usage for a memory-intensive application, you must install the CloudWatch agent on the instances. Without proper memory monitoring, you cannot track application performance or plan capacity effectively.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "An ecommerce company has an order-processing application that uses Amazon API Gateway \nand an AWS Lambda function.  \nThe application stores data in an Amazon Aurora PostgreSQL database.  \nDuring a recent sales event, a sudden surge in customer orders occurred.  \nSome customers experienced timeouts and the application did not process the orders of those \ncustomers.  \nA solutions architect determined that the CPU utilization and memory utilization were high on the \ndatabase because of a large number of open connections.  \nThe solutions architect needs to prevent the timeout errors while making the least possible \nchanges to the application. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure provisioned concurrency for the Lambda function.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS Proxy to create a proxy for the database.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a read replica for the database in a different AWS Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon RDS Proxy to create a proxy for the database is the correct solution that requires the least application changes. RDS Proxy is a fully managed database proxy service that pools and shares database connections, dramatically reducing the number of connections to the Aurora database. When Lambda functions connect directly to Aurora, each invocation can create a new connection, leading to connection exhaustion and high CPU/memory utilization. RDS Proxy maintains a pool of database connections and reuses them across Lambda invocations, reducing connection overhead. The solution requires minimal changes - just update the Lambda function's connection string to point to the RDS Proxy endpoint instead of the database endpoint. RDS Proxy handles connection pooling, failover, and query routing automatically, improving database efficiency and preventing timeout errors.\n\n**Why option 0 is incorrect:**\nThe option that says configure provisioned concurrency for the Lambda function is incorrect because provisioned concurrency keeps Lambda functions warm and ready to respond, but it doesn't solve the database connection exhaustion problem. Provisioned concurrency reduces cold starts but actually increases the number of concurrent Lambda executions, which could worsen the connection problem if each execution creates a new database connection. The root cause is too many database connections, not Lambda cold starts. This approach doesn't address the high CPU and memory utilization on the database caused by connection exhaustion.\n\n**Why option 2 is incorrect:**\nThe option that says create a read replica for the database in a different AWS Region is incorrect because read replicas are designed to offload read traffic, not to solve connection exhaustion issues. The problem is that too many connections are being opened to the database, causing high CPU and memory utilization. Creating a read replica in another region doesn't reduce connections to the primary database - it just provides another endpoint for reads. Additionally, cross-region replication adds latency and complexity. The requirement is to prevent timeout errors with the least changes, and read replicas require significant application changes to implement read/write splitting.\n\n**Why option 3 is incorrect:**\nThe option that says migrate the data from Aurora PostgreSQL to Amazon DynamoDB using AWS Database Migration Service is incorrect because migrating to a completely different database system requires extensive application changes, which violates the requirement to make the least possible changes. DynamoDB is a NoSQL database with a different data model and API than PostgreSQL, requiring significant code rewrites. Additionally, DynamoDB migration is a complex, time-consuming process that doesn't address the immediate timeout issue. The problem is connection management, not the database technology itself.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "A company runs a global web application on Amazon EC2 instances behind an Application Load \nBalancer. \nThe application stores data in Amazon Aurora.  \nThe company needs to create a disaster recovery solution and can tolerate up to 30 minutes of \ndowntime and potential data loss.  \nThe solution does not need to handle the load when the primary infrastructure is healthy. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n50 \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the application with the required infrastructure elements in place.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Host a scaled-down deployment of the application in a second AWS Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Replicate the primary infrastructure in a second AWS Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Back up data with AWS Backup.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nDeploying the application with required infrastructure elements in place, using Amazon Route 53 to configure active-passive failover, and creating an Aurora Replica in a second AWS Region is the correct solution for a disaster recovery scenario with 30-minute downtime tolerance. Active-passive failover uses Route 53 health checks to monitor the primary infrastructure. When the primary fails, Route 53 automatically routes traffic to the standby infrastructure in the second region. Since the solution doesn't need to handle load when primary is healthy, a minimal standby deployment is cost-effective. The Aurora Replica in the second region provides database availability, and since the company can tolerate potential data loss, asynchronous replication is acceptable. This approach minimizes costs while meeting the RTO (30 minutes) and RPO (some data loss acceptable) requirements.\n\n**Why option 1 is incorrect:**\nThe option that says host a scaled-down deployment in a second region, use Route 53 active-active failover, and create an Aurora Replica is incorrect because active-active failover distributes traffic across multiple endpoints simultaneously, which doesn't match the requirement that the solution doesn't need to handle load when primary is healthy. Active-active is designed for load distribution and high availability, not disaster recovery. Additionally, active-active requires both endpoints to be fully operational, which increases costs compared to a passive standby deployment.\n\n**Why option 2 is incorrect:**\nThe option that says replicate the primary infrastructure in a second region, use Route 53 active-active failover, and create an Aurora database restored from the latest snapshot is incorrect because this approach is more expensive and complex than necessary. Replicating the full primary infrastructure increases costs significantly, and restoring from snapshots adds recovery time, potentially exceeding the 30-minute tolerance. Active-active failover doesn't match the requirement for a passive DR solution. Aurora Replicas provide better RPO than snapshots since they continuously replicate data.\n\n**Why option 3 is incorrect:**\nThe option that says back up data with AWS Backup, use the backup to create infrastructure in a second region, use Route 53 active-passive failover, and create an Aurora second primary instance is incorrect because restoring from backups takes time and may exceed the 30-minute downtime tolerance. AWS Backup restores require time to provision infrastructure and restore data, which doesn't meet the RTO requirement efficiently. Additionally, creating a second primary instance (Aurora Global Database) is more complex and expensive than using a read replica, which can be promoted to primary when needed. Aurora Replicas provide faster failover than backup restores.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 26,
    "text": "A company wants to measure the effectiveness of its recent marketing campaigns.  \nThe company performs batch processing on csv files of sales data and stores the results in an \nAmazon S3 bucket once every hour.  \nThe S3 bipetabytes of objects. The company runs one-time queries in Amazon Athena to \ndetermine which products are most popular on a particular date for a particular region Queries \nsometimes fail or take longer than expected to finish.  \nWhich actions should a solutions architect take to improve the query performance and reliability? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Reduce the S3 object sizes to less than 126 MB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Partition the data by date and region in Amazon S3",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the files as large, single objects in Amazon S3.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon Kinosis Data Analytics to run the Queries as pan of the batch processing operation",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use an AWS Glue extract, transform, and load (ETL) process to convert the csv files into Apache Parquet format.",
        "correct": true
      }
    ],
    "correctAnswers": [
      2,
      4
    ],
    "explanation": "**Why option 2 is correct:**\nStoring files as large, single objects in Amazon S3 improves Athena query performance. Athena charges based on the amount of data scanned, and smaller objects increase metadata overhead and the number of API calls needed. Large objects (ideally 128 MB to 1 GB) reduce the number of files Athena needs to process, improving query performance and reducing the likelihood of query failures. When objects are too small, Athena spends more time on metadata operations and list operations, which can cause queries to fail or timeout. Large objects also reduce the number of S3 requests, improving reliability.\n\n**Why option 4 is correct:**\nUsing an AWS Glue ETL process to convert CSV files into Apache Parquet format significantly improves Athena query performance and reliability. Parquet is a columnar storage format that is compressed and optimized for analytics queries. Parquet files are typically 10-100x smaller than CSV files, which reduces the amount of data Athena needs to scan, lowering costs and improving query speed. Parquet also supports predicate pushdown, allowing Athena to skip reading irrelevant data. Since queries filter by date and region, Parquet's columnar structure and partitioning capabilities make these queries much faster and more reliable than scanning CSV files.\n\n**Why option 0 is incorrect:**\nThe option that says reduce S3 object sizes to less than 126 MB is incorrect because smaller objects actually degrade Athena performance. Small objects increase metadata overhead, require more S3 API calls for listing and reading, and can cause queries to fail or timeout due to the large number of files to process. Athena performs best with objects between 128 MB and 1 GB. Reducing object sizes below 126 MB would worsen the performance and reliability issues the company is experiencing.\n\n**Why option 1 is incorrect:**\nThe option that says partition the data by date and region in Amazon S3 is incorrect because while partitioning can improve query performance, the question asks for actions to improve performance and reliability. Partitioning alone doesn't address the fundamental issues with CSV format and small object sizes. However, partitioning combined with Parquet format (option 4) would be optimal. But partitioning CSV files without converting to an optimized format like Parquet provides limited benefit and doesn't solve the reliability issues.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon Kinesis Data Analytics to run queries as part of the batch processing operation is incorrect because Kinesis Data Analytics is designed for real-time stream processing, not for one-time ad-hoc queries on historical data. The requirement is to run one-time queries to determine product popularity for specific dates and regions, which is better suited for Athena's interactive query capabilities. Kinesis Data Analytics would require restructuring the entire data pipeline and doesn't address the performance issues with existing Athena queries on S3 data.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 27,
    "text": "A company is running several business applications in three separate VPCs within the us-east-1 \nRegion. The applications must be able to communicate between VPCs. The applications also \nmust be able to consistently send hundreds of gigabytes of data each day to a latency-sensitive \napplication that runs in a single on- premises data center. \nA solutions architect needs to design a network connectivity solution that maximizes cost-\neffectiveness. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure three AWS Site-to-Site VPN connections from the data center to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Launch a third-party virtual network appliance in each VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up three AWS Direct Connect connections from the data center to a Direct Connect",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up one AWS Direct Connect connection from the data center to AWS.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nSetting up one AWS Direct Connect connection from the data center to AWS, creating a Transit Gateway, attaching each VPC to the Transit Gateway, and establishing connectivity between the Direct Connect connection and the Transit Gateway is the most cost-effective solution. Direct Connect provides dedicated network connectivity with consistent, low-latency performance ideal for transferring hundreds of gigabytes daily to a latency-sensitive application. A single Direct Connect connection can be shared across multiple VPCs through Transit Gateway, eliminating the need for multiple connections. Transit Gateway acts as a central hub, enabling VPC-to-VPC communication and connecting all VPCs to the on-premises data center through a single Direct Connect connection. This approach minimizes costs by using one connection instead of three, while meeting all connectivity requirements.\n\n**Why option 0 is incorrect:**\nThe option that says configure three AWS Site-to-Site VPN connections from the data center to AWS, with one VPN connection for each VPC is incorrect because VPN connections have bandwidth limitations (typically up to 1.25 Gbps per tunnel) and higher latency compared to Direct Connect. For transferring hundreds of gigabytes daily to a latency-sensitive application, VPN may not provide sufficient bandwidth or consistent performance. Additionally, three VPN connections cost more than a single Direct Connect connection shared across VPCs. VPN connections also have higher data transfer costs and less predictable performance than Direct Connect.\n\n**Why option 1 is incorrect:**\nThe option that says launch a third-party virtual network appliance in each VPC and establish an IPsec VPN tunnel between the data center and each appliance is incorrect because this approach requires managing three separate network appliances, which increases operational overhead and costs. Third-party appliances incur EC2 instance costs, licensing fees, and management overhead. Additionally, VPN tunnels through appliances don't provide the same level of performance, reliability, or cost-effectiveness as Direct Connect. This solution doesn't maximize cost-effectiveness as required.\n\n**Why option 2 is incorrect:**\nThe option that says set up three AWS Direct Connect connections from the data center to a Direct Connect gateway, with each VPC using one connection is incorrect because this approach is unnecessarily expensive. A single Direct Connect connection can handle the bandwidth requirements (hundreds of GB per day) and can be shared across multiple VPCs through Transit Gateway or a Direct Connect Gateway. Setting up three separate connections triples the costs without providing additional benefits. The requirement is to maximize cost-effectiveness, and using one shared connection is the optimal approach.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 28,
    "text": "An online photo application lets users upload photos and perform image editing operations. The \napplication offers two classes of service free and paid Photos submitted by paid users are \nprocessed before those submitted by free users Photos are uploaded to Amazon S3 and the job \ninformation is sent to Amazon SQS. \nWhich configuration should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use one SQS FIFO queue.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use two SQS FIFO queues: one for paid and one for free.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use two SQS standard queues one for paid and one for free.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use one SQS standard queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing two SQS standard queues (one for paid and one for free) and configuring Amazon EC2 instances to prioritize polling for the paid queue over the free queue is the correct solution. Separate queues allow the application to implement priority processing by having workers poll the paid queue more frequently than the free queue. This ensures paid users' photos are processed before free users' photos. Standard queues are appropriate here because strict ordering (FIFO) isn't required - the requirement is just that paid photos are processed before free photos. The application can control priority by adjusting polling frequency and the number of workers dedicated to each queue.\n\n**Why option 0 is incorrect:**\nThe option that says use one SQS FIFO queue and assign a higher priority to paid photos is incorrect because SQS FIFO queues process messages in strict first-in-first-out order based on message groups, not priority levels. FIFO queues don't support priority-based message ordering - they ensure messages are processed exactly once and in order within a message group. You cannot assign priorities to messages in a FIFO queue to make paid photos process before free photos that arrived earlier.\n\n**Why option 1 is incorrect:**\nThe option that says use two SQS FIFO queues (one for paid and one for free) and set the free queue to use short polling and the paid queue to use long polling is incorrect because polling type (short vs long) doesn't determine processing priority. Short polling returns immediately even if no messages are available, while long polling waits up to 20 seconds for messages. However, if workers are polling both queues equally, there's no guarantee paid photos will be processed first. The solution needs explicit prioritization through polling frequency or dedicated workers, not just polling type.\n\n**Why option 3 is incorrect:**\nThe option that says use one SQS standard queue, set the visibility timeout of paid photos to zero, and configure EC2 instances to prioritize visibility settings is incorrect because visibility timeout determines how long a message is hidden after being received, not processing priority. Setting visibility timeout to zero would make messages immediately visible again after processing, potentially causing duplicate processing. Additionally, SQS doesn't support message-level priority settings in standard queues. You cannot assign different visibility timeouts to different messages based on user type in a single queue - visibility timeout is a queue-level setting, not a message-level setting.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 29,
    "text": "A company hosts its product information webpages on AWS. The existing solution uses multiple \nAmazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. The \nwebsite also uses a custom DNS name and communicates with HTTPS only using a dedicated \nSSL certificate. The company is planning a new product launch and wants to be sure that users \nfrom around the world have the best possible experience on the new website. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Redesign the application to use Amazon CloudFront",
        "correct": true
      },
      {
        "id": 1,
        "text": "Redesign the application to use AWS Elastic Beanstalk",
        "correct": false
      },
      {
        "id": 2,
        "text": "Redesign the application to use a Network Load Balancer.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Redesign the application to use Amazon S3 static website hosting",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nRedesigning the application to use Amazon CloudFront is the correct solution to provide the best possible experience for users around the world. CloudFront is a Content Delivery Network (CDN) that caches content at edge locations globally, reducing latency by serving content from locations closest to users. CloudFront integrates seamlessly with Application Load Balancers, allowing the existing ALB infrastructure to remain as the origin. CloudFront supports custom DNS names through Route 53 and can use custom SSL certificates, maintaining the existing HTTPS-only requirement. This solution improves global performance without requiring a complete application redesign - CloudFront can be added as a layer in front of the existing infrastructure.\n\n**Why option 1 is incorrect:**\nThe option that says redesign the application to use AWS Elastic Beanstalk is incorrect because Elastic Beanstalk is a platform-as-a-service that simplifies application deployment and management, but it doesn't improve global performance for users worldwide. Elastic Beanstalk deploys applications to a single region and doesn't provide CDN capabilities. While Elastic Beanstalk can help with deployment automation, it doesn't address the requirement to provide the best possible experience for global users, which requires content distribution through a CDN like CloudFront.\n\n**Why option 2 is incorrect:**\nThe option that says redesign the application to use a Network Load Balancer is incorrect because Network Load Balancers operate at Layer 4 and provide load balancing within a region, but they don't improve performance for global users. NLB distributes traffic across instances in the same region, reducing latency within that region, but users in other regions still experience high latency. The requirement is to provide the best experience for users \"from around the world,\" which requires global content distribution through a CDN, not just regional load balancing.\n\n**Why option 3 is incorrect:**\nThe option that says redesign the application to use Amazon S3 static website hosting is incorrect because S3 static website hosting is designed for static websites only. The existing solution uses EC2 instances behind an ALB, indicating a dynamic application. Migrating to S3 static hosting would require completely rewriting the application to be static, which may not be feasible. Additionally, S3 static hosting alone doesn't provide global content distribution - you would still need CloudFront in front of S3 to serve global users effectively.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company has 150 TB of archived image data stored on-premises that needs to be moved to the \nAWS Cloud within the next month. \nThe company's current network connection allows up to 100 Mbps uploads for this purpose \nduring the night only. \nWhat is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowmobile to ship the data to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Order multiple AWS Snowball devices to ship the data to AWS.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable Amazon S3 Transfer Acceleration and securely upload the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOrdering multiple AWS Snowball devices to ship the data to AWS is the most cost-effective solution that meets the migration deadline. With only 100 Mbps upload capacity available during nights (approximately 6 hours), the maximum data transfer per night is approximately 2.1 TB (100 Mbps  6 hours  3600 seconds = 2.16 TB). Transferring 150 TB over the internet would take approximately 71 nights, far exceeding the 1-month deadline. AWS Snowball Edge Storage Optimized devices can store up to 80 TB each, so two devices (160 TB total) would be sufficient for 150 TB of data. Snowball devices provide fast, secure, offline data transfer that bypasses internet bandwidth limitations. The cost of Snowball devices and shipping is significantly less than the time and potential business impact of a 71-day migration, making it the most cost-effective solution.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS Snowmobile to ship the data to AWS is incorrect because Snowmobile is designed for extremely large datasets of 10 PB or more in a single location. For 150 TB of data, Snowmobile is overkill and unnecessarily expensive. Snowmobile requires a high-speed network backbone and is designed for petabyte-scale migrations. Snowball devices are more appropriate and cost-effective for datasets in the hundreds of terabytes range.\n\n**Why option 2 is incorrect:**\nThe option that says enable Amazon S3 Transfer Acceleration and securely upload the data is incorrect because Transfer Acceleration optimizes data transfer to S3 using CloudFront edge locations, but it still relies on the available internet bandwidth. With only 100 Mbps available during nights, Transfer Acceleration cannot overcome the fundamental bandwidth limitation. Transferring 150 TB at 100 Mbps would take approximately 71 nights (150 TB / 2.1 TB per night), which exceeds the 1-month deadline. Transfer Acceleration improves transfer speeds but doesn't solve the bandwidth constraint problem.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon S3 VPC endpoint and establish a VPN to upload the data is incorrect because VPC endpoints are for private connectivity between VPC resources and AWS services, not for on-premises to AWS data transfer. Additionally, a VPN connection still uses the same 100 Mbps internet bandwidth limitation. VPN connections don't increase available bandwidth - they provide encrypted connectivity over the existing internet connection. This approach would still take approximately 71 nights to transfer 150 TB, exceeding the deadline.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 31,
    "text": "A company hosts its web application on AWS using seven Amazon EC2 instances. The company \nrequires that the IP addresses of all healthy EC2 instances be returned in response to DNS \nqueries. Which policy should be used to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Simple routing policy",
        "correct": false
      },
      {
        "id": 1,
        "text": "Latency routing policy",
        "correct": false
      },
      {
        "id": 2,
        "text": "Multivalue routing policy",
        "correct": true
      },
      {
        "id": 3,
        "text": "Geolocation routing policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing a multivalue answer routing policy is the correct solution. Multivalue answer routing returns multiple healthy IP addresses (up to 8) in response to DNS queries, allowing clients to choose which IP to connect to. Route 53 performs health checks on each resource and only returns healthy endpoints. Since the company requires that IP addresses of all healthy EC2 instances be returned, multivalue answer routing is ideal because it can return multiple healthy IPs simultaneously. Each record set can be associated with a health check, ensuring only healthy instances are included in DNS responses. This provides basic load balancing and fault tolerance at the DNS level.\n\n**Why option 0 is incorrect:**\nThe option that says use a simple routing policy is incorrect because simple routing policy returns a single IP address or value in response to DNS queries. It doesn't support returning multiple IP addresses for all healthy EC2 instances. Simple routing is used for a single resource or when you want Route 53 to respond to DNS queries with only one value. It doesn't meet the requirement to return IP addresses of all healthy instances.\n\n**Why option 1 is incorrect:**\nThe option that says use a latency routing policy is incorrect because latency routing returns the resource with the lowest latency based on the user's location, not all healthy instances. Latency routing is designed to route users to the endpoint that provides the best performance based on network latency measurements. It returns a single IP address per query based on latency, not multiple IPs for all healthy instances. This doesn't meet the requirement to return all healthy instance IPs.\n\n**Why option 3 is incorrect:**\nThe option that says use a geolocation routing policy is incorrect because geolocation routing returns resources based on the geographic location of the user making the DNS query, not all healthy instances. Geolocation routing allows you to route traffic based on the user's location (country/continent), returning different resources for different geographic regions. It returns a single IP address per query based on location, not multiple IPs for all healthy instances. This doesn't meet the requirement to return all healthy instance IPs.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "A company wants to use AWS Systems Manager to manage a fleet of Amazon EC2 instances. \nAccording to the company's security requirements, no EC2 instances can have internet access. A \nsolutions architect needs to design network connectivity from the EC2 instances to Systems \nManager while fulfilling this security obligation. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy the EC2 instances into a private subnet with no route to the internet.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an interface VPC endpoint for Systems Manager.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy a NAT gateway into a public subnet.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an internet gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nConfiguring an interface VPC endpoint for Systems Manager is the correct solution. VPC endpoints provide private connectivity between VPC resources and AWS services without requiring internet access. Interface endpoints use AWS PrivateLink to create a private connection to Systems Manager APIs, allowing EC2 instances in private subnets to communicate with Systems Manager without traversing the internet. This meets the security requirement that no EC2 instances can have internet access while still enabling Systems Manager functionality. Interface endpoints are deployed as ENIs in your VPC subnets and use private IP addresses, ensuring all traffic stays within the AWS network.\n\n**Why option 0 is incorrect:**\nThe option that says deploy EC2 instances into a private subnet with no route to the internet is incorrect because this only addresses part of the requirement. While private subnets don't have direct internet routes, EC2 instances still need a way to communicate with Systems Manager. Without a VPC endpoint or NAT gateway, instances in private subnets cannot reach AWS services like Systems Manager, which require API calls over the internet. Simply placing instances in private subnets doesn't solve the connectivity requirement.\n\n**Why option 2 is incorrect:**\nThe option that says deploy a NAT gateway into a public subnet is incorrect because NAT gateways allow outbound internet access for resources in private subnets, which violates the security requirement that no EC2 instances can have internet access. While NAT gateways enable connectivity to AWS services, they provide internet connectivity, which the security requirements explicitly prohibit. NAT gateways route traffic through the internet gateway, exposing traffic to the public internet.\n\n**Why option 3 is incorrect:**\nThe option that says deploy an internet gateway is incorrect because internet gateways provide direct internet access to resources in public subnets, which directly violates the security requirement that no EC2 instances can have internet access. Internet gateways enable bidirectional internet connectivity, allowing both inbound and outbound internet traffic. This approach doesn't meet the security requirement and doesn't provide the private connectivity needed for Systems Manager.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company needs to build a reporting solution on AWS. The solution must support SQL queries \nthat data analysts run on the data. \nThe data analysts will run lower than 10 total queries each day. The company generates 3 GB of \nnew data daily in an on-premises relational database. This data needs to be transferred to AWS \nto perform reporting tasks. \nWhat should a solutions architect recommend to meet these requirements at the LOWEST cost?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon Kinesis Data Firehose delivery stream to deliver the data into an Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Export a daily copy of the data from the on-premises database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate the data from the on-premises",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUsing AWS Database Migration Service (AWS DMS) to replicate data from the on-premises database to Amazon Redshift and using Amazon Redshift to run SQL queries is the lowest cost solution. DMS can perform continuous replication or one-time migration from the on-premises database to Redshift. With only 3 GB of new data daily and fewer than 10 queries per day, Redshift provides cost-effective analytics capabilities. Redshift's pay-as-you-go pricing means you only pay for the cluster when it's running, and with low query volume, you can use a small instance or even pause the cluster between queries. DMS handles the data transfer automatically, and Redshift provides SQL query capabilities that data analysts are familiar with, minimizing operational overhead.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DMS to replicate data to Amazon S3 and use Amazon Athena for queries is incorrect because while this approach can work, it's not the lowest cost option for this use case. DMS to S3 requires additional steps, and Athena charges per query and data scanned. For 3 GB daily with fewer than 10 queries, Redshift can be more cost-effective, especially if the cluster can be paused when not in use. Additionally, the requirement specifies SQL queries that data analysts run, and while Athena supports SQL, Redshift provides better performance for repeated queries on structured data.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon Kinesis Data Firehose to deliver data into Amazon Kinesis Data Analytics is incorrect because Kinesis Data Analytics is designed for real-time stream processing and continuous queries, not for on-demand SQL queries by data analysts. Kinesis services are optimized for streaming data and real-time analytics, which doesn't match the requirement for simple, on-demand queries. Additionally, Kinesis services have higher costs for low-volume use cases compared to Redshift. The requirement is for reporting with SQL queries, not real-time stream processing.\n\n**Why option 2 is incorrect:**\nThe option that says export a daily copy of the data from the on-premises database, upload it to S3, and use Amazon Athena for queries is incorrect because this manual approach increases operational overhead. Daily manual exports require scripting, monitoring, and error handling. While Athena can query S3 data, the manual export process adds complexity and potential points of failure. DMS provides automated, reliable replication with change data capture capabilities, reducing operational overhead. Additionally, for structured relational data, Redshift provides better query performance and cost-effectiveness than Athena for this volume.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 34,
    "text": "A company wants to monitor its AWS costs for financial review. The cloud operations team is \ndesigning an architecture in the AWS Organizations management account to query AWS Cost \nand Usage Reports for all member accounts. \nThe team must run this query once a month and provide a detailed analysis of the bill. \nWhich solution is the MOST scalable and cost-effective way to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3. Use Amazon Athena for analysis.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3. Use Amazon Redshift for analysis.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis. Use Amazon QuickSight for analysis.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nEnabling Cost and Usage Reports (CUR) for member accounts, delivering reports to Amazon S3, and using Amazon Redshift for analysis is the most scalable and cost-effective solution. When CUR is enabled for each member account, each account generates its own detailed billing reports, which scale automatically as new accounts are added. Reports are delivered to S3, providing durable, cost-effective storage. Redshift is a data warehouse optimized for analytics queries on large datasets, making it ideal for analyzing cost and usage data across all member accounts. Redshift can query data directly from S3 using Redshift Spectrum, or load data into Redshift for faster query performance. This approach scales to handle large volumes of cost data and provides powerful analytics capabilities.\n\n**Why option 0 is incorrect:**\nThe option that says enable CUR in the management account, deliver to Kinesis, and use EMR for analysis is incorrect because the management account's CUR only includes costs for the management account itself, not detailed usage data for all member accounts. Additionally, Kinesis is designed for real-time streaming data, not for batch cost reports that are generated daily. EMR is a big data processing framework that requires cluster management, which adds operational overhead and costs. This approach doesn't scale well and is more complex than necessary for monthly cost analysis.\n\n**Why option 1 is incorrect:**\nThe option that says enable CUR in the management account, deliver to S3, and use Athena for analysis is incorrect because the management account's CUR doesn't include detailed usage data for member accounts - it only includes aggregated billing information. To get detailed cost and usage analysis for all member accounts, you need CUR enabled for each member account. While Athena can query S3 data effectively, the limitation is that the management account's CUR doesn't provide the detailed data needed for comprehensive analysis across all accounts.\n\n**Why option 3 is incorrect:**\nThe option that says enable CUR for member accounts, deliver to Kinesis, and use QuickSight for analysis is incorrect because Kinesis is designed for real-time streaming data, not for batch cost reports. CUR reports are generated daily and delivered to S3, not streamed to Kinesis. QuickSight is a business intelligence tool that can visualize data, but it needs data in a queryable format (like S3 with Athena or Redshift). Delivering to Kinesis doesn't provide the storage and query capabilities needed for cost analysis. The correct approach is S3 for storage and Redshift or Athena for querying.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A company collects data for temperature, humidity, and atmospheric pressure in cities across \nmultiple continents. The average volume of data that the company collects from each site daily is \n500 GB. Each site has a high-speed Internet connection. \nThe company wants to aggregate the data from all these global sites as quickly as possible in a \nsingle Amazon S3 bucket. The solution must minimize operational complexity. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Turn on S3 Transfer Acceleration on the destination S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Upload the data from each site to an S3 bucket in the closest Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each",
        "correct": false
      },
      {
        "id": 3,
        "text": "Upload the data from each site to an Amazon EC2 instance in the closest Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTurning on S3 Transfer Acceleration on the destination S3 bucket is the correct solution. Transfer Acceleration uses CloudFront edge locations to optimize data transfer paths, routing uploads through the AWS edge network instead of the public internet. This significantly improves transfer speeds for long-distance transfers (50-500% faster) and is ideal when uploading to a centralized bucket from multiple global locations. With sites across multiple continents uploading 500 GB daily each, Transfer Acceleration optimizes the network path and reduces latency. It requires minimal operational complexity - just enable the feature on the bucket and use the transfer acceleration endpoint. This solution aggregates all data into a single S3 bucket as required while maximizing transfer speeds.\n\n**Why option 1 is incorrect:**\nThe option that says upload data from each site to an S3 bucket in the closest region is incorrect because this approach distributes data across multiple regions instead of aggregating it in a single bucket as required. The requirement explicitly states data must be aggregated in a single S3 bucket. While uploading to the closest region might improve individual site upload speeds, it doesn't meet the aggregation requirement. Additionally, this approach requires managing multiple buckets and potentially replicating data, which increases operational complexity.\n\n**Why option 2 is incorrect:**\nThe option that says schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site is incorrect because Snowball devices are designed for large-scale, one-time migrations or periodic bulk transfers, not for daily 500 GB transfers from multiple global sites. Snowball requires physical device shipping, which adds significant operational complexity and doesn't meet the requirement to minimize operational complexity. Additionally, with high-speed internet connections available at each site, Snowball is unnecessary and more expensive than Transfer Acceleration for regular daily transfers.\n\n**Why option 3 is incorrect:**\nThe option that says upload data from each site to an EC2 instance in the closest region is incorrect because this approach requires managing EC2 instances, which increases operational complexity. You would need to provision, configure, and manage EC2 instances in multiple regions, set up data transfer mechanisms, and then transfer data from EC2 to S3. This adds unnecessary infrastructure and operational overhead. Transfer Acceleration directly uploads to S3 without requiring intermediate infrastructure, minimizing operational complexity as required.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A company needs the ability to analyze the log files of its proprietary application. The logs are \nstored in JSON format in an Amazon S3 bucket Queries will be simple and will run on-demand. \nA solutions architect needs to perform the analysis with minimal changes to the existing \narchitecture. \nWhat should the solutions architect do to meet these requirements with the LEAST amount of \noperational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon CloudWatch Logs to store the logs",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Athena directly with Amazon S3 to run the queries as needed",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Glue to catalog the logs",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing Amazon Athena directly with Amazon S3 to run queries as needed is the solution with the least operational overhead. Athena is a serverless interactive query service that allows you to analyze data directly in S3 using standard SQL without provisioning infrastructure. Since the logs are already stored in JSON format in S3, Athena can query them directly without data movement or transformation. Athena requires minimal setup - just create a table schema pointing to the S3 bucket location, and you can start running SQL queries immediately. This approach requires minimal changes to the existing architecture (logs already in S3) and has no infrastructure to manage, making it operationally efficient.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Redshift to load all content into one place and run SQL queries is incorrect because Redshift requires provisioning and managing a data warehouse cluster, which increases operational overhead significantly. You need to size the cluster, manage scaling, handle backups, and pay for the cluster even when not running queries. Additionally, loading data from S3 into Redshift requires ETL processes, which adds complexity. For simple, on-demand queries on JSON logs in S3, Athena is more operationally efficient.\n\n**Why option 1 is incorrect:**\nThe option that says use Amazon CloudWatch Logs to store the logs is incorrect because this would require changing the existing architecture to send logs to CloudWatch Logs instead of S3. The requirement is to perform analysis with minimal changes to the existing architecture. CloudWatch Logs Insights can query logs, but migrating logs from S3 to CloudWatch Logs requires significant changes to the logging infrastructure. Additionally, CloudWatch Logs has retention limits and costs more than S3 for long-term log storage.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Glue to catalog the logs is incorrect because while Glue can create a data catalog, it doesn't provide query capabilities by itself. You would still need Athena or another query engine to actually run queries. Glue adds an extra step (cataloging) that may not be necessary if Athena can infer the schema from JSON files. For simple JSON logs, Athena can often query them directly without a Glue catalog, reducing operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company uses AWS Organizations to manage multiple AWS accounts for different \ndepartments. The management account has an Amazon S3 bucket that contains project reports. \nThe company wants to limit access to this S3 bucket to only users of accounts within the \norganization in AWS Organizations. \nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an organizational unit (OU) for each department.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization,",
        "correct": false
      },
      {
        "id": 3,
        "text": "Tag each user that needs access to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAdding the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy is the solution with the least operational overhead. The aws:PrincipalOrgID condition key allows you to restrict access to resources based on whether the principal belongs to a specific AWS Organization. By adding this condition to the bucket policy, you can allow access only to principals from accounts within your organization, without needing to list individual account IDs. This approach automatically includes new accounts added to the organization without requiring policy updates. It's a single policy change that scales automatically as the organization grows, minimizing operational overhead.\n\n**Why option 1 is incorrect:**\nThe option that says create an organizational unit (OU) for each department is incorrect because OUs are organizational structures for grouping accounts, not for controlling S3 bucket access. OUs don't provide access control mechanisms - they're used for applying service control policies (SCPs) and organizing accounts hierarchically. To restrict S3 access based on organization membership, you need to use IAM condition keys in bucket policies, not OUs. This approach doesn't directly address the S3 access requirement.\n\n**Why option 2 is incorrect:**\nThe option that says use AWS CloudTrail to monitor account creation events is incorrect because CloudTrail is an auditing and logging service, not an access control mechanism. CloudTrail can log when accounts are created or invited to the organization, but it cannot prevent or allow access to S3 buckets. CloudTrail provides visibility into API calls but doesn't enforce access policies. This approach doesn't meet the requirement to limit access to the S3 bucket.\n\n**Why option 3 is incorrect:**\nThe option that says tag each user that needs access to the S3 bucket is incorrect because IAM user tags don't provide a reliable way to restrict access based on organization membership. Tags are metadata that can be used in IAM policies with condition keys, but there's no built-in tag that indicates organization membership. Additionally, manually tagging users increases operational overhead and doesn't automatically include new users or accounts added to the organization. The aws:PrincipalOrgID condition key is the proper way to restrict access based on organization membership.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "An application runs on an Amazon EC2 instance in a VPC. The application processes logs that \nare stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without \nconnectivity to the internet. \nWhich solution will provide private network connectivity to Amazon S3?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint to the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an instance profile on Amazon EC2 to allow S3 access.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon API Gateway API with a private link to access the S3 endpoint.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCreating a gateway VPC endpoint for S3 is the correct solution to provide private network connectivity. Gateway VPC endpoints are free and provide private connectivity between your VPC and S3 without requiring internet access, NAT gateway, or VPN connections. After creating the gateway endpoint, you add a route in your VPC route table that routes S3 traffic (using the S3 prefix list) to the gateway endpoint. All traffic between your VPC and S3 stays within the AWS network, never traversing the public internet. This meets the requirement for private connectivity without internet access. Gateway endpoints are highly available and automatically scale, requiring no management.\n\n**Why option 1 is incorrect:**\nThe option that says stream logs to Amazon CloudWatch Logs and export to S3 is incorrect because this approach changes the architecture unnecessarily and doesn't provide private connectivity. CloudWatch Logs requires the EC2 instance to have connectivity to CloudWatch, which would still need internet access or VPC endpoints. Additionally, this adds complexity by introducing CloudWatch Logs as an intermediate step. The requirement is for the EC2 instance to access S3 directly without internet connectivity, which is best achieved with a VPC endpoint.\n\n**Why option 2 is incorrect:**\nThe option that says create an instance profile on EC2 to allow S3 access is incorrect because instance profiles provide IAM permissions for accessing S3, but they don't provide network connectivity. An instance profile grants the EC2 instance permission to make S3 API calls, but the instance still needs network connectivity to reach S3. Without a VPC endpoint or internet access, the instance cannot reach S3 even with proper IAM permissions. Instance profiles solve the authentication/authorization problem, not the network connectivity problem.\n\n**Why option 3 is incorrect:**\nThe option that says create an Amazon API Gateway API with a private link to access the S3 endpoint is incorrect because API Gateway doesn't provide direct private connectivity to S3. API Gateway is designed for building REST APIs, not for providing VPC-level connectivity to AWS services. Additionally, API Gateway would add unnecessary complexity, latency, and cost. VPC endpoints are the proper AWS service for providing private connectivity between VPCs and AWS services like S3.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company is hosting a web application on AWS using a single Amazon EC2 instance that stores \nuser-uploaded documents in an Amazon EBS volume. For better scalability and availability, the \ncompany duplicated the architecture and created a second EC2 instance and EBS volume in \nanother Availability Zone, placing both behind an Application Load Balancer. After completing this \nchange, users reported that, each time they refreshed the website, they could see one subset of \ntheir documents or the other, but never all of the documents at the same time. \nWhat should a solutions architect propose to ensure users see all of their documents at once?",
    "options": [
      {
        "id": 0,
        "text": "Copy the data so both EBS volumes contain all the documents.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the Application Load Balancer to direct a user to the server with the documents",
        "correct": false
      },
      {
        "id": 2,
        "text": "Copy the data from both EBS volumes to Amazon EFS.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the Application Load Balancer to send the request to both servers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCopying the data from both EBS volumes to Amazon EFS and mounting the EFS file system on both EC2 instances is the correct solution. Amazon EFS is a fully managed network file system that provides shared file storage accessible from multiple EC2 instances across multiple Availability Zones. When both EC2 instances mount the same EFS file system, they share access to all documents, ensuring users see all their documents regardless of which instance handles their request. EFS automatically replicates data across multiple AZs, providing high availability. This solves the problem of data fragmentation across separate EBS volumes by providing a single, shared storage layer that both instances can access simultaneously.\n\n**Why option 0 is incorrect:**\nThe option that says copy the data so both EBS volumes contain all the documents is incorrect because EBS volumes are instance-specific storage that cannot be shared between instances. Even if you copy all documents to both EBS volumes, you'll have synchronization issues - when a user uploads a new document, it will only be stored on one EBS volume (the one attached to the instance handling that request). The other instance won't see the new document until you manually sync, which is operationally complex and error-prone. EBS volumes don't provide shared storage capabilities.\n\n**Why option 1 is incorrect:**\nThe option that says configure the Application Load Balancer to direct a user to the server with the documents is incorrect because ALBs don't have visibility into which documents are stored on which server. ALBs route traffic based on health checks and load balancing algorithms, not based on data location. Additionally, this approach doesn't solve the fundamental problem - documents are still fragmented across two separate EBS volumes, and users won't see all their documents in a single view. Sticky sessions might route a user to the same instance, but this doesn't ensure all documents are accessible.\n\n**Why option 3 is incorrect:**\nThe option that says configure the Application Load Balancer to send the request to both servers is incorrect because ALBs cannot send a single request to multiple targets simultaneously. ALBs distribute requests across healthy targets, but each request goes to only one target. Sending requests to both servers would require the application to handle duplicate requests, which doesn't solve the data fragmentation problem. The issue is that documents are stored on separate EBS volumes, not that requests aren't being distributed properly.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company uses NFS to store large video files in on-premises network attached storage. Each \nvideo file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer \ngrowing. The company decides to migrate the video files to Amazon S3. The company must \nmigrate the video files as soon as possible while using the least possible network bandwidth. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Snowball Edge job.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy an S3 File Gateway on premises.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an AWS Direct Connect connection between the on-premises network and AWS.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreating an AWS Snowball Edge job is the correct solution for migrating 70 TB of data as soon as possible while using the least possible network bandwidth. Snowball Edge devices can transfer data at speeds up to 100 Gbps locally, allowing you to copy 70 TB in approximately 1.5-2 hours. The device is shipped to your location, you copy data to it locally (using no internet bandwidth), then ship it back to AWS. AWS then imports the data into S3. While the shipping process takes 6-9 business days total, this approach uses zero network bandwidth, which meets the requirement to use the least possible bandwidth. For large datasets like 70 TB, Snowball is the most bandwidth-efficient migration method.\n\n**Why option 0 is incorrect:**\nThe option that says create an S3 bucket and upload files directly is incorrect because uploading 70 TB over the internet would consume significant network bandwidth, which violates the requirement to use the least possible bandwidth. Even with high-speed internet, transferring 70 TB would take days or weeks depending on available bandwidth, and it would consume all available network capacity. This approach doesn't minimize bandwidth usage as required.\n\n**Why option 2 is incorrect:**\nThe option that says deploy an S3 File Gateway on-premises is incorrect because S3 File Gateway provides a file interface to S3 but still transfers data over the internet. When files are written to the File Gateway, they are uploaded to S3 over your network connection, consuming bandwidth. File Gateway is designed for ongoing file access, not for one-time bulk migration. It doesn't minimize bandwidth usage for a 70 TB migration - it still requires transferring all data over the internet.\n\n**Why option 3 is incorrect:**\nThe option that says set up an AWS Direct Connect connection between the on-premises network and AWS is incorrect because Direct Connect provides dedicated network connectivity but still uses network bandwidth. While Direct Connect offers consistent, low-latency connectivity, it doesn't eliminate bandwidth usage - you're still transferring 70 TB over the network connection. Direct Connect is better suited for ongoing connectivity needs, not for one-time migrations where bandwidth minimization is critical. Additionally, Direct Connect setup takes time and may not be the fastest migration method.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company has an application that ingests incoming messages. Dozens of other applications and \nmicroservices then quickly consume these messages. The number of messages varies drastically \nand sometimes increases suddenly to 100,000 each second. The company wants to decouple the \nsolution and increase scalability. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Persist the messages to Amazon Kinesis Data Analytics.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write the messages to Amazon Kinesis Data Streams with a single shard.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nPublishing messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) queues as subscribers is the correct solution. SNS provides pub/sub messaging that decouples the message producer from consumers. When messages are published to an SNS topic, they are automatically delivered to all subscribed SQS queues. This allows dozens of applications and microservices to consume messages independently, each reading from their own SQS queue. SNS supports nearly unlimited throughput (can handle 100,000+ messages per second), and SQS queues can scale independently based on consumption patterns. This architecture provides decoupling, scalability, and allows each consumer to process messages at its own rate.\n\n**Why option 0 is incorrect:**\nThe option that says persist messages to Amazon Kinesis Data Analytics is incorrect because Kinesis Data Analytics is designed for real-time stream processing and analytics, not for pub/sub message distribution to multiple consumers. Kinesis Data Analytics processes streaming data and runs SQL queries or applications on the stream, but it doesn't provide the same decoupling and fan-out capabilities as SNS. Additionally, Kinesis requires managing shards and has different scaling characteristics than SNS/SQS for this use case.\n\n**Why option 1 is incorrect:**\nThe option that says deploy the ingestion application on EC2 instances in an Auto Scaling group to scale based on message volume is incorrect because this only addresses scaling the ingestion layer, not decoupling and distributing messages to multiple consumers. Auto Scaling helps handle incoming message volume, but it doesn't solve the problem of distributing messages to dozens of applications and microservices. The requirement is to decouple the solution and allow multiple consumers to process messages independently, which requires a messaging service like SNS/SQS.\n\n**Why option 2 is incorrect:**\nThe option that says write messages to Amazon Kinesis Data Streams with a single shard is incorrect because a single shard has throughput limitations (1 MB/second or 1,000 records/second for writes, 2 MB/second for reads). With message volumes that can spike to 100,000 messages per second, a single shard would be a bottleneck and couldn't handle the load. Additionally, Kinesis requires consumers to manage shard processing and doesn't provide the same level of decoupling as SNS/SQS for multiple independent consumers.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 42,
    "text": "A company is migrating a distributed application to AWS. The application serves variable \nworkloads. The legacy platform consists of a primary server that coordinates jobs across multiple \ncompute nodes. The company wants to modernize the application with a solution that maximizes \nresiliency and scalability. \nHow should a solutions architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement the primary server and the compute nodes with Amazon EC2 instances that are",
        "correct": false
      },
      {
        "id": 3,
        "text": "implement the primary server and the compute nodes with Amazon EC2 instances that are",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nConfiguring an Amazon Simple Queue Service (Amazon SQS) queue as a destination for jobs and configuring EC2 Auto Scaling to scale compute nodes based on the SQS queue size is the correct solution. SQS decouples the primary server from compute nodes, allowing the primary server to submit jobs to the queue without needing to know about or directly communicate with compute nodes. This increases resiliency because if compute nodes fail, jobs remain in the queue and can be processed when nodes recover. Auto Scaling based on SQS queue size ensures compute nodes scale automatically to handle variable workloads - when the queue grows, more instances are added; when the queue shrinks, instances are terminated. This maximizes both resiliency and scalability.\n\n**Why option 0 is incorrect:**\nThe option that says configure an SQS queue as a destination but doesn't include Auto Scaling configuration is incomplete. While SQS provides decoupling, without Auto Scaling the compute nodes won't automatically scale to handle variable workloads. The requirement is to maximize resiliency and scalability, which requires both SQS for decoupling and Auto Scaling for dynamic scaling based on workload.\n\n**Why option 2 is incorrect:**\nThe option that says implement the primary server and compute nodes with EC2 instances that are manually scaled is incorrect because manual scaling doesn't maximize scalability or resiliency. Manual scaling requires human intervention to add or remove instances, which doesn't respond quickly to variable workloads. Additionally, if this approach doesn't use SQS for decoupling, the primary server remains tightly coupled to compute nodes, reducing resiliency. If the primary server fails or compute nodes fail, the system may lose jobs or become unavailable.\n\n**Why option 3 is incorrect:**\nThe option that says implement the primary server and compute nodes with EC2 instances (duplicate/incomplete) is incorrect for similar reasons as option 2. Without SQS for decoupling and Auto Scaling for dynamic scaling, the solution doesn't maximize resiliency and scalability. The legacy architecture with a single primary server coordinating jobs is a single point of failure and doesn't scale well for variable workloads.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 43,
    "text": "A company is running an SMB file server in its data center. The file server stores large files that \nare accessed frequently for the first few days after the files are created. After 7 days the files are \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n60 \nrarely accessed. \n \nThe total data size is increasing and is close to the company's total storage capacity. A solutions \narchitect must increase the company's available storage space without losing low-latency access \nto the most recently accessed files. The solutions architect must also provide file lifecycle \nmanagement to avoid future storage issues. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 File Gateway to extend the company's storage space.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon FSx for Windows File Server file system to extend the company's storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Install a utility on each user's computer to access Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCreating an Amazon S3 File Gateway to extend the company's storage space is the correct solution. S3 File Gateway provides an SMB or NFS file interface to S3, allowing the existing SMB file server infrastructure to seamlessly extend storage into S3. File Gateway caches frequently accessed files locally, providing low-latency access to recently created files (those accessed frequently in the first few days). Files are automatically stored in S3, which provides virtually unlimited storage capacity. S3 Lifecycle policies can be configured to transition files older than 7 days to cheaper storage classes (like S3 Glacier), providing file lifecycle management to avoid future storage issues. This solution increases available storage without losing low-latency access to recent files.\n\n**Why option 0 is incorrect:**\nThe option that says use AWS DataSync to copy data older than 7 days from the SMB file server to AWS is incorrect because DataSync is a one-time or scheduled data transfer service, not a file gateway. DataSync would copy files to S3, but users would lose access to those files from the SMB server. The requirement is to extend storage space while maintaining access, not to archive old files. Additionally, DataSync doesn't provide the low-latency caching that File Gateway offers for frequently accessed files.\n\n**Why option 2 is incorrect:**\nThe option that says create an Amazon FSx for Windows File Server file system to extend storage is incorrect because FSx for Windows File Server is a fully managed Windows file server in AWS, not a gateway to extend on-premises storage. FSx would require migrating the entire file server to AWS, which doesn't meet the requirement to extend existing storage. Additionally, FSx doesn't provide the same cost-effective lifecycle management as S3 for files that are rarely accessed after 7 days.\n\n**Why option 3 is incorrect:**\nThe option that says install a utility on each user's computer to access S3 is incorrect because this approach requires changes on every user's computer, increases operational overhead, and doesn't provide the seamless integration that File Gateway offers. Users would need to learn new tools and workflows, and the solution doesn't integrate with the existing SMB file server infrastructure. File Gateway provides transparent access through the existing SMB protocol without requiring client-side changes.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company is building an ecommerce web application on AWS. The application sends \ninformation about new orders to an Amazon API Gateway REST API to process. The company \nwants to ensure that orders are processed in the order that they are received. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an API Gateway integration to publish a message to an Amazon Simple Notification",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an API Gateway authorizer to block any requests while the application processes an order.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue is the correct solution. SQS FIFO queues guarantee that messages are processed exactly once and in the exact order they are received. When orders are sent to the API Gateway, they are forwarded to the FIFO queue, which maintains strict first-in-first-out ordering. This ensures orders are processed in the order they are received, meeting the requirement. FIFO queues are designed for scenarios where message ordering is critical, such as ecommerce order processing.\n\n**Why option 0 is incorrect:**\nThe option that says use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic is incorrect because SNS is a pub/sub messaging service that delivers messages to multiple subscribers, but it doesn't guarantee message ordering. SNS messages can arrive out of order, especially when there are multiple subscribers or when messages are retried. SNS is designed for fan-out messaging, not for maintaining strict order like FIFO queues.\n\n**Why option 2 is incorrect:**\nThe option that says use an API Gateway authorizer to block requests while the application processes an order is incorrect because API Gateway authorizers are used for authentication and authorization, not for controlling message processing order. Authorizers validate requests before they reach the backend, but they cannot ensure orders are processed in sequence. This approach would block all requests during processing, which doesn't scale and doesn't guarantee order - it just serializes all requests, which is inefficient.\n\n**Why option 3 is incorrect:**\nThe option that says use an API Gateway integration to send a message to an SQS standard queue is incorrect because SQS standard queues provide high throughput and at-least-once delivery, but they don't guarantee message ordering. Messages in standard queues may be delivered out of order, especially under high load. The requirement explicitly states orders must be processed in the order received, which requires a FIFO queue, not a standard queue.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 45,
    "text": "A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora \ndatabase. The EC2 instances connect to the database by using user names and passwords that \nare stored locally in a file. The company wants to minimize the operational overhead of credential \nmanagement. \nWhat should a solutions architect do to accomplish this goal? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n61",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Systems Manager Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket lo store objects that are encrypted with an AWS Key.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume or each EC2 instance.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing AWS Secrets Manager is the correct solution to minimize operational overhead of credential management. Secrets Manager provides a centralized service for storing, retrieving, and rotating database credentials. Instead of storing usernames and passwords in local files on EC2 instances (which requires manual updates and is insecure), Secrets Manager allows EC2 instances to retrieve credentials programmatically using IAM roles. Secrets Manager can automatically rotate RDS database credentials on a schedule, reducing the operational burden of manual credential rotation. The service integrates with RDS to update database passwords automatically, and applications can retrieve the latest credentials without code changes.\n\n**Why option 1 is incorrect:**\nThe option that says use AWS Systems Manager Parameter Store is incorrect because while Parameter Store can store encrypted parameters (including SecureString parameters), it doesn't provide automatic credential rotation capabilities. Secrets Manager is specifically designed for secrets management with built-in rotation support for RDS databases. Parameter Store is better suited for configuration data and application parameters, not for credentials that need regular rotation. Using Parameter Store would still require manual rotation processes, increasing operational overhead.\n\n**Why option 2 is incorrect:**\nThe option that says create an S3 bucket to store encrypted objects with AWS KMS is incorrect because storing credentials in S3 adds unnecessary complexity and doesn't provide the same level of integration and automation as Secrets Manager. You would need to build custom logic to retrieve and decrypt credentials from S3, manage access permissions, and implement rotation manually. This approach increases operational overhead rather than minimizing it. Secrets Manager is purpose-built for this use case.\n\n**Why option 3 is incorrect:**\nThe option that says create an encrypted EBS volume for each EC2 instance to store credentials is incorrect because this approach still requires storing credentials on the instances themselves, which doesn't solve the operational overhead problem. You would still need to manually update credentials on each EBS volume when they change, and there's no centralized management or automatic rotation. Additionally, EBS volumes are instance-specific, making credential management across multiple instances complex and error-prone.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 46,
    "text": "A global company hosts its web application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The web application has static data and dynamic data. The company \nstores its static data in an Amazon S3 bucket. The company wants to improve performance and \nreduce latency for the static data and dynamic data. The company is using its own domain name \nregistered with Amazon Route 53. \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon CloudFront distribution that has the ALB as an origin.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribution that has the ALB as an origin.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCreating an Amazon CloudFront distribution that has both the S3 bucket and the ALB as origins is the correct solution. CloudFront is a Content Delivery Network (CDN) that caches content at edge locations globally, improving performance and reducing latency for users worldwide. By configuring CloudFront with multiple origins, you can route static content requests to the S3 bucket origin and dynamic content requests to the ALB origin. CloudFront caches static content at edge locations, serving it directly to users without hitting S3 or the ALB, dramatically reducing latency. Dynamic content is routed to the ALB, which forwards to EC2 instances. CloudFront integrates seamlessly with Route 53 for custom domain names and supports custom SSL certificates, meeting all requirements.\n\n**Why option 1 is incorrect:**\nThe option that says create a CloudFront distribution with only the ALB as an origin is incorrect because this approach doesn't optimize static content delivery. Static content stored in S3 should be served directly from S3 through CloudFront for best performance, not routed through the ALB and EC2 instances. Routing static content through the ALB adds unnecessary latency and load on the application servers. The requirement is to improve performance for both static and dynamic data, which requires both origins.\n\n**Why option 2 is incorrect:**\nThe option that says create a CloudFront distribution with only the S3 bucket as an origin is incorrect because this approach doesn't handle dynamic content. Dynamic content from the EC2 instances behind the ALB cannot be served from S3. Users requesting dynamic content would either get errors or stale cached content. The requirement is to improve performance for both static and dynamic data, which requires CloudFront to route to both S3 (for static) and ALB (for dynamic) based on the request path or behavior.\n\n**Why option 3 is incorrect:**\nThe option that says create a CloudFront distribution with the ALB as an origin (duplicate of option 1) is incorrect for the same reasons as option 1. Without S3 as an origin, static content performance isn't optimized, and the solution doesn't fully meet the requirement to improve performance for both static and dynamic data.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 47,
    "text": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance \nactivities, the company needs to rotate the credentials tor its Amazon ROS tor MySQL databases \nacross multiple AWS Regions. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Store the credentials as secrets in AWS Secrets Manager.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Store the credentials as secrets in AWS Systems Manager by creating a secure string",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nStoring credentials as secrets in AWS Secrets Manager is the solution with the least operational overhead. Secrets Manager is specifically designed for storing and rotating secrets, with built-in support for automatic credential rotation for Amazon RDS databases (including MySQL). Secrets Manager can automatically rotate RDS credentials on a schedule using Lambda functions, eliminating the need for manual rotation processes. The service integrates directly with RDS to update database passwords automatically. For multi-region deployments, Secrets Manager supports replicating secrets across regions, allowing centralized management of credentials across multiple AWS Regions. This minimizes operational overhead by automating the rotation process.\n\n**Why option 1 is incorrect:**\nThe option that says store credentials as secrets in AWS Systems Manager Parameter Store by creating a secure string is incorrect because Parameter Store doesn't provide automatic credential rotation capabilities. While Parameter Store can store encrypted SecureString parameters, you would need to build custom automation (Lambda functions, scripts) to rotate credentials manually. Secrets Manager has built-in rotation support for RDS, which significantly reduces operational overhead compared to building custom rotation logic for Parameter Store.\n\n**Why option 2 is incorrect:**\nThe option that says store credentials in an S3 bucket with server-side encryption enabled is incorrect because S3 is not designed for secrets management. You would need to build custom logic to retrieve, decrypt, and rotate credentials, which increases operational overhead. Additionally, managing access permissions, implementing rotation schedules, and updating RDS passwords would all require custom development and ongoing maintenance. Secrets Manager provides these capabilities out of the box.\n\n**Why option 3 is incorrect:**\nThe option that says encrypt credentials using AWS KMS multi-region keys is incorrect because KMS is a key management service, not a secrets management service. While KMS can encrypt data, it doesn't provide storage, retrieval, or rotation capabilities for secrets. You would still need to build a complete secrets management solution on top of KMS, including storage, access control, rotation logic, and RDS integration, which significantly increases operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company is planning to run a group of Amazon EC2 instances that connect to an Amazon \nAurora database. The company has built an AWS CloudFormation template to deploy the EC2 \ninstances and the Aurora DB cluster. The company wants to allow the instances to authenticate \nto the database in a secure way. The company does not want to maintain static database \ncredentials. \nWhich solution meets these requirements with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a database user with a user name and password. Add parameters for the database user name and password to the CloudFormation template. Pass the parameters to the EC2 instances when the instances are launched.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a database user with a user name and password. Store the user name and password in AWS Systems Manager Parameter Store. Configure the EC2 instances to retrieve the database credentials from Parameter Store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the DB cluster to use IAM database authentication. Create a database user to use with IAM authentication. Associate a role with the EC2 instances to allow applications on the instances to access the database.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure the DB cluster to use IAM database authentication with an IAM user. Create a database user that has a name that matches the IAM user. Associate the IAM user with the EC2 instances to allow applications on the instances to access the database.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nConfiguring the DB cluster to use IAM database authentication, creating a database user for IAM authentication, and associating an IAM role with the EC2 instances is the solution with the least operational effort that meets the requirement to avoid static credentials. IAM database authentication eliminates the need for static usernames and passwords. EC2 instances use their IAM role to generate authentication tokens, which are temporary and automatically rotated. This eliminates credential management overhead - no passwords to store, rotate, or manage. The CloudFormation template can create the IAM role and associate it with EC2 instances, and configure Aurora to enable IAM authentication, all in a single deployment.\n\n**Why option 0 is incorrect:**\nThe option that says create a database user with username and password, add parameters to CloudFormation template, and pass to EC2 instances is incorrect because this approach still uses static credentials (username and password). Even though credentials are passed via CloudFormation parameters, they are still static and need to be managed, rotated, and updated manually. This violates the requirement to avoid maintaining static database credentials. Additionally, storing credentials in CloudFormation parameters or passing them to EC2 instances increases security risk and operational overhead.\n\n**Why option 1 is incorrect:**\nThe option that says create a database user with username and password, store in Parameter Store, and configure EC2 instances to retrieve from Parameter Store is incorrect because this approach still uses static credentials. Parameter Store can securely store credentials, but they are still static username/password combinations that need manual rotation and management. This doesn't eliminate static credential maintenance as required. While Parameter Store is better than local files, it doesn't meet the requirement to avoid static credentials.\n\n**Why option 3 is incorrect:**\nThe option that says configure IAM database authentication with an IAM user (not a role) and associate the IAM user with EC2 instances is incorrect because IAM users are for human access or programmatic access with long-term credentials, not for EC2 instances. EC2 instances should use IAM roles, not IAM users. Roles provide temporary credentials that are automatically rotated, while IAM users require managing access keys. Using IAM users with EC2 instances is an anti-pattern and increases operational overhead compared to using roles.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A solutions architect is designing a shared storage solution for a web application that is deployed \nacross multiple Availability Zones. The web application runs on \nAmazon EC2 instances that are in an Auto Scaling group. The company plans to make frequent \nchanges to the content. The solution must have strong consistency in returning the new content \nas soon as the changes occur. \nWhich solutions meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block storage that is mounted to the individual EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the individual EC2 instances.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a shared Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon S3 bucket to store the web content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0,
      1
    ],
    "explanation": "**Why option 0 is correct:**\nUsing AWS Storage Gateway Volume Gateway with iSCSI block storage mounted to individual EC2 instances is correct. Volume Gateway provides iSCSI block storage that can be mounted by multiple EC2 instances across Availability Zones. When content is written to the volume, all instances see the changes immediately due to strong consistency. Volume Gateway caches frequently accessed data locally while storing data durably in S3, providing low-latency access with strong consistency guarantees. This solution supports frequent content changes with immediate visibility across all instances.\n\n**Why option 1 is correct:**\nCreating an Amazon Elastic File System (Amazon EFS) file system and mounting it on individual EC2 instances is correct. EFS is a fully managed network file system that provides shared storage accessible from multiple EC2 instances across multiple Availability Zones. EFS provides strong read-after-write consistency, meaning that when content is written, subsequent reads immediately see the new content. EFS automatically scales storage capacity and performance, making it ideal for applications with frequent content changes. All EC2 instances in the Auto Scaling group can mount the same EFS file system and see changes immediately.\n\n**Why option 2 is incorrect:**\nThe option that says create a shared Amazon EBS volume and mount it on individual EC2 instances is incorrect because EBS volumes cannot be shared across multiple EC2 instances simultaneously in a read-write fashion. EBS Multi-Attach allows a volume to be attached to multiple instances, but it's designed for specific use cases like clustered applications and doesn't provide the same level of consistency and performance as EFS or Volume Gateway for general file sharing. EBS volumes are instance-specific storage, not shared file systems.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS DataSync to perform continuous synchronization of data between EC2 hosts is incorrect because DataSync is a data transfer service for one-time or scheduled migrations, not for real-time shared storage. DataSync would require continuous synchronization jobs running on a schedule, which introduces latency between when content changes and when it's visible on other instances. This doesn't provide strong consistency \"as soon as changes occur\" - there's a delay based on the sync schedule. Additionally, this approach adds operational complexity and doesn't scale well with Auto Scaling groups.\n\n**Why option 4 is incorrect:**\nThe option that says create an S3 bucket to store web content, set Cache-Control to no-cache, and use CloudFront to deliver content is incorrect because S3 is object storage, not a file system. While S3 can store web content, it doesn't provide the same file system semantics and strong consistency guarantees as EFS or Volume Gateway. Additionally, even with no-cache headers, CloudFront and S3 eventual consistency models may introduce slight delays. For applications that need immediate visibility of changes across all instances, a shared file system like EFS or Volume Gateway is more appropriate.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 50,
    "text": "A company that operates a web application on premises is preparing to launch a newer version of \nthe application on AWS. The company needs to route requests to either the AWS-hosted or the \non-premises-hosted application based on the URL query string. The on-premises application is \nnot available from the internet, and a VPN connection is established between Amazon VPC and \nthe company's data center. The company wants to use an Application Load Balancer (ALB) for \nthis launch. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use two ALBs: one for on-premises and one for the AWS resource.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use two ALBs: one for on-premises and one for the AWS resource.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use one ALB with two target groups: one for the AWS resource and one for on premises.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use one ALB with two AWS Auto Scaling groups: one for the AWS resource and one for on",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing one ALB with two target groups (one for AWS resources and one for on-premises) is the correct solution. Application Load Balancers support advanced request routing based on query string parameters. You can create listener rules that examine the query string and route traffic to different target groups accordingly. The on-premises target group can use IP addresses as targets, pointing to the on-premises application accessible through the VPN connection. ALB supports routing based on query string, path, host header, HTTP headers, and source IP, making it ideal for this use case. This approach uses a single ALB, simplifying the architecture while meeting the routing requirement.\n\n**Why option 0 is incorrect:**\nThe option that says use two ALBs (one for on-premises and one for AWS resource) is incorrect because ALBs cannot be directly connected to on-premises resources - they route to targets within AWS (EC2 instances, IP addresses, Lambda functions, etc.). While you could potentially use an ALB with IP targets pointing to on-premises resources through VPN, having two separate ALBs doesn't provide query string-based routing between them. You would need an additional service (like Route 53) to route between ALBs based on query strings, which adds complexity. A single ALB with query string routing rules is simpler and more efficient.\n\n**Why option 1 is incorrect:**\nThe option that says use two ALBs (duplicate of option 0) is incorrect for the same reasons as option 0. Two ALBs don't provide integrated query string-based routing, and the architecture is more complex than necessary.\n\n**Why option 3 is incorrect:**\nThe option that says use one ALB with two AWS Auto Scaling groups (one for AWS resource and one for on-premises) is incorrect because Auto Scaling groups are AWS resources that manage EC2 instances, not on-premises resources. You cannot create an Auto Scaling group for on-premises infrastructure. Additionally, Auto Scaling groups are not target groups - ALBs route to target groups, which can contain Auto Scaling groups, EC2 instances, IP addresses, or Lambda functions. For on-premises resources, you would use an IP target group, not an Auto Scaling group.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 51,
    "text": "A company wants to move from many standalone AWS accounts to a consolidated, multi-account \narchitecture. The company plans to create many new AWS accounts for different business units. \nThe company needs to authenticate access to these AWS accounts by using a centralized \ncorporate directory service \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Select TWO )",
    "options": [
      {
        "id": 0,
        "text": "Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an Amazon Cognito identity pool. Configure AWS Single Sign-On to accept Amazon Cognito authentication.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS Single Sign-On to AWS Directory Service.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up AWS Single Sign-On (AWS SSO) in the organization. Configure AWS SSO and integrate it with the company's corporate directory service.",
        "correct": true
      }
    ],
    "correctAnswers": [
      0,
      4
    ],
    "explanation": "**Why option 0 is correct:**\nCreating a new organization in AWS Organizations with all features turned on and creating the new AWS accounts in the organization is the first step. AWS Organizations provides centralized management of multiple AWS accounts, allowing you to consolidate billing, apply policies, and manage accounts as a single unit. Creating accounts within the organization enables centralized governance and is a prerequisite for using AWS SSO (now IAM Identity Center) for centralized authentication.\n\n**Why option 4 is correct:**\nSetting up AWS Single Sign-On (AWS SSO, now IAM Identity Center) in the organization and configuring it to integrate with the company's corporate directory service is the correct solution for centralized authentication. AWS SSO integrates with corporate directory services like Active Directory, Okta, Azure AD, and other SAML 2.0 identity providers. This allows users to authenticate using their corporate credentials and access AWS accounts and applications through a single sign-on experience. AWS SSO eliminates the need to manage IAM users in each account separately.\n\n**Why option 1 is incorrect:**\nThe option that says set up an Amazon Cognito identity pool and configure AWS SSO to accept Cognito authentication is incorrect because Cognito is designed for end-user authentication in web and mobile applications, not for corporate directory integration in multi-account AWS environments. Cognito provides user pools and identity pools for application users, but it doesn't integrate with corporate directory services like Active Directory. AWS SSO integrates directly with corporate directories, not through Cognito.\n\n**Why option 2 is incorrect:**\nThe option that says configure a service control policy (SCP) to manage AWS accounts and add AWS SSO to AWS Directory Service is incorrect because SCPs are used for permission management and governance, not for authentication. While SCPs are important for managing accounts in an organization, they don't provide authentication capabilities. Additionally, \"adding AWS SSO to AWS Directory Service\" is not the correct integration approach - AWS SSO integrates with existing directory services, it doesn't add to AWS Directory Service.\n\n**Why option 3 is incorrect:**\nThe option that says create a new organization and configure the organization's authentication mechanism to use AWS Directory Service directly is incorrect because AWS Organizations doesn't have a built-in authentication mechanism that directly uses AWS Directory Service. AWS Directory Service provides managed Active Directory, but authentication for AWS accounts is handled through AWS SSO (IAM Identity Center), which integrates with directory services. You cannot directly configure Organizations to use Directory Service for authentication without AWS SSO.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "An entertainment company is using Amazon DynamoDB to store media metadata. \nThe application is read intensive and experiencing delays. \nThe company does not have staff to handle additional operational overhead and needs to \nimprove the performance efficiency of DynamoDB without reconfiguring the application. \nWhat should a solutions architect recommend to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon ElastiCache for Redis",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon DynamoDB Accelerate (DAX)",
        "correct": true
      },
      {
        "id": 2,
        "text": "Replicate data by using DynamoDB global tables",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Memcached with Auto Discovery enabled",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon DynamoDB Accelerator (DAX) is the correct solution. DAX is a fully managed, in-memory caching service for DynamoDB that provides microsecond latency for read-heavy workloads. DAX is a drop-in replacement for DynamoDB - applications can use the same DynamoDB API calls, and DAX automatically caches frequently accessed items. This requires no application reconfiguration, meeting the requirement to improve performance without reconfiguring the application. DAX handles cache management, scaling, and failover automatically, requiring no operational overhead from staff. It's specifically designed for read-intensive DynamoDB workloads.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon ElastiCache for Redis is incorrect because ElastiCache requires application changes to implement caching logic. You would need to modify the application to check ElastiCache first, then fall back to DynamoDB if there's a cache miss, and update the cache when data changes. This requires significant application reconfiguration, which violates the requirement. Additionally, ElastiCache doesn't integrate seamlessly with DynamoDB like DAX does - it's a separate service that requires custom integration code.\n\n**Why option 2 is incorrect:**\nThe option that says replicate data using DynamoDB global tables is incorrect because global tables provide multi-region replication for high availability and disaster recovery, but they don't improve read performance or reduce latency. Global tables replicate data across regions but don't cache data or reduce read latency. The requirement is to improve performance efficiency for a read-intensive workload experiencing delays, which requires caching, not replication. Global tables also add operational overhead for managing replication.\n\n**Why option 3 is incorrect:**\nThe option that says use Amazon ElastiCache for Memcached with Auto Discovery enabled is incorrect because, like Redis, Memcached requires application changes to implement caching logic. You would need to modify the application to use Memcached as a cache layer, which requires reconfiguration. Additionally, Memcached doesn't integrate seamlessly with DynamoDB - it's a separate caching service that requires custom application logic to manage cache hits, misses, and invalidation. DAX provides transparent caching without application changes.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company has an application that provides marketing services to stores. The services are based \non previous purchases by store customers. The stores upload transaction data to the company \nthrough SFTP, and the data is processed and analyzed to generate new marketing offers. Some \nof the files can exceed 200 GB in size. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n66 \nRecently, the company discovered that some of the stores have uploaded files that contain \npersonally identifiable information (PII) that should not have been included. The company wants \nadministrators to be alerted if PII is shared again. The company also wants to automate \nremediation. \n \nWhat should a solutions architect do to meet these requirements with the LEAST development \neffort?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the objects that contain PII.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing an Amazon S3 bucket as a secure transfer point, Amazon Macie to scan objects, and Amazon SNS to notify administrators is the solution with the least development effort. Macie is a fully managed data security service that automatically discovers, classifies, and protects sensitive data in S3. Macie uses machine learning to identify PII and other sensitive data without requiring custom scanning algorithms. When Macie detects PII, it publishes findings to Amazon EventBridge, which can trigger SNS notifications to administrators. This approach requires minimal development - just configure Macie, set up EventBridge rules, and configure SNS topics. Macie handles the scanning automatically, eliminating the need to write custom PII detection code.\n\n**Why option 0 is incorrect:**\nThe option that says use S3 as a transfer point, Amazon Inspector to scan objects, and S3 Lifecycle policy to remove objects containing PII is incorrect because Amazon Inspector is designed for security assessment of EC2 instances and container images, not for scanning S3 objects for PII. Inspector assesses applications for vulnerabilities and deviations from security best practices, but it doesn't detect PII in data files. Additionally, automatically deleting files via Lifecycle policy based on Inspector findings isn't a standard workflow and would require custom integration.\n\n**Why option 2 is incorrect:**\nThe option that says implement custom scanning algorithms in a Lambda function, trigger on object upload, and use SNS for notifications is incorrect because this approach requires significant development effort. You would need to write custom PII detection algorithms, handle large files (200+ GB), manage Lambda timeouts and memory limits, and implement proper scanning logic. This violates the requirement for least development effort. Macie provides pre-built PII detection capabilities that don't require custom code.\n\n**Why option 3 is incorrect:**\nThe option that says implement custom scanning algorithms in Lambda, use SES for notifications, and trigger Lifecycle policy is incorrect for the same reasons as option 2 - it requires significant development effort to build custom PII detection. Additionally, using SES for notifications and Lifecycle policies for automated deletion adds complexity. Macie with EventBridge and SNS provides a more integrated, managed solution that requires less development effort.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a \nspecific AWS Region for an upcoming event that will last 1 week. \n \nWhat should the company do to guarantee the EC2 capacity?",
    "options": [
      {
        "id": 0,
        "text": "Purchase Reserved instances that specify the Region needed",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an On Demand Capacity Reservation that specifies the Region needed",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase Reserved instances that specify the Region and three Availability Zones needed",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nCreating an On-Demand Capacity Reservation that specifies the Region and three Availability Zones is the correct solution. Capacity Reservations guarantee EC2 capacity in specific Availability Zones for a specified duration. You can create separate Capacity Reservations for each of the three required Availability Zones, ensuring capacity is available in all three zones. Capacity Reservations are ideal for short-term, predictable workloads like a 1-week event. They guarantee capacity without requiring a 1- or 3-year commitment like Reserved Instances. You pay only for the capacity reserved, and you can cancel the reservation when the event ends.\n\n**Why option 0 is incorrect:**\nThe option that says purchase Reserved Instances that specify only the Region is incorrect because Reserved Instances don't guarantee capacity in specific Availability Zones - they provide discounts but capacity is subject to availability. Additionally, Reserved Instances require 1- or 3-year commitments, which is not cost-effective for a 1-week event. Reserved Instances are designed for long-term, steady-state workloads, not short-term events.\n\n**Why option 1 is incorrect:**\nThe option that says create an On-Demand Capacity Reservation that specifies only the Region is incorrect because Capacity Reservations must be created for specific Availability Zones, not just Regions. You cannot create a single Capacity Reservation that covers multiple Availability Zones - you need separate reservations for each AZ. The requirement explicitly states capacity is needed in three specific Availability Zones, so you must create reservations for each zone.\n\n**Why option 2 is incorrect:**\nThe option that says purchase Reserved Instances that specify the Region and three Availability Zones is incorrect because Reserved Instances don't guarantee capacity - they provide billing discounts but capacity is still subject to availability. Additionally, Reserved Instances require long-term commitments (1-3 years) and are not suitable for a 1-week event. You would be paying for Reserved Instances for years when you only need capacity for one week, which is not cost-effective.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company's website uses an Amazon EC2 instance store for its catalog of items. The company \nwants to make sure that the catalog is highly available and that the catalog is stored in a durable \nlocation. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n67 \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the catalog to Amazon ElastiCache for Redis.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a larger EC2 instance with a larger instance store.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nMoving the catalog to an Amazon Elastic File System (Amazon EFS) file system is the correct solution. EFS provides a fully managed, highly available network file system that can be accessed by multiple EC2 instances simultaneously. EFS automatically replicates data across multiple Availability Zones, providing high availability and durability. Unlike instance store, which is ephemeral and lost when the instance stops or fails, EFS provides persistent, durable storage. EFS is designed for shared file storage that needs to be accessible from multiple instances, making it ideal for a website catalog that may need to be accessed by multiple web servers.\n\n**Why option 0 is incorrect:**\nThe option that says move the catalog to Amazon ElastiCache for Redis is incorrect because ElastiCache is an in-memory caching service, not durable storage. ElastiCache data is stored in memory and can be lost if the cache cluster fails or is restarted. While ElastiCache provides high performance, it doesn't meet the durability requirement. Additionally, ElastiCache is designed for caching frequently accessed data, not as primary storage for a catalog. The requirement is for durable storage, not caching.\n\n**Why option 1 is incorrect:**\nThe option that says deploy a larger EC2 instance with a larger instance store is incorrect because instance store is ephemeral storage that is lost when the instance stops, terminates, or fails. Instance store provides high-performance local storage but is not durable or highly available. If the instance fails, all data on the instance store is lost. This doesn't meet the requirement for durable storage or high availability. Larger instance store doesn't solve the durability problem.\n\n**Why option 2 is incorrect:**\nThe option that says move the catalog from instance store to Amazon S3 Glacier Deep Archive is incorrect because S3 Glacier Deep Archive is designed for long-term archival storage with retrieval times of 12 hours, not for active website catalogs that need immediate access. Glacier Deep Archive is the lowest-cost storage class but has the slowest retrieval times, making it unsuitable for a website catalog that users need to access in real-time. Additionally, Glacier doesn't provide file system semantics - it's object storage, not a file system.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 \nyear of the call, but users access the files infrequently after 1 year. The company wants to \noptimize its solution by giving users the ability to query and retrieve files that are less than 1-year-\nold as quickly as possible. A delay in retrieving older files is acceptable. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Store individual files with tags in Amazon S3 Glacier Instant Retrieval.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store individual files in Amazon S3 Intelligent-Tiering.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store individual files with tags in Amazon S3 Standard storage.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store individual files in Amazon S3 Standard storage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nStoring individual files in Amazon S3 Intelligent-Tiering is the most cost-effective solution. Intelligent-Tiering automatically moves objects between access tiers (Frequent Access, Infrequent Access, Archive Instant Access, Archive Access, Deep Archive Access) based on access patterns. Files accessed within 1 year will remain in the Frequent Access tier, providing fast retrieval. Files not accessed for extended periods will automatically move to cheaper tiers, reducing costs. Intelligent-Tiering monitors access patterns and optimizes storage costs automatically without requiring manual lifecycle policies. This solution provides fast access to recent files while cost-optimizing older files, meeting both the performance requirement for files less than 1 year old and the cost optimization requirement.\n\n**Why option 0 is incorrect:**\nThe option that says store files with tags in Amazon S3 Glacier Instant Retrieval is incorrect because Glacier Instant Retrieval is designed for archive data that is accessed once or twice per quarter, not for files that are accessed randomly within 1 year. While Glacier Instant Retrieval provides millisecond access times, it has higher storage costs than Standard storage and is optimized for long-term archival, not for active access patterns. Additionally, using tags to query files doesn't provide the same query capabilities as S3's native features.\n\n**Why option 2 is incorrect:**\nThe option that says store files with tags in Amazon S3 Standard storage is incorrect because storing all files in Standard storage, regardless of age, is not cost-effective. Files older than 1 year that are infrequently accessed should be moved to cheaper storage classes. Standard storage is the most expensive storage class and should only be used for frequently accessed data. While tags can help organize files, they don't automatically optimize storage costs based on access patterns like Intelligent-Tiering does.\n\n**Why option 3 is incorrect:**\nThe option that says store files in Amazon S3 Standard storage (without tags) is incorrect for the same reasons as option 2. Storing all files in Standard storage regardless of access patterns is not cost-effective. Files older than 1 year that are rarely accessed should be in cheaper storage tiers. Standard storage doesn't automatically optimize costs based on access patterns - you would need to implement lifecycle policies manually, which is less efficient than Intelligent-Tiering's automatic optimization.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 57,
    "text": "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The \nworkload is powered by third-party software. The company needs to patch the third-party \nsoftware on all EC2 instances as quickly as possible to remediate a critical security vulnerability. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n68",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to apply the patch to all EC2 instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Run Command to run a custom command that applies the patch to",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nUsing AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances is the correct solution for patching third-party software quickly. Run Command allows you to execute commands or scripts on multiple EC2 instances simultaneously, making it ideal for applying patches to 1,000 instances quickly. Since the software is third-party (not OS-level), Patch Manager may not have the patches available in its repository. Run Command provides the flexibility to execute custom patch installation commands or scripts across all instances. You can target instances by tags, instance IDs, or all instances, and Run Command executes the commands in parallel, significantly reducing the time to patch all instances.\n\n**Why option 0 is incorrect:**\nThe option that says create an AWS Lambda function to apply the patch to all EC2 instances is incorrect because Lambda functions don't have direct access to EC2 instances to execute commands. Lambda would need to use Systems Manager Run Command or another service to actually apply patches, adding an unnecessary layer. Additionally, Lambda has execution time limits and concurrency limits that may not be suitable for patching 1,000 instances quickly. Run Command is the direct, efficient solution for executing commands on EC2 instances.\n\n**Why option 1 is incorrect:**\nThe option that says configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances is incorrect because Patch Manager is designed for operating system patches and patches from supported software repositories (like Windows Update, Amazon Linux, Ubuntu, etc.). Third-party software patches may not be available in Patch Manager's repositories, and Patch Manager may not support the specific third-party software. For custom or third-party software patches, Run Command provides the flexibility to execute custom installation commands.\n\n**Why option 2 is incorrect:**\nThe option that says schedule an AWS Systems Manager maintenance window to apply the patch is incorrect because maintenance windows are designed for scheduled maintenance activities, not for urgent security patches that need to be applied \"as quickly as possible.\" Maintenance windows run on a schedule and may have delays before execution. Additionally, maintenance windows typically use Patch Manager or Run Command as the execution method, so you would still need to choose the appropriate patching mechanism. For urgent patches, Run Command can be executed immediately without waiting for a maintenance window schedule.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A company is developing an application that provides order shipping statistics for retrieval by a \nREST API. The company wants to extract the shipping statistics, organize the data into an easy-\nto-read HTML format, and send the report to several email addresses at the same time every \nmorning. \n \nWhich combination of steps should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Configure the application to send the data to Amazon Kinesis Data Firehose.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1,
      3
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon Simple Email Service (Amazon SES) to format the data and send the report by email is correct. SES supports sending HTML-formatted emails, which meets the requirement to organize data into an easy-to-read HTML format. SES can send emails to multiple recipients simultaneously, meeting the requirement to send to several email addresses. SES provides a simple API for sending formatted emails programmatically.\n\n**Why option 3 is correct:**\nCreating an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data is correct. EventBridge supports scheduled rules (cron expressions or rate expressions) that can trigger Lambda functions at specific times, such as every morning. The Lambda function can query the REST API to extract shipping statistics, format the data into HTML, and then use SES to send the email. This combination provides the automation needed to run the process every morning.\n\n**Why option 0 is incorrect:**\nThe option that says configure the application to send data to Amazon Kinesis Data Firehose is incorrect because Kinesis Data Firehose is designed for streaming data to destinations like S3, Redshift, or Elasticsearch for analytics, not for scheduled email reporting. Firehose processes data streams continuously, not on a schedule. Additionally, Firehose doesn't provide email sending capabilities or HTML formatting - you would still need additional services to meet the email requirement.\n\n**Why option 2 is incorrect:**\nThe option that says create an EventBridge scheduled event that invokes an AWS Glue job to query the API is incorrect because AWS Glue is designed for ETL (extract, transform, load) jobs on large datasets stored in data stores like S3, databases, etc. Glue is not designed to query REST APIs or send emails. Glue jobs are better suited for batch processing of data already in storage, not for querying APIs and sending formatted emails. Lambda is more appropriate for this use case.\n\n**Why option 4 is incorrect:**\nThe option that says store application data in S3, create an SNS topic as an S3 event destination, and send the report by email is incorrect because S3 event notifications trigger when objects are created/modified, not on a schedule. The requirement is to send reports every morning, which requires scheduled execution, not event-driven execution. Additionally, SNS can send notifications but doesn't provide HTML email formatting capabilities like SES does. SNS is better for simple notifications, while SES provides full email capabilities including HTML formatting.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company wants to migrate its on-premises application to AWS. The application produces output \nfiles that vary in size from tens of gigabytes to hundreds of terabytes The application data must \nbe stored in a standard file system structure. The company wants a solution that scales \nautomatically, is highly available, and requires minimum operational overhead. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon EFS for storage.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon EFS for storage.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon S3 for storage.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nMigrating the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group and using Amazon EFS for storage is the correct solution. EFS provides a standard file system structure (NFS) that the application requires, and it scales automatically to petabytes without provisioning. EFS is highly available, automatically replicating data across multiple Availability Zones. EC2 instances in a Multi-AZ Auto Scaling group provide high availability for the application, and EFS can be mounted on multiple instances simultaneously. This solution requires minimal operational overhead - EFS is fully managed, and Auto Scaling handles instance management automatically.\n\n**Why option 0 is incorrect:**\nThe option that says migrate to ECS containers and use Amazon S3 for storage is incorrect because S3 is object storage, not a standard file system. Applications that require a file system structure cannot directly use S3 as a file system without significant modifications. S3 uses REST APIs and object keys, not file paths and directory structures. While S3 can store large files, it doesn't provide the POSIX-compliant file system interface that many applications require.\n\n**Why option 1 is incorrect:**\nThe option that says migrate to EKS containers and use Amazon EFS for storage is partially correct regarding EFS, but EKS adds unnecessary complexity for this use case. EKS requires managing Kubernetes clusters, nodes, and container orchestration, which increases operational overhead. If the application doesn't already use containers, migrating to EKS requires containerization efforts. EC2 with EFS provides a simpler migration path with less operational overhead while still meeting all requirements.\n\n**Why option 3 is incorrect:**\nThe option that says migrate to EC2 instances in Multi-AZ Auto Scaling group and use Amazon S3 for storage is incorrect because S3 is object storage, not a file system. The requirement explicitly states \"the application data must be stored in a standard file system structure,\" which S3 cannot provide. S3 uses object keys and REST APIs, not file paths and directory structures. Applications that require file system semantics need EFS or another file system solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 60,
    "text": "A company needs to store its accounting records in Amazon S3. The records must be \nimmediately accessible for 1 year and then must be archived for an additional 9 years. No one at \nthe company, including administrative users and root users, can be able to delete the records \nduring the entire 10-year period. The records must be stored with maximum resiliency. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Store the records in S3 Glacier for the entire 10-year period.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Store the records by using S3 Intelligent-Tiering.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nUsing an S3 Lifecycle policy to transition records from S3 Standard to S3 Glacier Deep Archive after 1 year, combined with S3 Object Lock in compliance mode, is the correct solution. S3 Standard provides immediate accessibility for the first year. After 1 year, the Lifecycle policy automatically transitions objects to S3 Glacier Deep Archive for cost-effective long-term archival (years 2-10). S3 Object Lock in compliance mode prevents anyone, including root users and administrators, from deleting or modifying objects during the retention period. Compliance mode provides the strongest protection - even root users cannot override the retention settings. S3 Glacier Deep Archive provides maximum resiliency with 99.999999999% (11 9's) durability and stores data across multiple Availability Zones.\n\n**Why option 0 is incorrect:**\nThe option that says store records in S3 Glacier for the entire 10-year period is incorrect because S3 Glacier has retrieval times (minutes to hours depending on retrieval tier), which doesn't meet the requirement for immediate accessibility during the first year. The requirement states records must be \"immediately accessible for 1 year,\" which requires S3 Standard or Intelligent-Tiering, not Glacier. Additionally, this approach doesn't address the requirement to prevent deletion - you would still need Object Lock.\n\n**Why option 1 is incorrect:**\nThe option that says store records using S3 Intelligent-Tiering is incorrect because Intelligent-Tiering doesn't prevent deletion by administrators or root users. While Intelligent-Tiering optimizes costs by moving objects between tiers based on access patterns, it doesn't provide the immutability protection required. To prevent deletion during the 10-year period, you need S3 Object Lock, which is not mentioned in this option. Intelligent-Tiering alone doesn't meet the requirement that \"no one at the company, including administrative users and root users, can be able to delete the records.\"\n\n**Why option 3 is incorrect:**\nThe option that says use an S3 Lifecycle policy to transition records from S3 Standard to S3 One Zone-IA after 1 year is incorrect because S3 One Zone-IA stores data in a single Availability Zone, which doesn't provide maximum resiliency. The requirement explicitly states records must be stored with \"maximum resiliency,\" which requires multi-AZ storage. One Zone-IA has lower durability (99.5%) compared to Standard (99.999999999%) and doesn't provide the same level of protection against AZ failures. Additionally, this option doesn't address the requirement to prevent deletion.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "A company runs multiple Windows workloads on AWS. The company's employees use Windows \nfile shares that are hosted on two Amazon EC2 instances. The file shares synchronize data \nbetween themselves and maintain duplicate copies. The company wants a highly available and \ndurable storage solution that preserves how users currently access the files. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate all the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon S3 File Gateway.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ",
        "correct": true
      },
      {
        "id": 3,
        "text": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nExtending the file share environment to Amazon FSx for Windows File Server with a Multi-AZ deployment is the correct solution. FSx for Windows File Server provides a fully managed Windows file server that supports the SMB protocol, preserving how users currently access files. Multi-AZ deployment provides high availability by automatically replicating data synchronously to a standby file server in another Availability Zone. If the primary file server fails, FSx automatically fails over to the standby with minimal downtime. FSx provides durable, highly available storage that integrates seamlessly with existing Windows environments and Active Directory, requiring no changes to how users access files.\n\n**Why option 0 is incorrect:**\nThe option that says migrate all data to Amazon S3 is incorrect because S3 is object storage, not a file system. Users currently access files through Windows file shares (SMB protocol), which requires a file system interface. S3 uses REST APIs and object keys, not SMB file shares. Migrating to S3 would require significant changes to how users access files, violating the requirement to preserve current access methods. Additionally, S3 doesn't provide the same file system semantics and permissions model as Windows file shares.\n\n**Why option 1 is incorrect:**\nThe option that says set up an Amazon S3 File Gateway is incorrect because S3 File Gateway provides a file interface to S3 but doesn't provide the same level of high availability and durability as FSx Multi-AZ. File Gateway runs on-premises or on EC2 instances, which still requires managing the gateway infrastructure. Additionally, File Gateway doesn't provide the same Windows file server features, Active Directory integration, and SMB protocol support that FSx provides. FSx is purpose-built for Windows file shares with native Windows features.\n\n**Why option 3 is incorrect:**\nThe option that says extend to Amazon Elastic File System (Amazon EFS) with Multi-AZ is incorrect because EFS uses the NFS protocol, not SMB. Windows file shares require the SMB protocol, which EFS doesn't support. Users accessing Windows file shares expect SMB protocol support, Active Directory integration, and Windows file permissions, which EFS (being Linux-based) doesn't provide. FSx for Windows File Server is specifically designed for Windows file shares with SMB protocol support.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A solutions architect is developing a multiple-subnet VPC architecture. The solution will consist of \nsix subnets in two Availability Zones. The subnets are defined as public, private and dedicated for \ndatabases. Only the Amazon EC2 instances running in the private subnets should be able to \naccess a database. \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a now route table that excludes the route to the public subnets' CIDR blocks.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a security group that denies ingress from the security group used by instances in the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a security group that allows ingress from the security group used by instances in the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a new peering connection between the public subnets and the private subnets.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCreating a security group that allows ingress from the security group used by instances in the private subnets is the correct solution. Security groups are stateful firewalls that control traffic at the instance level. By creating a security group for the database that only allows inbound traffic from the security group associated with private subnet instances, you ensure that only EC2 instances in private subnets can access the database. Security groups use a default-deny model - all inbound traffic is blocked by default, and you explicitly allow traffic from specific sources. This approach provides fine-grained access control and is the standard way to restrict database access in AWS.\n\n**Why option 0 is incorrect:**\nThe option that says create a new route table that excludes the route to the public subnets' CIDR blocks is incorrect because route tables control routing between subnets and networks, not access to specific resources like databases. Route tables determine where network traffic is directed, but they don't control which instances can access a database. Even if you modify routing, instances in public subnets could still access the database if security groups allow it. Security groups are the proper mechanism for controlling database access, not route tables.\n\n**Why option 1 is incorrect:**\nThe option that says create a security group that denies ingress from the security group used by instances in public subnets is incorrect because security groups don't support deny rules - they only support allow rules. Security groups use a whitelist model where you specify what is allowed, and everything else is denied by default. You cannot create explicit deny rules in security groups. To restrict access, you create allow rules only for the sources you want to permit (private subnet security groups), and deny is implicit for all other sources.\n\n**Why option 3 is incorrect:**\nThe option that says create a new peering connection between public subnets and private subnets is incorrect because VPC peering is used to connect different VPCs, not to control access within the same VPC. Since all subnets are in the same VPC, peering is not applicable. Additionally, peering connections enable connectivity but don't restrict access - you still need security groups to control which instances can access the database. Peering would actually enable more connectivity, which is the opposite of what's needed.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company has registered its domain name with Amazon Route 53. The company uses Amazon \nAPI Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. \nThird-party services consume the APIs securely. The company wants to design its API Gateway \nURL with the company's domain name and corresponding certificate so that the third-party \nservices can use HTTPS. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nCreating a Regional API Gateway endpoint, associating it with the company's domain name, importing the SSL/TLS certificate into ACM in the same Region (ca-central-1), attaching the certificate to the API Gateway endpoint, and configuring Route 53 to route traffic to the endpoint is the correct solution. Regional API Gateway endpoints require certificates to be in the same Region as the API. Since the API is in ca-central-1, the certificate must be imported into ACM in ca-central-1, not us-east-1. Route 53 can then create DNS records (A or AAAA) that point to the custom domain name, enabling third-party services to access the API using HTTPS with the company's domain name.\n\n**Why option 0 is incorrect:**\nThe option that says create stage variables in API Gateway to overwrite the default URL and import certificate into ACM is incorrect because stage variables are used to pass configuration data to backend integrations, not to configure custom domain names. Stage variables cannot change the API Gateway endpoint URL or domain name. To use a custom domain name, you must create a custom domain name configuration in API Gateway, not use stage variables.\n\n**Why option 1 is incorrect:**\nThe option that says create Route 53 DNS records pointing to the Regional API Gateway endpoint and import certificate into ACM in us-east-1 is incorrect because Regional API Gateway endpoints require certificates to be in the same Region as the API. Since the API is in ca-central-1, the certificate must be imported into ACM in ca-central-1, not us-east-1. Edge-optimized endpoints require certificates in us-east-1, but Regional endpoints require certificates in the same Region as the API.\n\n**Why option 3 is incorrect:**\nThe option that says create Regional API Gateway endpoint, import certificate into ACM in us-east-1, attach to APIs, and create Route 53 A record pointing to domain name is incorrect because Regional endpoints require certificates in the same Region as the API (ca-central-1), not us-east-1. Additionally, you attach certificates to the custom domain name configuration, not directly to APIs. The Route 53 configuration should point to the API Gateway custom domain name, not create a circular reference pointing to itself.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company is running a popular social media website. The website gives users the ability to \nupload images to share with other users. The company wants to make sure that the images do \nnot contain inappropriate content. The company needs a solution that minimizes development \neffort. What should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Comprehend to detect inappropriate content.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Rekognition to detect inappropriate content.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon SageMaker to detect inappropriate content.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nUsing Amazon Rekognition to detect inappropriate content is the correct solution that minimizes development effort. Amazon Rekognition Content Moderation is a fully managed service that uses machine learning to detect inappropriate, unwanted, or offensive content in images and videos. Rekognition can detect adult content, violent content, and other inappropriate material without requiring you to build custom machine learning models. The service provides a simple API that can be integrated into the image upload workflow - when users upload images, the application can call Rekognition's moderation API to check for inappropriate content before allowing the image to be shared. This requires minimal development effort compared to building custom detection algorithms.\n\n**Why option 0 is incorrect:**\nThe option that says use Amazon Comprehend to detect inappropriate content is incorrect because Amazon Comprehend is a natural language processing (NLP) service designed for analyzing text, not images. Comprehend can detect sentiment, entities, and key phrases in text, but it cannot analyze image content to detect inappropriate visual content. For image moderation, you need a computer vision service like Rekognition, not a text analysis service.\n\n**Why option 2 is incorrect:**\nThe option that says use Amazon SageMaker to detect inappropriate content is incorrect because SageMaker is a machine learning platform that requires you to build, train, and deploy custom models. This approach requires significant development effort - you would need to collect training data, train a model, deploy it, and manage the infrastructure. This violates the requirement to minimize development effort. Rekognition provides pre-trained models that work out of the box without any model development.\n\n**Why option 3 is incorrect:**\nThe option that says use AWS Fargate to deploy a custom machine learning model to detect inappropriate content is incorrect because this approach requires building a custom machine learning model, which involves significant development effort. You would need to develop the model, create container images, deploy to Fargate, and manage the infrastructure. This is much more complex than using Rekognition's pre-built content moderation capabilities. Fargate is for running containers, not for providing ML services - you would still need to build the ML model yourself.",
    "domain": "Design Resilient Architectures"
  }
]