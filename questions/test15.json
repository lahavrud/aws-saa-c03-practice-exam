[
  {
    "id": 0,
    "text": "A company recently created a disaster recovery site in a different AWS Region. The company \nneeds to transfer large amounts of data back and forth between NFS file systems in the two \nRegions on a periodic basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Snowball devices.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an SFTP server on Amazon EC2.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS DataSync is specifically designed for efficient and automated data transfer between on-premises storage and AWS storage services, as well as between different AWS Regions. It supports NFS file systems directly and provides features like incremental transfers, encryption, and scheduling, which significantly reduce operational overhead compared to other manual or custom solutions. It automates the data transfer process, handles network optimization, and provides monitoring capabilities, making it the ideal choice for periodic data synchronization between NFS file systems in different Regions.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Snowball devices are primarily intended for large, one-time data migrations to or from AWS. While they can handle large amounts of data, they are not suitable for periodic data transfers due to the logistical overhead involved in shipping the devices back and forth between Regions. This option would introduce significant delays and operational complexity.\n\n**Why option 2 is incorrect:**\nThis is incorrect because setting up an SFTP server on Amazon EC2 would require manual configuration, management, and maintenance of the server, including security patching, scaling, and monitoring. This approach introduces significant operational overhead compared to using a managed service like AWS DataSync. Furthermore, transferring large amounts of data over SFTP might not be as efficient as using DataSync's optimized transfer protocols.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS Database Migration Service (DMS) is designed for migrating databases, not file systems. It is not suitable for transferring data between NFS file systems. Using DMS for this purpose would be inappropriate and ineffective.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 1,
    "text": "A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same \nVPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure \nthat enough funds are available before a stock can be purchased. The company has noticed in \nthe VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web \nservice over the internet instead of through the VPC. A solutions architect must implement a \nsolution so that the APIs communicate through the VPC. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
    "options": [
      {
        "id": 0,
        "text": "Add an X-API-Key header in the HTTP header for authorization.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an interface endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a gateway endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating a private connection between the API Gateway and the CheckFunds API within the VPC. An interface endpoint provides a private entry point within the VPC for accessing services like API Gateway. By configuring the BuyStock API to use the interface endpoint's DNS name, the traffic will be routed directly to the CheckFunds API through the VPC network, avoiding the internet. This approach requires minimal changes to the existing code, primarily updating the endpoint URL used by the BuyStock API to point to the interface endpoint.\n\n**Why option 0 is incorrect:**\nAdding an X-API-Key header is primarily for authorization and does not directly address the routing of traffic within the VPC. While API keys enhance security, they don't force traffic to stay within the VPC. The traffic would still potentially route over the internet if the API endpoint is not configured to use a VPC-internal route.\n\n**Why option 2 is incorrect:**\nGateway endpoints are used for accessing services like S3 and DynamoDB from within a VPC without using an internet gateway or NAT gateway. They do not support API Gateway. Therefore, using a gateway endpoint is not a viable solution for enabling communication between two APIs within the same VPC.\n\n**Why option 3 is incorrect:**\nIntroducing an Amazon SQS queue would decouple the two APIs, but it would also require significant code changes to both APIs. The BuyStock API would need to enqueue messages, and the CheckFunds API would need to consume them. This introduces asynchronous communication and requires a complete redesign of the interaction between the two APIs, violating the requirement of minimal code changes.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company wants to run an in-memory database for a latency-sensitive application that runs on \nAmazon EC2 instances. The application processes more than 100,000 transactions each minute \nand requires high network throughput. A solutions architect needs to provide a cost-effective \nnetwork design that minimizes data transfer charges. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n195 \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-effective and performant solution. Launching all EC2 instances in the same Availability Zone eliminates inter-AZ data transfer costs, which can be significant with high transaction volumes. Specifying a placement group ensures that the instances are physically located close together within the AZ, further reducing latency and increasing network throughput. Placement groups are designed for applications that require low network latency, high network throughput, or both. This directly addresses the application's requirements for low latency and high throughput while minimizing costs.\n\n**Why option 1 is incorrect:**\nWhile launching instances in different Availability Zones provides higher availability, it introduces data transfer costs between the zones. This contradicts the requirement to minimize data transfer charges. Also, the added network latency between AZs could negatively impact the performance of the latency-sensitive application.\n\n**Why option 2 is incorrect:**\nDeploying an Auto Scaling group across multiple Availability Zones, while beneficial for high availability, introduces inter-AZ data transfer costs. The question explicitly asks for a solution that minimizes data transfer charges. Step scaling policies are useful for scaling based on demand but do not address the core requirement of minimizing data transfer costs.\n\n**Why option 3 is incorrect:**\nSimilar to option 2, deploying an Auto Scaling group with step scaling across multiple Availability Zones increases availability but also increases data transfer costs. The step scaling policy doesn't directly contribute to minimizing data transfer charges, which is a primary concern.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A company that primarily runs its application servers on premises has decided to migrate to AWS. \nThe company wants to minimize its need to scale its Internet Small Computer Systems Interface \n(iSCSI) storage on premises. The company wants only its recently accessed data to remain \nstored locally. \n \nWhich AWS solution should the company use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 File Gateway",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Storage Gateway Tape Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Storage Gateway Volume Gateway stored volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway Volume Gateway cached volumes",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by storing the entire dataset in AWS S3 while caching only the frequently accessed data on-premises. This minimizes the on-premises storage footprint and ensures that only the recently accessed data remains stored locally. The cached volumes gateway allows the company to leverage the scalability and cost-effectiveness of S3 while maintaining low-latency access to frequently used data on-premises.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Amazon S3 File Gateway provides file-based access to S3, not block-based iSCSI access. The company uses iSCSI storage on-premises, so a file gateway is not a suitable replacement.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because AWS Storage Gateway Tape Gateway is designed for backup and archival purposes, not for primary storage or minimizing on-premises storage footprint. It's used to replace physical tape libraries with virtual tape libraries in AWS.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company has multiple AWS accounts that use consolidated billing. The company runs several \nactive high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The \ncompany's finance team has access to AWS Trusted Advisor in the consolidated billing account \nand all other AWS accounts. \n \nThe finance team needs to use the appropriate AWS account to access the Trusted Advisor \ncheck recommendations for RDS. The finance team must review the appropriate Trusted Advisor \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n196 \ncheck to reduce RDS costs. \n \nWhich combination of steps should the finance team take to meet these requirements? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Use the Trusted Advisor recommendations from the account where the RDS instances are",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Review the Trusted Advisor check for Amazon RDS Idle DB Instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because in a consolidated billing setup, the management account (also known as the payer account or consolidated billing account) provides a centralized view of Trusted Advisor recommendations across all linked accounts. The finance team can access the consolidated billing account to see aggregated recommendations for all RDS instances across all accounts, which is essential for a comprehensive cost optimization strategy.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while the finance team *could* access Trusted Advisor in each individual account where the RDS instances reside, it would be inefficient and time-consuming to gather a complete picture of RDS costs across the entire organization. The consolidated billing account provides a centralized view, which is the more appropriate approach.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while Reserved Instances are a good way to save money on RDS, the question states that the instances were On-Demand and only ran for 90 days. Reserved Instances are best for long-term, consistent usage. The Idle DB Instances check is more relevant in this scenario.\n\n**Why option 3 is incorrect:**\nThis is correct because the Trusted Advisor check for Amazon RDS Idle DB Instances identifies RDS instances that are not being utilized effectively. Shutting down or resizing idle instances can lead to significant cost savings, directly addressing the requirement to reduce RDS costs. This check helps identify resources that are consuming resources without providing value.\n\n**Why option 4 is incorrect:**\nThis is incorrect because the question specifically mentions Amazon RDS for Oracle, not Amazon Redshift. The Amazon Redshift Reserved Node Optimization check is irrelevant to the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A solutions architect needs to optimize storage costs. The solutions architect must identify any \nAmazon S3 buckets that are no longer being accessed or are rarely accessed. \n \nWhich solution will accomplish this goal with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity",
        "correct": true
      },
      {
        "id": 1,
        "text": "Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because S3 Storage Lens provides an organization-wide view of object storage, with drill-down capabilities to understand usage trends and identify cost optimization opportunities. It automatically aggregates metrics and provides dashboards to analyze access patterns, identify infrequently accessed buckets, and recommend storage tiering strategies, minimizing operational overhead.\n\n**Why option 1 is incorrect:**\nThis is incorrect because the S3 dashboard in the AWS Management Console provides basic storage metrics but lacks the advanced analytics and aggregated views necessary to efficiently identify infrequently accessed buckets across an entire organization. It would require manual examination of each bucket, increasing operational overhead.\n\n**Why option 2 is incorrect:**\nThis is incorrect because CloudWatch BucketSizeBytes only provides information about the size of the bucket, not access patterns. Analyzing only the size will not help identify infrequently accessed buckets. It also requires enabling and managing CloudWatch metrics, adding operational overhead.\n\n**Why option 3 is incorrect:**\nThis is incorrect because enabling CloudTrail for S3 object monitoring generates a large volume of logs that require significant processing and analysis to determine access patterns. This approach has high operational overhead and is not the most efficient way to identify infrequently accessed buckets. It also requires additional services like Athena or EMR to analyze the logs.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company sells datasets to customers who do research in artificial intelligence and machine \nlearning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket \nin the us-east-1 Region. The company hosts a web application that the customers use to \npurchase access to a given dataset. The web application is deployed on multiple Amazon EC2 \ninstances behind an Application Load Balancer. After a purchase is made, customers receive an \nS3 signed URL that allows access to the files. \n \nThe customers are distributed across North America and Europe. The company wants to reduce \nthe cost that is associated with data transfers and wants to maintain or improve performance. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n197",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the web application to enable streaming of the datasets to end users. Configure the web",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by caching the datasets closer to the customers. CloudFront is a CDN that caches content at edge locations around the world. By using CloudFront, customers in North America and Europe will download the datasets from edge locations closer to them, reducing latency and data transfer costs from the us-east-1 Region. This improves performance and reduces costs, fulfilling the requirements.\n\n**Why option 0 is incorrect:**\nS3 Transfer Acceleration improves transfer speeds to S3 by using CloudFront edge locations for uploads. However, it primarily benefits uploads to S3, not downloads from S3, and it doesn't cache the data closer to the users like a full CloudFront distribution. It also doesn't necessarily reduce data transfer costs as effectively as CloudFront.\n\n**Why option 2 is incorrect:**\nSetting up a second S3 bucket in eu-central-1 with S3 Cross-Region Replication would duplicate the data and incur additional storage costs. While it would bring the data closer to European customers, it doesn't address the cost reduction requirement and adds complexity. Furthermore, it requires the application to be aware of which bucket to use based on the customer's location, adding more complexity.\n\n**Why option 3 is incorrect:**\nModifying the web application to stream the datasets might improve the user experience for some types of data, but it doesn't inherently reduce data transfer costs. It also adds significant complexity to the web application and might not be suitable for all types of datasets. The primary goal is to reduce data transfer costs and improve performance for downloads, which is better addressed by a CDN.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company is using AWS to design a web application that will process insurance quotes. Users \nwill request quotes from the application. Quotes must be separated by quote type, must be \nresponded to within 24 hours, and must not get lost. The solution must maximize operational \nefficiency and must minimize maintenance. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by using a single SNS topic to receive all quote requests. Subscribers to this topic can then filter messages based on the 'quote type' attribute using SNS message filtering. This allows for routing quotes to different processing components (e.g., Lambda functions, SQS queues) based on their type. SNS provides a highly available and durable messaging platform, ensuring no data loss. Using a single topic simplifies management and reduces operational overhead compared to creating multiple topics or streams. The 24-hour response time can be managed by the subscribers, which can be configured to trigger alerts or escalate if processing takes too long. This approach maximizes operational efficiency and minimizes maintenance by leveraging a managed service with built-in scalability and reliability.\n\n**Why option 0 is incorrect:**\nCreating multiple Kinesis data streams based on quote type is not the most efficient solution. While it allows for separation of quote types, it introduces complexity in managing multiple streams. Kinesis Data Streams are primarily designed for real-time data streaming and analytics, which is not the primary focus of this scenario. The question emphasizes the need for reliable delivery and separation by quote type, which can be more efficiently achieved with SNS filtering. Kinesis also requires more configuration and maintenance compared to SNS.\n\n**Why option 1 is incorrect:**\nCreating a Lambda function and an SNS topic does not provide a complete solution for routing quotes based on type. While the Lambda function could receive the quote and publish it to the SNS topic, it doesn't address the requirement of separating quotes by type for processing. Without filtering mechanisms, all subscribers to the SNS topic would receive all quotes, requiring them to implement their own filtering logic, which adds complexity and reduces operational efficiency. This solution also doesn't inherently guarantee the 24-hour response time requirement or prevent data loss as effectively as using SNS filtering and durable subscribers.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company has an application that runs on several Amazon EC2 instances. Each EC2 instance \nhas multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The \napplication's EC2 instance configuration and data need to be backed up nightly. The application \nalso needs to be recoverable in a different AWS Region. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by using AWS Backup, a fully managed service designed for centralized backup and restore management. AWS Backup simplifies the process of creating consistent, application-consistent backups of EC2 instances and their associated EBS volumes. The cross-region copy feature of AWS Backup enables replication of backups to another AWS Region, fulfilling the disaster recovery requirement. Using AWS Backup is more operationally efficient than writing custom Lambda functions because it handles scheduling, retention, and lifecycle management automatically.\n\n**Why option 0 is incorrect:**\nWhile Lambda can be used to schedule EBS snapshots, it requires writing and maintaining custom code for snapshot creation, retention, and cross-region replication. This approach is less operationally efficient than using AWS Backup, which provides a managed solution for these tasks. Additionally, this option does not inherently back up the EC2 instance configuration, requiring additional custom scripting.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it is identical to option 1 and therefore cannot be incorrect if option 1 is correct.\n\n**Why option 3 is incorrect:**\nThis option is incorrect for the same reasons as option 0. It involves writing and maintaining custom Lambda functions for snapshot management, which is less efficient than using the managed AWS Backup service. It also doesn't inherently back up the EC2 instance configuration.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company is building a mobile app on AWS. The company wants to expand its reach to millions \nof users. The company needs to build a platform so that authorized users can watch the \ncompany's content on their mobile devices. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up IPsec VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront. Provide signed URLs to stream content.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up AWS Client VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement of distributing content to a large audience by using Amazon CloudFront, a content delivery network (CDN). CloudFront caches content at edge locations globally, reducing latency and improving performance for users. Signed URLs provide a mechanism to control access to the content, ensuring that only authorized users can stream it. This meets the security requirement by restricting access based on authentication and authorization.\n\n**Why option 0 is incorrect:**\nPublishing content to a public S3 bucket makes the content accessible to anyone with the URL. While AWS KMS can encrypt the content at rest, it doesn't prevent unauthorized access if the bucket is public. This option fails to meet the security requirement of only allowing authorized users to access the content.\n\n**Why option 1 is incorrect:**\nSetting up an IPsec VPN between each mobile app and the AWS environment is not a scalable solution for millions of users. Managing and maintaining a large number of VPN connections would be complex and resource-intensive. VPNs are more suitable for connecting networks or a smaller number of users, not for large-scale content distribution to mobile devices. Additionally, streaming content over a VPN adds significant overhead and latency.\n\n**Why option 3 is incorrect:**\nSimilar to IPsec VPN, AWS Client VPN is not designed for streaming content to millions of mobile users. It's intended for secure remote access to AWS resources for a smaller number of users, not for large-scale content delivery. The overhead and complexity of managing VPN connections for millions of users would be impractical and inefficient.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company hosts a three-tier web application that includes a PostgreSQL database The database \nstores the metadata from documents The company searches the metadata for key terms to \nretrieve documents that the company reviews in a report each month The documents are stored \nin Amazon S3 The documents are usually written only once, but they are updated frequency The \nreporting process takes a few hours with the use of relational queries The reporting process must \nnot affect any document modifications or the addition of new documents. \nWhat are the MOST operationally efficient solutions that meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a new Amazon RDS for PostgreSQL Reserved Instance and an On-Demand read replica",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a new Amazon Aurora PostgreSQL DB cluster that includes a Reserved Instance and an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a new Amazon RDS for PostgreSQL Multi-AZ Reserved Instance Configure the reporting",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up a new Amazon DynamoDB table to store the documents Use a fixed write capacity to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement for non-impactful reporting by creating a read replica. A read replica allows the reporting queries to be executed against a copy of the data, isolating the main database from the reporting workload. Using a Reserved Instance for the primary database and an On-Demand read replica balances cost and performance. The Reserved Instance provides cost savings for the primary database, which is likely to be consistently used, while the On-Demand read replica provides flexibility and cost-effectiveness for the reporting process, which is only needed monthly. This approach is operationally efficient because it leverages existing PostgreSQL knowledge and infrastructure, minimizing the learning curve and management overhead.\n\n**Why option 0 is incorrect:**\nSwitching to DocumentDB would require significant data migration and application code changes, which is not operationally efficient. The question states the existing database is PostgreSQL, and the focus is on reporting without impacting the existing system. Introducing a new database technology adds complexity and cost.\n\n**Why option 2 is incorrect:**\nWhile Aurora PostgreSQL is a good database option, setting up a new Aurora cluster solely for reporting is overkill and less operationally efficient than using a read replica of the existing RDS PostgreSQL database. It introduces unnecessary complexity and cost. Also, the question mentions the existing database is PostgreSQL, so migrating to Aurora would be an unnecessary change.\n\n**Why option 3 is incorrect:**\nA Multi-AZ setup primarily provides high availability and disaster recovery. While important, it doesn't directly address the requirement of isolating the reporting workload to prevent impact on the primary database. Configuring the reporting tool to use the standby instance is not a supported or recommended practice. The standby instance is for failover purposes and should not be used for read operations.\n\n**Why option 4 is incorrect:**\nDynamoDB is a NoSQL database and is not suitable for complex relational queries required for the monthly report. The existing data is in a relational database (PostgreSQL), and the reporting process uses relational queries. Migrating to DynamoDB would require significant data model changes and application code modifications, making it operationally inefficient. Also, DynamoDB is not designed for storing documents directly; it's better suited for storing metadata or key-value pairs.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company experienced a breach that affected several applications in its on-premises data \ncenter. The attacker took advantage of vulnerabilities in the custom applications that were \nrunning on the servers. The company is now migrating its applications to run on Amazon EC2 \ninstances. The company wants to implement a solution that actively scans for vulnerabilities on \nthe EC2 instances and sends a report that details the findings. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution directly addresses the requirements. Amazon Inspector is specifically designed for automated security assessments and vulnerability management of EC2 instances and container images. Deploying the Inspector agent allows it to perform deep scans of the operating system and applications running on the EC2 instances. Configuring Inspector will generate detailed findings reports that identify vulnerabilities, security misconfigurations, and deviations from best practices.\n\n**Why option 0 is incorrect:**\nAWS Shield is designed to protect against DDoS attacks, not to scan EC2 instances for vulnerabilities in applications or operating systems. It focuses on protecting the infrastructure from external threats, not internal application security.\n\n**Why option 1 is incorrect:**\nAmazon Macie is a data security and data privacy service that uses machine learning and pattern matching to discover and protect sensitive data in AWS. It's primarily used for identifying sensitive information stored in S3 buckets, not for scanning EC2 instances for application vulnerabilities. While Lambda functions can be used for various tasks, they are not a substitute for a dedicated vulnerability scanning service like Inspector. This combination does not provide a comprehensive vulnerability scanning solution for EC2 instances.\n\n**Why option 2 is incorrect:**\nAmazon GuardDuty is a threat detection service that monitors your AWS accounts and workloads for malicious activity and unauthorized behavior. While it can detect some vulnerabilities and suspicious activities, it is not designed for detailed vulnerability scanning of EC2 instances at the application level. It focuses on identifying broader security threats and anomalies, not specific software vulnerabilities. Deploying GuardDuty agents is not a standard practice, as GuardDuty primarily uses AWS CloudTrail, VPC Flow Logs, and DNS logs for analysis.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A company uses an Amazon EC2 instance to run a script to poll for and process messages in an \nAmazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational \ncosts while maintaining its ability to process a growing number of messages that are added to the \nqueue. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the size of the EC2 instance to process messages faster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Run Command to run the script on demand.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by leveraging AWS Lambda's serverless architecture. Lambda functions can be triggered directly by SQS messages, eliminating the need for continuous polling by an EC2 instance. This approach significantly reduces operational costs because you only pay for the compute time used to process messages. Lambda also scales automatically with the number of messages in the queue, ensuring that the growing message volume can be handled efficiently.\n\n**Why option 0 is incorrect:**\nIncreasing the EC2 instance size might process messages faster, but it doesn't address the fundamental problem of inefficient resource utilization. The EC2 instance will still be running even when there are no messages to process, leading to unnecessary costs. This approach does not provide cost optimization, especially when the message volume fluctuates.\n\n**Why option 1 is incorrect:**\nWhile turning off the EC2 instance when underutilized might save some costs, it introduces complexity and potential delays. Amazon EventBridge would need to monitor the EC2 instance's utilization and trigger the shutdown and startup processes. This adds operational overhead and might not be as responsive as a serverless solution like Lambda. Furthermore, the startup time of the EC2 instance could lead to message processing delays. This solution is less efficient than using a serverless approach.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company uses a legacy application to produce data in CSV format. The legacy application \nstores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf \n(COTS) application that can perform complex SQL queries to analyze data that is stored in \nAmazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv \nfiles that the legacy application produces. \n \nThe company cannot update the legacy application to produce data in another format. The \ncompany needs to implement a solution so that the COTS application can use the data that the \nlegacy application produces. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure",
        "correct": true
      },
      {
        "id": 1,
        "text": "Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by using AWS Glue, a fully managed ETL service, to transform the CSV data into a format suitable for the COTS application. Glue can automatically discover the schema of the CSV files, transform the data (e.g., convert data types, flatten nested structures), and load the transformed data into a format like Parquet or ORC in S3 or directly into Redshift. Scheduling the Glue job ensures that the transformation happens automatically on a regular basis. Using Glue minimizes operational overhead because it's a managed service, eliminating the need to manage servers or infrastructure.\n\n**Why option 1 is incorrect:**\nDeveloping a Python script on EC2 instances introduces significant operational overhead. It requires managing EC2 instances, writing and maintaining the Python script, handling scaling, and ensuring the script runs reliably. This approach is more complex and less scalable than using a managed ETL service like AWS Glue.\n\n**Why option 2 is incorrect:**\nUsing Lambda and DynamoDB is not suitable for transforming large CSV files. Lambda functions have execution time limits and memory constraints, making them unsuitable for processing potentially large datasets. DynamoDB is a NoSQL database and not designed for storing or querying the transformed CSV data in a way that the COTS application can use. This option also adds unnecessary complexity.\n\n**Why option 3 is incorrect:**\nLaunching an EMR cluster on a weekly schedule is overkill for this scenario. EMR is designed for large-scale data processing and analytics using frameworks like Hadoop and Spark. Using EMR for a simple CSV transformation introduces significant operational overhead and cost compared to using AWS Glue. EMR also requires more configuration and management.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company recently migrated its entire IT environment to the AWS Cloud. The company \ndiscovers that users are provisioning oversized Amazon EC2 instances and modifying security \ngroup rules without using the appropriate change control process. A solutions architect must \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n201 \ndevise a strategy to track and audit these inventory and configuration changes. \n \nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS CloudTrail and use it for auditing.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use data lifecycle policies for the Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable AWS Trusted Advisor and reference the security dashboard.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Config and create rules for auditing and compliance purposes.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Restore previous resource configurations with an AWS CloudFormation template.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS CloudTrail logs API calls made to AWS services. By enabling CloudTrail, the company can track who provisioned the oversized EC2 instances and who modified the security group rules. The logs provide a detailed audit trail of actions taken within the AWS environment, which is essential for compliance and security monitoring.\n\n**Why option 1 is incorrect:**\nThis is incorrect because data lifecycle policies are primarily used for managing the lifecycle of data stored in services like S3 and EBS. They are not directly related to tracking or auditing EC2 instance sizes or security group changes.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while AWS Trusted Advisor provides recommendations on cost optimization, security, fault tolerance, and performance, it doesn't provide a detailed audit trail of configuration changes or user actions. It offers high-level guidance but not the granular tracking required in this scenario.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while AWS Config is useful for auditing and compliance, it was not selected as a correct answer. AWS Config is a good option for this scenario, but the question only allows for two correct answers, and CloudTrail is a more fundamental auditing tool.\n\n**Why option 4 is incorrect:**\nThis is incorrect because restoring previous resource configurations with CloudFormation might revert unwanted changes, but it doesn't address the underlying problem of unauthorized modifications or provide a mechanism for tracking future changes. It's a reactive measure, not a proactive auditing solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems \nadministrators have used shared SSH keys to manage the instances. After a recent audit, the \ncompany's security team is mandating the removal of all shared keys. A solutions architect must \ndesign a solution that provides secure access to the EC2 instances. \n \nWhich solution will meet this requirement with the LEAST amount of administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager Session Manager to connect to the EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Systems Manager Session Manager provides secure access to EC2 instances without the need for SSH keys. It uses IAM roles to control access, eliminating the need to manage and distribute SSH keys. Session Manager also provides auditing capabilities and centralizes access management, reducing administrative overhead compared to other solutions. It also allows connection to instances even if they are in private subnets without opening SSH ports to the internet.\n\n**Why option 1 is incorrect:**\nThis is incorrect because generating one-time SSH keys using AWS STS would be complex to implement and manage at scale. It would require significant custom scripting and integration, increasing administrative overhead. Furthermore, it still involves the concept of keys, albeit temporary ones, which the security team is trying to eliminate.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while bastion hosts can improve security by limiting SSH access points, they still require managing SSH keys for the bastion hosts themselves. This doesn't eliminate the problem of shared keys, it just concentrates it. Configuring all other instances to only allow SSH from the bastion hosts adds complexity and administrative overhead. It also introduces a single point of failure.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon Cognito is primarily used for authenticating users for web and mobile applications, not for SSH access to EC2 instances. Using a custom authorizer and Lambda function for SSH authentication would be overly complex and introduce significant administrative overhead. It is not the intended use case for Cognito and Lambda.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A company is building a data analysis platform on AWS by using AWS Lake Formation. The \nplatform will ingest data from different sources such as Amazon S3 and Amazon RDS. The \ncompany needs a secure solution to prevent access to portions of the data that contain sensitive \ninformation.",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that includes permissions to access Lake Formation tables.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create data filters to implement row-level security and cell-level security.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function that removes sensitive information before Lake Formation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that perodically Queries and removes sensitive information from",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Lake Formation uses IAM roles to grant permissions to access the data lake resources, including tables. By creating an IAM role with specific permissions to access Lake Formation tables, you can control which users or services can access the data. This is the foundational step for securing data in Lake Formation.\n\n**Why option 1 is incorrect:**\nWhile data filters in Lake Formation can implement row-level and cell-level security, they are not the initial step. Data filters are applied on top of existing IAM role-based access control. Creating an IAM role is the fundamental requirement for granting access to Lake Formation resources before applying more granular controls like data filters.\n\n**Why option 2 is incorrect:**\nUsing a Lambda function to remove sensitive information before Lake Formation is an option, but it's not the ideal solution for secure access control. It modifies the original data, which might not be desirable and can introduce complexities in data governance and auditing. It's better to control access to the sensitive data rather than removing it altogether. Also, this approach doesn't scale well and can become a bottleneck.\n\n**Why option 3 is incorrect:**\nSimilar to Option 2, using a Lambda function to periodically query and remove sensitive information is not the best approach for secure access control. It modifies the original data, which might not be desirable and can introduce complexities in data governance and auditing. It's better to control access to the sensitive data rather than removing it altogether. This approach also introduces operational overhead and potential data inconsistencies.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n202 \nWhat should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket \nare encrypted?",
    "options": [
      {
        "id": 0,
        "text": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct approach because the `x-amz-server-side-encryption` header in a `PutObject` request specifies the type of server-side encryption to be used. By denying any `PutObject` requests that do not include this header, the bucket policy enforces that all objects uploaded to the bucket must be encrypted using server-side encryption. This ensures data at rest is protected.\n\n**Why option 0 is incorrect:**\nThis is incorrect because the `s3:x-amz-acl` header controls access permissions to the object, not encryption. While ACLs are important for security, they don't enforce encryption at rest. Denying uploads based on the absence of an ACL header would not guarantee that objects are encrypted.\n\n**Why option 1 is incorrect:**\nThis is incorrect because, similar to option 0, the `s3:x-amz-acl` header controls access permissions, not encryption. Specifying a particular ACL value doesn't enforce encryption. The focus should be on the encryption header.\n\n**Why option 2 is incorrect:**\nThis is incorrect because `aws:SecureTransport` is not a standard S3 header for enforcing encryption. While using HTTPS (which provides secure transport) is a good practice, it doesn't guarantee that the objects are encrypted at rest within S3. The header related to encryption at rest is `x-amz-server-side-encryption`.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A solutions architect is designing a multi-tier application for a company. The application's users \nupload images from a mobile device. The application generates a thumbnail of each image and \nreturns a message to the user to confirm that the image was uploaded successfully. \n \nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster \nresponse time to its users to notify them that the original image was received. The solutions \narchitect must design the application to asynchronously dispatch requests to the different \napplication tiers. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by using Amazon SQS to decouple the image upload process from the thumbnail generation process. When an image is uploaded, a message is placed in the SQS queue. A separate process (e.g., a Lambda function or an EC2 instance) can then consume messages from the queue and generate the thumbnail. This allows the application to immediately respond to the user after the image is uploaded, without waiting for the thumbnail to be generated. SQS provides a reliable and scalable way to handle asynchronous tasks.\n\n**Why option 0 is incorrect:**\nWhile a Lambda function can generate the thumbnail, it doesn't inherently provide asynchronous processing or decouple the upload process from the thumbnail generation. Directly invoking a Lambda function to generate the thumbnail would still require the user to wait for the thumbnail generation to complete before receiving a response. It doesn't address the need for immediate feedback to the user.\n\n**Why option 1 is incorrect:**\nAWS Step Functions is a good choice for orchestrating complex workflows, but it's not the most efficient or cost-effective solution for a simple asynchronous task like thumbnail generation. Step Functions is better suited for scenarios involving multiple steps, branching logic, and error handling. For this specific use case, SQS provides a simpler and more direct way to decouple the upload and thumbnail generation processes. Also, Step Functions would still need to be triggered by something, and SQS provides the triggering mechanism.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 19,
    "text": "A solution architect needs to assign a new microsoft for a company's application. Clients must be \nable to call an HTTPS endpoint to reach the micoservice. The microservice also must use AWS \nidentity and Access Management (IAM) to authentication calls. The soltions architect will write the \nlogic for this microservice by using a single AWS Lambda function that is written in Go 1.x. \nWhich solution will deploy the function in the in the MOST operationally efficient way? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n203",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribuion. Deploy the function to CloudFront Functions. Specify",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution directly addresses all requirements. Amazon API Gateway provides an HTTPS endpoint for clients to call the microservice. By configuring the API Gateway method to integrate with the Lambda function and enabling IAM authentication on the API Gateway method, the microservice can authenticate calls using IAM roles and policies. API Gateway handles the HTTPS termination, request routing, and authentication, making it an operationally efficient solution for managing the microservice endpoint.\n\n**Why option 1 is incorrect:**\nWhile Lambda function URLs with AWS_IAM authentication can provide an HTTPS endpoint and IAM authentication, they lack the features and operational maturity of API Gateway. API Gateway offers features like request validation, throttling, caching, and transformation, which are essential for managing a microservice endpoint in a production environment. Lambda function URLs are more suitable for simpler use cases or testing.\n\n**Why option 2 is incorrect:**\nCloudFront is a CDN primarily used for caching and distributing content. While it can invoke Lambda@Edge functions, using it solely for routing requests to a Lambda function for a microservice is not the most operationally efficient solution. API Gateway is specifically designed for this purpose and provides better management and control over the API endpoint. Additionally, Lambda@Edge functions have limitations on execution time and memory, which might not be suitable for all microservice workloads. Integrating IAM directly with Lambda@Edge for authentication is complex and less streamlined than using API Gateway's built-in IAM integration.\n\n**Why option 3 is incorrect:**\nCloudFront Functions are designed for lightweight transformations and manipulations of HTTP requests and responses at the edge. They are not suitable for implementing the core logic of a microservice. CloudFront Functions also do not support IAM authentication directly. This option does not meet the requirements of the question.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A company wants to implement a disaster recovery plan for its primary on-premises file storage \nvolume. The file storage volume is mounted from an Internet Small Computer Systems Interface \n(iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes \n(TB) of data. \n \nThe company wants to ensure that end users retain immediate access to all file types from the \non-premises systems without experiencing latency. \n \nWhich solution will meet these requirements with the LEAST amount of change to the company's \nexisting infrastructure?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by creating a full copy of the on-premises data in AWS. The Volume Gateway stored volume replicates the entire dataset to AWS, providing a complete disaster recovery copy. In a disaster scenario, the stored volume in AWS can be accessed directly, providing immediate access to all file types. Because the entire volume is stored in AWS, there's no dependency on the on-premises cache, ensuring low latency access during a DR event. Using a Volume Gateway aligns with the existing iSCSI-based infrastructure, minimizing changes to the company's setup. While initial replication will take time, it provides the best balance of immediate access and minimal changes for a large dataset.\n\n**Why option 0 is incorrect:**\nAn S3 File Gateway is designed for object storage and not block storage. While it can provide access to files stored in S3, it requires migrating the data to S3 first, which is a significant change to the existing infrastructure. It also introduces latency as the data needs to be retrieved from S3. The question specifically asks for immediate access without latency, which S3 File Gateway cannot guarantee for a large dataset like hundreds of TB.\n\n**Why option 1 is incorrect:**\nA Tape Gateway is designed for long-term archival and backup, not for immediate disaster recovery. Restoring data from tapes is a slow process and does not meet the requirement of immediate access to all file types without experiencing latency. It also requires a separate data backup solution, adding complexity to the existing infrastructure. The primary use case for Tape Gateway is archival, not DR with immediate access.\n\n**Why option 2 is incorrect:**\nA Volume Gateway cached volume stores the primary data in AWS S3 and caches frequently accessed data on-premises. In a disaster recovery scenario, accessing data not present in the local cache would incur latency as it needs to be retrieved from S3. Setting the local cache to only 10% of the volume size means that 90% of the data would need to be retrieved from S3 during a DR event, violating the 'without experiencing latency' requirement. While it minimizes on-premises storage, it compromises the immediate access requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A company is hosting a web application from an Amazon S3 bucket. The application uses \nAmazon Cognito as an identity provider to authenticate users and return a JSON Web Token \n(JWT) that provides access to protected resources that are stored in another S3 bucket. \n \nUpon deployment of the application, users report errors and are unable to access the protected \ncontent. A solutions architect must resolve this issue by providing proper permissions so that \nusers can access the protected content. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n204 \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the Amazon Cognito identity pool to assume the proper IAM role for access to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Update the S3 ACL to allow the application to access the protected content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Cognito identity pools allow you to define IAM roles that users assume after they authenticate. By updating the identity pool to assume an IAM role with the necessary permissions to access the protected S3 bucket, you grant authenticated users the required access. This is the standard and secure way to grant access to AWS resources based on Cognito authentication.\n\n**Why option 1 is incorrect:**\nThis is incorrect because S3 ACLs are generally not the preferred method for managing permissions, especially for complex scenarios involving authentication providers like Cognito. ACLs are more suitable for simple access control scenarios. Using IAM roles associated with Cognito identities provides more granular control and better security practices.\n\n**Why option 2 is incorrect:**\nThis is incorrect because eventual consistency in S3 is not the issue here. The problem is related to permissions, not data availability. Redeploying the application will not solve the permission problem.\n\n**Why option 3 is incorrect:**\nThis is incorrect because custom attribute mappings are used to map attributes from the identity provider (e.g., social sign-in providers) to Cognito user pool attributes. While attribute mappings can be useful for other purposes, they do not directly grant access to S3 resources. The core issue is the lack of an IAM role with the correct permissions associated with the Cognito identity pool.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "An image hosting company uploads its large assets to Amazon S3 Standard buckets. The \ncompany uses multipart upload in parallel by using S3 APIs and overwrites if the same object is \nuploaded again. For the first 30 days after upload, the objects will be accessed frequently. The \nobjects will be used less frequently after 30 days, but the access patterns for each object will be \ninconsistent. The company must optimize its S3 storage costs while maintaining high availability \nand resiliency of stored assets. \n \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Move assets to S3 Intelligent-Tiering after 30 days.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an S3 Lifecycle policy to clean up expired object delete markers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because S3 Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on changing access patterns. This addresses the requirement of optimizing storage costs for objects that are accessed less frequently after 30 days with inconsistent access patterns, without requiring manual intervention or lifecycle policies based on fixed timeframes. It maintains high availability and resiliency by storing data across multiple Availability Zones.\n\n**Why option 1 is incorrect:**\nWhile cleaning up incomplete multipart uploads is a good practice for cost optimization and preventing storage waste, it doesn't directly address the primary requirement of optimizing storage costs based on access patterns after the initial 30 days. The question focuses on the infrequent access after the initial period, not the incomplete uploads themselves.\n\n**Why option 2 is incorrect:**\nCleaning up expired object delete markers is a good practice for cost optimization and preventing unexpected behavior, but it doesn't directly address the primary requirement of optimizing storage costs based on access patterns after the initial 30 days. The question focuses on the infrequent access after the initial period, not the delete markers.\n\n**Why option 3 is incorrect:**\nWhile S3 Standard-IA is cheaper than S3 Standard for infrequently accessed data, it requires a minimum storage duration charge of 30 days and a retrieval fee. Since the access patterns are inconsistent after 30 days, some objects might still be accessed frequently, making S3 Intelligent-Tiering a better choice as it automatically moves objects back to the frequent access tier when needed. Also, a lifecycle policy would be needed to move the objects, adding complexity.\n\n**Why option 4 is incorrect:**\nS3 One Zone-IA is the cheapest option for infrequently accessed data, but it stores data in a single Availability Zone, which compromises the high availability and resiliency requirements. The question explicitly states the need to maintain high availability and resiliency, making this option unsuitable.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 \ninstances contain highly sensitive data and run in a private subnet. According to company policy, \nthe EC2 instances that run in the VPC can access only approved third-party software repositories \non the internet for software product updates that use the third party's URL. Other internet traffic \nmust be blocked. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the route table for the private subnet to route the outbound traffic to an AWS Network",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement strict inbound security group rules. Configure an outbound rule that allows traffic only",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Network Firewall allows for deep packet inspection and URL filtering. By updating the route table for the private subnet to route outbound traffic to the Network Firewall, the firewall can be configured with rules to allow traffic only to the approved third-party software repositories based on their URLs. All other outbound traffic will be blocked, fulfilling the requirement of securing the VPC network and restricting access to only approved destinations.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS WAF is designed to protect web applications from common web exploits and attacks. It operates at Layer 7 (Application Layer) and is primarily used to filter inbound HTTP/HTTPS traffic. It is not suitable for controlling outbound traffic from EC2 instances to specific third-party software repositories based on URLs. WAF is typically associated with Application Load Balancers or API Gateways, not general outbound traffic filtering.\n\n**Why option 2 is incorrect:**\nThis is incorrect because security groups operate at the instance level and control traffic based on IP addresses, protocols, and port numbers. While inbound security group rules can restrict access to the EC2 instances, outbound rules cannot filter traffic based on URLs. Security groups are stateful, meaning that if traffic is allowed outbound, the corresponding inbound traffic is automatically allowed. Therefore, security groups alone cannot meet the requirement of allowing access only to specific third-party software repositories based on URLs.\n\n**Why option 3 is incorrect:**\nThis is incorrect because an Application Load Balancer (ALB) is designed to distribute incoming application traffic across multiple targets, such as EC2 instances. It is not designed for controlling outbound traffic from EC2 instances to the internet. While an ALB can be used to route traffic to the EC2 instances, it cannot be used to filter outbound traffic based on URLs. Using an ALB for outbound traffic would also introduce unnecessary complexity and cost.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the scalability requirements effectively. CloudFront caching the static content reduces the load on the S3 bucket and improves website performance. Adding SQS decouples the API from the backend workers, allowing the API to quickly accept sales requests and enqueue them for asynchronous processing. This prevents the API from being overwhelmed during peak events, ensuring that all requests are eventually processed. SQS also provides buffering and retry mechanisms, ensuring that no sales requests are lost even if the backend workers experience temporary issues.\n\n**Why option 0 is incorrect:**\nWhile CloudFront can cache dynamic content, it's generally more effective for static content. Increasing the number of EC2 instances might help, but it doesn't address the potential for the API to be overwhelmed by a sudden surge in requests. It also doesn't decouple the API from the backend workers, which can lead to performance bottlenecks.\n\n**Why option 1 is incorrect:**\nWhile caching static content with CloudFront is beneficial, placing the EC2 instances in an Auto Scaling group only addresses the scaling of the API layer. It doesn't address the potential for the API to be overwhelmed by a sudden surge in requests or decouple the API from the backend workers. The backend processing could still become a bottleneck.\n\n**Why option 2 is incorrect:**\nWhile CloudFront can cache dynamic content, it's generally more effective for static content. Adding ElastiCache might improve the performance of the API by caching frequently accessed data, but it doesn't address the fundamental issue of decoupling the API from the backend workers. The API could still be overwhelmed by a sudden surge in requests.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions \narchitect needs to provide a solution that will run regular security scans across a large fleet of \nEC2 instances. The solution should also patch the EC2 instances on a regular schedule and \nprovide a report of each instance's patch status. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements because Amazon Inspector is a vulnerability management service that automatically assesses EC2 instances for software vulnerabilities and unintended network exposure. It can be configured to run regular scans. AWS Systems Manager Patch Manager, integrated with Inspector findings, can be used to automate the patching of EC2 instances. Systems Manager also provides reporting on patch compliance, fulfilling the reporting requirement.\n\n**Why option 0 is incorrect:**\nAmazon Macie is a data security and data privacy service that uses machine learning and pattern matching to discover and protect sensitive data in AWS. It's primarily focused on S3 buckets and does not directly address EC2 instance patching or vulnerability scanning in the way required by the question. While Macie can integrate with other services, it's not the primary tool for the stated requirements.\n\n**Why option 1 is incorrect:**\nAmazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and unauthorized behavior. It identifies suspicious behavior such as unusual API calls or potentially unauthorized deployments. While GuardDuty is important for security, it does not provide vulnerability scanning or patching capabilities for EC2 instances. It focuses on detecting threats, not remediating vulnerabilities.\n\n**Why option 2 is incorrect:**\nAmazon Detective analyzes, investigates, and quickly identifies the root cause of suspicious activities or security findings. It gathers log data from various AWS services. While Detective is useful for security investigations, it does not perform vulnerability scanning or patching of EC2 instances. It's a forensic tool, not a vulnerability management or patching solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company is planning to store data on Amazon RDS DB instances. The company must encrypt \nthe data at rest. \n \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct solution because AWS KMS is the recommended service for managing encryption keys within AWS. Enabling encryption for the DB instance and using a KMS key ensures that the data at rest is encrypted using a key managed by AWS KMS. RDS integrates directly with KMS for encryption at rest.\n\n**Why option 1 is incorrect:**\nStoring encryption keys in AWS Secrets Manager is generally used for storing secrets like passwords, API keys, and other sensitive information. While Secrets Manager can store encryption keys, it's not the primary service for managing encryption keys for RDS encryption at rest. RDS directly integrates with KMS for this purpose, making KMS the more appropriate and secure choice. Also, the question asks about encrypting the DB, not just storing a key.\n\n**Why option 2 is incorrect:**\nAWS Certificate Manager (ACM) is used for managing SSL/TLS certificates for securing network traffic (data in transit), not for encrypting data at rest. Enabling SSL/TLS on the DB instances encrypts the connection between the client and the database, but it does not encrypt the data stored on the disk.\n\n**Why option 3 is incorrect:**\nIAM is used for managing access control and permissions within AWS. It does not provide functionality for generating certificates or enabling encryption for RDS DB instances. SSL/TLS certificates are managed by ACM, and data at rest encryption is managed by KMS.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The \ncompany's network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowball.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS DataSync.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a secure VPN connection.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Transfer Acceleration.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Snowball is a physical data transport solution designed for transferring large amounts of data into and out of AWS. Given the limited network bandwidth and the large data volume, transferring the data over the internet within the 30-day timeframe is unlikely. Snowball allows the company to ship the data to AWS, bypassing the network limitations.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS DataSync is an online data transfer service. While it can automate and accelerate data transfers, it still relies on network bandwidth. With only 15 Mbps available (and a 70% utilization limit), transferring 20 TB of data within 30 days would be extremely challenging and likely impossible. DataSync is more suitable for ongoing synchronization or smaller data volumes where network bandwidth is sufficient.\n\n**Why option 2 is incorrect:**\nThis is incorrect because a secure VPN connection, while providing secure data transfer, still relies on the available network bandwidth. The limited bandwidth of 15 Mbps (with a 70% utilization limit) would make transferring 20 TB of data within 30 days impractical. A VPN adds overhead, further reducing the effective bandwidth.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon S3 Transfer Acceleration uses Amazon CloudFront's globally distributed edge locations to accelerate uploads to Amazon S3. While it can improve transfer speeds, it still relies on the available network bandwidth. The limited bandwidth of 15 Mbps (with a 70% utilization limit) would make transferring 20 TB of data within 30 days impractical. Transfer Acceleration is more beneficial when the bottleneck is network latency, not bandwidth.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A company needs to provide its employees with secure access to confidential and sensitive files. \nThe company wants to ensure that the files can be accessed only by authorized users. The files \nmust be downloaded securely to the employees' devices. \nThe files are stored in an on-premises Windows file server. However, due to an increase in \nremote usage, the file server is running out of capacity. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by providing a fully managed, scalable, and secure Windows file server in AWS. Amazon FSx for Windows File Server integrates seamlessly with Active Directory, allowing for existing user authentication and authorization mechanisms to be used. This ensures that only authorized users can access the files. The files can be downloaded securely to employees' devices using standard Windows file sharing protocols over a secure connection (e.g., VPN or Direct Connect). The managed nature of FSx eliminates the need for managing the underlying infrastructure, addressing the capacity limitations of the on-premises server.\n\n**Why option 0 is incorrect:**\nPlacing the file server in a public subnet exposes it to the internet, which is a security risk. While security groups can provide some protection, it's not the best practice for sensitive data. Also, simply migrating to EC2 doesn't inherently solve the capacity issue without proper scaling considerations. The question emphasizes secure access and authorized users, which this option doesn't address as effectively as FSx.\n\n**Why option 2 is incorrect:**\nWhile S3 offers scalability and security, it's primarily an object storage service, not a file server. Creating a private VPC endpoint enhances security by keeping traffic within the AWS network. However, using signed URLs for file access is not ideal for a large number of users and frequent access, as it requires managing and distributing these URLs. It also doesn't provide the same level of integration with Windows file sharing protocols and Active Directory as FSx. S3 is more suited for storing static content and less for a traditional file server workload.\n\n**Why option 3 is incorrect:**\nCreating a public VPC endpoint for S3 exposes the data to the internet, which is a major security risk, especially for confidential and sensitive files. Allowing employees to sign on directly to S3 is also not a secure or manageable approach for controlling access to specific files. This option fails to meet the security requirements of the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company's application runs on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability \nZones. On the first day of every month at midnight, the application becomes much slower when \nthe month-end financial calculation batch runs. This causes the CPU utilization of the EC2 \ninstances to immediately peak to 100%, which disrupts the application. \n \nWhat should a solutions architect recommend to ensure the application is able to handle the \nworkload and avoid downtime?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon CloudFront distribution in front of the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by proactively scaling the EC2 Auto Scaling group based on a known schedule. Since the slowdown occurs on the first day of every month at midnight, a scheduled scaling policy can be configured to increase the number of EC2 instances before the batch process starts, thus distributing the load and preventing CPU utilization from peaking to 100% and causing downtime. This is the most efficient and cost-effective way to handle predictable workload spikes.\n\n**Why option 0 is incorrect:**\nUsing Amazon CloudFront is primarily for caching static content and reducing latency for geographically dispersed users. While it can help with some types of workloads, it doesn't directly address the issue of high CPU utilization caused by the month-end financial calculation batch. The application slowdown is due to the processing load on the EC2 instances, not the delivery of static content. CloudFront would not prevent the EC2 instances from becoming overloaded.\n\n**Why option 1 is incorrect:**\nA simple scaling policy based on CPU utilization reacts to the high CPU utilization *after* it occurs. This means there will still be a period of performance degradation while the Auto Scaling group scales out. The goal is to *prevent* the high CPU utilization in the first place, not just react to it. A reactive scaling policy is less effective than a proactive, scheduled scaling policy in this scenario because the problem is predictable.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "A company wants to give a customer the ability to use on-premises Microsoft Active Directory to \ndownload files that are stored in Amazon S3. The customer's application uses an SFTP client to \ndownload the files. \n \nWhich solution will meet these requirements with the LEAST operational overhead and no \nchanges to the customer's application?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up AWS DataSync to synchronize between the on-premises location and the S3 location by",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution directly addresses the requirements. AWS Transfer Family with SFTP for Amazon S3 allows secure file transfers using SFTP. The integrated Active Directory configuration enables the customer's existing on-premises Active Directory to authenticate users, eliminating the need for separate user management. This approach minimizes operational overhead because AWS Transfer Family is a managed service, handling the underlying infrastructure. Finally, since the customer is already using an SFTP client, no changes to the application are needed.\n\n**Why option 1 is incorrect:**\nAWS Database Migration Service (DMS) is designed for database migrations, not file transfers. It doesn't provide SFTP access or integrate with Active Directory for authentication in the context of file downloads from S3. Therefore, it's not suitable for this scenario.\n\n**Why option 2 is incorrect:**\nAWS DataSync is used to synchronize data between on-premises storage and AWS storage services like S3. While it can move files to S3, it doesn't provide SFTP access for the customer's application. The customer would need to change their application to use DataSync's synchronization mechanism, which violates the requirement of no application changes. Also, it doesn't directly integrate with Active Directory for authentication.\n\n**Why option 3 is incorrect:**\nSetting up a Windows EC2 instance with SFTP would provide SFTP access, but it introduces significant operational overhead. The company would need to manage the EC2 instance, including patching, security, and scaling. While it's possible to integrate Active Directory with the EC2 instance, it's more complex than using AWS Transfer Family's built-in integration. This solution also requires more manual configuration and management, increasing operational overhead.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A company is experiencing sudden increases in demand. The company needs to provision large \nAmazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto \nScaling group. The company needs a solution that provides minimum initialization latency to meet \nthe demand. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the need for minimal initialization latency. EBS fast snapshot restore allows you to pre-initialize EBS volumes created from snapshots, which are used when creating AMIs. By enabling this feature on the underlying snapshot, new instances launched from AMIs created from that snapshot will have significantly reduced latency because the data is readily available. This is crucial for quickly scaling up in response to sudden demand increases.\n\n**Why option 0 is incorrect:**\nWhile creating an AMI from a snapshot is a valid approach, the `aws ec2 register-image` command itself doesn't inherently reduce initialization latency. It simply registers an existing image. Step Functions are not relevant to this scenario.\n\n**Why option 2 is incorrect:**\nAmazon Data Lifecycle Manager (DLM) primarily focuses on automating the creation, retention, and deletion of EBS snapshots and AMIs. While it helps manage the lifecycle of these resources, it doesn't directly address the requirement of minimizing initialization latency. DLM focuses on automation and compliance, not speed of provisioning.\n\n**Why option 3 is incorrect:**\nAmazon EventBridge and AWS Backup are focused on scheduling and managing backups. While AWS Backup can create AMIs, it doesn't inherently address the need for minimizing initialization latency. The focus is on data protection and recovery, not rapid provisioning of instances.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for \nstorage. The application tier is hosted on Amazon EC2 instances. The company's IT security \nguidelines mandate that the database credentials be encrypted and rotated every 14 days. \n \nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirements by utilizing AWS Secrets Manager. Secrets Manager allows you to store database credentials, encrypt them using AWS KMS, and automatically rotate them. The rotation process can be configured to occur every 14 days, meeting the security guidelines. This approach minimizes operational effort because Secrets Manager handles the encryption, storage, and rotation automatically, reducing the need for manual intervention or custom scripting. The EC2 instances can retrieve the credentials programmatically from Secrets Manager.\n\n**Why option 1 is incorrect:**\nWhile Systems Manager Parameter Store can store secrets, it doesn't natively support automatic rotation. Implementing rotation with Parameter Store would require custom scripting and scheduling, increasing operational overhead. Also, while Parameter Store offers encryption, Secrets Manager is specifically designed for managing secrets like database credentials, providing built-in rotation capabilities.\n\n**Why option 2 is incorrect:**\nStoring the credentials in a file and encrypting it with KMS provides encryption at rest, but it doesn't address the automatic rotation requirement. Implementing rotation would require significant custom scripting to update the file, re-encrypt it, and distribute the updated credentials to the EC2 instances, leading to high operational overhead.\n\n**Why option 3 is incorrect:**\nSimilar to option 2, storing the credentials in a file and encrypting it with KMS provides encryption at rest, but it doesn't address the automatic rotation requirement. Implementing rotation would require significant custom scripting to update the file, re-encrypt it, and distribute the updated credentials to the EC2 instances, leading to high operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company has deployed a web application on AWS. The company hosts the backend database \non Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling \nneeds. The read replicas must lag no more than 1 second behind the primary DB instance. The \ndatabase routinely runs scheduled stored procedures. \n \nAs traffic on the website increases, the replicas experience additional lag during periods of peak \nload. A solutions architect must reduce the replication lag as much as possible. The solutions \narchitect must minimize changes to the application code and must minimize ongoing operational \noverhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas,",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large,",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because Amazon Aurora MySQL offers significantly faster replication compared to standard MySQL replication. Aurora uses a shared storage layer, which allows replicas to access the same data as the primary instance, resulting in near real-time replication. Aurora Replicas typically have a replication lag of less than 100 milliseconds, which easily meets the 1-second requirement. Furthermore, migrating to Aurora from MySQL is relatively straightforward and requires minimal application code changes. Aurora also supports stored procedures, satisfying that requirement. Finally, Aurora is a managed service, minimizing ongoing operational overhead.\n\n**Why option 1 is incorrect:**\nWhile ElastiCache for Redis can improve read performance, it introduces eventual consistency and requires significant application code changes to implement caching logic. It does not directly address the replication lag issue between the primary RDS instance and its read replicas. The application would need to be modified to write to the cache and handle cache invalidation, which increases complexity and operational overhead. Also, ElastiCache doesn't directly help with the performance of stored procedures.\n\n**Why option 2 is incorrect:**\nMigrating to a MySQL database on EC2 instances would increase operational overhead significantly. The company would be responsible for managing the database instances, including patching, backups, and scaling. While choosing larger EC2 instances might improve performance, it doesn't inherently solve the replication lag problem. Standard MySQL replication would still be used, and it's unlikely to meet the 1-second replication lag requirement consistently during peak load. This option also increases the operational burden, which is against the requirements.\n\n**Why option 3 is incorrect:**\nMigrating to Amazon DynamoDB would require significant application code changes, as DynamoDB is a NoSQL database with a different data model and query language than MySQL. While DynamoDB can provide high read and write throughput, it's not a direct replacement for a relational database like MySQL, especially when stored procedures are involved. The effort to rewrite the application to use DynamoDB would be substantial and would not minimize changes to the application code. Also, DynamoDB doesn't directly support stored procedures.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a \nservice (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB \ncluster. \nThe DR plan must replicate data to a secondary AWS Region. \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of replicating data to a secondary region for disaster recovery using Aurora Global Database. Aurora Global Database is designed for disaster recovery and low latency global reads. By specifying a minimum of one DB instance in the secondary region, the solution ensures that the secondary region is ready to take over in case of a disaster. This provides a low RTO and RPO, which is important for a high-volume SaaS platform. While it's not the absolute cheapest option, it provides a good balance between cost and performance for DR, making it the most cost-effective solution considering the SaaS platform's requirements.\n\n**Why option 0 is incorrect:**\nWhile MySQL binary log replication can be used for DR, it is not as efficient or managed as Aurora Global Database. Setting up and maintaining binary log replication requires more manual effort and configuration, increasing operational overhead and potentially leading to higher costs in the long run, especially for a high-volume SaaS platform. It also doesn't provide the same level of integration and automation as Aurora Global Database for failover and recovery. The recovery time objective (RTO) would likely be higher than with Aurora Global Database.\n\n**Why option 1 is incorrect:**\nSetting up an Aurora Global Database and then removing the DB instance in the secondary region defeats the purpose of disaster recovery. Without a DB instance in the secondary region, there is no standby database to failover to in case of a disaster. This option would not meet the DR requirements and is therefore incorrect.\n\n**Why option 2 is incorrect:**\nAWS DMS is primarily designed for database migration and continuous data replication between heterogeneous databases. While it can be used for DR, it is not the most cost-effective or efficient solution for replicating data between Aurora MySQL clusters. DMS introduces additional overhead and complexity, and it is not optimized for the low RTO/RPO requirements of a high-volume SaaS platform. Aurora Global Database is a more native and efficient solution for replicating Aurora MySQL data for DR.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 35,
    "text": "A company has a custom application with embedded credentials that retrieves information from \nan Amazon RDS MySQL DB instance. Management says the application must be made more \nsecure with the least amount of programming effort. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by leveraging IAM roles for authentication. By configuring the application to assume an IAM role with the necessary permissions to access the RDS instance, the need for embedded credentials is eliminated. AWS Secrets Manager securely stores the database credentials, and the application can retrieve them programmatically. This approach minimizes code changes compared to other options that might require significant modifications to the application's authentication mechanism. The use of Secrets Manager also provides features like rotation and auditing, enhancing security.\n\n**Why option 0 is incorrect:**\nWhile AWS KMS is used for managing encryption keys, it doesn't directly address the problem of embedded credentials in the application. Using KMS to encrypt the credentials would still require the application to have access to the KMS key, which could become another security risk if not handled properly. Furthermore, it would require more programming effort to integrate KMS into the application for decryption.\n\n**Why option 1 is incorrect:**\nStoring credentials directly in the application configuration, even if encrypted, is not a secure practice. It still leaves the credentials vulnerable if the configuration file is compromised. This approach doesn't address the underlying issue of embedded credentials and doesn't provide the benefits of a dedicated secrets management service like AWS Secrets Manager.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A media company hosts its website on AWS. The website application's architecture includes a \nfleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is \nhosted on Amazon Aurora. The company's cybersecurity team reports that the application is \nvulnerable to SQL injection. \n \nHow should the company resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an ALB listener rule to reply to SQL injections with a fixed response.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon Inspector to block all SQL injection attempts automatically.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS WAF (Web Application Firewall) is designed to protect web applications from common web exploits, including SQL injection. By placing WAF in front of the ALB, it can inspect incoming HTTP requests and block those that match SQL injection patterns. Web ACLs (Access Control Lists) are used to define the rules that WAF uses to identify and block malicious requests. This is a standard and recommended practice for protecting web applications on AWS.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while an ALB can respond with a fixed response, it's not an effective way to prevent SQL injection. It doesn't analyze the request content to identify malicious patterns. It would simply block all requests that *might* be SQL injection attempts, leading to false positives and potentially blocking legitimate traffic. It's a crude and unreliable approach.\n\n**Why option 2 is incorrect:**\nThis is incorrect because AWS Shield Advanced primarily protects against DDoS attacks, not SQL injection vulnerabilities. While it offers some protection against application-layer attacks, it's not its primary focus, and it's not a substitute for a WAF. Shield Advanced is more about availability and resilience than specific application-level security vulnerabilities like SQL injection.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon Inspector is a vulnerability assessment service that identifies security vulnerabilities and deviations from best practices within your AWS environment. It doesn't automatically block SQL injection attempts. It identifies the vulnerability, but it's up to the user to remediate it. Inspector provides findings and recommendations, but it doesn't act as a real-time protection mechanism like a WAF.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company \nwants to create a visualization in Amazon QuickSight by joining the data in the data lake with \noperational data that is stored in an Amazon Aurora MySQL database. The company wants to \nenforce column-level authorization so that the company's marketing team can access only a \nsubset of columns in the database. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by leveraging a Lake Formation blueprint to automate the ingestion of data from the Aurora MySQL database into the S3 data lake. Lake Formation blueprints are designed to simplify and accelerate data ingestion and transformation. By using a blueprint, the company can easily set up a repeatable process for moving data from the database to the data lake. Lake Formation's integration with IAM and its fine-grained access control features enable the enforcement of column-level authorization for the marketing team. This approach minimizes operational overhead because the blueprint automates the data ingestion process and Lake Formation handles the security aspects.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because using Amazon EMR to directly ingest data into QuickSight SPICE bypasses the S3 data lake and Lake Formation governance. It doesn't provide a mechanism for enforcing column-level authorization in a manageable way. While EMR can extract data from the database, it doesn't integrate with Lake Formation for centralized governance and security. Also, directly loading into SPICE might not be suitable for large datasets or frequently updated data.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because while AWS Glue Studio can ingest data from the database to the S3 data lake, simply attaching an IAM role to the Glue job does not provide column-level authorization. IAM roles provide access control at the resource level (e.g., S3 bucket, Glue job), but not at the column level within the data. Lake Formation is required to enforce column-level access control. Glue Studio on its own doesn't provide the necessary integration with Lake Formation for fine-grained access control.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 \ninstances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, \nbut the baseline CPU utilization that is noted on each run is at least 60%. The company needs to \nprovision the capacity 30 minutes before the jobs run. \n \nCurrently, engineers complete this task by manually modifying the Auto Scaling group \nparameters. The company does not have the resources to analyze the required capacity trends \nfor the Auto Scaling group counts. The company needs an automated way to modify the Auto \nScaling group's desired capacity. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by leveraging predictive scaling, which uses machine learning to forecast future traffic patterns, including cyclical trends like weekly batch jobs. Predictive scaling can anticipate the need for increased capacity 30 minutes before the job runs, as specified in the requirements. This approach minimizes operational overhead because it automates the scaling process based on historical data and forecasts, eliminating the need for manual adjustments or complex configurations. It is particularly suitable when the company lacks resources for detailed capacity trend analysis, as the predictive scaling service handles the analysis automatically.\n\n**Why option 0 is incorrect:**\nWhile dynamic scaling policies react to real-time metrics like CPU utilization, they are not ideal for pre-provisioning capacity 30 minutes before a known event. Dynamic scaling responds to changes in demand as they occur, rather than anticipating them. The question specifies that the company needs to provision capacity *before* the jobs run, making dynamic scaling less suitable.\n\n**Why option 1 is incorrect:**\nScheduled scaling policies can adjust capacity at specific times. However, they require manual configuration of the scaling schedule and desired capacity. This approach increases operational overhead because engineers need to determine and configure the appropriate capacity for each scheduled event. The question explicitly states that the company wants to minimize operational overhead and lacks resources for detailed capacity trend analysis, making scheduled scaling a less desirable option compared to predictive scaling.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 39,
    "text": "A solutions architect is designing a company's disaster recovery (DR) architecture. The company \nhas a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled \nbackup. The DR design needs to include multiple AWS Regions. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n213",
    "options": [
      {
        "id": 0,
        "text": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by leveraging Amazon Aurora Global Database. Aurora Global Database is designed for disaster recovery and allows for low RTO (Recovery Time Objective) and RPO (Recovery Point Objective) across multiple AWS Regions. It provides automatic replication to a secondary region, simplifying the DR setup and minimizing operational overhead compared to managing replication manually or through backups.\n\n**Why option 0 is incorrect:**\nThis approach involves managing multiple EC2 instances and configuring replication between them. While it can achieve multi-region DR, it requires significant operational overhead for setup, monitoring, and failover management. It also doesn't inherently provide low RTO/RPO compared to Aurora Global Database.\n\n**Why option 1 is incorrect:**\nWhile RDS Multi-AZ provides high availability within a single region, it doesn't inherently provide a multi-region DR solution. Read replicas can be promoted to become standalone instances in another region, but this requires manual intervention and configuration, increasing operational overhead. It also doesn't offer the same level of automated replication and failover capabilities as Aurora Global Database.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to \nparse messages. The application cannot parse messages that are larger than 256 KB in size. \nThe company wants to implement a solution to give the application the ability to parse messages \nas large as 50 MB. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
    "options": [
      {
        "id": 0,
        "text": "Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the limit in Amazon SQS to handle messages that are larger than 256 KB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution addresses the requirement by leveraging the Amazon SQS Extended Client Library for Java. This library allows sending messages larger than 256 KB by storing the message payload in Amazon S3 and sending a reference to the S3 object in the SQS message. The library handles the complexity of storing and retrieving the large message, minimizing code changes in the application. It seamlessly integrates with existing SQS workflows, making it the most efficient solution.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because replacing SQS with EventBridge would require significant code changes. EventBridge is designed for event-driven architectures and not as a direct replacement for message queuing, especially for large message payloads. It would necessitate a complete redesign of the application's message processing logic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because the 256 KB message size limit is a hard limit imposed by Amazon SQS and cannot be changed. SQS is designed for smaller messages, and attempting to bypass this limit directly is not possible.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because storing messages in Amazon EFS would require significant code changes to manage the storage and retrieval of messages. The application would need to handle file management, potentially leading to increased complexity and latency. It also doesn't integrate well with the existing SQS workflow, making it a less desirable solution compared to using the Extended Client Library.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company wants to restrict access to the content of one of its main web applications and to \nprotect the content by using authorization techniques available on AWS. The company wants to \nimplement a serverless architecture and an authentication solution for fewer than 100 users. The \nsolution needs to integrate with the main web application and serve web content globally. The \nsolution must also scale as the company's user base grows while providing the lowest login \nlatency possible. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution effectively addresses all the requirements. Amazon Cognito provides a scalable and cost-effective authentication solution, especially suitable for smaller user bases and scales well as the user base grows. Lambda@Edge allows for authorization logic to be executed at edge locations globally, ensuring low latency and global content delivery. This approach is serverless, cost-effective, and integrates well with a web application, providing authorization before content is served.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because AWS Directory Service for Microsoft Active Directory is an expensive and complex solution for a small user base. It is more suitable for larger organizations already using Active Directory. While Lambda can be used for authorization, it would not be as globally distributed and low-latency as Lambda@Edge.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because while Cognito is a good choice for authentication, using standard AWS Lambda for authorization would not provide the low-latency, globally distributed authorization required. Lambda functions are region-specific, and routing all authorization requests to a single region would introduce latency and potentially scalability bottlenecks. Also, the mention of Amazon S3 is irrelevant to the authorization aspect of the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because AWS Directory Service for Microsoft Active Directory is an expensive and complex solution for a small user base. It is more suitable for larger organizations already using Active Directory. While Lambda@Edge is a good choice for authorization, the authentication component makes the entire solution less cost-effective.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A company has an aging network-attached storage (NAS) array in its data center. The NAS array \npresents SMB shares and NFS shares to client workstations. The company does not want to \npurchase a new NAS array. The company also does not want to incur the cost of renewing the \nNAS array's support contract. Some of the data is accessed frequently, but much of the data is \ninactive. \n \nA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 \nLifecycle policies, and maintains the same look and feel for the client workstations. The solutions \narchitect has identified AWS Storage Gateway as part of the solution. \n \nWhich type of storage gateway should the solutions architect provision to meet these \nrequirements?",
    "options": [
      {
        "id": 0,
        "text": "Volume Gateway",
        "correct": false
      },
      {
        "id": 1,
        "text": "Tape Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon FSx File Gateway",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 File Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Amazon S3 File Gateway provides low-latency access to data stored in Amazon S3 for on-premises applications via standard file protocols like SMB and NFS. It caches frequently accessed data locally, reducing latency, and allows the use of S3 Lifecycle policies to move inactive data to lower-cost storage tiers within S3. This directly addresses the requirements of migrating the data to S3, maintaining the existing file share access, and implementing cost optimization.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Volume Gateway presents block storage volumes to on-premises applications. It does not provide file-based access via SMB or NFS shares, which is a key requirement in the scenario. Volume Gateway is suitable for applications that require block-level storage, such as databases or virtual machine storage, not file shares.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Tape Gateway is designed for archiving data to virtual tapes stored in AWS. It is not suitable for providing ongoing file-based access to data via SMB or NFS shares. Tape Gateway is used for backup and archival purposes, not for replacing a NAS array with active file shares.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 43,
    "text": "A company has an application that is running on Amazon EC2 instances. A solutions architect \nhas standardized the company on a particular instance family and various instance sizes based \non the current needs of the company. \n \nThe company wants to maximize cost savings for the application over the next 3 years. The \ncompany needs to be able to change the instance family and sizes in the next 6 months based on \napplication popularity and usage. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n215 \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Compute Savings Plan",
        "correct": true
      },
      {
        "id": 1,
        "text": "EC2 Instance Savings Plan",
        "correct": false
      },
      {
        "id": 2,
        "text": "Zonal Reserved Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Standard Reserved Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most cost-effective option because Compute Savings Plans offer significant discounts (up to 66%) in exchange for a commitment to a consistent amount of compute usage (measured in $/hour) for a 1- or 3-year term. Crucially, Compute Savings Plans apply to EC2, AWS Lambda, and AWS Fargate. The flexibility comes from the fact that the commitment applies regardless of the instance family, size, Availability Zone, operating system, or tenancy used. This allows the company to change instance types and sizes as needed within the 6-month window and beyond, while still benefiting from the cost savings.\n\n**Why option 1 is incorrect:**\nThis is incorrect because EC2 Instance Savings Plans provide discounts on specific instance families within a region. While they offer significant savings, they lack the flexibility to change instance families. The company anticipates changing instance families within 6 months, making this option unsuitable.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Zonal Reserved Instances provide capacity reservations in a specific Availability Zone, along with a discount on the hourly usage. While they offer cost savings, they are tied to a specific instance type and Availability Zone. The company needs the flexibility to change instance families, and zonal RIs do not provide this.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Standard Reserved Instances offer discounts on EC2 usage but are less flexible than Compute Savings Plans. While they can be modified, the process is more complex and less seamless than the flexibility offered by Compute Savings Plans. Also, Standard RIs don't cover Lambda or Fargate.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company collects data from a large number of participants who use wearable devices. The \ncompany stores the data in an Amazon DynamoDB table and uses applications to analyze the \ndata. The data workload is constant and predictable. The company wants to stay at or below its \nforecasted budget for DynamoDB. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the most cost-effective solution because provisioned mode allows you to specify the exact read and write capacity units (RCUs and WCUs) needed for the constant and predictable workload. By accurately provisioning the capacity, the company can avoid overspending on unused capacity, which would be the case with on-demand mode or over-provisioning in provisioned mode. This approach allows the company to stay within its forecasted budget.\n\n**Why option 0 is incorrect:**\nWhile DynamoDB Standard-IA is cheaper for storage, it's designed for infrequently accessed data. The question doesn't state that the data is infrequently accessed; it only mentions a constant workload. Using Standard-IA would only be beneficial if a significant portion of the data is rarely accessed, which is not implied. Furthermore, using provisioned mode alone (option 1) is more cost-effective if all data is accessed regularly because it avoids the added complexity and potential cost overhead of managing data tiers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nOn-demand mode is designed for unpredictable workloads where you don't know the read and write capacity requirements in advance. Setting the RCUs and WCUs in on-demand mode doesn't make sense because on-demand mode automatically scales capacity based on demand. Specifying RCUs and WCUs is a feature of provisioned mode, not on-demand mode. On-demand mode is generally more expensive than provisioned mode for predictable workloads.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 45,
    "text": "A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-\nsoutheast-3 Region. The database is encrypted with an AWS Key Management Service (AWS \nKMS) customer managed key. The company was recently acquired and must securely share a \nbackup of the database with the acquiring company's AWS account in ap-southeast-3. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a database snapshot. Add the acquiring company's AWS account to the KMS key policy.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a database snapshot. Download the database snapshot. Upload the database snapshot to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating a database snapshot and then granting the acquiring company's AWS account access to the KMS key used to encrypt the snapshot. By adding the acquiring company's account as a principal in the KMS key policy, the acquiring company's IAM roles or users can be granted permission to use the KMS key to decrypt the snapshot in their account. This allows them to restore the database without compromising the security of the data. The snapshot itself remains encrypted, fulfilling the 'securely share' requirement. This is the most efficient and secure way to share the encrypted snapshot.\n\n**Why option 0 is incorrect:**\nCreating an unencrypted snapshot defeats the purpose of encryption and violates the requirement to securely share the data. Unencrypted data is vulnerable to unauthorized access and compromise.\n\n**Why option 2 is incorrect:**\nCreating a snapshot with an AWS managed KMS key would mean the acquiring company would not be able to access the data, as they do not have access to the key. The question specifies that the database is encrypted with a customer managed key, and the solution should leverage that. Changing the encryption key to an AWS managed key would require decrypting and re-encrypting the data, which is not the most efficient or secure approach.\n\n**Why option 3 is incorrect:**\nDownloading and uploading the database snapshot introduces significant security risks. The snapshot would be unencrypted during transit and storage on the local machine, making it vulnerable to interception and unauthorized access. This approach is also inefficient and impractical for large databases.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the \nus-east-1 Region to store customer transactions. The company needs high availability and \nautomatic recovery for the DB instance. \n \nThe company must also run reports on the RDS database several times a year. The report \nprocess causes transactions to take longer than usual to post to the customers' accounts. The \ncompany needs a solution that will improve the performance of the report process. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read replica of the DB instance in a different Availability Zone. Point all requests for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to RDS Custom.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use RDS Proxy to limit reporting requests to the maintenance window.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a Multi-AZ deployment provides high availability and automatic failover. If the primary DB instance fails, RDS automatically fails over to the standby instance in another Availability Zone, ensuring minimal downtime and automatic recovery. This addresses the high availability requirement.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while restoring a snapshot to a new RDS instance would create a separate reporting database, it doesn't provide automatic recovery for the primary production database. Also, restoring from a snapshot is a manual process and not automatic.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while a read replica can offload reporting workload from the primary database, creating it in a *different* Availability Zone does not directly address the high availability requirement for the *primary* database. A read replica in the same AZ as the primary would not help with HA. The question specifically asks for HA for the primary database.\n\n**Why option 3 is incorrect:**\nThis is incorrect because RDS Custom is more complex and generally used when you need operating system level access. It doesn't directly address the need for high availability and improved reporting performance in a simpler way than the other options. It adds unnecessary complexity.\n\n**Why option 4 is incorrect:**\nThis is incorrect because RDS Proxy helps manage database connections and improve application scalability, but it doesn't directly improve the performance of the report process itself. Limiting reporting requests to the maintenance window might reduce the impact on transactions, but it doesn't improve the report process's performance. It also doesn't address the high availability requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 47,
    "text": "A company is moving its data management application to AWS. The company wants to transition \nto an event-driven architecture. The architecture needs to be more distributed and to use \nserverless concepts while performing the different aspects of the workflow. The company also \nwants to minimize operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process",
        "correct": false
      },
      {
        "id": 1,
        "text": "Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by using AWS Step Functions to create a state machine. Step Functions is a serverless orchestration service that allows you to define workflows as state machines, coordinating multiple AWS services, including Lambda functions, in a reliable and scalable manner. This aligns perfectly with the need for an event-driven architecture, serverless components, and minimal operational overhead, as Step Functions manages the state and execution flow without requiring you to manage servers.\n\n**Why option 0 is incorrect:**\nWhile AWS Glue can perform ETL (Extract, Transform, Load) operations and invoke Lambda functions, it is primarily designed for data integration and ETL pipelines. Building the entire workflow in Glue might not be the most efficient or flexible approach for a general-purpose event-driven architecture. Glue is more focused on data processing tasks, and while it can trigger Lambda functions, it doesn't provide the same level of workflow orchestration and state management as Step Functions. Also, using Glue for the entire workflow might lead to increased operational overhead compared to Step Functions.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because deploying the application on Amazon EC2 instances contradicts the requirement for a serverless architecture. EC2 instances require manual provisioning, scaling, and patching, which increases operational overhead. While Step Functions can be used to orchestrate tasks, deploying the application on EC2 negates the benefits of a serverless approach and increases management complexity.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company is designing the network for an online multi-player game. The game uses the UDP \nnetworking protocol and will be deployed in eight AWS Regions. The network architecture needs \nto minimize latency and packet loss to give end users a high-quality gaming experience. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Setup a transit gateway in each Region. Create inter-Region peering attachments between each",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by leveraging AWS Global Accelerator's ability to route traffic to the nearest healthy endpoint based on network conditions and geographic proximity. Global Accelerator uses the AWS global network to minimize latency and packet loss by optimizing the path between users and the game servers. UDP listeners enable the game's UDP traffic to be handled efficiently. Endpoint groups in each Region allow Global Accelerator to distribute traffic across multiple game server instances within each region, providing redundancy and scalability. This ensures a consistent and high-quality gaming experience for users regardless of their location.\n\n**Why option 0 is incorrect:**\nWhile Transit Gateway facilitates inter-region connectivity, setting up a transit gateway in each region and creating inter-region peering attachments between each of them does not inherently minimize latency or packet loss for UDP traffic. Transit Gateway is more suited for managing connectivity between VPCs and on-premises networks, and the inter-region peering can still introduce latency. It doesn't provide the same level of optimized routing and global network utilization as Global Accelerator.\n\n**Why option 2 is incorrect:**\nAmazon CloudFront is primarily designed for caching and delivering static and dynamic web content. While it can improve performance for HTTP/HTTPS traffic, it is not optimized for real-time UDP traffic required by online multiplayer games. CloudFront's caching mechanisms are not suitable for the low-latency, bidirectional communication needed in this scenario. Also, CloudFront's UDP support is limited and not designed for this type of application.\n\n**Why option 3 is incorrect:**\nSetting up a VPC peering mesh between each region would create a complex and difficult-to-manage network. While VPC peering allows direct network connectivity between VPCs, it doesn't inherently minimize latency or packet loss. The routing between regions would not be optimized for low latency, and managing a full mesh of VPC peering connections (N*(N-1)/2 connections, which is 28 in this case) becomes operationally challenging and doesn't provide the benefits of a globally optimized network like Global Accelerator. Turning on UDP for each VPC simply enables UDP traffic but doesn't address the latency and packet loss concerns.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company hosts a three-tier web application on Amazon EC2 instances in a single Availability \nZone. The web application uses a self-managed MySQL database that is hosted on an EC2 \ninstance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL \ndatabase currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects \ntraffic of 1,000 IOPS for both reads and writes at peak traffic. \n \nThe company wants to minimize any disruptions, stabilize performance, and reduce costs while \nretaining the capacity for double the IOPS. The company wants to move the database tier to a \nfully managed solution that is highly available and fault tolerant. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Intelligent-Tiering access tiers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use two large EC2 instances to host the database in active-passive mode.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution meets the requirements by providing a fully managed database service (RDS for MySQL) with Multi-AZ deployment for high availability and fault tolerance. General Purpose SSD (gp3) volumes offer a cost-effective balance of performance and price and can be scaled to handle the required 2000 IOPS. RDS handles backups, patching, and other administrative tasks, reducing operational overhead and minimizing disruptions. gp3 volumes are designed to provide a baseline performance level with the ability to burst to higher levels, making them suitable for handling peak traffic. gp3 is generally more cost-effective than io2 for the given IOPS requirement.\n\n**Why option 0 is incorrect:**\nWhile io2 Block Express EBS volumes offer high performance, they are significantly more expensive than gp3 volumes. The question emphasizes cost-effectiveness, and for the specified IOPS requirement (2000 IOPS), a gp3 volume can provide sufficient performance at a lower cost. Choosing io2 would be an unnecessary expense.\n\n**Why option 2 is incorrect:**\nAmazon S3 Intelligent-Tiering is an object storage service and is not suitable for hosting a relational database like MySQL. It's designed for storing and retrieving files, not for transactional database operations. This option does not address the requirement of migrating the database to a managed solution.\n\n**Why option 3 is incorrect:**\nWhile using two EC2 instances in active-passive mode can provide high availability, it does not meet the requirement of using a fully managed solution. It also requires manual configuration and management of failover, backups, patching, and other administrative tasks, which increases operational overhead and does not reduce costs compared to a managed service like RDS.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 50,
    "text": "A company hosts a serverless application on AWS. The application uses Amazon API Gateway, \nAWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase \nin application errors that result from database connection timeouts during times of peak traffic or \nunpredictable traffic. The company needs a solution that reduces the application failures with the \nleast amount of change to the code. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Reduce the Lambda concurrency rate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable RDS Proxy on the RDS DB instance.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Resize the RDS DB instance class to accept more connections.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon DynamoDB with on-demand scaling.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the problem of database connection exhaustion by pooling and sharing database connections. RDS Proxy sits between the Lambda functions and the RDS database, multiplexing connections. This allows a large number of Lambda functions to share a smaller pool of database connections, preventing the database from being overwhelmed. It requires minimal code changes, as the Lambda functions only need to be configured to connect to the RDS Proxy endpoint instead of directly to the RDS instance.\n\n**Why option 0 is incorrect:**\nReducing the Lambda concurrency rate would limit the number of Lambda functions that can run concurrently. While this might reduce the load on the database, it would also reduce the application's ability to handle traffic, potentially leading to increased latency and a poor user experience. This does not address the root cause of the connection timeouts and is counterproductive to handling peak traffic.\n\n**Why option 2 is incorrect:**\nResizing the RDS DB instance class to a larger instance might temporarily alleviate the connection timeout issue by increasing the number of connections the database can handle. However, it doesn't address the underlying problem of inefficient connection management. It's also a more expensive solution than using RDS Proxy and might not scale effectively for unpredictable traffic spikes. It also doesn't minimize code changes.\n\n**Why option 3 is incorrect:**\nMigrating the database to Amazon DynamoDB would require significant code changes, as DynamoDB is a NoSQL database with a different data model and query language than PostgreSQL. This option violates the requirement of minimizing code changes and is a much more complex and time-consuming solution than using RDS Proxy. Additionally, DynamoDB might not be suitable for all types of data and application requirements.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "A company is migrating an old application to AWS. The application runs a batch job every hour \nand is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. \nThe server has 64 virtual CPU (vCPU) and 512 GiB of memory. \n \nWhich solution will run the batch job within 15 minutes with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda with functional scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Lightsail with AWS Auto Scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Batch on Amazon EC2.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution is correct because AWS Batch is specifically designed for running batch computing workloads. It allows you to define job definitions that specify the compute resources required (CPU, memory, etc.). AWS Batch automatically provisions and manages the underlying EC2 instances based on the job requirements, scaling up or down as needed. This eliminates the need for manual instance management, reducing operational overhead. By configuring the AWS Batch environment with appropriate EC2 instance types (e.g., compute-optimized instances) and specifying the required vCPUs and memory in the job definition, the batch job can be executed within the desired 15-minute timeframe. AWS Batch handles the scheduling, queuing, and execution of the jobs, further simplifying the process.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because AWS Lambda has limitations on execution time (currently 15 minutes) and memory (currently 10 GB). While Lambda offers functional scaling, the CPU-intensive nature of the batch job and the required memory (512 GiB) exceed Lambda's capabilities. Therefore, Lambda is not suitable for this workload.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because while ECS with Fargate is a container orchestration service that eliminates the need to manage the underlying EC2 instances, it may require more operational overhead compared to AWS Batch for this specific scenario. You would need to build and manage a container image for the batch job, configure the ECS cluster and task definitions, and potentially manage scaling policies. AWS Batch simplifies this process by providing a managed batch processing service that handles the underlying infrastructure and job scheduling automatically. Also, ensuring the Fargate task has sufficient CPU and memory to complete in 15 minutes requires careful configuration and testing, adding to the operational overhead.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because Amazon Lightsail is designed for simpler workloads like websites and small applications. It does not offer the flexibility and scalability required for a CPU-intensive batch job that requires 64 vCPUs and 512 GiB of memory. While Lightsail offers Auto Scaling, it is not optimized for batch processing and would likely be more expensive and less efficient than AWS Batch for this use case.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 52,
    "text": "A company stores its data objects in Amazon S3 Standard storage. A solutions architect has \nfound that 75% of the data is rarely accessed after 30 days. The company needs all the data to \nremain immediately accessible with the same high availability and resiliency, but the company \nwants to minimize storage costs. \n \nWhich storage solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the data objects to S3 Glacier Deep Archive after 30 days.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement of minimizing storage costs for infrequently accessed data while maintaining immediate accessibility and high availability. S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It offers lower storage costs compared to S3 Standard, but with a retrieval fee. It maintains the same high availability (99.99%) and durability (99.999999999%) as S3 Standard, making it suitable for this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because S3 Glacier Deep Archive is designed for long-term archival of data with infrequent access and retrieval times measured in hours. The requirement states that data needs to be immediately accessible, which Glacier Deep Archive does not provide. Retrieval times can be 12 hours or more, making it unsuitable for immediate access.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because S3 One Zone-IA stores data in a single Availability Zone, which reduces costs but also reduces availability and durability compared to S3 Standard and S3 Standard-IA. The question specifies that the company needs to maintain the same high availability and resiliency, which S3 One Zone-IA does not guarantee. Data loss is more likely with S3 One Zone-IA if the Availability Zone becomes unavailable.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because while S3 One Zone-IA offers lower storage costs, it sacrifices availability and durability by storing data in a single Availability Zone. The question explicitly states that the company needs to maintain the same high availability and resiliency as S3 Standard, which S3 One Zone-IA does not provide. Moving the data immediately doesn't change the fact that it doesn't meet the availability requirements.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company has a three-tier application on AWS that ingests sensor data from its users devices. \nThe traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the \nweb tier, and finally to EC2 instances for the application tier. The application tier makes calls to a \ndatabase. \n \nWhat should a solutions architect do to improve the security of the data in transit?",
    "options": [
      {
        "id": 0,
        "text": "Configure a TLS listener. Deploy the server certificate on the NLB.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS Shield Advanced. Enable AWS WAF on the NLB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\n{\n    \"analysis\": \"The question focuses on securing data in transit for a three-tier application. The application uses a Network Load Balancer (NLB) to distribute traffic to EC2 instances in the web and application tiers, with the application tier interacting with a database. The primary goal is to encrypt the data as it moves between the different tiers. The key is to implement TLS encryption at the NLB to secure the initial connection and ensure data is encrypted throughout the communication p\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A social media company runs its application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The \napplication has more than a billion images stored in an Amazon S3 bucket and processes \nthousands of images each second. The company wants to resize the images dynamically and \nserve appropriate formats to clients. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Install an external image management library on an EC2 instance. Use the image management",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a CloudFront origin request policy. Use the policy to automatically resize images and to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Lambda@Edge function with an external image management library. Associate the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a CloudFront response headers policy. Use the policy to automatically resize images and",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements effectively because Lambda@Edge allows you to execute code at CloudFront edge locations. This proximity to the user reduces latency. Using an external image management library within the Lambda@Edge function enables dynamic image resizing and format conversion based on the client's request (e.g., User-Agent header). This approach avoids the need to manage additional infrastructure like EC2 instances, minimizing operational overhead. Lambda@Edge scales automatically with the request volume, making it suitable for handling thousands of images per second.\n\n**Why option 0 is incorrect:**\nInstalling an external image management library on an EC2 instance introduces significant operational overhead. It requires managing the EC2 instance, including patching, scaling, and ensuring high availability. This approach is not as scalable or cost-effective as a serverless solution like Lambda@Edge, especially given the high volume of image processing required. Furthermore, routing requests to the EC2 instance would add latency and complexity to the architecture.\n\n**Why option 1 is incorrect:**\nCloudFront origin request policies primarily control which headers, cookies, and query strings are forwarded to the origin. They do not provide a mechanism for dynamically resizing images or converting their formats. Therefore, this option does not meet the core requirements of the question. While origin request policies can be used to modify requests before they reach the origin, they don't inherently provide image processing capabilities.\n\n**Why option 3 is incorrect:**\nCloudFront response headers policies are used to add or modify HTTP headers in the responses that CloudFront sends to viewers. They do not provide a mechanism for dynamically resizing images or converting their formats. Therefore, this option does not meet the core requirements of the question. While response header policies can be used for security or caching purposes, they are not relevant to image processing.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 55,
    "text": "A hospital needs to store patient records in an Amazon S3 bucket. The hospital's compliance \nteam must ensure that all protected health information (PHI) is encrypted in transit and at rest. \nThe compliance team must administer the encryption key for data at rest. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by enforcing encryption in transit using the `aws:SecureTransport` condition in the S3 bucket policy. This condition ensures that all requests to the S3 bucket must be made over HTTPS. Additionally, using Server-Side Encryption with Customer Master Keys (SSE-KMS) allows the compliance team to manage the encryption keys within AWS KMS, fulfilling the requirement for compliance team-administered encryption keys for data at rest.\n\n**Why option 0 is incorrect:**\nWhile a public SSL/TLS certificate in ACM is necessary for HTTPS, it doesn't directly enforce encryption in transit for S3. The certificate is used by services like CloudFront or API Gateway, not directly by S3. Also, this option doesn't address encryption at rest or key management.\n\n**Why option 1 is incorrect:**\nThis option only addresses encryption in transit using the `aws:SecureTransport` condition in the S3 bucket policy. It does not address the requirement for encryption at rest or the compliance team's need to administer the encryption key for data at rest. Server-Side Encryption with S3 Managed Keys (SSE-S3) does not allow the compliance team to administer the encryption key.\n\n**Why option 3 is incorrect:**\nThis option only addresses encryption in transit using the `aws:SecureTransport` condition in the S3 bucket policy. It does not address the requirement for encryption at rest or the compliance team's need to administer the encryption key for data at rest. Server-Side Encryption with Customer-Provided Keys (SSE-C) requires the client to provide the encryption key with each request, which is not ideal for key management by the compliance team and can be complex to implement and manage securely.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A data analytics company wants to migrate its batch processing system to AWS. The company \nreceives thousands of small data files periodically during the day through FTP. A on-premises \nbatch job processes the data files overnight. However, the batch job takes hours to finish running. \nThe company wants the AWS solution to process incoming data files are possible with minimal \nchanges to the FTP clients that send the files. The solution must delete the incoming data files \nthe files have been processed successfully. Processing for each file needs to take 3-8 minutes. \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by using AWS Transfer Family to create an FTP server. AWS Transfer Family is a managed service, reducing the operational overhead of managing an FTP server on EC2. Storing the files on Amazon Elastic File System (EFS) allows for easy access by compute resources for processing. EFS provides a shared file system that can be mounted on multiple instances, enabling parallel processing of the files. This approach minimizes changes to the FTP clients, provides a scalable and managed FTP service, and allows for efficient processing of the files. The question doesn't explicitly state the processing mechanism, but EFS allows for easy integration with various compute services like EC2, Lambda, or ECS for processing. The ability to delete files after processing can be easily implemented within the processing logic.\n\n**Why option 0 is incorrect:**\nWhile using an EC2 instance as an FTP server is possible, it requires manual management of the FTP server software, security patching, and scaling. Storing files directly as objects in S3 from the FTP server is not a standard FTP functionality and would require custom scripting and integration, increasing complexity and operational overhead. Also, directly uploading to S3 from an FTP server on EC2 is not a straightforward process and might require significant custom development.\n\n**Why option 1 is incorrect:**\nSimilar to option 0, using an EC2 instance as an FTP server requires manual management. Storing files on Amazon Elastic Block Storage (EBS) attached to the EC2 instance limits scalability and availability, as EBS volumes are tied to a specific Availability Zone and EC2 instance. This makes it less operationally efficient than using a managed service like AWS Transfer Family and a shared file system like EFS.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. \nThe application has three tiers, a business tier, and a database tier with Microsoft SQL Server. \nThe company wants to use specific features of SQL Server such as native backups and Data \nQuality Services. The company also needs to share files for process between the tiers. \n \nHow should a solution architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host all three on Amazon instances. Use Mmazon FSx File Gateway for file sharing between",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host all three on Amazon EC2 instances. Use Amazon FSx for Windows file sharing between the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by hosting all three tiers (application, business, and database) on Amazon EC2 instances. EC2 provides the flexibility to run Windows Server and Microsoft SQL Server, allowing the company to utilize native backups and Data Quality Services. Amazon FSx for Windows File Server provides a fully managed, highly available, and scalable file system that is compatible with Windows environments. This allows for seamless file sharing between the tiers, fulfilling the file sharing requirement.\n\n**Why option 0 is incorrect:**\nWhile hosting all three tiers on Amazon EC2 instances is a valid approach, using Amazon FSx File Gateway is not the most efficient solution for file sharing within the same AWS environment. FSx File Gateway is designed for hybrid environments, connecting on-premises applications to FSx file shares in AWS. For internal file sharing between EC2 instances, using Amazon FSx for Windows File Server directly provides better performance and lower latency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The \ncompany has VPCs with public subnets and private subnets in its AWS account. The EC2 \ninstances run in a private subnet in one of the VPCs. The Lambda functions need direct network \naccess to the EC2 instances for the application to work. \n \nThe application will run for at least 1 year. The company expects the number of Lambda functions \nthat the application uses to increase during that time. The company wants to maximize its savings \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n222 \non all application resources and to keep network latency between the services low. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a Compute Savings Plan. Optimize the Lambda functions duration and memory usage,",
        "correct": true
      },
      {
        "id": 3,
        "text": "Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage,",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by leveraging a Compute Savings Plan, which offers cost savings on EC2 and Lambda usage. Optimizing Lambda function duration and memory usage further reduces costs. Using Lambda functions within the VPC provides low latency network access to the EC2 instances, as they are in the same network. The Compute Savings Plan is suitable for a variety of compute workloads, including both EC2 and Lambda, making it ideal for the expected increase in Lambda function usage. This is more flexible than an EC2 instance Savings Plan, which is tied to EC2 usage only.\n\n**Why option 0 is incorrect:**\nPurchasing an EC2 instance Savings Plan only covers EC2 costs and does not provide any cost savings for Lambda functions. While optimizing Lambda duration is beneficial, it doesn't address the cost of the Lambda functions themselves. This option is not the most cost-effective solution for the entire application, as it only focuses on EC2.\n\n**Why option 1 is incorrect:**\nPurchasing an EC2 instance Savings Plan only covers EC2 costs and does not provide any cost savings for Lambda functions. While optimizing Lambda duration is beneficial, it doesn't address the cost of the Lambda functions themselves. This option is not the most cost-effective solution for the entire application, as it only focuses on EC2.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n223 \nA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the scalability and reliability requirements by leveraging CloudFront for static content caching and SQS for asynchronous request processing. CloudFront reduces the load on the EC2 instances by serving static content directly from its edge locations, improving website performance and reducing latency. SQS decouples the API from the backend workers, allowing the API to quickly enqueue sales requests without waiting for immediate processing. This prevents the API from being overwhelmed during peak traffic and ensures that all requests are eventually processed by the backend workers, even if there is a backlog.\n\n**Why option 0 is incorrect:**\nWhile adding CloudFront for dynamic content can improve performance to some extent, it's less effective than caching static content. Dynamic content, by its nature, changes frequently, reducing the cache hit ratio. Increasing the number of EC2 instances alone might not be sufficient to handle sudden spikes in traffic if the backend processing is the bottleneck. It also doesn't address the potential for the API to become overwhelmed while waiting for backend processing.\n\n**Why option 1 is incorrect:**\nWhile adding CloudFront for static content is a good practice, placing EC2 instances in an Auto Scaling group only addresses the scaling of the API tier. It doesn't address the potential bottleneck in the asynchronous processing of sales requests. If the backend workers cannot keep up with the incoming requests, the system will still be overloaded, and requests might be dropped or delayed.\n\n**Why option 2 is incorrect:**\nAdding CloudFront for dynamic content has limited benefits compared to caching static content. Adding ElastiCache might improve the performance of the API by caching frequently accessed data, but it doesn't address the fundamental problem of handling a large influx of sales requests asynchronously. The API might still become overwhelmed while waiting for the backend workers to process the requests.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A companys web application consists of an Amazon API Gateway API in front of an AWS \nLambda function and an Amazon DynamoDB database. The Lambda function handles the \nbusiness logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito \nuser pools to identify the individual users of the application. A solutions architect needs to update \nthe application so that only users who have a subscription can access premium content. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Enable API caching and throttling on the API Gateway API.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS WAF on the API Gateway API Create a rule to filter users who have a subscription.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Apply fine-grained IAM permissions to the premium content in the DynamoDB table.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement API usage plans and API keys to limit the access of users who do not have a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by leveraging API Gateway's built-in features for access control. API usage plans allow you to define limits and quotas for API usage, and API keys can be associated with these plans. By associating API keys with subscription status (e.g., assigning a key only to subscribed users), you can effectively control access to premium content. This approach minimizes operational overhead because it utilizes existing API Gateway functionality without requiring code changes in the Lambda function or complex IAM configurations. The subscription status can be managed outside the core application logic, making it easier to update and maintain.\n\n**Why option 0 is incorrect:**\nAPI caching and throttling are primarily for performance optimization and preventing abuse, not for controlling access based on user subscription status. While throttling can limit overall usage, it doesn't differentiate between subscribed and unsubscribed users. Caching, on the other hand, could potentially expose premium content to unauthorized users if not implemented carefully with subscription awareness, adding complexity and operational overhead.\n\n**Why option 1 is incorrect:**\nWhile AWS WAF can filter requests based on various criteria, using it to determine subscription status would require complex rules and potentially integration with an external subscription management system. This adds significant operational overhead and complexity to the solution. WAF is better suited for protecting against common web exploits rather than implementing fine-grained access control based on business logic.\n\n**Why option 2 is incorrect:**\nApplying fine-grained IAM permissions to the DynamoDB table would require significant changes to the Lambda function to assume different IAM roles based on the user's subscription status. This adds complexity to the Lambda function and increases operational overhead. It also tightly couples the application logic with IAM roles, making it harder to maintain and update. Furthermore, it bypasses the API Gateway layer, which is the intended point of entry for requests, potentially creating security vulnerabilities.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company's application runs on AWS. The application stores large documents in an Amazon S3 \nbucket that uses the S3 Standard-infrequent Access (S3 Standerd-IA) storage class. The \ncompany will continue paying to store the data but wants to save on its total S3 costs. The \ncompany wants authorized external users to have the ability to access the documents in \nmilliseconds. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the S3 bucket to be a Requester Pays bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the storage tier to S3 Standard for all existing and future objects.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on S3 Transfer Acceleration tor the S3 Docket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront to handle all the requests to the S3 bucket.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by caching the S3 objects at edge locations closer to the external users. This reduces latency and provides millisecond access. While CloudFront does have associated costs, it can be configured to be more cost-effective than S3 Standard for frequently accessed objects, especially when combined with S3 Standard-IA for the origin. CloudFront also provides security benefits by allowing you to control access to your content and protect your S3 bucket from direct access.\n\n**Why option 0 is incorrect:**\nThis option shifts the cost of data transfer to the requester (external users). While it might reduce the company's costs, it doesn't address the requirement of providing millisecond access. It also might deter external users from accessing the documents if they have to pay for the data transfer.\n\n**Why option 1 is incorrect:**\nChanging the storage tier to S3 Standard would increase storage costs compared to S3 Standard-IA. While it might slightly improve access latency compared to S3 Standard-IA, it's not the most cost-effective way to achieve millisecond access for external users. CloudFront provides a much more significant improvement in latency at a potentially lower cost, especially for frequently accessed objects.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  }
]