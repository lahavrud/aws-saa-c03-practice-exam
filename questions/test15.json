[
  {
    "id": 0,
    "text": "A company recently created a disaster recovery site in a different AWS Region. The company \nneeds to transfer large amounts of data back and forth between NFS file systems in the two \nRegions on a periodic basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Snowball devices.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an SFTP server on Amazon EC2.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS DataSync is a fully managed data transfer service that simplifies moving large amounts of data between on-premises storage systems and AWS services. It can also transfer data between different AWS services, including different AWS Regions. DataSync provides a simple, scalable, and automated solution to transfer data, and it minimizes the operational overhead because it is fully managed by AWS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same \nVPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure \nthat enough funds are available before a stock can be purchased. The company has noticed in \nthe VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web \nservice over the internet instead of through the VPC. A solutions architect must implement a \nsolution so that the APIs communicate through the VPC. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
    "options": [
      {
        "id": 0,
        "text": "Add an X-API-Key header in the HTTP header for authorization.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an interface endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a gateway endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAn interface endpoint is a horizontally scaled, redundant VPC endpoint that provides private connectivity to a service. It is an elastic network interface with a private IP address that serves as an entry point for traffic destined to the AWS service. Interface endpoints are used to connect VPCs with AWS services. https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company wants to run an in-memory database for a latency-sensitive application that runs on \nAmazon EC2 instances. The application processes more than 100,000 transactions each minute \nand requires high network throughput. A solutions architect needs to provide a cost-effective \nnetwork design that minimizes data transfer charges. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n195 \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTo achieve low latency, high throughput, and cost-effectiveness, the optimal solution is to launch EC2 instances as a placement group with the cluster strategy within the same Availability Zone.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A company that primarily runs its application servers on premises has decided to migrate to AWS. \nThe company wants to minimize its need to scale its Internet Small Computer Systems Interface \n(iSCSI) storage on premises. The company wants only its recently accessed data to remain \nstored locally. \n \nWhich AWS solution should the company use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 File Gateway",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Storage Gateway Tape Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Storage Gateway Volume Gateway stored volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway Volume Gateway cached volumes",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Storage Gateway Volume Gateway provides two configurations for connecting to iSCSI storage, namely, stored volumes and cached volumes. The stored volume configuration stores the entire data set on-premises and asynchronously backs up the data to AWS. The cached volume configuration stores recently accessed data on-premises, and the remaining data is stored in Amazon S3. Since the company wants only its recently accessed data to remain stored locally, the cached volume configuration would be the most appropriate. It allows the company to keep frequently accessed data on-premises and reduce the need for scaling its iSCSI storage while still providing access to all data through the AWS cloud. This configuration also provides low-latency access to frequently accessed data and cost-effective off-site backups for less frequently accessed data. https://docs.amazonaws.cn/en_us/storagegateway/latest/vgw/StorageGatewayConcepts.html#sto rage-gateway-cached-concepts\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company has multiple AWS accounts that use consolidated billing. The company runs several \nactive high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The \ncompany's finance team has access to AWS Trusted Advisor in the consolidated billing account \nand all other AWS accounts. \n \nThe finance team needs to use the appropriate AWS account to access the Trusted Advisor \ncheck recommendations for RDS. The finance team must review the appropriate Trusted Advisor \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n196 \ncheck to reduce RDS costs. \n \nWhich combination of steps should the finance team take to meet these requirements? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Use the Trusted Advisor recommendations from the account where the RDS instances are",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Review the Trusted Advisor check for Amazon RDS Idle DB Instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/trusted-advisor-cost-optimization/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A solutions architect needs to optimize storage costs. The solutions architect must identify any \nAmazon S3 buckets that are no longer being accessed or are rarely accessed. \n \nWhich solution will accomplish this goal with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity",
        "correct": true
      },
      {
        "id": 1,
        "text": "Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nS3 Storage Lens is a fully managed S3 storage analytics solution that provides a comprehensive view of object storage usage, activity trends, and recommendations to optimize costs. Storage Lens allows you to analyze object access patterns across all of your S3 buckets and generate detailed metrics and reports. https://aws.amazon.com/blogs/aws/s3-storage-lens/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company sells datasets to customers who do research in artificial intelligence and machine \nlearning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket \nin the us-east-1 Region. The company hosts a web application that the customers use to \npurchase access to a given dataset. The web application is deployed on multiple Amazon EC2 \ninstances behind an Application Load Balancer. After a purchase is made, customers receive an \nS3 signed URL that allows access to the files. \n \nThe customers are distributed across North America and Europe. The company wants to reduce \nthe cost that is associated with data transfers and wants to maintain or improve performance. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n197",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the web application to enable streaming of the datasets to end users. Configure the web",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nTo reduce the cost associated with data transfers and maintain or improve performance, a solutions architect should use Amazon CloudFront, a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. Deploying a CloudFront distribution with the existing S3 bucket as the origin will allow the company to serve the data to customers from edge locations that are closer to them, reducing data transfer costs and improving performance. Directing customer requests to the CloudFront URL and switching to CloudFront signed URLs for access control will enable customers to access the data securely and efficiently. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company is using AWS to design a web application that will process insurance quotes. Users \nwill request quotes from the application. Quotes must be separated by quote type, must be \nresponded to within 24 hours, and must not get lost. The solution must maximize operational \nefficiency and must minimize maintenance. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nQuote types need to be separated: SNS message filtering can be used to publish messages to the appropriate SQS queue based on the quote type, ensuring that quotes are separated by type. Quotes must be responded to within 24 hours and must not get lost: SQS provides reliable and scalable queuing for messages, ensuring that quotes will not get lost and can be processed in a Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company has an application that runs on several Amazon EC2 instances. Each EC2 instance \nhas multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The \napplication's EC2 instance configuration and data need to be backed up nightly. The application \nalso needs to be recoverable in a different AWS Region. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/vi/blogs/aws/aws-backup-ec2-instances-efs-single-file-restore-and- cross-region-backup/ When you back up an EC2 instance, AWS Backup will protect all EBS volumes attached to the instance, and it will attach them to an AMI that stores all parameters from the original EC2 instance except for two.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company is building a mobile app on AWS. The company wants to expand its reach to millions \nof users. The company needs to build a platform so that authorized users can watch the \ncompany's content on their mobile devices. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up IPsec VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront. Provide signed URLs to stream content.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up AWS Client VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront supports signed URLs that provide authorized access to your content. This feature allows the company to control who can access their content and for how long, providing a secure Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company hosts a three-tier web application that includes a PostgreSQL database The database \nstores the metadata from documents The company searches the metadata for key terms to \nretrieve documents that the company reviews in a report each month The documents are stored \nin Amazon S3 The documents are usually written only once, but they are updated frequency The \nreporting process takes a few hours with the use of relational queries The reporting process must \nnot affect any document modifications or the addition of new documents. \nWhat are the MOST operationally efficient solutions that meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a new Amazon RDS for PostgreSQL Reserved Instance and an On-Demand read replica",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a new Amazon Aurora PostgreSQL DB cluster that includes a Reserved Instance and an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a new Amazon RDS for PostgreSQL Multi-AZ Reserved Instance Configure the reporting",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up a new Amazon DynamoDB table to store the documents Use a fixed write capacity to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company experienced a breach that affected several applications in its on-premises data \ncenter. The attacker took advantage of vulnerabilities in the custom applications that were \nrunning on the servers. The company is now migrating its applications to run on Amazon EC2 \ninstances. The company wants to implement a solution that actively scans for vulnerabilities on \nthe EC2 instances and sends a report that details the findings. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. https://aws.amazon.com/inspector/features/?nc=sn&loc=2 Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A company uses an Amazon EC2 instance to run a script to poll for and process messages in an \nAmazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational \ncosts while maintaining its ability to process a growing number of messages that are added to the \nqueue. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the size of the EC2 instance to process messages faster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Run Command to run the script on demand.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nBy migrating the script to AWS Lambda, the company can take advantage of the auto-scaling feature of the service. AWS Lambda will automatically scale resources to match the size of the workload. This means that the company will not have to worry about provisioning or managing instances as the number of messages increases, resulting in lower operational costs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company uses a legacy application to produce data in CSV format. The legacy application \nstores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf \n(COTS) application that can perform complex SQL queries to analyze data that is stored in \nAmazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv \nfiles that the legacy application produces. \n \nThe company cannot update the legacy application to produce data in another format. The \ncompany needs to implement a solution so that the COTS application can use the data that the \nlegacy application produces. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure",
        "correct": true
      },
      {
        "id": 1,
        "text": "Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company recently migrated its entire IT environment to the AWS Cloud. The company \ndiscovers that users are provisioning oversized Amazon EC2 instances and modifying security \ngroup rules without using the appropriate change control process. A solutions architect must \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n201 \ndevise a strategy to track and audit these inventory and configuration changes. \n \nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS CloudTrail and use it for auditing.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use data lifecycle policies for the Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable AWS Trusted Advisor and reference the security dashboard.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Config and create rules for auditing and compliance purposes.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Restore previous resource configurations with an AWS CloudFormation template.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems \nadministrators have used shared SSH keys to manage the instances. After a recent audit, the \ncompany's security team is mandating the removal of all shared keys. A solutions architect must \ndesign a solution that provides secure access to the EC2 instances. \n \nWhich solution will meet this requirement with the LEAST amount of administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager Session Manager to connect to the EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Systems Manager Session Manager provides secure and auditable instance management without the need for any inbound connections or open ports. It allows you to manage your instances through an interactive one-click browser-based shell or through the AWS CLI. This means that you don't have to manage any SSH keys, and you don't have to worry about securing access to your instances as access is controlled through IAM policies.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A company is building a data analysis platform on AWS by using AWS Lake Formation. The \nplatform will ingest data from different sources such as Amazon S3 and Amazon RDS. The \ncompany needs a secure solution to prevent access to portions of the data that contain sensitive \ninformation.",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that includes permissions to access Lake Formation tables.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create data filters to implement row-level security and cell-level security.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function that removes sensitive information before Lake Formation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that perodically Queries and removes sensitive information from",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n202 \nWhat should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket \nare encrypted?",
    "options": [
      {
        "id": 0,
        "text": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo ensure that all objects uploaded to an Amazon S3 bucket are encrypted, the solutions architect should update the bucket policy to deny any PutObject requests that do not have an x- amz-server-side-encryption header set. This will prevent any objects from being uploaded to the bucket unless they are encrypted using server-side encryption. https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A solutions architect is designing a multi-tier application for a company. The application's users \nupload images from a mobile device. The application generates a thumbnail of each image and \nreturns a message to the user to confirm that the image was uploaded successfully. \n \nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster \nresponse time to its users to notify them that the original image was received. The solutions \narchitect must design the application to asynchronously dispatch requests to the different \napplication tiers. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 19,
    "text": "A solution architect needs to assign a new microsoft for a company's application. Clients must be \nable to call an HTTPS endpoint to reach the micoservice. The microservice also must use AWS \nidentity and Access Management (IAM) to authentication calls. The soltions architect will write the \nlogic for this microservice by using a single AWS Lambda function that is written in Go 1.x. \nWhich solution will deploy the function in the in the MOST operationally efficient way? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n203",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribuion. Deploy the function to CloudFront Functions. Specify",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A company wants to implement a disaster recovery plan for its primary on-premises file storage \nvolume. The file storage volume is mounted from an Internet Small Computer Systems Interface \n(iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes \n(TB) of data. \n \nThe company wants to ensure that end users retain immediate access to all file types from the \non-premises systems without experiencing latency. \n \nWhich solution will meet these requirements with the LEAST amount of change to the company's \nexisting infrastructure?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A company is hosting a web application from an Amazon S3 bucket. The application uses \nAmazon Cognito as an identity provider to authenticate users and return a JSON Web Token \n(JWT) that provides access to protected resources that are stored in another S3 bucket. \n \nUpon deployment of the application, users report errors and are unable to access the protected \ncontent. A solutions architect must resolve this issue by providing proper permissions so that \nusers can access the protected content. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n204 \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the Amazon Cognito identity pool to assume the proper IAM role for access to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Update the S3 ACL to allow the application to access the protected content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/tutorial-create-identity-pool.html You have to create an custom role such as read-only.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "An image hosting company uploads its large assets to Amazon S3 Standard buckets. The \ncompany uses multipart upload in parallel by using S3 APIs and overwrites if the same object is \nuploaded again. For the first 30 days after upload, the objects will be accessed frequently. The \nobjects will be used less frequently after 30 days, but the access patterns for each object will be \ninconsistent. The company must optimize its S3 storage costs while maintaining high availability \nand resiliency of stored assets. \n \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Move assets to S3 Intelligent-Tiering after 30 days.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an S3 Lifecycle policy to clean up expired object delete markers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nS3 Intelligent-Tiering - Data with unknown, changing, or unpredictable access patterns and moves objects that have not been accessed in 30 consecutive days to the Infrequent Access tier. https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 \ninstances contain highly sensitive data and run in a private subnet. According to company policy, \nthe EC2 instances that run in the VPC can access only approved third-party software repositories \non the internet for software product updates that use the third party's URL. Other internet traffic \nmust be blocked. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the route table for the private subnet to route the outbound traffic to an AWS Network",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement strict inbound security group rules. Configure an outbound rule that allows traffic only",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nSend the outbound connection from EC2 to Network Firewall. In Network Firewall, create stateful outbound rules to allow certain domains for software patch download and deny all other domains. https://docs.aws.amazon.com/network-firewall/latest/developerguide/suricata- examples.html#suricata-example-domain-filtering\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nStatic content can include images and style sheets that are the same across all users and are best cached at the edges of the content distribution network (CDN). Dynamic content includes information that changes frequently or is personalized based on user preferences, behavior, location or other factors - all content is sales requests.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions \narchitect needs to provide a solution that will run regular security scans across a large fleet of \nEC2 instances. The solution should also patch the EC2 instances on a regular schedule and \nprovide a report of each instance's patch status. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on Amazon Web Services (AWS). It automatically assesses applications for vulnerabilities or deviations from best practices. Amazon Inspector can be used to identify security issues and recommend fixes for them. It is an ideal solution for running regular security scans across a large fleet of EC2 instances. AWS Systems Manager Patch Manager is a service that helps you automate the process of patching Windows and Linux instances. It provides a simple, automated way to patch your instances with the latest security patches and updates. Patch Manager helps you maintain compliance with security policies and regulations by providing detailed reports on the patch status of your instances.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company is planning to store data on Amazon RDS DB instances. The company must encrypt \nthe data at rest. \n \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTo encrypt data at rest in Amazon RDS, you can use the encryption feature of Amazon RDS, which uses AWS Key Management Service (AWS KMS). With this feature, Amazon RDS encrypts each database instance with a unique key. This key is stored securely by AWS KMS. You can manage your own keys or use the default AWS-managed keys. When you enable encryption for a DB instance, Amazon RDS encrypts the underlying storage, including the automated backups, read replicas, and snapshots.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The \ncompany's network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowball.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS DataSync.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a secure VPN connection.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Transfer Acceleration.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS Snowball is a secure data transport solution that accelerates moving large amounts of data into and out of the AWS cloud. It can move up to 80 TB of data at a time, and provides a network bandwidth of up to 50 Mbps, so it is well-suited for the task. Additionally, it is secure and easy to use, making it the ideal solution for this migration. https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A company needs to provide its employees with secure access to confidential and sensitive files. \nThe company wants to ensure that the files can be accessed only by authorized users. The files \nmust be downloaded securely to the employees' devices. \nThe files are stored in an on-premises Windows file server. However, due to an increase in \nremote usage, the file server is running out of capacity. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIt provides a secure way for employees to access confidential and sensitive files from anywhere using AWS Client VPN. The Amazon FSx for Windows File Server file system is designed to provide native support for Windows file system features such as NTFS permissions, Active Directory integration, and Distributed File System (DFS). This means that the company can continue to use their on-premises Active Directory to manage user access to files.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company's application runs on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability \nZones. On the first day of every month at midnight, the application becomes much slower when \nthe month-end financial calculation batch runs. This causes the CPU utilization of the EC2 \ninstances to immediately peak to 100%, which disrupts the application. \n \nWhat should a solutions architect recommend to ensure the application is able to handle the \nworkload and avoid downtime?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon CloudFront distribution in front of the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "A company wants to give a customer the ability to use on-premises Microsoft Active Directory to \ndownload files that are stored in Amazon S3. The customer's application uses an SFTP client to \ndownload the files. \n \nWhich solution will meet these requirements with the LEAST operational overhead and no \nchanges to the customer's application?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up AWS DataSync to synchronize between the on-premises location and the S3 location by",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/transfer/latest/userguide/directory-services-users.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A company is experiencing sudden increases in demand. The company needs to provision large \nAmazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto \nScaling group. The company needs a solution that provides minimum initialization latency to meet \nthe demand. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nEnabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot allows you to quickly create a new Amazon Machine Image (AMI) from a snapshot, which can help reduce the initialization latency when provisioning new instances. Once the AMI is provisioned, you can replace the AMI in the Auto Scaling group with the new AMI. This will ensure that new instances are launched from the updated AMI and are able to meet the increased demand quickly. Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for \nstorage. The application tier is hosted on Amazon EC2 instances. The company's IT security \nguidelines mandate that the database credentials be encrypted and rotated every 14 days. \n \nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTo implement password rotation lifecycles, use AWS Secrets Manager. You can rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle using Secrets Manager. https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials- amazon-rds-database-types-oracle/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company has deployed a web application on AWS. The company hosts the backend database \non Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling \nneeds. The read replicas must lag no more than 1 second behind the primary DB instance. The \ndatabase routinely runs scheduled stored procedures. \n \nAs traffic on the website increases, the replicas experience additional lag during periods of peak \nload. A solutions architect must reduce the replication lag as much as possible. The solutions \narchitect must minimize changes to the application code and must minimize ongoing operational \noverhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas,",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large,",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nYou can scale reads for your Amazon RDS for PostgreSQL DB instance by adding read replicas to the instance. As with other Amazon RDS database engines, RDS for PostgreSQL uses the native replication mechanisms of PostgreSQL to keep read replicas up to date with changes on the source DB. For general information about read replicas and Amazon RDS, see Working with read replicas. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PostgreSQL.Replication.Re adReplicas.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a \nservice (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB \ncluster. \nThe DR plan must replicate data to a secondary AWS Region. \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAn Aurora DB cluster can contain up to 15 Aurora Replicas. The Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans WITHIN an AWS Region. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html You can replicate data across multiple Regions by using an Aurora global database.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 35,
    "text": "A company has a custom application with embedded credentials that retrieves information from \nan Amazon RDS MySQL DB instance. Management says the application must be made more \nsecure with the least amount of programming effort. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://ws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically- with-aws-secrets-manager/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A media company hosts its website on AWS. The website application's architecture includes a \nfleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is \nhosted on Amazon Aurora. The company's cybersecurity team reports that the application is \nvulnerable to SQL injection. \n \nHow should the company resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an ALB listener rule to reply to SQL injections with a fixed response.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon Inspector to block all SQL injection attempts automatically.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nProtect against SQL injection and cross-site scripting To protect your applications against SQL injection and cross-site scripting (XSS) attacks, use the built-in SQL injection and cross-site scripting engines. Remember that attacks can be performed on different parts of the HTTP request, such as the HTTP header, query string, or URI. Configure the AWS WAF rules to inspect different parts of the HTTP request against the built-in mitigation engines.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company \nwants to create a visualization in Amazon QuickSight by joining the data in the data lake with \noperational data that is stored in an Amazon Aurora MySQL database. The company wants to \nenforce column-level authorization so that the company's marketing team can access only a \nsubset of columns in the database. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution leverages AWS Lake Formation to ingest data from the Aurora MySQL database into the S3 data lake, while enforcing column-level access control for QuickSight users. Lake Formation can be used to create and manage the data lake's metadata and enforce security and governance policies, including column-level access control. This solution then uses Amazon Athena as the data source in QuickSight to query the data in the S3 data lake. This solution minimizes operational overhead by leveraging AWS services to manage and secure the data, and by using a standard query service (Amazon Athena) to provide a SQL interface to the data. https://aws.amazon.com/blogs/big-data/enforce-column-level-authorization-with-amazon- quicksight-and-aws-lake-formation/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 \ninstances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, \nbut the baseline CPU utilization that is noted on each run is at least 60%. The company needs to \nprovision the capacity 30 minutes before the jobs run. \n \nCurrently, engineers complete this task by manually modifying the Auto Scaling group \nparameters. The company does not have the resources to analyze the required capacity trends \nfor the Auto Scaling group counts. The company needs an automated way to modify the Auto \nScaling group's desired capacity. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 39,
    "text": "A solutions architect is designing a company's disaster recovery (DR) architecture. The company \nhas a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled \nbackup. The DR design needs to include multiple AWS Regions. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n213",
    "options": [
      {
        "id": 0,
        "text": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to \nparse messages. The application cannot parse messages that are larger than 256 KB in size. \nThe company wants to implement a solution to give the application the ability to parse messages \nas large as 50 MB. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
    "options": [
      {
        "id": 0,
        "text": "Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the limit in Amazon SQS to handle messages that are larger than 256 KB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nTo send messages larger than 256 KiB, you can use the Amazon SQS Extended Client Library for Java. This library allows you to send an Amazon SQS message that contains a reference to a message payload in Amazon S3. The maximum payload size is 2 GB. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas- messages.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company wants to restrict access to the content of one of its main web applications and to \nprotect the content by using authorization techniques available on AWS. The company wants to \nimplement a serverless architecture and an authentication solution for fewer than 100 users. The \nsolution needs to integrate with the main web application and serve web content globally. The \nsolution must also scale as the company's user base grows while providing the lowest login \nlatency possible. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon CloudFront is a global content delivery network (CDN) service that can securely deliver web content, videos, and APIs at scale. It integrates with Cognito for authentication and with Lambda@Edge for authorization, making it an ideal choice for serving web content globally. Lambda@Edge is a service that lets you run AWS Lambda functions globally closer to users, providing lower latency and faster response times. It can also handle authorization logic at the edge to secure content in CloudFront. For this scenario, Lambda@Edge can provide authorization for the web application while leveraging the low-latency benefit of running at the edge.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A company has an aging network-attached storage (NAS) array in its data center. The NAS array \npresents SMB shares and NFS shares to client workstations. The company does not want to \npurchase a new NAS array. The company also does not want to incur the cost of renewing the \nNAS array's support contract. Some of the data is accessed frequently, but much of the data is \ninactive. \n \nA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 \nLifecycle policies, and maintains the same look and feel for the client workstations. The solutions \narchitect has identified AWS Storage Gateway as part of the solution. \n \nWhich type of storage gateway should the solutions architect provision to meet these \nrequirements?",
    "options": [
      {
        "id": 0,
        "text": "Volume Gateway",
        "correct": false
      },
      {
        "id": 1,
        "text": "Tape Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon FSx File Gateway",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 File Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon S3 File Gateway provides a file interface to objects stored in S3. It can be used for a file- based interface with S3, which allows the company to migrate their NAS array data to S3 while maintaining the same look and feel for client workstations. Amazon S3 File Gateway supports SMB and NFS protocols, which will allow clients to continue to access the data using these protocols. Additionally, Amazon S3 Lifecycle policies can be used to automate the movement of data to lower-cost storage tiers, reducing the storage cost of inactive data. https://aws.amazon.com/about-aws/whats-new/2018/06/aws-storage-gateway-adds-smb-support- to-store-objects-in-amazon-s3/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 43,
    "text": "A company has an application that is running on Amazon EC2 instances. A solutions architect \nhas standardized the company on a particular instance family and various instance sizes based \non the current needs of the company. \n \nThe company wants to maximize cost savings for the application over the next 3 years. The \ncompany needs to be able to change the instance family and sizes in the next 6 months based on \napplication popularity and usage. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n215 \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Compute Savings Plan",
        "correct": true
      },
      {
        "id": 1,
        "text": "EC2 Instance Savings Plan",
        "correct": false
      },
      {
        "id": 2,
        "text": "Zonal Reserved Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Standard Reserved Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nCompute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, Region, OS or tenancy, and also apply to Fargate or Lambda usage. EC2 Instance Savings Plans provide the lowest prices, offering savings up to 72% in exchange for commitment to usage of individual instance families in a Region https://aws.amazon.com/savingsplans/compute-pricing/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company collects data from a large number of participants who use wearable devices. The \ncompany stores the data in an Amazon DynamoDB table and uses applications to analyze the \ndata. The data workload is constant and predictable. The company wants to stay at or below its \nforecasted budget for DynamoDB. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\n\"The data workload is constant and predictable.\" https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html \"With provisioned capacity you pay for the provision of read and write capacity units for your DynamoDB tables. Whereas with DynamoDB on-demand you pay per request for the data reads and writes that your application performs on your tables.\"\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 45,
    "text": "A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-\nsoutheast-3 Region. The database is encrypted with an AWS Key Management Service (AWS \nKMS) customer managed key. The company was recently acquired and must securely share a \nbackup of the database with the acquiring company's AWS account in ap-southeast-3. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a database snapshot. Add the acquiring company's AWS account to the KMS key policy.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a database snapshot. Download the database snapshot. Upload the database snapshot to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/premiumsupport/knowledge-center/aurora-share-encrypted-snapshot/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the \nus-east-1 Region to store customer transactions. The company needs high availability and \nautomatic recovery for the DB instance. \n \nThe company must also run reports on the RDS database several times a year. The report \nprocess causes transactions to take longer than usual to post to the customers' accounts. The \ncompany needs a solution that will improve the performance of the report process. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read replica of the DB instance in a different Availability Zone. Point all requests for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to RDS Custom.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use RDS Proxy to limit reporting requests to the maintenance window.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 47,
    "text": "A company is moving its data management application to AWS. The company wants to transition \nto an event-driven architecture. The architecture needs to be more distributed and to use \nserverless concepts while performing the different aspects of the workflow. The company also \nwants to minimize operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process",
        "correct": false
      },
      {
        "id": 1,
        "text": "Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company is designing the network for an online multi-player game. The game uses the UDP \nnetworking protocol and will be deployed in eight AWS Regions. The network architecture needs \nto minimize latency and packet loss to give end users a high-quality gaming experience. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Setup a transit gateway in each Region. Create inter-Region peering attachments between each",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nGlobal Accelerator supports the User Datagram Protocol (UDP) and Transmission Control Protocol (TCP), making it an excellent choice for an online multi-player game using UDP networking protocol. By setting up Global Accelerator with UDP listeners and endpoint groups in each Region, the network architecture can minimize latency and packet loss, giving end users a high-quality gaming experience.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company hosts a three-tier web application on Amazon EC2 instances in a single Availability \nZone. The web application uses a self-managed MySQL database that is hosted on an EC2 \ninstance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL \ndatabase currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects \ntraffic of 1,000 IOPS for both reads and writes at peak traffic. \n \nThe company wants to minimize any disruptions, stabilize performance, and reduce costs while \nretaining the capacity for double the IOPS. The company wants to move the database tier to a \nfully managed solution that is highly available and fault tolerant. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Intelligent-Tiering access tiers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use two large EC2 instances to host the database in active-passive mode.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nRDS does not support IO2 or IO2express . GP2 can do the required IOPS. RDS supported Storage > https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html GP2 max IOPS > https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/general-purpose.html#gp2- performance Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 50,
    "text": "A company hosts a serverless application on AWS. The application uses Amazon API Gateway, \nAWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase \nin application errors that result from database connection timeouts during times of peak traffic or \nunpredictable traffic. The company needs a solution that reduces the application failures with the \nleast amount of change to the code. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Reduce the Lambda concurrency rate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable RDS Proxy on the RDS DB instance.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Resize the RDS DB instance class to accept more connections.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon DynamoDB with on-demand scaling.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://aws.amazon.com/rds/proxy/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "A company is migrating an old application to AWS. The application runs a batch job every hour \nand is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. \nThe server has 64 virtual CPU (vCPU) and 512 GiB of memory. \n \nWhich solution will run the batch job within 15 minutes with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda with functional scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Lightsail with AWS Auto Scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Batch on Amazon EC2.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Batch is a fully-managed service that can launch and manage the compute resources needed to execute batch jobs. It can scale the compute environment based on the size and timing of the batch jobs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 52,
    "text": "A company stores its data objects in Amazon S3 Standard storage. A solutions architect has \nfound that 75% of the data is rarely accessed after 30 days. The company needs all the data to \nremain immediately accessible with the same high availability and resiliency, but the company \nwants to minimize storage costs. \n \nWhich storage solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the data objects to S3 Glacier Deep Archive after 30 days.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nMove the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days - will meet the requirements of keeping the data immediately accessible with high availability and resiliency, while minimizing storage costs. S3 Standard-IA is designed for infrequently accessed data, and it provides a lower storage cost than S3 Standard, while still offering the same low latency, high throughput, and high durability as S3 Standard.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company has a three-tier application on AWS that ingests sensor data from its users devices. \nThe traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the \nweb tier, and finally to EC2 instances for the application tier. The application tier makes calls to a \ndatabase. \n \nWhat should a solutions architect do to improve the security of the data in transit?",
    "options": [
      {
        "id": 0,
        "text": "Configure a TLS listener. Deploy the server certificate on the NLB.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS Shield Advanced. Enable AWS WAF on the NLB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nNetwork Load Balancers now support TLS protocol. With this launch, you can now offload resource intensive decryption/encryption from your application servers to a high throughput, and low latency Network Load Balancer. Network Load Balancer is now able to terminate TLS traffic and set up connections with your targets either over TCP or TLS protocol. https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html https://exampleloadbalancer.com/nlbtls_demo.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A social media company runs its application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The \napplication has more than a billion images stored in an Amazon S3 bucket and processes \nthousands of images each second. The company wants to resize the images dynamically and \nserve appropriate formats to clients. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Install an external image management library on an EC2 instance. Use the image management",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a CloudFront origin request policy. Use the policy to automatically resize images and to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Lambda@Edge function with an external image management library. Associate the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a CloudFront response headers policy. Use the policy to automatically resize images and",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nhttps://aws.amazon.com/cn/blogs/networking-and-content-delivery/resizing-images-with-amazon- cloudfront-lambdaedge-aws-cdn-blog/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 55,
    "text": "A hospital needs to store patient records in an Amazon S3 bucket. The hospital's compliance \nteam must ensure that all protected health information (PHI) is encrypted in transit and at rest. \nThe compliance team must administer the encryption key for data at rest. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nIt allows the compliance team to manage the KMS keys used for server-side encryption, thereby providing the necessary control over the encryption keys. Additionally, the use of the \"aws:SecureTransport\" condition on the bucket policy ensures that all connections to the S3 bucket are encrypted in transit.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A data analytics company wants to migrate its batch processing system to AWS. The company \nreceives thousands of small data files periodically during the day through FTP. A on-premises \nbatch job processes the data files overnight. However, the batch job takes hours to finish running. \nThe company wants the AWS solution to process incoming data files are possible with minimal \nchanges to the FTP clients that send the files. The solution must delete the incoming data files \nthe files have been processed successfully. Processing for each file needs to take 3-8 minutes. \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 57,
    "text": "A company is migrating a Linux-based web server group to AWS. The web servers must access \nfiles in a shared file store for some content. The company must not make any changes to the \napplication. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon S3 Standard bucket with access to the web servers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. \nThe application has three tiers, a business tier, and a database tier with Microsoft SQL Server. \nThe company wants to use specific features of SQL Server such as native backups and Data \nQuality Services. The company also needs to share files for process between the tiers. \n \nHow should a solution architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host all three on Amazon instances. Use Mmazon FSx File Gateway for file sharing between",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host all three on Amazon EC2 instances. Use Amazon FSx for Windows file sharing between the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nData Quality Services: If this feature is critical to your workload, consider choosing Amazon RDS Custom or Amazon EC2. https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/comparison.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The \ncompany has VPCs with public subnets and private subnets in its AWS account. The EC2 \ninstances run in a private subnet in one of the VPCs. The Lambda functions need direct network \naccess to the EC2 instances for the application to work. \n \nThe application will run for at least 1 year. The company expects the number of Lambda functions \nthat the application uses to increase during that time. The company wants to maximize its savings \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n222 \non all application resources and to keep network latency between the services low. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a Compute Savings Plan. Optimize the Lambda functions duration and memory usage,",
        "correct": true
      },
      {
        "id": 3,
        "text": "Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage,",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nBy purchasing a Compute Savings Plan, the company can save on the costs of running both EC2 instances and Lambda functions. The Lambda functions can be connected to the private subnet that contains the EC2 instances through a VPC endpoint for AWS services or a VPC peering connection. This provides direct network access to the EC2 instances while keeping the traffic within the private network, which helps to minimize network latency. Optimizing the Lambda functions duration, memory usage, number of invocations, and amount of data transferred can help to further minimize costs and improve performance. Additionally, using a private subnet helps to ensure that the EC2 instances are not directly accessible from the public internet, which is a security best practice.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A company is building a mobile app on AWS. The company wants to expand its reach to millions \nof users. The company needs to build a platform so that authorized users can watch the \ncompany's content on their mobile devices. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up IPsec VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront Provide signed URLs to stream content.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up AWS Client VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront supports signed URLs that provide authorized access to your content. This feature allows the company to control who can access their content and for how long, providing a secure and scalable solution for millions of users. https://www.amazonaws.cn/en/cloudfront/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n223 \nA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nStatic content can include images and style sheets that are the same across all users and are best cached at the edges of the content distribution network (CDN). Dynamic content includes information that changes frequently or is personalized based on user preferences, behavior, location or other factors - all content is sales requests.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A companys web application consists of an Amazon API Gateway API in front of an AWS \nLambda function and an Amazon DynamoDB database. The Lambda function handles the \nbusiness logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito \nuser pools to identify the individual users of the application. A solutions architect needs to update \nthe application so that only users who have a subscription can access premium content. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Enable API caching and throttling on the API Gateway API.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS WAF on the API Gateway API Create a rule to filter users who have a subscription.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Apply fine-grained IAM permissions to the premium content in the DynamoDB table.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement API usage plans and API keys to limit the access of users who do not have a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nTo meet the requirement with the least operational overhead, you can implement API usage plans and API keys to limit the access of users who do not have a subscription. This way, you can control access to your API Gateway APIs by requiring clients to submit valid API keys with requests. You can associate usage plans with API keys to configure throttling and quota limits on individual client accounts. https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage- plans.html Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company's application runs on AWS. The application stores large documents in an Amazon S3 \nbucket that uses the S3 Standard-infrequent Access (S3 Standerd-IA) storage class. The \ncompany will continue paying to store the data but wants to save on its total S3 costs. The \ncompany wants authorized external users to have the ability to access the documents in \nmilliseconds. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the S3 bucket to be a Requester Pays bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the storage tier to S3 Standard for all existing and future objects.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on S3 Transfer Acceleration tor the S3 Docket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront to handle all the requests to the S3 bucket.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 64,
    "text": "A company recently created a disaster recovery site in a different AWS Region. The company \nneeds to transfer large amounts of data back and forth between NFS file systems in the two \nRegions on a periodic basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Snowball devices",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an SFTP server on Amazon EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS DataSync is a fully managed data transfer service that simplifies moving large amounts of data between on-premises storage systems and AWS services. It can also transfer data between different AWS services, including different AWS Regions. DataSync provides a simple, scalable, and automated solution to transfer data, and it minimizes the operational overhead because it is fully managed by AWS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  }
]