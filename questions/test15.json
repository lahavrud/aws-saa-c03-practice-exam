[
  {
    "id": 0,
    "text": "A company recently created a disaster recovery site in a different AWS Region. The company \nneeds to transfer large amounts of data back and forth between NFS file systems in the two \nRegions on a periodic basis. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Snowball devices.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an SFTP server on Amazon EC2.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAWS DataSync is a fully managed data transfer service that simplifies moving large amounts of data between on-premises storage systems and AWS services. It can also transfer data between different AWS services, including different AWS Regions. DataSync provides a simple, scalable, and automated solution to transfer data, and it minimizes the operational overhead because it is fully managed by AWS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same \nVPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure \nthat enough funds are available before a stock can be purchased. The company has noticed in \nthe VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web \nservice over the internet instead of through the VPC. A solutions architect must implement a \nsolution so that the APIs communicate through the VPC. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
    "options": [
      {
        "id": 0,
        "text": "Add an X-API-Key header in the HTTP header for authorization.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an interface endpoint.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use a gateway endpoint.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it uses Amazon GuardDuty to monitor malicious activity on data stored in Amazon S3 and Amazon Inspector to check for vulnerabilities on Amazon EC2 instances. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, including EC2 instances, by identifying software vulnerabilities and unintended network exposure.\n\n**Why option 0 is incorrect:**\nis incorrect because while GuardDuty can detect malicious activity, it's not primarily designed for vulnerability assessments on EC2 instances. Inspector is the service designed for that purpose. GuardDuty can provide some security findings related to EC2, but Inspector provides a more comprehensive vulnerability assessment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A company wants to run an in-memory database for a latency-sensitive application that runs on \nAmazon EC2 instances. The application processes more than 100,000 transactions each minute \nand requires high network throughput. A solutions architect needs to provide a cost-effective \nnetwork design that minimizes data transfer charges. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n195 \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a",
        "correct": true
      },
      {
        "id": 1,
        "text": "Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it configures a lifecycle policy to transition objects to S3 One Zone-IA after 30 days. S3 One Zone-IA offers a lower storage cost compared to S3 Standard and S3 Standard-IA. While the question states high access for the first few days and reduced access after a week, transitioning after 30 days still addresses the requirement of cost reduction while maintaining immediate accessibility. S3 One Zone-IA is suitable because the assets are re-creatable, mitigating the risk of data loss in a single availability zone. The infrequent access requirement is also met.\n\n**Why option 1 is incorrect:**\nis incorrect because S3 Standard-IA, while cheaper than S3 Standard, is more expensive than S3 One Zone-IA. Since the assets are re-creatable, the slightly higher availability of S3 Standard-IA is not necessary, and the agency's primary goal is cost reduction. Transitioning after 7 days also suffers from the same timing issue as option 0.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 3,
    "text": "A company that primarily runs its application servers on premises has decided to migrate to AWS. \nThe company wants to minimize its need to scale its Internet Small Computer Systems Interface \n(iSCSI) storage on premises. The company wants only its recently accessed data to remain \nstored locally. \n \nWhich AWS solution should the company use to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 File Gateway",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS Storage Gateway Tape Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Storage Gateway Volume Gateway stored volumes",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway Volume Gateway cached volumes",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Server-Side Encryption with AWS Key Management Service keys (SSE-KMS), is the best solution. SSE-KMS allows S3 to encrypt the data using keys managed by KMS. This fulfills the requirement of encrypting data at rest in S3. Importantly, KMS provides a full audit trail via AWS CloudTrail, showing when the key was used, by whom, and from which IP address. This satisfies the audit requirement. The startup doesn't have to manage the keys directly, as KMS handles the key management aspects.\n\n**Why option 0 is incorrect:**\nusing Server-Side Encryption with Amazon S3 managed keys (SSE-S3), is incorrect because while it encrypts the data at rest in S3 and the startup doesn't manage the keys, it does NOT provide an audit trail of key usage. SSE-S3 is the simplest encryption option, but lacks the auditing capabilities required by the question.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nusing client-side encryption with client-provided keys, is incorrect because it places the burden of encryption and key management entirely on the startup. The question explicitly states the startup does not want to manage the encryption keys. Also, while the data is encrypted before reaching S3, it adds complexity to the application and doesn't leverage AWS's built-in encryption features.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 4,
    "text": "A company has multiple AWS accounts that use consolidated billing. The company runs several \nactive high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The \ncompany's finance team has access to AWS Trusted Advisor in the consolidated billing account \nand all other AWS accounts. \n \nThe finance team needs to use the appropriate AWS account to access the Trusted Advisor \ncheck recommendations for RDS. The finance team must review the appropriate Trusted Advisor \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n196 \ncheck to reduce RDS costs. \n \nWhich combination of steps should the finance team take to meet these requirements? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Use the Trusted Advisor recommendations from the account where the RDS instances are",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Review the Trusted Advisor check for Amazon RDS Idle DB Instances.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Amazon ElastiCache for Redis is an in-memory data store specifically designed for low-latency read and write operations. It offers high availability through replication and automatic failover. Redis's data structures are well-suited for leaderboard implementations, allowing for efficient ranking and retrieval of user data. Option 3 is also correct because DynamoDB with DAX (DynamoDB Accelerator) provides an in-memory cache layer for DynamoDB. DAX significantly improves read performance by caching frequently accessed data, reducing latency and improving the overall responsiveness of the leaderboard application. DynamoDB itself provides high availability and scalability, and DAX enhances its performance for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB offers low latency and high availability, it is not an in-memory data store by default. It is a NoSQL database that stores data on SSDs. While it can provide fast read/write speeds, it doesn't match the performance characteristics of a true in-memory solution for the specific requirements of a real-time leaderboard.\n\n**Why option 2 is incorrect:**\nis incorrect because Amazon RDS for Aurora is a relational database service. While Aurora offers good performance, it is not an in-memory data store and is not optimized for the ultra-low latency requirements of a real-time leaderboard. It is designed for transactional workloads and not for the high-frequency read/write operations typical of a leaderboard.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nis incorrect because Amazon Neptune is a graph database service. While it provides low latency for graph-based queries, it is not the best choice for a simple leaderboard application. The data structure and query patterns of a leaderboard are better suited for key-value stores or sorted sets, which are offered by ElastiCache for Redis or DynamoDB with DAX. Neptune is optimized for complex relationship analysis, which is not a primary requirement for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 5,
    "text": "A solutions architect needs to optimize storage costs. The solutions architect must identify any \nAmazon S3 buckets that are no longer being accessed or are rarely accessed. \n \nWhich solution will accomplish this goal with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity",
        "correct": true
      },
      {
        "id": 1,
        "text": "Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and Amazon CloudFront for S3, is the most efficient solution. DAX is a fully managed, highly available, in-memory cache for DynamoDB. It significantly improves read performance by caching frequently accessed data, reducing the load on DynamoDB and lowering latency. Amazon CloudFront is a content delivery network (CDN) that caches static content like images from S3 at edge locations globally. This reduces latency for users accessing the static content and offloads traffic from the S3 bucket. Since 90% of read requests are for commonly accessed data, both DAX and CloudFront will provide substantial performance improvements.\n\n**Why option 1 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and Amazon CloudFront for S3, is less efficient than using DAX. While ElastiCache Redis can be used as a cache, DAX is specifically designed for DynamoDB and offers tighter integration and easier management. DAX also handles invalidation and consistency automatically, which would need to be manually managed with ElastiCache Redis. CloudFront for S3 is a good choice for caching static content.\n\n**Why option 2 is incorrect:**\nenabling Amazon DynamoDB Accelerator (DAX) for DynamoDB and ElastiCache Memcached for S3, is not ideal. DAX is a good choice for DynamoDB caching. However, ElastiCache Memcached is not the best choice for caching static content in S3. CloudFront is a more suitable CDN for this purpose, offering global edge locations and integration with S3.\n\n**Why option 3 is incorrect:**\nenabling ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3, is the least efficient option. While ElastiCache Redis can be used as a cache for DynamoDB, DAX is a better-suited and more tightly integrated solution. ElastiCache Memcached is not the best choice for caching static content in S3; CloudFront is a more appropriate CDN.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company sells datasets to customers who do research in artificial intelligence and machine \nlearning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket \nin the us-east-1 Region. The company hosts a web application that the customers use to \npurchase access to a given dataset. The web application is deployed on multiple Amazon EC2 \ninstances behind an Application Load Balancer. After a purchase is made, customers receive an \nS3 signed URL that allows access to the files. \n \nThe customers are distributed across North America and Europe. The company wants to reduce \nthe cost that is associated with data transfers and wants to maintain or improve performance. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n197",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the web application to enable streaming of the datasets to end users. Configure the web",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThe correct answer is 'Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS'. Here's why:\n\n*   **Amazon S3 Standard:** S3 charges only for the storage used. For 1 GB, the cost will be relatively low.\n*   **Amazon EFS Standard:** EFS also charges for the storage used. While the blogger only stored 1 GB, EFS has a minimum storage duration charge. This makes it more expensive than S3 for small amounts of data stored for short periods.\n*   **Amazon EBS (General Purpose SSD gp2):** EBS charges for the *provisioned* storage, not the used storage. The blogger provisioned 100 GB, so they will be charged for the entire 100 GB, even though they only used 1 GB. This makes EBS the most expensive option in this scenario.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because EFS is more expensive than S3 due to minimum storage duration charges, and EBS is the most expensive due to the provisioned storage model.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company is using AWS to design a web application that will process insurance quotes. Users \nwill request quotes from the application. Quotes must be separated by quote type, must be \nresponded to within 24 hours, and must not get lost. The solution must maximize operational \nefficiency and must minimize maintenance. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because S3 Object Lock allows different versions of a single object to have different retention modes (Governance or Compliance) and retention periods. This is crucial for managing data lifecycle and compliance requirements where different versions might have varying retention needs.\nOption 3 is correct because when applying a retention period to an object version explicitly, you specify a 'Retain Until Date' for that specific object version. This date determines when the object version becomes eligible for deletion. This is a fundamental aspect of how S3 Object Lock enforces retention.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because explicit retention modes or periods set on an object version take precedence over bucket default settings. Bucket default settings apply only when no explicit retention is set on the object version itself.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company has an application that runs on several Amazon EC2 instances. Each EC2 instance \nhas multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The \napplication's EC2 instance configuration and data need to be backed up nightly. The application \nalso needs to be recoverable in a different AWS Region. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Lustre is the best choice because it is designed for high-performance computing (HPC) workloads like EDA that require parallel processing and fast storage. It provides a high-performance, scalable file system optimized for compute-intensive applications. It can handle the 'hot data' requirements effectively. Furthermore, FSx for Lustre can be integrated with Amazon S3 for cost-effective storage of 'cold data'. Data can be moved between FSx for Lustre and S3 based on access patterns, providing a tiered storage solution.\n\n**Why option 0 is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based applications and shared file storage. While it provides file storage, it is not optimized for the high-performance, parallel processing requirements of EDA 'hot data'. It is more suitable for general-purpose file sharing within a Windows environment.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company is building a mobile app on AWS. The company wants to expand its reach to millions \nof users. The company needs to build a platform so that authorized users can watch the \ncompany's content on their mobile devices. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up IPsec VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon CloudFront. Provide signed URLs to stream content.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up AWS Client VPN between the mobile app and the AWS environment to stream content.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nOptions 1 and 3 are the correct answers because they represent invalid lifecycle transitions in Amazon S3. \n\n*   **Option 1: Amazon S3 Intelligent-Tiering => Amazon S3 Standard:** This transition is invalid. Intelligent-Tiering automatically moves objects between frequent, infrequent, and archive access tiers based on access patterns. You cannot directly transition an object *from* Intelligent-Tiering *to* Standard. Intelligent-Tiering manages the tiering automatically. To move an object to Standard, you would need to copy the object to a new object in the Standard storage class. \n\n*   **Option 3: Amazon S3 One Zone-IA => Amazon S3 Standard-IA:** This transition is invalid. One Zone-IA stores data in a single Availability Zone, making it cheaper but less resilient than Standard-IA, which stores data in multiple Availability Zones. You cannot directly transition from One Zone-IA to Standard-IA. To achieve this, you would need to copy the object to a new object in the Standard-IA storage class. The key reason is the difference in data redundancy and availability guarantees. One Zone-IA is designed for data that can tolerate loss of availability in a single AZ, whereas Standard-IA is designed for higher availability with multi-AZ redundancy.\n\n**Why option 0 is incorrect:**\nis incorrect because transitioning from Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering is a valid lifecycle transition. Standard-IA is for infrequently accessed data, and Intelligent-Tiering can further optimize costs by automatically moving data between tiers based on access patterns. Therefore, it's a logical and supported transition.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 10,
    "text": "A company hosts a three-tier web application that includes a PostgreSQL database The database \nstores the metadata from documents The company searches the metadata for key terms to \nretrieve documents that the company reviews in a report each month The documents are stored \nin Amazon S3 The documents are usually written only once, but they are updated frequency The \nreporting process takes a few hours with the use of relational queries The reporting process must \nnot affect any document modifications or the addition of new documents. \nWhat are the MOST operationally efficient solutions that meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a new Amazon RDS for PostgreSQL Reserved Instance and an On-Demand read replica",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up a new Amazon Aurora PostgreSQL DB cluster that includes a Reserved Instance and an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a new Amazon RDS for PostgreSQL Multi-AZ Reserved Instance Configure the reporting",
        "correct": false
      },
      {
        "id": 4,
        "text": "Set up a new Amazon DynamoDB table to store the documents Use a fixed write capacity to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it utilizes AWS Directory Service AD Connector and AWS IAM Identity Center (successor to AWS SSO). AD Connector allows AWS to connect to the on-premises Active Directory without replicating the directory in AWS, reducing operational overhead. IAM Identity Center provides centralized management of access to multiple AWS accounts within an AWS Organization. Permission sets in IAM Identity Center can be assigned based on Active Directory group membership, simplifying access control and ensuring compliance. This approach minimizes ongoing operational management by leveraging managed AWS services for identity federation and access control.\n\n**Why option 0 is incorrect:**\nis incorrect because while it establishes a trust relationship between AWS Directory Service for Microsoft Active Directory and the on-premises AD, it requires deploying and managing a full-fledged Active Directory in AWS. This increases operational overhead compared to AD Connector. Also, managing IAM roles linked to AD groups directly can become complex across multiple accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying and managing an open-source identity provider (IdP) on Amazon EC2 adds significant operational overhead. It requires managing the EC2 instance, the IdP software, and the synchronization with Active Directory. While SAML federation is a valid approach, using a managed service like IAM Identity Center is more efficient and reduces the operational burden.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company experienced a breach that affected several applications in its on-premises data \ncenter. The attacker took advantage of vulnerabilities in the custom applications that were \nrunning on the servers. The company is now migrating its applications to run on Amazon EC2 \ninstances. The company wants to implement a solution that actively scans for vulnerabilities on \nthe EC2 instances and sends a report that details the findings. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nconfiguring AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC), is the correct solution. AWS WAF allows you to create rules based on various criteria, including the originating country of the request. By attaching AWS WAF to the ALB, you can inspect incoming traffic and block requests originating from the two prohibited countries, while allowing traffic from the home country. This provides a centralized and effective way to enforce geo-restrictions at the application layer.\n\n**Why option 0 is incorrect:**\nconfiguring the security group for the Amazon EC2 instances, is incorrect. While security groups provide network-level access control, they primarily operate based on IP addresses and ports. Implementing geo-restrictions using security groups would be complex and impractical, requiring constantly updating the security group rules with IP address ranges associated with the prohibited countries. This approach is not scalable, maintainable, or reliable, as IP address ranges can change frequently.\n\n**Why option 1 is incorrect:**\nusing the Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC), is incorrect. CloudFront is a Content Delivery Network (CDN) service. While CloudFront does offer geo-restriction capabilities, it's designed for caching and distributing content globally. In this scenario, the application is already running behind an ALB, and the question doesn't explicitly mention the need for content caching or distribution. Using CloudFront solely for geo-restriction would add unnecessary complexity and cost. Also, CloudFront is not deployed inside a VPC, it's a global service.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 12,
    "text": "A company uses an Amazon EC2 instance to run a script to poll for and process messages in an \nAmazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational \ncosts while maintaining its ability to process a growing number of messages that are added to the \nqueue. \n \nWhat should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the size of the EC2 instance to process messages faster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Systems Manager Run Command to run the script on demand.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nleveraging Amazon API Gateway with Amazon Kinesis Data Analytics, is the correct solution. Amazon API Gateway provides a REST API endpoint for the multi-tier application to send location data. Kinesis Data Analytics can then process this data in real-time and make it available to the analytics platform. Kinesis Data Analytics allows for real-time processing and analysis of streaming data, which perfectly fits the requirement of real-time accessibility for the analytics platform. The processed data can then be stored in a suitable data store (e.g., S3, Redshift) for further analysis and reporting.\n\n**Why option 0 is incorrect:**\nleveraging Amazon API Gateway with AWS Lambda, is incorrect because while API Gateway can provide the REST API endpoint and Lambda can process the data, Lambda is typically used for event-driven, short-lived tasks. For real-time data processing and analysis of streaming data, Kinesis Data Analytics is a more suitable choice. Lambda would require additional infrastructure and logic to handle the continuous stream of location data and make it available in real-time to the analytics platform.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nleveraging Amazon Athena with Amazon S3, is incorrect because Athena is a query service that allows you to analyze data stored in S3 using SQL. While S3 can be used to store the location data, Athena is not designed for real-time data processing and analysis. It's more suitable for ad-hoc queries and batch processing of data at rest.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company uses a legacy application to produce data in CSV format. The legacy application \nstores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf \n(COTS) application that can perform complex SQL queries to analyze data that is stored in \nAmazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv \nfiles that the legacy application produces. \n \nThe company cannot update the legacy application to produce data in another format. The \ncompany needs to implement a solution so that the COTS application can use the data that the \nlegacy application produces. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure",
        "correct": true
      },
      {
        "id": 1,
        "text": "Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing AWS Glue DataBrew, is the best solution. DataBrew is specifically designed for data preparation and cleaning with a visual, code-free interface. It allows data engineers and business analysts to collaborate on data preparation workflows. DataBrew recipes provide data lineage tracking, allowing users to audit and share transformation steps. Data profiling capabilities enable inspection of column statistics, null values, and data types. This directly addresses all the requirements of the question.\n\n**Why option 1 is incorrect:**\nusing Amazon AppFlow, is incorrect because AppFlow is primarily designed for transferring data between SaaS applications and AWS services. While AppFlow can perform some basic transformations during data transfer, it is not a comprehensive data preparation tool like DataBrew. It lacks the robust data profiling, data lineage, and visual workflow capabilities needed for the scenario. Sharing flows through IAM policies is not as intuitive or collaborative as DataBrew's recipe sharing.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 14,
    "text": "A company recently migrated its entire IT environment to the AWS Cloud. The company \ndiscovers that users are provisioning oversized Amazon EC2 instances and modifying security \ngroup rules without using the appropriate change control process. A solutions architect must \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n201 \ndevise a strategy to track and audit these inventory and configuration changes. \n \nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS CloudTrail and use it for auditing.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use data lifecycle policies for the Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable AWS Trusted Advisor and reference the security dashboard.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Config and create rules for auditing and compliance purposes.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Restore previous resource configurations with an AWS CloudFormation template.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThese are correct because they represent fundamental security best practices for the AWS account root user. Creating a strong password (Option 1) makes it harder for unauthorized individuals to guess or crack the password. Enabling Multi-Factor Authentication (MFA) (Option 2) adds an extra layer of security, requiring a second authentication factor in addition to the password. This significantly reduces the risk of unauthorized access, even if the password is compromised. AWS strongly recommends enabling MFA for the root user.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing the root user credentials, even with the business owner, is a major security risk. The root user has unrestricted access to all AWS resources in the account, and its credentials should be protected at all costs. Sharing the credentials increases the risk of accidental or malicious misuse. Instead of sharing the root user credentials, the consultant should create IAM users with appropriate permissions for the business owner and other users.\n\n**Why option 4 is incorrect:**\nis incorrect because creating and sharing root user access keys is a dangerous practice. Root user access keys grant full administrative access to the AWS account. Sharing these keys, even with the business owner, significantly increases the risk of unauthorized access and potential security breaches. It is strongly recommended to avoid creating root user access keys whenever possible. Instead, use IAM users and roles with least privilege access.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems \nadministrators have used shared SSH keys to manage the instances. After a recent audit, the \ncompany's security team is mandating the removal of all shared keys. A solutions architect must \ndesign a solution that provides secure access to the EC2 instances. \n \nWhich solution will meet this requirement with the LEAST amount of administrative overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager Session Manager to connect to the EC2 instances.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nleveraging multi-AZ configuration of Amazon RDS Custom for Oracle, is the best solution. RDS Custom allows for customization of the underlying OS and database environment, which is a key requirement for the legacy applications. The multi-AZ configuration provides high availability by replicating the database across multiple Availability Zones. RDS Custom also reduces the operational overhead compared to managing Oracle on EC2 instances, as AWS handles patching, backups, and recovery. This option balances the need for customization with the benefits of a managed service.\n\n**Why option 1 is incorrect:**\nis incorrect because standard RDS for Oracle multi-AZ configuration does not allow for customization of the underlying operating system or database environment. While it provides high availability, it fails to meet the customization requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 16,
    "text": "A company is building a data analysis platform on AWS by using AWS Lake Formation. The \nplatform will ingest data from different sources such as Amazon S3 and Amazon RDS. The \ncompany needs a secure solution to prevent access to portions of the data that contain sensitive \ninformation.",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that includes permissions to access Lake Formation tables.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create data filters to implement row-level security and cell-level security.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Lambda function that removes sensitive information before Lake Formation",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function that perodically Queries and removes sensitive information from",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because enabling AWS Multi-Factor Authentication (MFA) for privileged users adds an extra layer of security, protecting against unauthorized access even if credentials are compromised. MFA requires users to provide two or more verification factors to gain access to AWS resources, significantly reducing the risk of security breaches.\nOption 4 is correct because configuring AWS CloudTrail to log all AWS Identity and Access Management (IAM) actions provides an audit trail of all IAM activities. This is crucial for security monitoring, compliance, and troubleshooting. CloudTrail logs capture information about who did what, when, and from where, enabling organizations to track changes to IAM policies, roles, and users.\n\n**Why option 1 is incorrect:**\nis incorrect because using user credentials to provide access to EC2 instances is a security risk. Instead, IAM roles should be used to grant permissions to EC2 instances. Roles provide temporary credentials and avoid the need to embed long-term credentials in the instance.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 17,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n202 \nWhat should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket \nare encrypted?",
    "options": [
      {
        "id": 0,
        "text": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing permissions boundaries, is the most effective solution. Permissions boundaries allow you to set the maximum permissions that an IAM principal (user or role) can have. By attaching a permissions boundary to the developer's IAM role, you can restrict the maximum permissions they can grant to other IAM principals or use themselves, even if their IAM policy grants them more. This provides a safety net and prevents accidental or malicious privilege escalation. It is a scalable and centrally managed approach.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because restricting full database access to only the root user is not a practical or secure solution for day-to-day operations. The root user should be used sparingly and only for tasks that require its elevated privileges. Relying solely on the root user for database access creates a single point of failure and auditability issues. It also hinders collaboration and development efficiency.\n\n**Why option 2 is incorrect:**\nis incorrect because relying on the CTO to manually review each new developer's permissions is not a scalable or sustainable solution. It introduces a bottleneck and is prone to human error. While manual reviews can be part of a security process, they should not be the primary control. This approach does not scale well as the team grows.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A solutions architect is designing a multi-tier application for a company. The application's users \nupload images from a mobile device. The application generates a thumbnail of each image and \nreturns a message to the user to confirm that the image was uploaded successfully. \n \nThe thumbnail generation can take up to 60 seconds, but the company wants to provide a faster \nresponse time to its users to notify them that the original image was received. The solutions \narchitect must design the application to asynchronously dispatch requests to the different \napplication tiers. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Auto Scaling will detect the unhealthy instance via the ALB health checks and initiate a scaling activity to terminate it. After termination, it will launch a new instance to maintain the desired capacity. Option 2 is also correct. Because of the manual termination of instances in AZ A, the ASG will detect an imbalance across Availability Zones. Auto Scaling's rebalancing feature will launch new instances in the under-represented AZ (AZ A) *before* terminating instances in the over-represented AZ (AZ B). This ensures that application performance and availability are not compromised during the rebalancing process. Launching before terminating is the default and preferred behavior.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because Auto Scaling launches new instances *before* terminating old ones during rebalancing to avoid capacity dips and maintain application availability. Terminating before launching would lead to a temporary reduction in capacity and potentially impact application performance.\n\n**Why option 3 is incorrect:**\nis incorrect because while Auto Scaling will eventually replace the unhealthy instance, the termination and launch are not necessarily simultaneous. The termination is triggered by the health check failure, and the launch is triggered by the need to maintain desired capacity. These are separate activities, even if they occur in relatively quick succession.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 19,
    "text": "A solution architect needs to assign a new microsoft for a company's application. Clients must be \nable to call an HTTPS endpoint to reach the micoservice. The microservice also must use AWS \nidentity and Access Management (IAM) to authentication calls. The soltions architect will write the \nlogic for this microservice by using a single AWS Lambda function that is written in Go 1.x. \nWhich solution will deploy the function in the in the MOST operationally efficient way? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n203",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon CloudFront distribuion. Deploy the function to CloudFront Functions. Specify",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution because it leverages Amazon Comprehend's custom entity recognition feature. This allows the company to train Comprehend to identify ingredient names specifically, without requiring deep ML expertise. S3 Event Notifications trigger a Lambda function upon file upload, which then uses Comprehend to extract the ingredient names. These names are then stored in DynamoDB, enabling the front-end application to query for health scores. This approach is cost-effective because Comprehend is a managed service, minimizing operational overhead. Lambda is also cost-effective due to its pay-per-use model. The solution is fully automated and can handle invalid submissions by simply not finding any ingredient names, thus not querying DynamoDB.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using Amazon SageMaker with a custom-trained NLP model. This requires significant ML expertise to build, train, fine-tune, and maintain the model. It also involves managing a SageMaker endpoint, which adds operational overhead and cost. While SageMaker can be powerful, it's overkill for this scenario, especially given the company's lack of in-house ML experts and the requirement for a cost-effective solution. Using EventBridge adds unnecessary complexity compared to direct S3 event triggers.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A company wants to implement a disaster recovery plan for its primary on-premises file storage \nvolume. The file storage volume is mounted from an Internet Small Computer Systems Interface \n(iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes \n(TB) of data. \n \nThe company wants to ensure that end users retain immediate access to all file types from the \non-premises systems without experiencing latency. \n \nWhich solution will meet these requirements with the LEAST amount of change to the company's \nexisting infrastructure?",
    "options": [
      {
        "id": 0,
        "text": "Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10",
        "correct": false
      },
      {
        "id": 3,
        "text": "Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nusing Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation, is the correct solution. SQS FIFO queues guarantee that messages are processed in the exact order they are sent. While FIFO queues have a lower throughput limit than standard queues, batching allows for increased throughput. Each batch counts as a single transaction towards the SQS limits. With a batch size of 4, the system only needs to perform 250 operations per second (1000 messages / 4 messages per batch = 250 operations), which is well within the SQS FIFO queue limits. This approach balances the need for ordered processing with the required throughput.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon SQS FIFO (First-In-First-Out) queue to process the messages without batching, is incorrect because it might not be able to handle the peak rate of 1000 messages per second. While FIFO queues guarantee order, processing each message individually would likely exceed the default throughput limits of SQS FIFO queues, potentially leading to message throttling and performance issues. Without batching, the system would need to perform 1000 operations per second, which is significantly higher than the batched approach.\n\n**Why option 2 is incorrect:**\nusing Amazon SQS standard queue to process the messages, is incorrect because standard queues do not guarantee message order. While standard queues offer higher throughput, the requirement for processing messages in order is a primary constraint, making standard queues unsuitable for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A company is hosting a web application from an Amazon S3 bucket. The application uses \nAmazon Cognito as an identity provider to authenticate users and return a JSON Web Token \n(JWT) that provides access to protected resources that are stored in another S3 bucket. \n \nUpon deployment of the application, users report errors and are unable to access the protected \ncontent. A solutions architect must resolve this issue by providing proper permissions so that \nusers can access the protected content. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n204 \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the Amazon Cognito identity pool to assume the proper IAM role for access to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Update the S3 ACL to allow the application to access the protected content.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket",
        "correct": false
      },
      {
        "id": 3,
        "text": "Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Outposts allows you to run AWS services, including Amazon EKS Anywhere, on-premises. EKS Anywhere on Outposts provides a consistent Kubernetes experience with automated upgrades and integration with AWS services like CloudWatch and IAM. This solution fulfills all the requirements: modernization with AWS-managed services, data residency on-premises, and integration with AWS APIs.\n\n**Why option 1 is incorrect:**\nis incorrect because while Snowball Edge provides compute capabilities, it's primarily designed for data transfer and edge computing scenarios. It's not a suitable solution for running a production Kubernetes environment with the required level of integration with AWS services like CloudWatch and IAM. Orchestrating workloads in batches using the Snowball console is also not a scalable or efficient approach for a microservices architecture.\n\n**Why option 2 is incorrect:**\nis incorrect because deploying Amazon ECS with Fargate in a Local Zone still involves running compute resources in AWS, which violates the requirement of keeping all data and workloads on-premises. While a VPN connection can provide connectivity, it doesn't address the fundamental issue of data residency. Also, ECS is not Kubernetes, which the company is already using.\n\n**Why option 3 is incorrect:**\nis incorrect because deploying Amazon EKS in the cloud and connecting it to the on-premises Kubernetes cluster creates a hybrid environment where some workloads and data reside in AWS, violating the data residency requirement. While Direct Connect provides a dedicated network connection, it doesn't solve the problem of data leaving the on-premises environment. Using API Gateway for hybrid workloads is also not the primary requirement; the focus is on keeping everything on-premises.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 22,
    "text": "An image hosting company uploads its large assets to Amazon S3 Standard buckets. The \ncompany uses multipart upload in parallel by using S3 APIs and overwrites if the same object is \nuploaded again. For the first 30 days after upload, the objects will be accessed frequently. The \nobjects will be used less frequently after 30 days, but the access patterns for each object will be \ninconsistent. The company must optimize its S3 storage costs while maintaining high availability \nand resiliency of stored assets. \n \nWhich combination of actions should a solutions architect recommend to meet these \nrequirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Move assets to S3 Intelligent-Tiering after 30 days.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an S3 Lifecycle policy to clean up expired object delete markers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages both multipart upload and Amazon S3 Transfer Acceleration (S3TA). Multipart upload allows breaking the file into smaller parts, which can be uploaded in parallel, improving throughput and resilience. S3TA uses Amazon's globally distributed edge locations to optimize the network path between the on-premises data center and the S3 bucket, reducing latency and improving transfer speeds, especially over long distances. This combination provides the fastest upload method.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while multipart upload is beneficial for large files, it doesn't utilize S3 Transfer Acceleration. S3TA can significantly improve transfer speeds, especially when uploading from geographically distant locations. Without S3TA, the upload speed will be limited by the network latency between the on-premises data center and the S3 bucket.\n\n**Why option 3 is incorrect:**\nis incorrect because uploading a 2GB file in a single operation is less efficient and more prone to errors than using multipart upload. If the upload fails mid-transfer, the entire file needs to be re-uploaded. Multipart upload allows resuming interrupted uploads and improves overall reliability.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 23,
    "text": "A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 \ninstances contain highly sensitive data and run in a private subnet. According to company policy, \nthe EC2 instances that run in the VPC can access only approved third-party software repositories \non the internet for software product updates that use the third party's URL. Other internet traffic \nmust be blocked. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Update the route table for the private subnet to route the outbound traffic to an AWS Network",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on",
        "correct": false
      },
      {
        "id": 2,
        "text": "Implement strict inbound security group rules. Configure an outbound rule that allows traffic only",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an Application Load Balancer (ALB) provides content-based routing (e.g., routing based on the URL path, HTTP headers). It distributes traffic across EC2 instances in different AZs, ensuring high availability. The Auto Scaling group automatically replaces failed instances, maintaining the desired capacity and further enhancing availability. ALBs are designed for HTTP/HTTPS traffic and offer advanced routing capabilities.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because while an Auto Scaling group ensures high availability, it doesn't provide content-based routing. Public IP addresses are assigned to instances, but they are not the best way to handle failures. When an instance fails, its public IP is released, and a new instance will get a new public IP. This would require DNS updates, which is slow and unreliable. Also, managing public IPs directly on instances is not a scalable or maintainable approach.\n\n**Why option 3 is incorrect:**\nis incorrect because a Network Load Balancer (NLB) operates at Layer 4 (TCP/UDP) and does not provide content-based routing. NLBs are suitable for high-performance, low-latency applications that require TCP or UDP traffic. Private IP addresses are used for internal communication within the VPC, not for external access or masking failures. NLB also doesn't directly integrate with Auto Scaling in the same way as ALB.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThese are correct because they provide mechanisms to restrict content access based on geographic location.\n\n*   **Option 0: Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through an Amazon CloudFront web distribution:** CloudFront's geo-restriction feature allows you to block requests from specific countries or allow requests only from specific countries. This directly addresses the requirement of allowing access only from the USA.\n*   **Option 1: Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights:** Route 53's geolocation routing policy allows you to route traffic to different resources based on the geographic location of the user. By configuring Route 53 to route traffic to the streaming servers only for users in the USA, you can effectively restrict access to users from other countries.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because failover routing policy is primarily used for high availability and disaster recovery scenarios. It does not directly address the requirement of restricting access based on geographic location. Failover routing directs traffic to a secondary resource when the primary resource becomes unavailable, not based on user location.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions \narchitect needs to provide a solution that will run regular security scans across a large fleet of \nEC2 instances. The solution should also patch the EC2 instances on a regular schedule and \nprovide a report of each instance's patch status. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on",
        "correct": false
      },
      {
        "id": 1,
        "text": "Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because an inter-region VPC peering connection allows network connectivity between VPCs in different AWS regions. By establishing a peering connection between the VPC in us-east-1 (where the EFS is located) and the VPCs in other regions, the sourcing teams in those regions can directly access the EFS file system. This approach minimizes operational overhead as it avoids data replication or migration, and leverages the existing EFS setup. The EFS file system would need appropriate security group rules to allow access from the peered VPCs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because while Amazon S3 is globally accessible, copying the spreadsheet to S3 introduces additional operational overhead for synchronization and version control. Furthermore, it doesn't facilitate real-time collaboration on the *same* spreadsheet. Users would need to download, edit, and re-upload, leading to potential conflicts and versioning issues. This adds complexity and is not the least amount of operational overhead.\n\n**Why option 2 is incorrect:**\nis incorrect because moving the spreadsheet data into an Amazon RDS for MySQL database is a significant architectural change and introduces a lot of operational overhead. It requires data transformation, database schema design, and application development to interact with the database. This is far more complex than necessary and not the right solution for collaborating on a spreadsheet.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company is planning to store data on Amazon RDS DB instances. The company must encrypt \nthe data at rest. \n \nWhat should a solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nis the most suitable solution. Deploying an AWS Storage Gateway File Gateway on premises allows the company to present an NFS-compatible file share to their workloads. The File Gateway stores the uploaded files in Amazon S3, which provides virtually unlimited scalability and durability. S3 Lifecycle policies can then be used to automatically transition infrequently accessed objects to lower-cost storage classes like S3 Standard-IA or S3 Glacier, fulfilling the automated tiering requirement. This solution minimizes costs by leveraging S3's storage classes and allows the company to continue using their existing NFS-based tools and protocols, minimizing application changes. The File Gateway acts as a bridge between the on-premises NFS clients and the cloud-based S3 storage.\n\n**Why option 1 is incorrect:**\nis incorrect because using a Storage Gateway Volume Gateway in cached mode and attaching it as a block device to an on-premises file server adds unnecessary complexity and overhead. It requires maintaining an on-premises file server, which defeats the purpose of migrating to a cloud-based solution to address scalability issues. While snapshots can be stored in S3 Glacier Deep Archive, this approach is not as cost-effective or efficient as directly storing files in S3 and using S3 Lifecycle policies for tiering. Also, managing recovery operations and tiering with AWS Backup is not the most efficient way to handle archival and tiering in this scenario. The primary goal is to move away from on-premises infrastructure, which this solution doesn't fully achieve.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The \ncompany's network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Snowball.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS DataSync.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a secure VPN connection.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Transfer Acceleration.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Lambda functions have a concurrency limit per account per region. When SNS triggers Lambda functions at a high rate (5000 requests per second), the Lambda function might exceed its concurrency limit, leading to throttling and undelivered notifications. Increasing the account concurrency quota for Lambda is the appropriate solution to handle the increased load during peak season. This allows Lambda to scale and process the increased number of SNS messages without throttling.\n\n**Why option 1 is incorrect:**\nis incorrect because Lambda is a serverless service. You don't provision servers for Lambda. Lambda automatically scales based on the number of incoming requests, up to the account concurrency limit. While increasing concurrency is the solution, the problem isn't provisioning servers, but rather the account limit on existing Lambda concurrency.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A company needs to provide its employees with secure access to confidential and sensitive files. \nThe company wants to ensure that the files can be accessed only by authorized users. The files \nmust be downloaded securely to the employees' devices. \nThe files are stored in an on-premises Windows file server. However, due to an increase in \nremote usage, the file server is running out of capacity. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nOptions 1 (Throughput Optimized Hard Disk Drive - st1) and 3 (Cold Hard Disk Drive - sc1) are correct. These volume types are designed for infrequently accessed data and large sequential workloads. They are not suitable for boot volumes because the operating system requires frequent random access to files, which these drives are not optimized for. Using st1 or sc1 as a boot volume would result in very poor performance and slow boot times.\n\n**Why option 0 is incorrect:**\nGeneral Purpose Solid State Drive - gp2) is incorrect. gp2 volumes are designed for a wide variety of workloads and provide a balance of price and performance. They are commonly used as boot volumes for EC2 instances.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company's application runs on Amazon EC2 instances behind an Application Load Balancer \n(ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability \nZones. On the first day of every month at midnight, the application becomes much slower when \nthe month-end financial calculation batch runs. This causes the CPU utilization of the EC2 \ninstances to immediately peak to 100%, which disrupts the application. \n \nWhat should a solutions architect recommend to ensure the application is able to handle the \nworkload and avoid downtime?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon CloudFront distribution in front of the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the best solution because it utilizes Amazon SQS and AWS Lambda to create a fully serverless and automatically scaling architecture. SQS acts as a buffer for the incoming sensor data, decoupling the data producers (cars) from the data consumers (Lambda functions). Lambda functions are triggered by SQS messages and process the data in batches before writing it to the auto-scaled DynamoDB table. This solution requires no manual provisioning or scaling of compute resources, fulfilling the question's requirements. Lambda's ability to scale automatically based on the number of messages in the SQS queue ensures that the system can handle varying volumes of sensor data without manual intervention.\n\n**Why option 0 is incorrect:**\nis incorrect because while Kinesis Data Firehose can directly write to DynamoDB, it's primarily designed for streaming data to data lakes or analytics services. While it can scale, it might not be the most cost-effective or flexible solution for this specific use case compared to Lambda and SQS. Also, Kinesis Data Firehose might not handle the specific data transformation requirements as efficiently as Lambda.\n\n**Why option 1 is incorrect:**\nis incorrect because it involves using an Amazon EC2 instance to poll the SQS queue. This introduces the need to manage and scale the EC2 instance, which contradicts the requirement for a fully serverless solution. EC2 instances require manual provisioning and scaling, which the development team wants to avoid. While DynamoDB is auto-scaled, the EC2 instance becomes a bottleneck and a point of operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because it uses Kinesis Data Streams with an EC2 instance. Similar to option 1, using an EC2 instance to poll Kinesis Data Streams violates the serverless requirement. Kinesis Data Streams is a good choice for real-time streaming data, but requires a consumer application to process the data, and using EC2 for this purpose negates the serverless aspect. Furthermore, Kinesis Data Streams requires more configuration and management than SQS for this specific use case.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 30,
    "text": "A company wants to give a customer the ability to use on-premises Microsoft Active Directory to \ndownload files that are stored in Amazon S3. The customer's application uses an SFTP client to \ndownload the files. \n \nWhich solution will meet these requirements with the LEAST operational overhead and no \nchanges to the customer's application?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up AWS DataSync to synchronize between the on-premises location and the S3 location by",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nPath-based routing allows the Application Load Balancer to route requests to different target groups based on the URL path of the HTTP request. In this case, requests to `/orders` are routed to one microservice, and requests to `/products` are routed to another. This directly addresses the requirement of routing traffic based on the URL path.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nHTTP header-based routing allows routing based on the value of any HTTP header, not just the Host header. While technically possible to inspect the 'path' header, path-based routing is the more direct and efficient method to achieve the desired outcome. It is more complex to configure and maintain than path-based routing for this specific scenario.\n\n**Why option 3 is incorrect:**\nQuery string parameter-based routing allows routing based on the values of query string parameters in the URL. While this could be used, it's not the most appropriate solution for the given scenario where the routing is based on the path itself, not parameters. Path-based routing is more suitable and cleaner for this use case.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 31,
    "text": "A company is experiencing sudden increases in demand. The company needs to provision large \nAmazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto \nScaling group. The company needs a solution that provides minimum initialization latency to meet \nthe demand. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because Amazon SQS FIFO (First-In, First-Out) queues guarantee that messages are processed in the order they are sent and exactly once. This is crucial for ensuring that permit requests are processed in the correct sequence and that no request is lost or processed multiple times. The FIFO queue acts as a buffer between the web application and the processing tier, decoupling them and allowing the processing tier to handle requests at its own pace, even during periods of high traffic. This ensures reliability and scalability.\n\n**Why option 0 is incorrect:**\nis incorrect because while API Gateway and Lambda can handle web requests, they don't inherently guarantee exactly-once processing. Lambda functions can be invoked multiple times in case of errors or retries, potentially leading to duplicate processing. While mechanisms can be implemented to mitigate this, it adds complexity and doesn't provide the built-in guarantee of exactly-once processing that SQS FIFO offers. Also, directly invoking Lambda from API Gateway for every form submission might not be the most cost-effective or scalable solution for high traffic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because Amazon SQS standard queues do not guarantee exactly-once processing or message order. While they offer high throughput and best-effort ordering, messages can be delivered out of order or even duplicated. This violates the requirement that every request is processed exactly once, making it unsuitable for this scenario where data integrity is paramount. Standard queues are designed for scenarios where occasional duplicates or out-of-order processing are acceptable, which is not the case here.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 32,
    "text": "A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for \nstorage. The application tier is hosted on Amazon EC2 instances. The company's IT security \nguidelines mandate that the database credentials be encrypted and rotated every 14 days. \n \nWhat should a solutions architect do to meet this requirement with the LEAST operational effort?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nusing Amazon EC2 Spot Instances, is the most cost-effective solution. Spot Instances offer significant discounts compared to On-Demand and Reserved Instances, often up to 90%. Since the workflow can withstand disruptions, the risk of Spot Instances being terminated is acceptable. The workflow can be designed to handle interruptions and resume from where it left off. This makes Spot Instances the ideal choice for cost optimization in this scenario.\n\n**Why option 1 is incorrect:**\nusing Amazon EC2 On-Demand instances, is incorrect because while it provides guaranteed availability, it's more expensive than Spot Instances. Given the workflow's tolerance for interruptions, the higher cost of On-Demand instances is not justified.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 Reserved Instances, is incorrect. Reserved Instances offer cost savings compared to On-Demand instances, but they require a commitment to a specific instance type and availability zone for a longer period (1 or 3 years). While they can be cost-effective in general, they are not the *most* cost-effective option when the workload is interruptible. Spot Instances will almost always be cheaper than reserved instances for interruptible workloads.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 33,
    "text": "A company has deployed a web application on AWS. The company hosts the backend database \non Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling \nneeds. The read replicas must lag no more than 1 second behind the primary DB instance. The \ndatabase routinely runs scheduled stored procedures. \n \nAs traffic on the website increases, the replicas experience additional lag during periods of peak \nload. A solutions architect must reduce the replication lag as much as possible. The solutions \narchitect must minimize changes to the application code and must minimize ongoing operational \noverhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas,",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large,",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 2 and 3 are the correct answers.\n\n*   **Option 2: Implement Amazon RDS Proxy between the application and the Aurora PostgreSQL cluster. Deploy EC2 instances in an Auto Scaling group to retry transactions as needed.** RDS Proxy helps manage database connections, reducing the overhead on the Aurora database. It pools and reuses database connections, preventing connection exhaustion during peak loads. Deploying EC2 instances in an Auto Scaling group ensures that the application can scale horizontally to handle increased traffic. Retrying transactions handles transient failures effectively. This combination addresses scalability and resilience without requiring database changes.\n\n*   **Option 3: Modify the application to publish purchase events to an Amazon SQS queue. Launch an Auto Scaling group of EC2 workers that poll the queue and process purchases asynchronously.** This implements an asynchronous processing pattern. Instead of directly processing purchases synchronously, the application publishes purchase events to an SQS queue. An Auto Scaling group of EC2 workers then consumes these events and processes the purchases in the background. This decouples the front-end application from the back-end processing, allowing the front-end to remain responsive even during peak loads. SQS provides buffering and ensures that no purchase events are lost. The Auto Scaling group scales the processing capacity based on the queue length, providing scalability and cost-efficiency.\n\n**Why option 1 is incorrect:**\nOption 1: Deploy read replicas for the Aurora database in another Region and configure EC2 instances to read and write from the nearest replica based on latency. This option focuses on improving read performance and disaster recovery, but the problem is with write performance and application server scalability during peak purchase times. Read replicas do not help with write operations, which are the bottleneck in this scenario. Writing to a replica in another region would introduce significant latency and potential data consistency issues.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 34,
    "text": "A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a \nservice (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB \ncluster. \nThe DR plan must replicate data to a secondary AWS Region. \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because Amazon API Gateway supports two primary types of APIs: RESTful APIs and WebSocket APIs. RESTful APIs are inherently stateless, meaning each request from the client to the server contains all the information needed to understand and process the request. The server does not retain any client context between requests. WebSocket APIs, on the other hand, adhere to the WebSocket protocol, which enables stateful, full-duplex communication. This means a persistent connection is established between the client and server, allowing for real-time, bidirectional data transfer. The server maintains the connection state, enabling it to send data to the client without the client explicitly requesting it. This fulfills the requirement of supporting both stateful and stateless communication.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it incorrectly states that RESTful APIs enable stateful communication. RESTful APIs are designed to be stateless.\n\n**Why option 2 is incorrect:**\nis a duplicate of option 0 and therefore is the correct answer.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 35,
    "text": "A company has a custom application with embedded credentials that retrieves information from \nan Amazon RDS MySQL DB instance. Management says the application must be made more \nsecure with the least amount of programming effort. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create credentials on the RDS for MySQL database for the application user and store the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages Aurora Global Database for the `games` table, providing low-latency global reads. Aurora Global Database replicates data across multiple AWS regions, making it ideal for globally accessible data. Using standard Aurora instances for the `users` and `games_played` tables allows for regional data residency and compliance requirements. This approach minimizes application refactoring as the application can continue to interact with Aurora in a familiar way for regional data, while leveraging Aurora Global Database for global data.\n\n**Why option 0 is incorrect:**\nis incorrect because while DynamoDB Global Tables provide global data replication, switching to DynamoDB for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data. Using DynamoDB for `users` and `games_played` might be a viable option, but the primary reason this is incorrect is the forced migration of `games` to DynamoDB.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because using DynamoDB Global Tables for the `games` table would require significant application refactoring. The question specifically asks for minimal refactoring. Also, while DynamoDB is suitable for some workloads, Aurora is generally preferred for transactional workloads like those likely associated with game data.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 36,
    "text": "A media company hosts its website on AWS. The website application's architecture includes a \nfleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is \nhosted on Amazon Aurora. The company's cybersecurity team reports that the application is \nvulnerable to SQL injection. \n \nHow should the company resolve this issue?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an ALB listener rule to reply to SQL injections with a fixed response.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon Inspector to block all SQL injection attempts automatically.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 1 and 4 are the most efficient solutions.\n\nOption 1: Putting the instance into the Standby state allows you to detach the instance from the Auto Scaling group's active management without terminating it. While in Standby, the instance is not subject to health checks or scaling policies. After applying the patch and verifying its functionality, you can return the instance to service, re-integrating it into the Auto Scaling group. This is a quick and clean way to perform maintenance without triggering replacements.\n\nOption 4: Suspending the ReplaceUnhealthy process type prevents the Auto Scaling group from automatically replacing instances that fail health checks. This allows the team to apply the maintenance patch without the Auto Scaling group launching a new instance. After the patch is applied and the instance's health is restored, the ReplaceUnhealthy process can be resumed. This approach directly addresses the problem of premature instance replacement.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because suspending ScheduledActions will not prevent the Auto Scaling group from replacing an unhealthy instance. ScheduledActions are used to scale the group based on a schedule, not in response to health check failures. The ReplaceUnhealthy process is what triggers the replacement of unhealthy instances.\n\n**Why option 3 is incorrect:**\nis incorrect because deleting the Auto Scaling group and recreating it is the most disruptive and time-consuming option. It involves significant configuration overhead and potential downtime. This is not a time/resource-efficient solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 37,
    "text": "A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company \nwants to create a visualization in Amazon QuickSight by joining the data in the data lake with \noperational data that is stored in an Amazon Aurora MySQL database. The company wants to \nenforce column-level authorization so that the company's marketing team can access only a \nsubset of columns in the database. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it accurately describes the pricing models for both launch types. With the EC2 launch type, you are responsible for provisioning and managing the underlying EC2 instances. Therefore, you are charged for the EC2 instances themselves, as well as any EBS volumes attached to those instances. With the Fargate launch type, you don't manage the underlying infrastructure. Instead, you specify the vCPU and memory resources required for your containers, and you are charged based on the amount of those resources consumed by your tasks or services. This is a pay-as-you-go model for container resources.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that both launch types are charged based on EC2 instances and EBS volumes. This is only true for the EC2 launch type. Fargate abstracts away the underlying infrastructure, so you are not charged for EC2 instances or EBS volumes directly.\n\n**Why option 2 is incorrect:**\nis incorrect because it states that both launch types are charged based on vCPU and memory resources. While this is true for Fargate, it's not true for EC2 launch type. With EC2, you pay for the EC2 instance regardless of the container's resource utilization.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 38,
    "text": "A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 \ninstances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, \nbut the baseline CPU utilization that is noted on each run is at least 60%. The company needs to \nprovision the capacity 30 minutes before the jobs run. \n \nCurrently, engineers complete this task by manually modifying the Auto Scaling group \nparameters. The company does not have the resources to analyze the required capacity trends \nfor the Auto Scaling group counts. The company needs an automated way to modify the Auto \nScaling group's desired capacity. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it leverages AWS PrivateLink to establish a secure and private connection. The partner creates a Network Load Balancer (NLB) in front of the RDS instance. AWS PrivateLink then exposes this NLB as an interface VPC endpoint in the research company's VPC. This allows the research company to access the RDS database as if it were located within their own VPC, without traversing the public internet. PrivateLink provides a highly secure and scalable solution for private connectivity between VPCs, minimizing complexity by avoiding the need for VPNs, Direct Connect, or public IP addresses. It also complies with data security requirements by keeping all traffic within the AWS network.\n\n**Why option 0 is incorrect:**\nis incorrect because enabling public access on the RDS instance and using a NAT Gateway exposes the database to the public internet, which violates the data security requirements. It also introduces unnecessary complexity and security risks.\n\n**Why option 1 is incorrect:**\nis incorrect because while VPC peering can establish connectivity between the two VPCs, using AWS Transit Gateway in the partner's account to route traffic from the companys VPC to the database adds unnecessary complexity. Also, modifying the RDS subnet route tables to allow access from the companys CIDR block is a less secure approach than using PrivateLink, as it requires opening up the entire subnet to the company's CIDR block rather than just the specific endpoint. PrivateLink offers a more granular and secure connection.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 39,
    "text": "A solutions architect is designing a company's disaster recovery (DR) architecture. The company \nhas a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled \nbackup. The DR design needs to include multiple AWS Regions. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n213",
    "options": [
      {
        "id": 0,
        "text": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because Multi-AZ deployments in RDS use synchronous replication to ensure high availability and data durability. The primary database synchronously replicates data to a standby instance in a different Availability Zone (AZ) within the same region. Read Replicas, on the other hand, use asynchronous replication. This means that changes made to the primary database are replicated to the Read Replica with a slight delay. Read Replicas can be created within the same AZ as the primary, in a different AZ (Cross-AZ), or even in a different AWS Region (Cross-Region). This flexibility allows for scaling read operations and disaster recovery.\n\n**Why option 0 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication. Multi-AZ deployments use synchronous replication for high availability.\n\n**Why option 1 is incorrect:**\nis incorrect because it states that Multi-AZ follows asynchronous replication and that Read Replicas follow synchronous replication. Multi-AZ uses synchronous replication, and Read Replicas use asynchronous replication.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to \nparse messages. The application cannot parse messages that are larger than 256 KB in size. \nThe company wants to implement a solution to give the application the ability to parse messages \nas large as 50 MB. \n \nWhich solution will meet these requirements with the FEWEST changes to the code?",
    "options": [
      {
        "id": 0,
        "text": "Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the limit in Amazon SQS to handle messages that are larger than 256 KB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS).",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because it utilizes AWS Glue for ETL, Amazon Redshift Serverless for the data warehouse, and Amazon Redshift ML for machine learning. AWS Glue provides a serverless ETL service to transform and clean the data in S3. Amazon Redshift Serverless offers MPP capabilities in a serverless model, eliminating the need to manage infrastructure. Amazon Redshift ML allows analysts to build and train ML models using SQL syntax directly within Redshift, fulfilling the requirement of SQL-based ML without custom Python code. This solution effectively addresses all the requirements while minimizing operational overhead through serverless services.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon RDS for PostgreSQL, even with Aurora, is not designed for the scale and performance requirements of a large-scale data warehouse. While Aurora ML exists, it's not as tightly integrated with the data warehouse as Redshift ML, and PostgreSQL's architecture is not inherently MPP like Redshift. Also, it doesn't fully leverage serverless capabilities for the data warehouse component, requiring more management than Redshift Serverless.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 41,
    "text": "A company wants to restrict access to the content of one of its main web applications and to \nprotect the content by using authorization techniques available on AWS. The company wants to \nimplement a serverless architecture and an authentication solution for fewer than 100 users. The \nsolution needs to integrate with the main web application and serve web content globally. The \nsolution must also scale as the company's user base grows while providing the lowest login \nlatency possible. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct answer because it utilizes Route 53 Resolver outbound endpoints and forwarding rules. An outbound endpoint allows DNS queries originating from the VPC to be forwarded to on-premises DNS servers. The forwarding rule specifies which domain names (e.g., *.internal.example.com) should be routed to the on-premises DNS servers. This approach is secure because it only forwards specific queries to the on-premises DNS servers, rather than exposing the entire VPC's DNS resolution to the on-premises environment. Associating the rule with the VPC ensures that only resources within that VPC can use the rule.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because creating a Route 53 private hosted zone for the on-premises domain would require manually replicating the DNS records from the on-premises DNS servers into the Route 53 hosted zone. This is not a dynamic solution and would require constant synchronization, making it impractical and error-prone. The question implies the need for dynamic resolution of on-premises DNS records.\n\n**Why option 3 is incorrect:**\nis incorrect because there is no AWS service called 'hybrid connectivity gateway'. This option is misleading. Furthermore, Route 53 does not directly attach to on-premises DNS servers as authoritative zones for internal domains in this manner. While Route 53 can *be* an authoritative DNS server, this option does not address the requirement of forwarding queries to existing on-premises DNS servers.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 42,
    "text": "A company has an aging network-attached storage (NAS) array in its data center. The NAS array \npresents SMB shares and NFS shares to client workstations. The company does not want to \npurchase a new NAS array. The company also does not want to incur the cost of renewing the \nNAS array's support contract. Some of the data is accessed frequently, but much of the data is \ninactive. \n \nA solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 \nLifecycle policies, and maintains the same look and feel for the client workstations. The solutions \narchitect has identified AWS Storage Gateway as part of the solution. \n \nWhich type of storage gateway should the solutions architect provision to meet these \nrequirements?",
    "options": [
      {
        "id": 0,
        "text": "Volume Gateway",
        "correct": false
      },
      {
        "id": 1,
        "text": "Tape Gateway",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon FSx File Gateway",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon S3 File Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 2 and 3 are the correct answers because they directly address the need to minimize application refactoring while migrating from SQL Server to Aurora PostgreSQL.\n\n*   **Option 2: Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data:** AWS SCT helps convert the database schema from SQL Server to PostgreSQL. It identifies potential compatibility issues and provides recommendations for resolving them. AWS DMS then migrates the data from the source SQL Server database to the target Aurora PostgreSQL database. This combination ensures that the database structure and data are migrated effectively.\n*   **Option 3: Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands:** Babelfish for Aurora PostgreSQL is a translation layer that allows Aurora PostgreSQL to understand and execute T-SQL commands. This is crucial because the existing applications are tightly integrated with T-SQL. By deploying Babelfish, the company can minimize the need to rewrite T-SQL queries in the applications, significantly reducing application refactoring effort. Babelfish translates the T-SQL commands into PostgreSQL-compatible SQL, allowing the applications to continue functioning with minimal changes.\n\n**Why option 0 is incorrect:**\nis incorrect because while custom endpoints can be configured in Aurora, they cannot emulate the behavior of Microsoft SQL Server to the extent required for T-SQL compatibility. Custom endpoints are primarily for routing connections based on read/write workloads or other criteria, not for translating database dialects.\n\n**Why option 1 is incorrect:**\nis incorrect because using AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during migration is not a practical solution for minimizing application refactoring. AWS Glue is primarily used for ETL (Extract, Transform, Load) processes and data integration. It's not designed for real-time or on-the-fly query translation. Furthermore, modifying queries within Glue would require significant changes to the application logic and deployment pipelines, which contradicts the requirement to minimize refactoring. This approach would be more suitable for data warehousing scenarios where data is transformed before loading into the target database.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 43,
    "text": "A company has an application that is running on Amazon EC2 instances. A solutions architect \nhas standardized the company on a particular instance family and various instance sizes based \non the current needs of the company. \n \nThe company wants to maximize cost savings for the application over the next 3 years. The \ncompany needs to be able to change the instance family and sizes in the next 6 months based on \napplication popularity and usage. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n215 \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Compute Savings Plan",
        "correct": true
      },
      {
        "id": 1,
        "text": "EC2 Instance Savings Plan",
        "correct": false
      },
      {
        "id": 2,
        "text": "Zonal Reserved Instances",
        "correct": false
      },
      {
        "id": 3,
        "text": "Standard Reserved Instances",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nOptions 0 and 4 are correct.\n\n*   **Option 0: Use VPC security groups to control the network traffic to and from your file system:** Security groups act as virtual firewalls for your EC2 instances and EFS mount targets. By configuring security group rules, you can allow only specific EC2 instances (or security groups representing those instances) to access the EFS mount targets on the required ports (typically NFS port 2049). This provides network-level access control.\n*   **Option 4: Use an IAM policy to control access for clients who can mount your file system with the required permissions:** IAM policies can be attached to IAM roles assumed by EC2 instances. These policies can grant or deny permissions to perform specific EFS actions, such as mounting the file system. By using IAM policies, you can control which EC2 instances are authorized to mount the EFS file system. This provides identity-based access control.\n\n**Why option 1 is incorrect:**\nOption 1: Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system: GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While GuardDuty can detect unusual access patterns, it doesn't directly control access to EFS. It's a monitoring tool, not an access control mechanism.\n\n**Why option 2 is incorrect:**\nOption 2: Set up the IAM policy root credentials to control and configure the clients accessing the Amazon EFS file system: Using root account credentials directly is a major security risk and a violation of AWS best practices. Root credentials should never be used for day-to-day operations or application access. IAM roles should be used instead.\n\n**Why option 3 is incorrect:**\nOption 3: Use network access control list (network ACL) to control the network traffic to and from your Amazon EC2 instance: While Network ACLs can control network traffic, they operate at the subnet level and are stateless. Security Groups are a better choice for controlling access to EFS because they are stateful and operate at the instance/mount target level, providing more granular control. Also, the question is asking about controlling access *to the EFS file system*, not the EC2 instance.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 44,
    "text": "A company collects data from a large number of participants who use wearable devices. The \ncompany stores the data in an Amazon DynamoDB table and uses applications to analyze the \ndata. The data workload is constant and predictable. The company wants to stay at or below its \nforecasted budget for DynamoDB. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because enabling Multi-Factor Authentication (MFA) Delete on an S3 bucket requires users to provide an MFA code when deleting objects. This adds an extra layer of security and significantly reduces the risk of accidental deletion. Option 3 is correct because enabling versioning on an S3 bucket automatically keeps multiple versions of an object. If an object is accidentally deleted, it can be easily recovered by restoring a previous version. This provides a robust mechanism for protecting against data loss.\n\n**Why option 0 is incorrect:**\nis incorrect because while managerial approval is a good practice, it's a process-based control and doesn't technically prevent accidental deletion. It relies on human intervention and is prone to errors or delays. It is not a technical solution as requested by the question.\n\n**Why option 2 is incorrect:**\nis incorrect because while sending an SNS notification upon deletion can alert the IT manager, it doesn't prevent the deletion from happening in the first place. It's a reactive measure, not a preventative one. By the time the notification is received, the object is already deleted.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 45,
    "text": "A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-\nsoutheast-3 Region. The database is encrypted with an AWS Key Management Service (AWS \nKMS) customer managed key. The company was recently acquired and must securely share a \nbackup of the database with the acquiring company's AWS account in ap-southeast-3. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a database snapshot. Add the acquiring company's AWS account to the KMS key policy.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a database snapshot. Download the database snapshot. Upload the database snapshot to",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because a cluster placement group is designed for HPC applications that benefit from low network latency and high network throughput. Cluster placement groups pack instances close together within a single Availability Zone. This tight proximity enables instances to achieve the lowest possible latency and the highest packet-per-second network performance, which is essential for tightly coupled HPC workloads that require frequent communication between instances.\n\n**Why option 0 is incorrect:**\nis incorrect because partition placement groups are primarily used for large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. While they provide fault tolerance by distributing instances across partitions, they do not offer the low latency and high throughput benefits of cluster placement groups, which are more critical for HPC applications.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the \nus-east-1 Region to store customer transactions. The company needs high availability and \nautomatic recovery for the DB instance. \n \nThe company must also run reports on the RDS database several times a year. The report \nprocess causes transactions to take longer than usual to post to the customers' accounts. The \ncompany needs a solution that will improve the performance of the report process. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a read replica of the DB instance in a different Availability Zone. Point all requests for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to RDS Custom.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use RDS Proxy to limit reporting requests to the maintenance window.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThe correct answer is that the junior scientist does not need to pay any S3TA transfer charges for the image upload. Amazon S3 Transfer Acceleration has a 'no acceleration, no charge' policy. If S3TA does not provide a faster transfer than a standard S3 upload, you are not charged for the S3TA portion of the transfer. You would still be charged for the standard S3 PUT request and storage, but not for S3TA itself. The question specifically asks about transfer charges related to the lack of acceleration.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. You would only pay standard S3 transfer charges (PUT request) and storage costs.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because if S3TA does not accelerate the transfer, you are not charged for S3TA. The question states that the transfer was *not* accelerated.\n\n**Why option 3 is incorrect:**\nThis option is partially correct in that the scientist *will* pay standard S3 transfer charges for the PUT request. However, the question is specifically asking about the charges related to S3TA, and since it didn't accelerate the transfer, there are no S3TA charges. The best answer is the one that addresses the S3TA charges, which is option 0.\n\n**Why option 4 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 47,
    "text": "A company is moving its data management application to AWS. The company wants to transition \nto an event-driven architecture. The architecture needs to be more distributed and to use \nserverless concepts while performing the different aspects of the workflow. The company also \nwants to minimize operational overhead. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process",
        "correct": false
      },
      {
        "id": 1,
        "text": "Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda",
        "correct": false
      },
      {
        "id": 3,
        "text": "Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) is the most cost-effective option. It offers lower storage costs compared to S3 Standard but still provides millisecond access times. Since the data is accessed only twice a year, the infrequent access charges are outweighed by the storage cost savings. It meets the requirement of millisecond latency when the audit reports are generated.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAmazon S3 Glacier Deep Archive is designed for long-term archival and has the lowest storage cost but retrieval times can take hours, which violates the millisecond latency requirement. It is unsuitable for this use case.\n\n**Why option 2 is incorrect:**\nAmazon S3 Standard provides millisecond access and is suitable for frequently accessed data. However, it has higher storage costs than S3 Standard-IA, making it less cost-effective for data accessed only twice a year.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 48,
    "text": "A company is designing the network for an online multi-player game. The game uses the UDP \nnetworking protocol and will be deployed in eight AWS Regions. The network architecture needs \nto minimize latency and packet loss to give end users a high-quality gaming experience. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Setup a transit gateway in each Region. Create inter-Region peering attachments between each",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because it leverages IAM roles for cross-account access. Here's why:\n\n*   **IAM Roles:** IAM roles are designed for delegation of permissions. They don't have associated credentials like passwords or access keys. Instead, they are assumed by trusted entities (in this case, users from the development account).\n*   **Cross-Account Access:** To enable cross-account access, you create an IAM role in the production account that trusts the development account. This trust is established through a trust policy that specifies the AWS account ID of the development account as the principal.\n*   **Granting Permissions:** The IAM role in the production account is granted the necessary permissions to access the required resources. This is done through an IAM policy attached to the role.\n*   **Assuming the Role:** Users in the development account can then assume this role using the AWS CLI, SDK, or Management Console. When they assume the role, they receive temporary security credentials that allow them to access the resources in the production account.\n\nThis approach is secure because it avoids sharing long-term credentials and provides temporary, least-privilege access.\n\n**Why option 0 is incorrect:**\nis incorrect because cross-account access is a fundamental feature of AWS IAM. It is possible and commonly used to grant access to resources in different AWS accounts.\n\n**Why option 2 is incorrect:**\nis incorrect because while IAM roles are designed for delegation and can be assumed, IAM users represent individual identities. IAM users are not typically used directly for cross-account access in this manner. Sharing user credentials is a major security risk. Roles are the preferred method for cross-account access.\n\n**Why option 3 is incorrect:**\nis incorrect because sharing IAM user credentials is a severe security risk. It violates the principle of least privilege and makes auditing and access control difficult. If the shared credentials are compromised, the entire production environment could be at risk. This is a highly discouraged practice.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 49,
    "text": "A company hosts a three-tier web application on Amazon EC2 instances in a single Availability \nZone. The web application uses a self-managed MySQL database that is hosted on an EC2 \ninstance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL \ndatabase currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects \ntraffic of 1,000 IOPS for both reads and writes at peak traffic. \n \nThe company wants to minimize any disruptions, stabilize performance, and reduce costs while \nretaining the capacity for double the IOPS. The company wants to move the database tier to a \nfully managed solution that is highly available and fault tolerant. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use Amazon S3 Intelligent-Tiering access tiers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use two large EC2 instances to host the database in active-passive mode.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nusing Instance Store based Amazon EC2 instances, is the most cost-optimal and resource-efficient solution. Instance store provides very high random I/O performance because the storage is physically attached to the host server. Since the application handles data replication and ensures data availability even if an instance fails, the ephemeral nature of instance store is acceptable. This avoids the cost overhead of persistent block storage like EBS or network file systems like EFS.\n\n**Why option 0 is incorrect:**\nusing Amazon Elastic Block Store (Amazon EBS) based EC2 instances, is incorrect because EBS adds cost and complexity compared to instance store. While EBS can provide good performance, it's not as cost-effective as instance store when the application handles data replication and instance failure scenarios. The persistence offered by EBS is not necessary given the application's resilience.\n\n**Why option 2 is incorrect:**\nusing Amazon EC2 instances with access to Amazon S3 based storage, is incorrect because S3 is object storage, not block storage, and is not designed for high random I/O performance. Accessing data from S3 for each I/O operation would introduce significant latency and be very inefficient. S3 is suitable for storing large objects and serving static content, not for the type of workload described in the question.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 50,
    "text": "A company hosts a serverless application on AWS. The application uses Amazon API Gateway, \nAWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase \nin application errors that result from database connection timeouts during times of peak traffic or \nunpredictable traffic. The company needs a solution that reduces the application failures with the \nleast amount of change to the code. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Reduce the Lambda concurrency rate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable RDS Proxy on the RDS DB instance.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Resize the RDS DB instance class to accept more connections.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon DynamoDB with on-demand scaling.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the correct answer because it leverages IAM Roles for Service Accounts (IRSA). IRSA allows you to associate an IAM role with a Kubernetes service account. By creating separate service accounts for the UI and data services, and then mapping each service account to an IAM role with only the necessary permissions (DynamoDB for UI and S3 for data), you can achieve the desired least privilege access. This approach avoids granting excessive permissions to the underlying EC2 nodes or using a single shared service account, which would violate the principle of least privilege. IRSA uses the AWS Security Token Service (STS) to provide temporary credentials to the Pods, ensuring secure access to AWS resources.\n\n**Why option 0 is incorrect:**\nis incorrect because it violates the principle of least privilege. Sharing a single service account across all Pods and attaching an IAM role with both AmazonS3FullAccess and AmazonDynamoDBFullAccess policies grants all Pods access to both S3 and DynamoDB, regardless of whether they need it. This is a security risk and should be avoided.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 51,
    "text": "A company is migrating an old application to AWS. The application runs a batch job every hour \nand is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. \nThe server has 64 virtual CPU (vCPU) and 512 GiB of memory. \n \nWhich solution will run the batch job within 15 minutes with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda with functional scaling.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Lightsail with AWS Auto Scaling.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Batch on Amazon EC2.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nstoring the data in Amazon S3 Standard, is the most cost-effective choice. While other storage classes might seem cheaper at first glance, the frequent access pattern within the 24-hour window makes S3 Standard the better option. S3 Standard is designed for frequently accessed data. The cost of retrieving data from S3 Standard is lower than retrieving data from Infrequent Access storage classes. Given the heavy referencing of the intermediary results, the retrieval costs from IA storage classes would quickly outweigh the slightly higher storage cost of S3 Standard over a 24-hour period. Furthermore, the simplicity of using S3 Standard avoids the added complexity and potential costs associated with data lifecycle management policies needed for other storage classes.\n\n**Why option 0 is incorrect:**\nstoring the data in Amazon S3 Standard-Infrequent Access (S3 Standard-IA), is incorrect because S3 Standard-IA is designed for data that is accessed less frequently. While the storage cost is lower than S3 Standard, the retrieval costs are significantly higher. Since the intermediary query results are heavily referenced within the 24-hour period, the retrieval costs from S3 Standard-IA would likely exceed the cost savings from the lower storage cost, making it less cost-effective. S3 Standard-IA also has a minimum storage duration charge of 30 days, which would apply even though the data is only stored for 24 hours, making it even less cost-effective.\n\n**Why option 1 is incorrect:**\nstoring the data in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA), is incorrect for similar reasons as S3 Standard-IA. While it offers even lower storage costs, it also has higher retrieval costs and a 30-day minimum storage duration charge. More importantly, S3 One Zone-IA stores data in a single Availability Zone, which makes it less durable than S3 Standard. While durability isn't explicitly mentioned as a primary concern, it's generally a good practice to use S3 Standard unless there's a very specific reason to use a lower-durability option. The frequent access pattern and the 30-day minimum storage duration make this option less cost-effective. Also, the loss of an AZ could impact the analytics pipeline.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 52,
    "text": "A company stores its data objects in Amazon S3 Standard storage. A solutions architect has \nfound that 75% of the data is rarely accessed after 30 days. The company needs all the data to \nremain immediately accessible with the same high availability and resiliency, but the company \nwants to minimize storage costs. \n \nWhich storage solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Move the data objects to S3 Glacier Deep Archive after 30 days.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is the best solution because it leverages CloudWatch metric filters to analyze CloudTrail logs for specific error codes related to illegal API calls. CloudTrail captures API activity, and CloudWatch metric filters can extract relevant data from these logs. Creating an alarm based on the metric's rate allows for near-real-time detection of unusual activity. The SNS notification ensures that the appropriate team is alerted promptly. This approach is cost-effective and efficient for monitoring specific events within CloudTrail logs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nis incorrect because Athena and QuickSight are primarily for historical analysis and reporting, not near-real-time alerting. Running Athena queries against CloudTrail logs and generating reports is a batch-oriented process that is not suitable for immediate detection of illegal API calls. It's more appropriate for post-incident analysis or trend identification.\n\n**Why option 3 is incorrect:**\nis incorrect because Trusted Advisor focuses on best practices and service limits, not on detecting illegal API calls. While exceeding service limits might indirectly indicate an issue, it's not a direct indicator of unauthorized API activity. Furthermore, Trusted Advisor checks are not performed in near-real-time, making it unsuitable for the required alerting mechanism.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company has a three-tier application on AWS that ingests sensor data from its users devices. \nThe traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the \nweb tier, and finally to EC2 instances for the application tier. The application tier makes calls to a \ndatabase. \n \nWhat should a solutions architect do to improve the security of the data in transit?",
    "options": [
      {
        "id": 0,
        "text": "Configure a TLS listener. Deploy the server certificate on the NLB.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure AWS Shield Advanced. Enable AWS WAF on the NLB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because target tracking scaling policies are designed to maintain a specific metric at a target value. By configuring the Auto Scaling group to use a target tracking policy with CPU utilization as the target metric and a target value of 50%, the Auto Scaling group will automatically adjust the number of EC2 instances to keep the average CPU utilization of the group as close to 50% as possible. This is the most efficient and automated way to achieve the desired performance state.\n\n**Why option 1 is incorrect:**\nis incorrect because while a CloudWatch alarm *can* trigger scaling actions, it doesn't inherently maintain a target CPU utilization. You would need to manually configure the alarm to trigger scaling actions, and the scaling actions themselves would need to be carefully calibrated. This approach is less automated and less precise than using a target tracking policy. It also doesn't automatically adjust to changes in workload patterns.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nis incorrect because step scaling policies react to alarms based on thresholds, similar to simple scaling. However, step scaling allows you to define multiple scaling adjustments based on the severity of the alarm breach. While more flexible than simple scaling, it still requires manual configuration of the scaling adjustments and doesn't automatically adjust to maintain a target value like target tracking scaling.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A social media company runs its application on Amazon EC2 instances behind an Application \nLoad Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The \napplication has more than a billion images stored in an Amazon S3 bucket and processes \nthousands of images each second. The company wants to resize the images dynamically and \nserve appropriate formats to clients. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Install an external image management library on an EC2 instance. Use the image management",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a CloudFront origin request policy. Use the policy to automatically resize images and to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use a Lambda@Edge function with an external image management library. Associate the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a CloudFront response headers policy. Use the policy to automatically resize images and",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because it leverages EFS resource policies for cross-account access, shared VPC or peered VPC for network connectivity, and EFS access points for controlled access. EFS resource policies allow the research partner's account to grant the central account access to the EFS file system. Using a shared VPC or peered VPC allows the Lambda function in the central account to access the EFS mount target. EFS access points provide a controlled entry point to the file system, enforcing a specific user identity and root directory. This solution is scalable because EFS is designed to handle growing data volumes. It is cost-efficient because it avoids data duplication or complex data transfer mechanisms. It minimizes operational overhead by using managed services and established AWS security mechanisms.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because packaging the genomic input data as a Lambda layer is not suitable for large files. Lambda layers have size limitations, and this approach would require frequent updates to the layer as the data changes. This is not scalable or cost-efficient for large genomic datasets. Furthermore, managing and updating Lambda layers across accounts adds operational overhead.\n\n**Why option 3 is incorrect:**\nis incorrect because copying EFS contents to S3 using DataSync introduces data duplication, which increases storage costs. It also adds operational overhead for managing DataSync jobs and S3 access points. The Lambda function would need to be modified to use S3 API calls instead of file system operations, which is less efficient for genomics data analysis workloads that typically rely on file system access patterns. This solution is also less performant due to the data transfer and the different access patterns required for S3.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 55,
    "text": "A hospital needs to store patient records in an Amazon S3 bucket. The hospital's compliance \nteam must ensure that all protected health information (PHI) is encrypted in transit and at rest. \nThe compliance team must administer the encryption key for data at rest. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nusing Amazon CloudFront with a custom origin pointing to the on-premises servers, is the correct solution. CloudFront is a content delivery network (CDN) that caches website content at edge locations around the world. By using CloudFront, the website's static content (images, CSS, JavaScript, etc.) will be cached at edge locations in Asia, reducing latency for Asian users. The custom origin allows CloudFront to retrieve content from the existing on-premises servers in the US. This solution meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nleveraging an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers, is incorrect. While Route 53 geo-proximity routing can direct users to the closest server, it doesn't cache content. Users in Asia would still need to connect to the on-premises servers in the US, resulting in high latency. This option doesn't address the need for content caching and optimization.\n\n**Why option 3 is incorrect:**\nusing Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53, is also correct. This is because Route 53 is used to resolve the on-premise server's IP address. CloudFront will then cache the content from the on-premise server. This option is functionally equivalent to option 0 and also meets all the requirements: it optimizes website loading times for Asian users, keeps the backend in the US, and can be implemented relatively quickly.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 56,
    "text": "A data analytics company wants to migrate its batch processing system to AWS. The company \nreceives thousands of small data files periodically during the day through FTP. A on-premises \nbatch job processes the data files overnight. However, the batch job takes hours to finish running. \nThe company wants the AWS solution to process incoming data files are possible with minimal \nchanges to the FTP clients that send the files. The solution must delete the incoming data files \nthe files have been processed successfully. Processing for each file needs to take 3-8 minutes. \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAWS Global Accelerator is the best solution because it provides static IP addresses that act as a fixed entry point for the application. It uses the AWS global network to route traffic to the nearest healthy endpoint based on network conditions and health checks. Global Accelerator supports UDP, which is crucial for the gaming application. Furthermore, it allows for fast regional failover, typically within seconds, by automatically redirecting traffic to a healthy region if one becomes unavailable. The static IP addresses also simplify integration with the company's custom DNS service since the DNS records can point to these static IPs, and Global Accelerator handles the underlying routing complexities.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nAWS Elastic Load Balancing (ELB) is primarily a regional service. While Application Load Balancer (ALB) and Network Load Balancer (NLB) support UDP, they do not inherently provide the global reach and fast failover capabilities required by the question. ELB requires integration with Route 53 for global distribution, which adds complexity and might not be as fast as Global Accelerator's failover mechanism. Also, the question mentions using a custom DNS service, making ELB less suitable for the global routing aspect.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) primarily designed for caching and delivering static and dynamic content. While it can improve performance by caching content closer to users, it is not the best solution for a gaming application that relies on UDP and requires fast regional failover. CloudFront is not designed for real-time, interactive applications like games that depend on low-latency UDP connections. Also, it's not the primary solution for handling regional failover in the context of a custom DNS service.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 58,
    "text": "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. \nThe application has three tiers, a business tier, and a database tier with Microsoft SQL Server. \nThe company wants to use specific features of SQL Server such as native backups and Data \nQuality Services. The company also needs to share files for process between the tiers. \n \nHow should a solution architect design the architecture to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Host all three on Amazon instances. Use Mmazon FSx File Gateway for file sharing between",
        "correct": false
      },
      {
        "id": 1,
        "text": "Host all three on Amazon EC2 instances. Use Amazon FSx for Windows file sharing between the",
        "correct": true
      },
      {
        "id": 2,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      },
      {
        "id": 3,
        "text": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage built on Windows Server. It supports the SMB protocol, which is the native file sharing protocol for Windows. Because the on-premises environment uses Microsoft DFS, FSx for Windows File Server is the ideal choice as it natively supports DFS Namespaces and DFS Replication. This allows for seamless migration and integration of the existing DFS infrastructure into AWS. FSx for Windows File Server can be used to host the DFS namespaces and replicate data from the on-premises DFS servers, enabling the organization to run data-intensive analytics workloads in AWS while maintaining compatibility with their existing DFS infrastructure.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nAmazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. While it's suitable for data-intensive analytics, it does *not* natively support Microsoft DFS. Therefore, it's not the best option for migrating and supporting an existing DFS environment.\n\n**Why option 3 is incorrect:**\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) provides a managed Active Directory service in AWS. While it's useful for managing users and groups and integrating with Windows-based applications, it does not directly address the requirement of supporting Microsoft's Distributed File System (DFS). It's more about identity and access management, not file storage and DFS compatibility.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The \ncompany has VPCs with public subnets and private subnets in its AWS account. The EC2 \ninstances run in a private subnet in one of the VPCs. The Lambda functions need direct network \naccess to the EC2 instances for the application to work. \n \nThe application will run for at least 1 year. The company expects the number of Lambda functions \nthat the application uses to increase during that time. The company wants to maximize its savings \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n222 \non all application resources and to keep network latency between the services low. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase on an EC2 instance Savings Plan. Optimize the Lambda functions duration and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Purchase a Compute Savings Plan. Optimize the Lambda functions duration and memory usage,",
        "correct": true
      },
      {
        "id": 3,
        "text": "Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage,",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct answer because it utilizes a combination of services that provide a fully managed, scalable, and cost-effective solution. Amazon API Gateway provides a secure and scalable endpoint for receiving transaction requests. AWS Lambda handles the lightweight validation step without requiring server management. Amazon ECS with the Fargate launch type is used for the compute- and memory-intensive backend processing. Fargate eliminates the need to manage EC2 instances, allowing ECS to automatically provision and scale compute resources based on demand. This approach minimizes operational overhead and aligns with the requirement for a fully managed solution.\n\n**Why option 0 is incorrect:**\nis incorrect because Amazon SQS is primarily a message queuing service, not an API endpoint for receiving requests directly from mobile devices. While SQS can be used in conjunction with other services, it doesn't inherently provide the secure endpoint and request handling capabilities of API Gateway. Amazon EventBridge is more suited for event-driven architectures and less ideal for direct request validation. Amazon Lightsail instances, while simple to use, require more management than Fargate and may not scale as efficiently for compute-intensive workloads. The dynamic scaling policies based on memory thresholds and instance health checks are also more complex to configure and maintain compared to Fargate's automatic scaling.\n\n**Why option 1 is incorrect:**\nis incorrect because Amazon EKS Anywhere involves managing Kubernetes clusters on-premises. This significantly increases operational overhead, contradicting the requirement for a fully managed solution and minimizing infrastructure maintenance. While EKS Anywhere provides flexibility, it requires managing the underlying infrastructure, including servers, networking, and storage. This includes patching, scaling, and troubleshooting, which are all handled automatically by fully managed services like ECS Fargate. Using on-premises servers also introduces latency and availability concerns compared to a fully cloud-native solution.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 61,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n223 \nA company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts \nthe website on Amazon S3 and integrates the website with an API that handles sales requests. \nThe company hosts the API on three Amazon EC2 instances behind an Application Load \nBalancer (ALB). The API consists of static and dynamic front-end content along with backend \nworkers that process sales requests asynchronously. \n \nThe company is expecting a significant and sudden increase in the number of sales requests \nduring events for the launch of new products. \n \nWhat should a solutions architect recommend to ensure that all the requests are processed \nsuccessfully?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because when an AMI is copied from Region A to Region B, the underlying snapshot data is also copied. When instance 1B is launched from the AMI in Region B, it exists as an EC2 instance. Therefore, Region B contains the EC2 instance (1B), the AMI (copied from Region A), and the snapshot (copied as part of the AMI copy process).\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nis incorrect because only one AMI is copied to Region B. The question does not state that another AMI was created in Region B. The snapshot is also present in Region B.\n\n**Why option 2 is incorrect:**\nis incorrect because the snapshot associated with the AMI is also copied to Region B during the AMI copy process.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A companys web application consists of an Amazon API Gateway API in front of an AWS \nLambda function and an Amazon DynamoDB database. The Lambda function handles the \nbusiness logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito \nuser pools to identify the individual users of the application. A solutions architect needs to update \nthe application so that only users who have a subscription can access premium content. \n \nWhich solution will meet this requirement with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Enable API caching and throttling on the API Gateway API.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS WAF on the API Gateway API Create a rule to filter users who have a subscription.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Apply fine-grained IAM permissions to the premium content in the DynamoDB table.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Implement API usage plans and API keys to limit the access of users who do not have a",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because it utilizes a scheduled action within the Auto Scaling Group. A scheduled action allows you to set the desired capacity of the ASG to 10 at the designated hour on the last day of each month. This ensures that the ASG scales out to the required number of instances *before* the peak traffic arrives, preventing performance lag. Setting the 'desired capacity' directly tells the ASG how many instances it should be running. The ASG will handle launching the necessary instances to meet this desired capacity.\n\n**Why option 0 is incorrect:**\nis incorrect because setting both the min and max count to 10 doesn't guarantee that the ASG will scale *before* the peak. It only ensures that the ASG can have a minimum and maximum of 10 instances. The initial capacity might still be 2, and the scaling event might happen *during* the peak, not before. The desired capacity is what triggers the scaling action.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOptions 2 and 3 are incorrect because simple and target tracking scaling policies are designed to respond to changes in metrics (like CPU utilization or network traffic) in real-time. They are not suitable for predictable, scheduled scaling events. While they *could* eventually scale to 10 instances if the load increases, they won't proactively scale *before* the peak, leading to the same performance lag problem. Also, these policies don't directly allow you to set the instance count at a specific time.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company's application runs on AWS. The application stores large documents in an Amazon S3 \nbucket that uses the S3 Standard-infrequent Access (S3 Standerd-IA) storage class. The \ncompany will continue paying to store the data but wants to save on its total S3 costs. The \ncompany wants authorized external users to have the ability to access the documents in \nmilliseconds. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure the S3 bucket to be a Requester Pays bucket.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the storage tier to S3 Standard for all existing and future objects.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on S3 Transfer Acceleration tor the S3 Docket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon CloudFront to handle all the requests to the S3 bucket.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nOptions 1 and 3 are the most cost-effective solutions.\n\n*   **Option 1: Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket:** S3 Transfer Acceleration utilizes globally distributed edge locations (CloudFront Edge Locations) to optimize the transfer of data to S3. When data is uploaded to an edge location, it is then transferred to the S3 bucket over the AWS backbone network, which is generally faster and more reliable than the public internet. This reduces latency and improves upload speeds, especially for geographically distant locations. It's designed specifically for this type of scenario and is generally cost-effective for large file transfers.\n\n*   **Option 3: Use multipart uploads for faster file uploads into the destination Amazon S3 bucket:** Multipart upload allows you to upload a single object as a set of parts. Each part can be uploaded independently, and in parallel. This can significantly improve upload speed, especially over unreliable networks, as individual parts can be retried without re-uploading the entire file. It also allows for uploading large files that exceed the single object size limit. It is a built-in S3 feature and doesn't incur additional costs beyond standard S3 storage and request charges. It also improves resilience to network interruptions.\n\n**Why option 0 is incorrect:**\nOption 0: Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket. While Global Accelerator can improve network performance, it's generally more expensive than S3 Transfer Acceleration for S3 uploads. S3 Transfer Acceleration is specifically designed for this use case and is more cost-effective. Global Accelerator is better suited for applications that need to be available from multiple regions and require dynamic routing.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nOption 2: Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3. While Direct Connect provides a dedicated network connection, it is very expensive to establish and maintain, especially across multiple geographically dispersed locations. It's overkill for simply improving S3 upload speeds and is not cost-effective. Direct Connect is more appropriate when you need consistent, high-bandwidth connectivity for hybrid cloud environments or have strict security requirements.",
    "domain": "Design Cost-Optimized Architectures"
  }
]