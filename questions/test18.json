[
  {
    "id": 0,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n285 \nA company is building a three-tier application on AWS. The presentation tier will serve a static \nwebsite The logic tier is a containerized application. This application will store data in a relational \ndatabase. The company wants to simplify deployment and to reduce operational costs. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon S3 is a highly scalable and cost-effective storage service that can be used to host static website content. It provides durability, high availability, and low latency access to the static files. Amazon ECS with AWS Fargate eliminates the need to manage the underlying infrastructure. It allows you to run containerized applications without provisioning or managing EC2 instances. This reduces operational overhead and provides scalability. By using a managed Amazon RDS cluster for the database, you can offload the management tasks such as backups, patching, and monitoring to AWS. This reduces the operational burden and ensures high availability and durability of the database.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 1,
    "text": "A company seeks a storage solution for its application. The solution must be highly available and \nscalable. The solution also must function as a file system be mountable by multiple Linux \ninstances in AWS and on premises through native protocols, and have no minimum size \nrequirements. The company has set up a Site-to-Site VPN for access from its on-premises \nnetwork to its VPC. \n \nWhich storage solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon FSx Multi-AZ deployments",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon Elastic File System (Amazon EFS) with multiple mount targets",
        "correct": true
      },
      {
        "id": 3,
        "text": "Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon EFS is a fully managed file system service that provides scalable, shared storage for Amazon EC2 instances. It supports the Network File System version 4 (NFSv4) protocol, which is a native protocol for Linux-based systems. EFS is designed to be highly available, durable, and scalable.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 2,
    "text": "A 4-year-old media company is using the AWS Organizations all features feature set to organize \nits AWS accounts. According to the company's finance team, the billing information on the \nmember accounts must not be accessible to anyone, including the root user of the member \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n286 \naccounts. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Add all finance team users to an IAM group. Attach an AWS managed policy named Billing to the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Attach an identity-based policy to deny access to the billing information to all users, including the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to",
        "correct": true
      },
      {
        "id": 3,
        "text": "Convert from the Organizations all features feature set to the Organizations consolidated billing",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nService control policy are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization's access control guidelines. SCPs are available only in an organization that has all features enabled.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 3,
    "text": "An ecommerce company runs an application in the AWS Cloud that is integrated with an on-\npremises warehouse solution. The company uses Amazon Simple Notification Service (Amazon \nSNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application \ncan process the orders. The local data center team has detected that some of the order \nmessages were not received. \n \nA solutions architect needs to retain messages that are not delivered and analyze the messages \nfor up to 14 days. \n \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe message retention period in Amazon SQS can be set between 1 minute and 14 days (the default is 4 days). Therefore, you can configure your SQS DLQ to retain undelivered SNS messages for 14 days. This will enable you to analyze undelivered messages with the least development effort.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 4,
    "text": "A gaming company uses Amazon DynamoDB to store user information such as geographic \nlocation, player data, and leaderboards. The company needs to configure continuous backups to \nan Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n287 \nof the application and must not affect the read capacity units (RCUs) that are defined for the \ntable. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nContinuous backups is a native feature of DynamoDB, it works at any scale without having to manage servers or clusters and allows you to export data across AWS Regions and accounts to any point-in-time in the last 35 days at a per-second granularity. Plus, it doesn't affect the read capacity or the availability of your production tables. https://aws.amazon.com/blogs/aws/new-export-amazon-dynamodb-table-data-to-data-lake- amazon-s3/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 5,
    "text": "A solutions architect is designing an asynchronous application to process credit card data \nvalidation requests for a bank. The application must be secure and be able to process each \nrequest at least once. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS)",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/zh_tw/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs- least-privilege-policy.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 6,
    "text": "A company has multiple AWS accounts for development work. Some staff consistently use \noversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the \ndevelopment accounts. The company wants to centrally restrict the creation of AWS resources in \nthese accounts. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n288 \nWhich solution will meet these requirements with the LEAST development effort?",
    "options": [
      {
        "id": 0,
        "text": "Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Organizations to organize the accounts into organizational units (OUs). Define and",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Organizations: AWS Organizations is a service that helps you centrally manage multiple AWS accounts. It enables you to group accounts into organizational units (OUs) and apply policies across those accounts. Service Control Policies (SCPs): SCPs in AWS Organizations allow you to define fine-grained permissions and restrictions at the account or OU level. By attaching an SCP to the development accounts, you can control the creation and usage of EC2 instance types. Least Development Effort: Option B requires minimal development effort as it leverages the built- in features of AWS Organizations and SCPs. You can define the SCP to restrict the use of oversized EC2 instance types and apply it to the appropriate OUs or accounts.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 7,
    "text": "A company wants to use artificial intelligence (AI) to determine the quality of its customer service \ncalls. The company currently manages calls in four different languages, including English. The \ncompany will offer new languages in the future. The company does not have the resources to \nregularly maintain machine learning (ML) models. \n \nThe company needs to create written sentiment analysis reports from the customer service call \nrecordings. The customer service call recording text must be translated into English. \n \nWhich combination of steps will meet these requirements? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Comprehend to translate the audio recordings into English.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Lex to create the written sentiment analysis reports.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Polly to convert the audio recordings into text.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Transcribe to convert the audio recordings in any language into text.",
        "correct": true
      },
      {
        "id": 4,
        "text": "Use Amazon Translate to translate text in any language to English.",
        "correct": false
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon Transcribe will convert the audio recordings into text, Amazon Translate will translate the text into English, and Amazon Comprehend will perform sentiment analysis on the translated text to generate sentiment analysis reports.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 8,
    "text": "A company uses Amazon EC2 instances to host its internal systems. As part of a deployment \noperation, an administrator tries to use the AWS CLI to terminate an EC2 instance. However, the \nadministrator receives a 403 (Access Denied) error message. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n289 \nThe administrator is using an IAM role that has the following IAM policy attached: \n \n \n \nWhat is the cause of the unsuccessful request?",
    "options": [
      {
        "id": 0,
        "text": "The EC2 instance has a resource-based policy with a Deny statement.",
        "correct": false
      },
      {
        "id": 1,
        "text": "The principal has not been specified in the policy statement.",
        "correct": false
      },
      {
        "id": 2,
        "text": "The \"Action\" field does not grant the actions that are required to terminate the EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 9,
    "text": "A company is conducting an internal audit. The company wants to ensure that the data in an \nAmazon S3 bucket that is associated with the company's AWS Lake Formation data lake does \nnot contain sensitive customer or employee data. The company wants to discover personally \nidentifiable information (PII) or financial information, including passport numbers and credit card \nnumbers. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n290 \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon Macie to run a data discovery job that uses managed identifiers for the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon S3 Select to run a report across the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nAmazon Macie is a service that helps discover, classify, and protect sensitive data stored in AWS. It uses machine learning algorithms and managed identifiers to detect various types of sensitive information, including personally identifiable information (PII) and financial information. By configuring Amazon Macie to run a data discovery job with the appropriate managed identifiers for the required data types (such as passport numbers and credit card numbers), the company can identify and classify any sensitive data present in the S3 bucket.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company uses on-premises servers to host its applications. The company is running out of \nstorage capacity. The applications use both block storage and NFS storage. The company needs \na high-performing solution that supports local caching without re-architecting its existing \napplications. \n \nWhich combination of actions should a solutions architect take to meet these requirements? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Mount Amazon S3 as a file system to the on-premises servers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS Storage Gateway file gateway to replace NFS storage.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Storage Gateway volume gateway to replace the block storage.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy combining the deployment of an AWS Storage Gateway file gateway and an AWS Storage Gateway volume gateway, the company can address both its block storage and NFS storage needs, while leveraging local caching capabilities for improved performance.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 11,
    "text": "A company has a service that reads and writes large amounts of data from an Amazon S3 bucket \nin the same AWS Region. The service is deployed on Amazon EC2 instances within the private \nsubnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public \nsubnet. However, the company wants a solution that will reduce the data output costs. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nA VPC gateway endpoint allows you to privately access Amazon S3 from within your VPC without using a NAT gateway or NAT instance. By provisioning a VPC gateway endpoint for S3, the service in the private subnet can directly communicate with S3 without incurring data transfer costs for traffic going through a NAT gateway.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 12,
    "text": "A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize \napplication changes, the company stores the pictures as the latest version of an S3 object. The \ncompany needs to retain only the two most recent versions of the pictures. \n \nThe company wants to reduce costs. The company has identified the S3 bucket as a large \nexpense. \n \nWhich solution will reduce the S3 costs with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use an AWS Lambda function to check for older versions and delete all but the two most recent",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deactivate versioning on the S3 bucket and retain the two most recent versions.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nS3 Lifecycle policies allow you to define rules that automatically transition or expire objects based on their age or other criteria. By configuring an S3 Lifecycle policy to delete expired object versions and retain only the two most recent versions, you can effectively manage the storage costs while maintaining the desired retention policy. This solution is highly automated and requires minimal operational overhead as the lifecycle management is handled by S3 itself.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 13,
    "text": "A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The \ncompany's average connection utilization is less than 10%. A solutions architect must \nrecommend a solution that will reduce the cost without compromising security. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up a new 1 Gbps Direct Connect connection. Share the connection with another AWS",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Contact an AWS Direct Connect Partner to order a 1 Gbps connection. Share the connection with",
        "correct": false
      },
      {
        "id": 3,
        "text": "Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nFor Dedicated Connections, 1 Gbps, 10 Gbps, and 100 Gbps ports are available. For Hosted Connections, connection speeds of 50 Mbps, 100 Mbps, 200 Mbps, 300 Mbps, 400 Mbps, 500 Mbps, 1 Gbps, 2 Gbps, 5 Gbps and 10 Gbps may be ordered from approved AWS Direct Connect Partners.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company has multiple Windows file servers on premises. The company wants to migrate and \nconsolidate its files into an Amazon FSx for Windows File Server file system. File permissions \nmust be preserved to ensure that access rights do not change. \n \nWhich solutions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule",
        "correct": false
      },
      {
        "id": 2,
        "text": "Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS",
        "correct": false
      },
      {
        "id": 4,
        "text": "Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nA - This option involves deploying DataSync agents on your on-premises file servers and using DataSync to transfer the data directly to the FSx for Windows File Server. DataSync ensures that file permissions are preserved during the migration process. D - This option involves using an AWS Snowcone device, a portable data transfer device. You would connect the Snowcone device to your on-premises network, launch DataSync agents on the device, and schedule DataSync tasks to transfer the data to FSx for Windows File Server. DataSync handles the migration process while preserving file permissions.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "A company wants to ingest customer payment data into the company's data lake in Amazon S3. \nThe company receives payment data every minute on average. The company wants to analyze \nthe payment data in real time. Then the company wants to ingest the data into the data lake. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Streams to ingest data. Use AWS Lambda to analyze the data in real",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Glue to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Kinesis Data Firehose to ingest data. Use Amazon Kinesis Data Analytics to",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway to ingest data. Use AWS Lambda to analyze the data in real time.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nBy leveraging the combination of Amazon Kinesis Data Firehose and Amazon Kinesis Data Analytics, you can efficiently ingest and analyze the payment data in real time without the need for manual processing or additional infrastructure management. This solution provides a streamlined and scalable approach to handle continuous data ingestion and analysis requirements.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 16,
    "text": "A company runs a website that uses a content management system (CMS) on Amazon EC2. The \nCMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance \nfor the data tier. Website images are stored on an Amazon Elastic Block Store (Amazon EBS) \nvolume that is mounted inside the EC2 instance. \n \nWhich combination of actions should a solutions architect take to improve the performance and \nresilience of the website? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance",
        "correct": false
      },
      {
        "id": 1,
        "text": "Share the website images by using an NFS share from the primary EC2 instance. Mount this",
        "correct": false
      },
      {
        "id": 2,
        "text": "Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nBy combining the use of Amazon EFS for shared file storage and Amazon CloudFront for content delivery, you can achieve improved performance and resilience for the website.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 17,
    "text": "A company runs an infrastructure monitoring service. The company is building a new feature that \nwill enable the service to monitor data in customer AWS accounts. The new feature will call AWS \nAPIs in customer accounts to describe Amazon EC2 instances and read Amazon CloudWatch \nmetrics. \n \nWhat should the company do to obtain access to customer accounts in the MOST secure way?",
    "options": [
      {
        "id": 0,
        "text": "Ensure that the customers create an IAM role in their account with read-only EC2 and",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a serverless API that implements a token vending machine to provide temporary AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Ensure that the customers create an IAM user in their account with read-only EC2 and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Ensure that the customers create an Amazon Cognito user in their account to use an IAM role",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy having customers create an IAM role with the necessary permissions in their own accounts, the company can use AWS Identity and Access Management (IAM) to establish cross-account access. The trust policy allows the company's AWS account to assume the customer's IAM role temporarily, granting access to the specified resources (EC2 instances and CloudWatch metrics) within the customer's account. This approach follows the principle of least privilege, as the company only requests the necessary permissions and does not require long-term access keys or user credentials from the customers.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS \naccounts. The company's networking team has its own AWS account to manage the cloud \nnetwork. \n \nWhat is the MOST operationally efficient solution to connect the VPCs?",
    "options": [
      {
        "id": 0,
        "text": "Set up VPC peering connections between each VPC. Update each associated subnet's route",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Transit Gateway in the networking team's AWS account. Configure static routes",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy VPN gateways in each VPC. Create a transit VPC in the networking team's AWS account",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nWS Transit Gateway is a highly scalable and centralized hub for connecting multiple VPCs, on- premises networks, and remote networks. It simplifies network connectivity by providing a single entry point and reducing the number of connections required. In this scenario, deploying an AWS Transit Gateway in the networking team's AWS account allows for efficient management and control over the network connectivity across multiple VPCs.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 19,
    "text": "A company has Amazon EC2 instances that run nightly batch jobs to process data. The EC2 \ninstances run in an Auto Scaling group that uses On-Demand billing. If a job fails on one \ninstance, another instance will reprocess the job. The batch jobs run between 12:00 AM and \n06:00 AM local time every day. \n \nWhich solution will provide EC2 instances to meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto",
        "correct": false
      },
      {
        "id": 1,
        "text": "Purchase a 1-year Reserved Instance for the specific instance type and operating system of the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a new launch template for the Auto Scaling group. Increase the instance size. Set a policy",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nPurchasing a 1-year Savings Plan (option A) or a 1-year Reserved Instance (option B) may provide cost savings, but they are more suitable for long-running, steady-state workloads. Since your batch jobs run for a specific period each day, using Spot Instances with the ability to scale out based on CPU usage is a more cost-effective choice.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 20,
    "text": "A social media company is building a feature for its website. The feature will give users the ability \nto upload photos. The company expects significant increases in demand during large events and \nmust ensure that the website can handle the upload traffic from users. \n \nWhich solution meets these requirements with the MOST scalability?",
    "options": [
      {
        "id": 0,
        "text": "Upload files from the user's browser to the application servers. Transfer the files to an Amazon S3",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an AWS Storage Gateway file gateway. Upload files directly from the user's browser to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Generate Amazon S3 presigned URLs in the application. Upload files directly from the user's",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision an Amazon Elastic File System (Amazon EFS) file system. Upload files directly from the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis approach allows users to upload files directly to S3 without passing through the application servers, reducing the load on the application and improving scalability. It leverages the client-side capabilities to handle the file uploads and offloads the processing to S3.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 21,
    "text": "A company has a web application for travel ticketing. The application is based on a database that \nruns in a single data center in North America. The company wants to expand the application to \nserve a global user base. The company needs to deploy the application to multiple AWS Regions. \nAverage latency must be less than 1 second on updates to the reservation database. \n \nThe company wants to have separate deployments of its web platform across multiple Regions. \nHowever, the company must maintain a single primary reservation database that is globally \nconsistent. \n \nWhich solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Convert the application to use Amazon DynamoDB. Use a global table for the center reservation",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the database to an Amazon Aurora MySQL database. Deploy Aurora Read Replicas in",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database to an Amazon RDS for MySQL database. Deploy MySQL read replicas in",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the application to an Amazon Aurora Serverless database. Deploy instances of the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nUsing DynamoDB's global tables feature, you can achieve a globally consistent reservation database with low latency on updates, making it suitable for serving a global user base. The automatic replication provided by DynamoDB eliminates the need for manual synchronization between Regions.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 22,
    "text": "A company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 \ninstances that run in the us-west-1 Region. The company manually backs up the workloads to \ncreate an image as needed. \n \nIn the event of a natural disaster in the us-west-1 Region, the company wants to recover \nworkloads quickly in the us-west-2 Region. The company wants no more than 24 hours of data \nloss on the EC2 instances. The company also wants to automate any backups of the EC2 \ninstances. \n \nWhich solutions will meet these requirements with the LEAST administrative effort? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create backup vaults in us-west-1 and in us-west-2 by using AWS Backup. Create a backup plan",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a backup vault by using AWS Backup. Use AWS Backup to create a backup plan for the",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create a backup vault by using AWS Backup. Use AWS Backup to create a backup plan for the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nSolutions are both automated and require no manual intervention to create or copy backupsã€‚\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 23,
    "text": "A company operates a two-tier application for image processing. The application uses two \nAvailability Zones, each with one public subnet and one private subnet. An Application Load \nBalancer (ALB) for the web tier uses the public subnets. Amazon EC2 instances for the \napplication tier use the private subnets. \n \nUsers report that the application is running more slowly than expected. A security audit of the web \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n297 \nserver log files shows that the application is receiving millions of illegitimate requests from a small \nnumber of IP addresses. A solutions architect needs to resolve the immediate performance \nproblem while the company investigates a more permanent solution. \n \nWhat should the solutions architect recommend to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Modify the inbound security group for the web tier. Add a deny rule for the IP addresses that are",
        "correct": false
      },
      {
        "id": 1,
        "text": "Modify the network ACL for the web tier subnets. Add an inbound deny rule for the IP addresses",
        "correct": true
      },
      {
        "id": 2,
        "text": "Modify the inbound security group for the application tier. Add a deny rule for the IP addresses",
        "correct": false
      },
      {
        "id": 3,
        "text": "Modify the network ACL for the application tier subnets. Add an inbound deny rule for the IP",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIn this scenario, the security audit reveals that the application is receiving millions of illegitimate requests from a small number of IP addresses. To address this issue, it is recommended to modify the network ACL (Access Control List) for the web tier subnets. By adding an inbound deny rule specifically targeting the IP addresses that are consuming resources, the network ACL can block the illegitimate traffic at the subnet level before it reaches the web servers. This will help alleviate the excessive load on the web tier and improve the application's performance.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 24,
    "text": "A global marketing company has applications that run in the ap-southeast-2 Region and the eu-\nwest-1 Region. Applications that run in a VPC in eu-west-1 need to communicate securely with \ndatabases that run in a VPC in ap-southeast-2. \n \nWhich network design will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nYou cannot reference the security group of a peer VPC that's in a different Region. Instead, use the CIDR block of the peer VPC. https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 25,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n298 \nA company is developing software that uses a PostgreSQL database schema. The company \nneeds to configure multiple development environments and databases for the company's \ndevelopers. On average, each development environment is used for half of the 8-hour workday. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure each development environment with its own Amazon Aurora PostgreSQL database",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure each development environment with its own Amazon RDS for PostgreSQL Single-AZ",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure each development environment with its own Amazon S3 bucket by using Amazon S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nWith Aurora Serverless, you create a database, specify the desired database capacity range, and connect your applications. You pay on a per-second basis for the database capacity that you use when the database is active, and migrate between standard and serverless configurations with a few steps in the Amazon Relational Database Service (Amazon RDS) console.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 26,
    "text": "A company uses AWS Organizations with resources tagged by account. The company also uses \nAWS Backup to back up its AWS infrastructure resources. The company needs to back up all \nAWS resources. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Config to identify all untagged resources. Tag the identified resources",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Config to identify all resources that are not running. Add those resources to the backup",
        "correct": false
      },
      {
        "id": 2,
        "text": "Require all AWS account owners to review their resources to identify the resources that need to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon Inspector to identify all noncompliant resources.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis solution allows you to leverage AWS Config to identify any untagged resources within your AWS Organizations accounts. Once identified, you can programmatically apply the necessary tags to indicate the backup requirements for each resource. By using tags in the backup plan configuration, you can ensure that only the tagged resources are included in the backup process, reducing operational overhead and ensuring all necessary resources are backed up.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 27,
    "text": "A social media company wants to allow its users to upload images in an application that is hosted \nin the AWS Cloud. The company needs a solution that automatically resizes the images so that \nthe images can be displayed on multiple device types. The application experiences unpredictable \ntraffic patterns throughout the day. The company is seeking a highly available solution that \nmaximizes scalability. \n \nWhat should a solutions architect do to meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n299",
    "options": [
      {
        "id": 0,
        "text": "Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a static website hosted in Amazon CloudFront that invokes AWS Step Functions to resize",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a dynamic website hosted on a web server that runs on an Amazon EC2 instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a dynamic website hosted on an automatically scaling Amazon Elastic Container Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nBy using Amazon S3 and AWS Lambda together, you can create a serverless architecture that provides highly scalable and available image resizing capabilities. Here's how the solution would work: Set up an Amazon S3 bucket to store the original images uploaded by users. Configure an event trigger on the S3 bucket to invoke an AWS Lambda function whenever a new image is uploaded. The Lambda function can be designed to retrieve the uploaded image, perform the necessary resizing operations based on device requirements, and store the resized images back in the S3 bucket or a different bucket designated for resized images. Configure the Amazon S3 bucket to make the resized images publicly accessible for serving to users.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 28,
    "text": "A company is running a microservices application on Amazon EC2 instances. The company \nwants to migrate the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster \nfor scalability. The company must configure the Amazon EKS control plane with endpoint private \naccess set to true and endpoint public access set to false to maintain security compliance. The \ncompany must also put the data plane in private subnets. However, the company has received \nerror notifications because the node cannot join the cluster. \n \nWhich solution will allow the node to join the cluster?",
    "options": [
      {
        "id": 0,
        "text": "Grant the required permission in AWS Identity and Access Management (IAM) to the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create interface VPC endpoints to allow nodes to access the control plane.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Recreate nodes in the public subnet. Restrict security groups for EC2 nodes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Allow outbound traffic in the security group of the nodes.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy creating interface VPC endpoints, you can enable the necessary communication between the Amazon EKS control plane and the nodes in private subnets. This solution ensures that the control plane maintains endpoint private access (set to true) and endpoint public access (set to false) for security compliance.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company is migrating an on-premises application to AWS. The company wants to use Amazon \nRedshift as a solution. \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n300 \nWhich use cases are suitable for Amazon Redshift in this scenario? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Supporting data APIs to access data with traditional, containerized, and event-driven applications",
        "correct": false
      },
      {
        "id": 1,
        "text": "Supporting client-side and server-side encryption",
        "correct": true
      },
      {
        "id": 2,
        "text": "Building analytics workloads during specified hours and when the application is not active",
        "correct": false
      },
      {
        "id": 3,
        "text": "Caching data to reduce the pressure on the backend database",
        "correct": false
      },
      {
        "id": 4,
        "text": "Scaling globally to support petabytes of data and tens of millions of requests per minute",
        "correct": false
      },
      {
        "id": 1,
        "text": "Supporting client-side and server-side encryption: Amazon Redshift supports both client-side",
        "correct": false
      },
      {
        "id": 2,
        "text": "Building analytics workloads during specified hours and when the application is not active:",
        "correct": false
      },
      {
        "id": 4,
        "text": "Scaling globally to support petabytes of data and tens of millions of requests per minute:",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nB. Supporting client-side and server-side encryption: Amazon Redshift supports both client-side and server-side encryption for improved data security. C. Building analytics workloads during specified hours and when the application is not active: Amazon Redshift is optimized for running complex analytic queries against very large datasets, making it a good choice for this use case. E. Scaling globally to support petabytes of data and tens of millions of requests per minute: Amazon Redshift is designed to handle petabytes of data, and to deliver fast query and I/O performance for virtually any size dataset.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A company provides an API interface to customers so the customers can retrieve their financial \ninformation. èˆŽe company expects a larger number of requests during peak usage times of the \nyear. \n \nThe company requires the API to respond consistently with low latency to ensure customer \nsatisfaction. The company needs to provide a compute host for the API. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an Application Load Balancer and an Amazon Elastic Kubernetes Service (Amazon EKS)",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway and AWS Lambda functions with reserved concurrency.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIn the context of the given scenario, where the company wants low latency and consistent performance for their API during peak usage times, it would be more suitable to use provisioned concurrency. By allocating a specific number of concurrent executions, the company can ensure that there are enough function instances available to handle the expected load and minimize the impact of cold starts. This will result in lower latency and improved performance for the API.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 31,
    "text": "A company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 \nbucket for archival purposes. \n \nWhich solution will meet this requirement with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Enable S3 logging in the Systems Manager console. Choose an S3 bucket to send the session",
        "correct": true
      },
      {
        "id": 1,
        "text": "Install the Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Export the logs",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a Systems Manager document to upload all server logs to a central S3 bucket. Use",
        "correct": false
      },
      {
        "id": 3,
        "text": "Install an Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Create a",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-logging.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "An application uses an Amazon RDS MySQL DB instance. The RDS database is becoming low \non disk space. A solutions architect wants to increase the disk space without downtime. \n \nWhich solution meets these requirements with the LEAST amount of effort?",
    "options": [
      {
        "id": 0,
        "text": "Enable storage autoscaling in RDS",
        "correct": true
      },
      {
        "id": 1,
        "text": "Increase the RDS database instance size",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the RDS database instance storage type to Provisioned IOPS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Back up the RDS database, increase the storage capacity, restore the database, and stop the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nEnabling storage autoscaling allows RDS to automatically adjust the storage capacity based on the application's needs. When the storage usage exceeds a predefined threshold, RDS will automatically increase the allocated storage without requiring manual intervention or causing downtime. This ensures that the RDS database has sufficient disk space to handle the increasing storage requirements.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 33,
    "text": "A consulting company provides professional services to customers worldwide. The company \nprovides solutions and tools for customers to expedite gathering and analyzing data on AWS. The \ncompany needs to centrally manage and deploy a common set of solutions and tools for \ncustomers to use for self-service purposes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create AWS CloudFormation templates for the customers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create AWS Service Catalog products for the customers.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create AWS Systems Manager templates for the customers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create AWS Config items for the customers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Service Catalog allows you to create and manage catalogs of IT services that can be deployed within your organization. With Service Catalog, you can define a standardized set of products (solutions and tools in this case) that customers can self-service provision. By creating Service Catalog products, you can control and enforce the deployment of approved and validated Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company is designing a new web application that will run on Amazon EC2 Instances. The \napplication will use Amazon DynamoDB for backend data storage. The application traffic will be \nunpredictable. The company expects that the application read and write throughput to the \ndatabase will be moderate to high. The company needs to scale in response to application traffic. \n \nWhich DynamoDB table configuration will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure DynamoDB with provisioned read and write by using the DynamoDB Standard table",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure DynamoDB with provisioned read and write by using the DynamoDB Standard",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure DynamoDB in on-demand mode by using the DynamoDB Standard Infrequent Access",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nAWS Service Catalog allows you to create and manage catalogs of IT services that can be deployed within your organization. With Service Catalog, you can define a standardized set of products (solutions and tools in this case) that customers can self-service provision. By creating Service Catalog products, you can control and enforce the deployment of approved and validated solutions and tools.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 35,
    "text": "A retail company has several businesses. The IT team for each business manages its own AWS \naccount. Each team account is part of an organization in AWS Organizations. Each team \nmonitors its product inventory levels in an Amazon DynamoDB table in the team's own AWS \naccount. \n \nThe company is deploying a central inventory reporting application into a shared AWS account. \nThe application must be able to read items from all the teams' DynamoDB tables. \n \nWhich authentication option will meet these requirements MOST securely?",
    "options": [
      {
        "id": 0,
        "text": "Integrate DynamoDB with AWS Secrets Manager in the inventory application account. Configure",
        "correct": false
      },
      {
        "id": 1,
        "text": "In every business account, create an IAM user that has programmatic access. Configure the",
        "correct": false
      },
      {
        "id": 2,
        "text": "In every business account, create an IAM role named BU_ROLE with a policy that gives the role",
        "correct": true
      },
      {
        "id": 3,
        "text": "Integrate DynamoDB with AWS Certificate Manager (ACM). Generate identity certificates to",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nIAM Roles: IAM roles provide a secure way to grant permissions to entities within AWS. By creating an IAM role in each business account named BU_ROLE with the necessary permissions to access the DynamoDB table, the access can be controlled at the IAM role level. Cross-Account Access: By configuring a trust policy in the BU_ROLE that trusts a specific role in the inventory application account (APP_ROLE), you establish a trusted relationship between the two accounts. Least Privilege: By creating a specific IAM role (BU_ROLE) in each business account and granting it access only to the required DynamoDB table, you can ensure that each team's table is accessed with the least privilege principle. Security Token Service (STS): The use of STS AssumeRole API operation in the inventory application account allows the application to assume the cross-account role (BU_ROLE) in each business account.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon \nEKS). The company's workload is not consistent throughout the day. The company wants \nAmazon EKS to scale in and out according to the workload. \n \nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use an AWS Lambda function to resize the EKS cluster.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon API Gateway and connect it to Amazon EKS.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS App Mesh to observe network activity.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nBy combining the Kubernetes Cluster Autoscaler (option C) to manage the number of nodes in the cluster and enabling horizontal pod autoscaling (option B) with the Kubernetes Metrics Server, you can achieve automatic scaling of your EKS cluster and container applications based on workload demand. This approach minimizes operational overhead as it leverages built-in Kubernetes functionality and automation mechanisms.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 37,
    "text": "A company runs a microservice-based serverless web application. The application must be able \nto retrieve data from multiple Amazon DynamoDB tables A solutions architect needs to give the \napplication the ability to retrieve the data with no impact on the baseline performance of the \napplication. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "AWS AppSync pipeline resolvers",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon CloudFront with Lambda@Edge functions",
        "correct": false
      },
      {
        "id": 2,
        "text": "Edge-optimized Amazon API Gateway with AWS Lambda functions",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon Athena Federated Query with a DynamoDB connector",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "A company wants to analyze and troubleshoot Access Denied errors and Unauthorized errors \nthat are related to IAM permissions. The company has AWS CloudTrail turned on. \n \nWhich solution will meet these requirements with the LEAST effort?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Glue and write custom scripts to query CloudTrail logs for the errors.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Batch and write custom scripts to query CloudTrail logs for the errors.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Search CloudTrail logs with Amazon Athena queries to identify the errors.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\n\"Using Athena with CloudTrail logs is a powerful way to enhance your analysis of AWS service activity.\" https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 39,
    "text": "A company wants to add its existing AWS usage cost to its operation cost dashboard. A solutions \narchitect needs to recommend a solution that will give the company access to its usage cost \nprogrammatically. The company must be able to access cost data for the current year and \nforecast costs for the next 12 months. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Access usage cost-related data by using the AWS Cost Explorer API with pagination.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Access usage cost-related data by using downloadable AWS Cost Explorer report .csv files.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure AWS Budgets actions to send usage cost data to the company through FTP.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create AWS Budgets reports for usage cost data. Send the data to the company through SMTP.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nYou can view your costs and usage using the Cost Explorer user interface free of charge. You can also access your data programmatically using the Cost Explorer API. Each paginated API request incurs a charge of $0.01. You can't disable Cost Explorer after you enable it. https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/clients/client-cost- explorer/interfaces/costexplorerpaginationconfiguration.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 40,
    "text": "A solutions architect is reviewing the resilience of an application. The solutions architect notices \nthat a database administrator recently failed over the application's Amazon Aurora PostgreSQL \ndatabase writer instance as part of a scaling exercise. The failover resulted in 3 minutes of \ndowntime for the application. \n \nWhich solution will reduce the downtime for scaling exercises with the LEAST operational \noverhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n305",
    "options": [
      {
        "id": 0,
        "text": "Create more Aurora PostgreSQL read replicas in the cluster to handle the load during failover.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up a secondary Aurora PostgreSQL cluster in the same AWS Region. During failover, update",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon ElastiCache for Memcached cluster to handle the load during failover.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up an Amazon RDS proxy for the database. Update the application to use the proxy endpoint.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAmazon RDS proxy allows you to automatically route write request to the healthy writer, minimizing downtime.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company has a regional subscription-based streaming service that runs in a single AWS \nRegion. The architecture consists of web servers and application servers on Amazon EC2 \ninstances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The \narchitecture includes an Amazon Aurora global database cluster that extends across multiple \nAvailability Zones. \n \nThe company wants to expand globally and to ensure that its application has minimal downtime. \n \nWhich solution will provide the MOST fault tolerance?",
    "options": [
      {
        "id": 0,
        "text": "Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy the web tier and the application tier to a second Region. Create an Aurora PostgreSQL",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAws Aurora Global Database allows you to read and write from any region in the global cluster. This enables you to distribute read and write workloads globally, improving performance and reducing latency. Data is replicated synchronously across regions, ensuring strong consistency.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 42,
    "text": "A data analytics company wants to migrate its batch processing system to AWS. The company \nreceives thousands of small data files periodically during the day through FTP. An on-premises \nbatch job processes the data files overnight. However, the batch job takes hours to finish running. \n \nThe company wants the AWS solution to process incoming data files as soon as possible with \nminimal changes to the FTP clients that send the files. The solution must delete the incoming \ndata files after the files have been processed successfully. Processing for each file needs to take \n3-8 minutes. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n306 \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company is migrating its workloads to AWS. The company has transactional and sensitive data \nin its databases. The company wants to use AWS Cloud solutions to increase security and \nreduce operational overhead for the databases. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the databases to Amazon EC2. Use an AWS Key Management Service (AWS KMS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Migrate the databases to Amazon RDS Configure encryption at rest.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Migrate the data to Amazon S3 Use Amazon Macie for data security and protection",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the database to Amazon RDS. Use Amazon CloudWatch Logs for data security and",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 44,
    "text": "A company has an online gaming application that has TCP and UDP multiplayer gaming \ncapabilities. The company uses Amazon Route 53 to point the application traffic to multiple \nNetwork Load Balancers (NLBs) in different AWS Regions. The company needs to improve \napplication performance and decrease latency for the online game in preparation for user growth. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Add an Amazon CloudFront distribution in front of the NLBs. Increase the Cache-Control max-age",
        "correct": false
      },
      {
        "id": 1,
        "text": "Replace the NLBs with Application Load Balancers (ALBs). Configure Route 53 to use latency-",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use",
        "correct": true
      },
      {
        "id": 3,
        "text": "Add an Amazon API Gateway endpoint behind the NLBs. Enable API caching. Override method",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 45,
    "text": "A company needs to integrate with a third-party data feed. The data feed sends a webhook to \nnotify an external service when new data is ready for consumption. A developer wrote an AWS \nLambda function to retrieve data when the company receives a webhook callback. The developer \nmust make the Lambda function available for the third party to call. \n \nWhich solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      {
        "id": 0,
        "text": "Create a function URL for the Lambda function. Provide the Lambda function URL to the third",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an Application Load Balancer (ALB) in front of the Lambda function. Provide the ALB URL",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Attach the topic to the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Attach the queue to the",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 46,
    "text": "A company has a workload in an AWS Region. Customers connect to and access the workload \nby using an Amazon API Gateway REST API. The company uses Amazon Route 53 as its DNS \nprovider. The company wants to provide individual and secure URLs for all customers. \n \nWhich combination of steps will meet these requirements with the MOST operational efficiency? \n(Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53",
        "correct": true
      },
      {
        "id": 1,
        "text": "Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create hosted zones for each customer as required in Route 53. Create zone records that point",
        "correct": false
      },
      {
        "id": 3,
        "text": "Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create multiple API endpoints for each customer in API Gateway.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A company stores data in Amazon S3. According to regulations, the data must not contain \npersonally identifiable information (PII). The company recently discovered that S3 buckets have \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n308 \nsome objects that contain PII. The company needs to automatically detect PII in S3 buckets and \nto notify the company's security team. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData event type",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 48,
    "text": "A company wants to build a logging solution for its multiple AWS accounts. The company \ncurrently stores the logs from all accounts in a centralized account. The company has created an \nAmazon S3 bucket in the centralized account to store the VPC flow logs and AWS CloudTrail \nlogs. All logs must be highly available for 30 days for frequent analysis, retained for an additional \n60 days for backup purposes, and deleted 90 days after creation. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Transition objects to the S3 Standard storage class 30 days after creation. Write an expiration",
        "correct": false
      },
      {
        "id": 1,
        "text": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class 30 days",
        "correct": false
      },
      {
        "id": 2,
        "text": "Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation. Write",
        "correct": true
      },
      {
        "id": 3,
        "text": "Transition objects to the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class 30",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 49,
    "text": "A company is building an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for its \nworkloads. All secrets that are stored in Amazon EKS must be encrypted in the Kubernetes etcd \nkey-value store. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a new AWS Key Management Service (AWS KMS) key. Use AWS Secrets Manager to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create the Amazon EKS cluster with default options. Use the Amazon Elastic Block Store",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new AWS Key Management Service (AWS KMS) key with the alias/aws/ebs alias.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 50,
    "text": "A company wants to provide data scientists with near real-time read-only access to the \ncompany's production Amazon RDS for PostgreSQL database. The database is currently \nconfigured as a Single-AZ database. The data scientists use complex queries that will not affect \nthe production database. The company needs a solution that is highly available. \n \nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Scale the existing production database in a maintenance window to provide enough power for the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the setup from a Single-AZ to a Multi-AZ instance deployment with a larger secondary",
        "correct": false
      },
      {
        "id": 2,
        "text": "Change the setup from a Single-AZ to a Multi-AZ instance deployment. Provide two additional",
        "correct": false
      },
      {
        "id": 3,
        "text": "Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nMulti-AZ instance: the standby instance doesn't serve any read or write traffic. Multi-AZ DB cluster: consists of primary instance running in one AZ serving read-write traffic and two other standby running in two different AZs serving read traffic. https://aws.amazon.com/blogs/database/choose-the-right-amazon-rds-deployment-option-single- az-instance-multi-az-instance-or-multi-az-database-cluster/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 51,
    "text": "A company runs a three-tier web application in the AWS Cloud that operates across three \nAvailability Zones. The application architecture has an Application Load Balancer, an Amazon \nEC2 web server that hosts user session states, and a MySQL database that runs on an EC2 \ninstance. The company expects sudden increases in application traffic. The company wants to be \nable to scale to meet future application capacity demands and to ensure high availability across \nall three Availability Zones. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the MySQL database to Amazon DynamoDB Use DynamoDB Accelerator (DAX) to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Migrate the MySQL database to Amazon RDS for MySQL in a single Availability Zone. Use",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nMemcached is best suited for caching data, while Redis is better for storing data that needs to be persisted. If you need to store data that needs to be accessed frequently, such as user profiles, session data, and application settings, then Redis is the better choice.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 52,
    "text": "A global video streaming company uses Amazon CloudFront as a content distribution network \n(CDN). The company wants to roll out content in a phased manner across multiple countries. The \ncompany needs to ensure that viewers who are outside the countries to which the company rolls \nout content are not able to view the content. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Add geographic restrictions to the content in CloudFront by using an allow list. Set up a custom",
        "correct": true
      },
      {
        "id": 1,
        "text": "Set up a new URL tor restricted content. Authorize access by using a signed URL and cookies.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Encrypt the data for the content that the company distributes. Set up a custom error message.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new URL for restricted content. Set up a time-restricted access policy for signed URLs.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 53,
    "text": "A company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) \nconfiguration. The company's core production business application uses Microsoft SQL Server \nStandard, which runs on a virtual machine (VM). The application has a recovery point objective \n(RPO) of 30 seconds or fewer and a recovery time objective (RTO) of 60 minutes. The DR \nsolution needs to minimize costs wherever possible. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure a multi-site active/active setup between the on-premises server and AWS by using",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a warm standby Amazon RDS for SQL Server database on AWS. Configure AWS",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use third-party backup software to capture backups every night. Store a secondary set of",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 54,
    "text": "Get Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n311 \nA company has an on-premises server that uses an Oracle database to process and store \ncustomer information. The company wants to use an AWS database service to achieve higher \navailability and to improve application performance. The company also wants to offload reporting \nfrom its primary database system. \n \nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB instance in",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS in a Single-AZ deployment to create an Oracle database. Create a read replica",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle database.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 55,
    "text": "A company wants to build a web application on AWS. Client access requests to the website are \nnot predictable and can be idle for a long time. Only customers who have paid a subscription fee \ncan have the ability to sign in and use the web application. \n \nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon Cognito user pool to authenticate users.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Cognito identity pool to authenticate users.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 56,
    "text": "A media company uses an Amazon CloudFront distribution to deliver content over the internet. \nThe company wants only premium customers to have access to the media streams and file \ncontent. The company stores all content in an Amazon S3 bucket. The company also delivers \ncontent on demand to customers for a specific purpose, such as movie rentals or music \ndownloads. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Generate and provide S3 signed cookies to premium customers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Generate and provide CloudFront signed URLs to premium customers.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use origin access control (OAC) to limit the access of non-premium customers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Generate and activate field-level encryption to block non-premium customers.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 57,
    "text": "A company runs Amazon EC2 instances in multiple AWS accounts that are individually bled. The \ncompany recently purchased a Savings Pian. Because of changes in the company's business \nrequirements, the company has decommissioned a large number of EC2 instances. The \ncompany wants to use its Savings Plan discounts on its other AWS accounts. \n \nWhich combination of steps will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "From the AWS Account Management Console of the management account, turn on discount",
        "correct": true
      },
      {
        "id": 1,
        "text": "From the AWS Account Management Console of the account that purchased the existing Savings",
        "correct": false
      },
      {
        "id": 2,
        "text": "From the AWS Organizations management account, use AWS Resource Access Manager (AWS",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an organization in AWS Organizations in a new payer account. Invite the other AWS",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an organization in AWS Organizations in the existing AWS account with the existing EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 58,
    "text": "A retail company uses a regional Amazon API Gateway API for its public REST APIs. The API \nGateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A \nsolutions architect needs to create a solution that has minimal effects on customers and minimal \ndata loss to release the new version of APIs. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new API Gateway endpoint with new versions of the API definitions. Create a custom",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nGet Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A recent analysis of a company's IT expenses highlights the need to reduce backup costs. The \ncompany's chief information officer wants to simplify the on-premises backup infrastructure and \nreduce costs by eliminating the use of physical backup tapes. The company must preserve the \nexisting investment in the on-premises backup applications and workflows. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Storage Gateway to connect with the backup applications using the NFS interface.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up an Amazon EFS file system that connects with the backup applications using the NFS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an Amazon EFS file system that connects with the backup applications using the iSCSI",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nhttps://aws.amazon.com/storagegateway/vtl/?nc1=h_ls\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 60,
    "text": "A company has data collection sensors at different locations. The data collection sensors stream \na high volume of data to the company. The company wants to design a platform on AWS to \ningest and process high-volume streaming data. The solution must be scalable and support data \ncollection in near real time. The company must store the data in Amazon S3 for future reporting. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use AWS Glue to deliver streaming data to Amazon S3.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Lambda to deliver streaming data and store the data to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Database Migration Service (AWS DMS) to deliver streaming data to Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon Kinesis Data Firehose: Capture, transform, and load data streams into AWS data stores (S3) in near real-time.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 61,
    "text": "A company has separate AWS accounts for its finance, data analytics, and development \ndepartments. Because of costs and security concerns, the company wants to control which \nservices each AWS account can use. \n \nWhich solution will meet these requirements with the LEAST operational overhead? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n314",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Systems Manager templates to control which AWS services each department can use.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create organization units (OUs) for each department in AWS Organizations. Attach service",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS CloudFormation to automatically provision only the AWS services that each department",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up a list of products in AWS Service Catalog in the AWS accounts to manage and control the",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 62,
    "text": "A company has created a multi-tier application for its ecommerce website. The website uses an \nApplication Load Balancer that resides in the public subnets, a web tier in the public subnets, and \na MySQL cluster hosted on Amazon EC2 instances in the private subnets. The MySQL database \nneeds to retrieve product catalog and pricing information that is hosted on the internet by a third-\nparty provider. A solutions architect must devise a strategy that maximizes security without \nincreasing operational overhead. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a NAT instance in the VPC. Route all the internet-based traffic through the NAT instance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an internet gateway and attach it to the VPModify the private subnet route table to",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure a virtual private gateway and attach it to the VPC. Modify the private subnet route table",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "Explanation not available.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 63,
    "text": "A company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda \nenvironment variables. A solutions architect needs to ensure that the required permissions are in \nplace to decrypt and use the environment variables. \n \nWhich steps must the solutions architect take to implement the correct permissions? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Add AWS KMS permissions in the Lambda resource policy.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add AWS KMS permissions in the Lambda execution role.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Add AWS KMS permissions in the Lambda function policy.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Allow the Lambda execution role in the AWS KMS key policy.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Allow the Lambda resource policy in the AWS KMS key policy.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nTo decrypt environment variables encrypted with AWS KMS, Lambda needs to be granted permissions to call KMS APIs. This is done in two places: The Lambda execution role needs kms:Decrypt and kms:GenerateDataKey permissions added. The execution role governs what AWS services the function code can access. The KMS key policy needs to allow the Lambda execution role to have kms:Decrypt and kms:GenerateDataKey permissions for that specific key. This allows the execution role to use that Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 64,
    "text": "A company has a financial application that produces reports. The reports average 50 KB in size \nand are stored in Amazon S3. The reports are frequently accessed during the first week after \nproduction and must be stored for several years. The reports must be retrievable within 6 hours. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Standard-Infrequent",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use S3 Intelligent-Tiering. Configure S3 Intelligent-Tiering to transition the reports to S3",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier Deep Archive",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nAmazon S3 Glacier: Expedited Retrieval: Provides access to data within 1-5 minutes. Standard Retrieval: Provides access to data within 3-5 hours. Bulk Retrieval: Provides access to data within 5-12 hours. Amazon S3 Glacier Deep Archive: Standard Retrieval: Provides access to data within 12 hours. Bulk Retrieval: Provides access to data within 48 hours.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  }
]