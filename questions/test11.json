[
  {
    "id": 0,
    "text": "A company wants to run its critical applications in containers to meet requirements tor scalability \nand availability The company prefers to focus on maintenance of the critical applications. The \ncompany does not want to be responsible for provisioning and managing the underlying \ninfrastructure that runs the containerized workload. \nWhat should a solutions architect do to meet those requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 Instances, and Install Docker on the Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nAWS Fargate is a serverless compute engine for containers that eliminates the need to provision, configure, or manage servers. When using ECS on Fargate, AWS handles all the underlying infrastructure management including server provisioning, patching, and scaling. This allows the company to focus entirely on maintaining their critical applications while AWS manages the container infrastructure. Fargate automatically scales containers based on demand and provides high availability across multiple Availability Zones, meeting the scalability and availability requirements without any infrastructure management overhead.\n\n**Why option A is incorrect:**\nUsing EC2 instances with Docker installed requires the company to provision, configure, and manage the EC2 instances themselves. This includes tasks like patching the operating system, managing security groups, configuring Auto Scaling, and ensuring high availability. This contradicts the requirement to not be responsible for provisioning and managing the underlying infrastructure.\n\n**Why option B is incorrect:**\nECS on EC2 worker nodes still requires the company to manage the EC2 instances that serve as worker nodes. The company must provision, configure, patch, and maintain these EC2 instances, handle Auto Scaling for the worker nodes, and ensure their availability. This does not meet the requirement of not managing the underlying infrastructure.\n\n**Why option D is incorrect:**\nThis option appears incomplete in the text, but even if it refers to ECS on EC2 instances, it still requires managing EC2 infrastructure. The company would need to provision, configure, and maintain EC2 instances, which contradicts the requirement to avoid infrastructure management responsibilities.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company hosts more than 300 global websites and applications. The company requires a \nplatform to analyze more than 30 TB of clickstream data each day. What should a solutions \narchitect do to transmit and process the clickstream data?",
    "options": [
      {
        "id": 0,
        "text": "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Cache the data to Amazon CloudFron.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Collect the data from Amazon Kinesis Data Streams.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nAmazon Kinesis Data Streams is designed specifically for real-time data ingestion and processing of large-scale streaming data. It can handle massive throughput (up to thousands of data sources writing millions of records per second) and automatically scales to handle the 30 TB of daily clickstream data. Kinesis Data Streams provides low-latency data collection and can integrate with various AWS analytics services like Amazon Kinesis Data Analytics, Amazon EMR, and Amazon Redshift for real-time processing. It's the ideal service for collecting and transmitting clickstream data from 300+ websites and applications in real-time.\n\n**Why option A is incorrect:**\nAWS Data Pipeline is designed for batch data processing workflows, not real-time streaming data. It's meant for periodic, scheduled data transfers and transformations, not for continuous real-time clickstream data ingestion. Data Pipeline would introduce significant latency as it processes data in batches rather than streaming, which doesn't meet the real-time requirements for clickstream analysis.\n\n**Why option B is incorrect:**\nCreating an Auto Scaling group of EC2 instances requires the company to manage the infrastructure, including provisioning instances, configuring scaling policies, managing instance health, and handling failures. This approach adds operational overhead and doesn't provide the managed, scalable solution that Kinesis Data Streams offers. Additionally, managing EC2 instances for this use case is more complex and less cost-effective than using a managed streaming service.\n\n**Why option C is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) service designed for caching and delivering static and dynamic content to end users with low latency. It's not designed for collecting or processing clickstream data. CloudFront doesn't have the capability to ingest, store, or process streaming data for analytics purposes. It's the wrong service for this data collection and processing requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company is running a multi-tier ecommerce web application in the AWS Cloud. \nThe web application is running on Amazon EC2 instances. \nThe database tier Is on a provisioned Amazon Aurora MySQL DB cluster with a writer and a \nreader in a Multi-AZ environment. \nThe new requirement for the database tier is to serve the application to achieve continuous write \navailability through an Instance failover. \nWhat should a solutions architect do to meet this new requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add a new AWS Region to the DB cluster for multiple writes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a new reader In the same Availability Zone as the writer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database tier to an Aurora multi-master cluster.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the database tier to an Aurora DB cluster with parallel query enabled.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nAurora multi-master clusters provide continuous write availability by allowing multiple database instances to accept write operations simultaneously. In a multi-master configuration, if one writer instance fails, the other writer instances can immediately continue handling write requests without any failover delay. This eliminates the single point of failure present in the standard Aurora configuration with one writer and multiple readers. The multi-master setup ensures that write operations can continue seamlessly through instance failover, meeting the requirement for continuous write availability.\n\n**Why option A is incorrect:**\nAdding a new AWS Region to the DB cluster creates a global database, but this doesn't provide continuous write availability through instance failover within the same region. Global databases are primarily designed for disaster recovery and read scaling across regions, not for handling failover scenarios within a single region. The failover process between regions would still involve some downtime and doesn't address the requirement for continuous write availability through instance failover.\n\n**Why option B is incorrect:**\nAdding a new reader instance in the same Availability Zone as the writer doesn't help with write availability. Reader instances in Aurora are read-only replicas that cannot accept write operations. Even if the writer fails, a reader cannot take over write operations. This option only improves read performance and redundancy for read operations, but does nothing to address continuous write availability through failover.\n\n**Why option D is incorrect:**\nAurora Parallel Query is a feature that accelerates analytical queries by parallelizing them across multiple Aurora replicas. However, it doesn't change the fundamental architecture of having a single writer instance. Parallel Query improves read performance for analytical workloads but doesn't provide multiple write endpoints or continuous write availability through failover. The single writer limitation remains, which doesn't meet the requirement.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 \nhour before the desired Amazon EC2 capacity is reached. The peak capacity is the â€˜same every \nnight and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-\neffective solution that will allow for the desired EC2 capacity to be reached quickly and allow the \nAuto Scaling group to scale down after the batch jobs are complete. \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the minimum capacity for the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the maximum capacity for the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure scheduled scaling to scale up to the desired compute level.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Change the scaling policy to add more EC2 instances during each scaling operation.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nScheduled scaling allows you to configure the Auto Scaling group to automatically scale to a specific capacity at a predetermined time. Since the batch jobs always start at 1 AM with the same peak capacity every night, scheduled scaling can be configured to scale up to the desired capacity exactly at 1 AM, eliminating the 1-hour warm-up period. After the jobs complete, scheduled scaling can scale the group back down, reducing costs. This approach ensures the desired capacity is reached immediately when needed while maintaining cost-effectiveness by scaling down during non-peak hours.\n\n**Why option A is incorrect:**\nIncreasing the minimum capacity would keep instances running 24/7, even when batch jobs aren't running. This wastes money by paying for compute resources that aren't needed most of the time. While it would ensure capacity is available immediately, it doesn't address the cost-effectiveness requirement and doesn't solve the problem of reaching capacity quickly - it just keeps capacity always available at unnecessary cost.\n\n**Why option B is incorrect:**\nIncreasing the maximum capacity only sets the upper limit for scaling, but doesn't help the Auto Scaling group reach the desired capacity faster. The scaling process would still take the same amount of time (1 hour) to reach the desired capacity because the scaling policies and instance launch times remain unchanged. This option doesn't address the requirement to reach capacity quickly.\n\n**Why option D is incorrect:**\nChanging the scaling policy to add more instances per scaling operation might reduce the time slightly, but it doesn't guarantee reaching capacity exactly at 1 AM. The scaling would still be reactive and based on metrics, which introduces delay. More importantly, this doesn't leverage the predictable, scheduled nature of the workload. Scheduled scaling is the appropriate solution for predictable, time-based workloads like this nightly batch job.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company runs an application in the AWS Cloud and uses Amazon DynamoDB as the database. \nThe company deploys Amazon EC2 instances to a private network to process data from the \ndatabase. \nThe company uses two NAT instances to provide connectivity to DynamoDB. \nThe company wants to retire the NAT instances. \nA solutions architect must implement a solution that provides connectivity to DynamoDB and that \ndoes not require ongoing management. \nWhat is the MOST cost-effective solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint to provide connectivity to DynamoDB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a managed NAT gateway to provide connectivity to DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Establish an AWS Direct Connect connection between the private network and DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS PrivateLink endpoint service between the private network and DynamoDB",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nA gateway VPC endpoint provides private connectivity between resources in a VPC and AWS services like DynamoDB without requiring internet access, NAT devices, VPN connections, or AWS Direct Connect. Gateway VPC endpoints are free - there are no data processing charges, hourly charges, or data transfer charges for traffic within the same region. This makes it the most cost-effective solution. Additionally, gateway VPC endpoints are fully managed by AWS, requiring no ongoing management, which meets the requirement of not needing ongoing management. The endpoint automatically scales and provides high availability.\n\n**Why option B is incorrect:**\nA managed NAT Gateway still requires ongoing management in terms of monitoring, cost optimization, and ensuring high availability (though it's managed, you still need to configure and maintain it). More importantly, NAT Gateways incur hourly charges and data processing charges, making them significantly more expensive than gateway VPC endpoints. Since the requirement is to access DynamoDB (which supports VPC endpoints), using a NAT Gateway is unnecessarily costly and doesn't meet the cost-effectiveness requirement.\n\n**Why option C is incorrect:**\nAWS Direct Connect is designed for establishing dedicated network connections from on-premises to AWS, not for connecting VPC resources to AWS services like DynamoDB. Direct Connect is expensive, requires physical infrastructure setup, and is overkill for this use case. It doesn't provide the same cost benefits as VPC endpoints and doesn't meet the cost-effectiveness requirement. Additionally, it requires more management and setup than a simple VPC endpoint.\n\n**Why option D is incorrect:**\nAWS PrivateLink endpoint services are used to expose your own services privately to other VPCs or AWS accounts, not to access AWS managed services like DynamoDB. For accessing AWS services like DynamoDB from a VPC, you use VPC endpoints (gateway endpoints for S3 and DynamoDB), not PrivateLink endpoint services. This option is conceptually incorrect for this use case.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 5,
    "text": "A company has an on-premises business application that generates hundreds of files each day. \nThese files are stored on an SMB file share and require a low-latency connection to the \napplication servers. \nA new company policy states all application-generated files must be copied to AWS. \nThere is already a VPN connection to AWS. \nThe application development team does not have time to make the necessary code modifications \nto move the application to AWS. \nWhich service should a solutions architect recommend to allow the application to copy files to \nAWS?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nAWS Storage Gateway provides a hybrid cloud storage service that allows on-premises applications to seamlessly use AWS cloud storage. The File Gateway mode presents an SMB file share interface to the on-premises application, maintaining low-latency local access while automatically copying files to Amazon S3 in the background. Since the files are stored locally on the Storage Gateway appliance, the application experiences low latency. The files are automatically replicated to S3 without requiring code changes, meeting the requirement to copy files to AWS without application modifications.\n\n**Why option A is incorrect:**\nAmazon EFS uses the NFS protocol, not SMB, so it wouldn't be compatible with the existing SMB file share setup without significant changes. More importantly, accessing EFS over a VPN connection from on-premises would introduce latency, as the data would need to traverse the VPN connection for every file operation. This doesn't meet the low-latency requirement. Additionally, EFS is designed for Linux workloads, not Windows SMB shares.\n\n**Why option B is incorrect:**\nAmazon FSx for Windows File Server provides a fully managed Windows file system in AWS, but accessing it from on-premises over a VPN would introduce significant latency. Every file read and write operation would need to traverse the VPN connection, which doesn't meet the low-latency requirement. FSx is designed for workloads running in AWS, not for on-premises applications that need low-latency file access.\n\n**Why option C is incorrect:**\nAWS Snowball is a data migration service designed for one-time, large-scale data transfers, not for ongoing daily file copying operations. Snowball requires physical devices to be shipped, data to be loaded onto the device, and the device to be shipped back to AWS. This process takes days or weeks and cannot handle the requirement of copying hundreds of files each day in near real-time. It's not suitable for continuous, ongoing file replication.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A company has an automobile sales website that stores its listings in an database on Amazon \nRDS. \nWhen an automobile is sold, the listing needs to be removed from the website and the data must \nbe sent to multiple target systems. \nWhich design should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nAWS Lambda functions can be triggered by RDS database events through Amazon SNS. When a database record is updated (like marking a car as sold), RDS can publish an event to an SNS topic, which then invokes a Lambda function. The Lambda function can process the event, remove the listing from the website, and then use SNS's fanout capability to send the data to multiple target systems simultaneously. This serverless architecture provides automatic scaling, eliminates the need to manage infrastructure, and efficiently handles the requirement to send data to multiple systems.\n\n**Why option B is incorrect:**\nWhile this option also mentions Lambda triggered by RDS updates, the specific implementation details likely differ in a way that doesn't efficiently handle the fanout to multiple target systems. The correct solution leverages SNS's native fanout capability, which allows one message to be delivered to multiple subscribers (target systems) simultaneously, which is more efficient than sequential processing.\n\n**Why option C is incorrect:**\nSubscribing to RDS event notifications and sending to Amazon SQS creates a queue-based system, but SQS doesn't natively support fanout to multiple target systems. You would need separate queues for each target system and additional processing logic to distribute messages. This adds complexity and doesn't leverage the built-in fanout pattern that SNS provides. Additionally, SQS requires polling, which introduces latency compared to the push-based SNS approach.\n\n**Why option D is incorrect:**\nWhile RDS event notifications can be sent to SNS, this option likely doesn't include the Lambda function component needed to process the event and remove the listing from the website. SNS alone can distribute messages, but you still need a compute component (Lambda) to handle the business logic of updating the website and processing the data before sending it to target systems. The complete solution requires both SNS for distribution and Lambda for processing.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A company is developing a video conversion application hosted on AWS. \nThe application will be available in two tiers: a free tier and a paid tier. \nUsers in the paid tier will have their videos converted first and then the tree tier users will have \ntheir videos converted. \nWhich solution meets these requirements and is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "One FIFO queue for the paid tier and one standard queue for the free tier",
        "correct": false
      },
      {
        "id": 1,
        "text": "A single FIFO Amazon Simple Queue Service (Amazon SQS) queue for all file types",
        "correct": false
      },
      {
        "id": 2,
        "text": "A single standard Amazon Simple Queue Service (Amazon SQS) queue for all file types",
        "correct": false
      },
      {
        "id": 3,
        "text": "Two standard Amazon Simple Queue Service (Amazon SQS) queues with one for the paid tier",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nUsing two standard SQS queues (one for paid tier, one for free tier) allows the application to prioritize paid tier videos by processing messages from the paid tier queue first, then the free tier queue. Standard queues are more cost-effective than FIFO queues and provide sufficient ordering for this use case where you just need to ensure paid videos are processed before free videos. The application logic can check the paid tier queue first, and only process free tier videos when the paid tier queue is empty. This meets the requirement while being the most cost-effective solution.\n\n**Why option A is incorrect:**\nUsing a FIFO queue for the paid tier and a standard queue for the free tier would work functionally, but FIFO queues are more expensive than standard queues due to the additional processing required to maintain strict ordering. Since the requirement only needs to ensure paid videos are processed before free videos (not strict ordering within each tier), standard queues for both tiers provide the same functionality at lower cost, making this option less cost-effective.\n\n**Why option B is incorrect:**\nA single FIFO queue for all file types would process videos in strict first-in-first-out order, which means if a free tier video arrives before a paid tier video, it would be processed first. This doesn't meet the requirement that paid tier users should have their videos converted first. Additionally, FIFO queues are more expensive than standard queues, making this less cost-effective than using two standard queues.\n\n**Why option C is incorrect:**\nA single standard queue for all file types would process videos in the order they arrive, regardless of tier. This means free tier videos could be processed before paid tier videos if they arrive first, which violates the requirement that paid tier videos must be converted first. You need separate queues to implement priority processing.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 8,
    "text": "A company runs an ecommerce application on Amazon EC2 instances behind an Application \nLoad Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple \nAvailability Zones. The Auto Scaling group scales based on CPU utilization metrics. The \necommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a \nlarge EC2 instance. \n \nThe database's performance degrades quickly as application load increases. The application \nhandles more read requests than write transactions. The company wants a solution that will \nautomatically scale the database to meet the demand of unpredictable read workloads while \nmaintaining high availability. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Redshift with a single node for leader and compute functionality.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS with a Single-AZ deployment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Aurora with a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Memcached with EC2 Spot Instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nAmazon Aurora MySQL provides up to 5x better performance than standard MySQL on RDS and is specifically designed to handle high read workloads efficiently. Aurora automatically scales read capacity by adding up to 15 read replicas across multiple Availability Zones, which can automatically handle unpredictable read workloads. The Multi-AZ deployment ensures high availability with automatic failover capabilities. Aurora's storage automatically grows as needed and can handle the performance degradation issues seen with the single EC2 instance MySQL database.\n\n**Why option A is incorrect:**\nAmazon Redshift is a data warehousing service designed for analytical workloads and large-scale data processing, not for transactional database workloads like an ecommerce application. Redshift uses columnar storage optimized for analytics queries, not row-based transactional operations. Additionally, a single-node Redshift cluster doesn't provide the automatic read scaling or high availability features required. Redshift is the wrong database type for this use case.\n\n**Why option B is incorrect:**\nAmazon RDS with Single-AZ deployment doesn't provide high availability - if the database instance fails, there will be downtime during recovery. More importantly, RDS doesn't automatically scale read capacity to handle unpredictable read workloads. While you can manually add read replicas, the requirement asks for automatic scaling. Single-AZ also doesn't meet the high availability requirement specified in the scenario.\n\n**Why option D is incorrect:**\nAmazon ElastiCache for Memcached is an in-memory caching service, not a primary database. It's designed to cache frequently accessed data to improve application performance, but it doesn't replace the need for a persistent database. Additionally, using EC2 Spot Instances for ElastiCache introduces the risk of instance termination, which would cause data loss since Memcached doesn't provide persistence. This doesn't meet the requirements for a scalable, highly available database solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company recently migrated to AWS and wants to implement a solution to protect the traffic that \nflows in and out of the production VPC. The company had an inspection server in its on-premises \ndata center. The inspection server performed specific operations such as traffic flow inspection \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n76 \nand traffic filtering. The company wants to have the same functionalities in the AWS Cloud. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nAWS Network Firewall is a managed network firewall service that provides stateful inspection, intrusion prevention, and web filtering capabilities at the VPC perimeter. It can inspect and filter all traffic flowing in and out of the VPC through internet gateways, NAT gateways, VPN connections, or AWS Direct Connect. Network Firewall provides the same traffic inspection and filtering functionalities that the on-premises inspection server provided, but as a fully managed AWS service. It supports custom rule sets and integrates with AWS managed rule sets for common protections.\n\n**Why option A is incorrect:**\nAmazon GuardDuty is a threat detection service that uses machine learning to identify potential security threats by analyzing CloudTrail logs, VPC Flow Logs, and DNS logs. However, GuardDuty is a monitoring and detection service, not a firewall that can actively inspect and filter traffic in real-time. It doesn't perform traffic flow inspection or filtering at the network level like the on-premises inspection server did. GuardDuty detects threats but doesn't block or filter traffic.\n\n**Why option B is incorrect:**\nTraffic Mirroring is a feature that copies network traffic from EC2 instances to monitoring and security appliances for analysis. However, Traffic Mirroring only mirrors traffic - it doesn't perform active inspection and filtering of traffic flowing in and out of the VPC. It's a passive monitoring tool that requires additional security appliances to analyze the mirrored traffic. It doesn't replace the functionality of an inspection server that actively filters traffic.\n\n**Why option D is incorrect:**\nAWS Firewall Manager is a security management service that helps centrally configure and manage firewall rules across multiple AWS accounts and resources. However, Firewall Manager is a management and policy enforcement tool, not the actual firewall that performs traffic inspection and filtering. Firewall Manager works with AWS WAF, AWS Shield, and AWS Network Firewall to apply rules, but it doesn't itself inspect or filter traffic. You still need AWS Network Firewall to actually perform the inspection and filtering functions.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon \nRDS for PostgreSQL. The company needs a reporting solution that provides data visualization \nand includes all the data sources within the data lake. Only the company's management team \nshould have full access to all the visualizations. The rest of the company should have only limited \naccess. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an analysis in Amazon QuickSight.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an analysis in Amazon OuickSighl.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Glue table and crawler for the data in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Glue table and crawler for the data in Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option B is correct:**\nAmazon QuickSight is a cloud-based business intelligence service that provides data visualization and reporting capabilities. QuickSight can connect to multiple data sources including Amazon S3 and Amazon RDS for PostgreSQL, allowing it to create visualizations from all data sources in the data lake. QuickSight supports fine-grained access control through IAM and resource-level permissions, allowing you to grant full access to the management team while providing limited access to the rest of the company. QuickSight dashboards and analyses can be shared with specific users or groups with different permission levels.\n\n**Why option A is incorrect:**\nWhile this option also mentions creating an analysis in Amazon QuickSight (note: option B has a typo \"OuickSighl\" but refers to the same service), the key difference is likely in the implementation details. However, since both refer to QuickSight, the correct answer is the one that properly implements the access control requirements. QuickSight is indeed the right service, but the implementation must ensure proper IAM and resource-level permissions are configured.\n\n**Why option C is incorrect:**\nAWS Glue is a serverless ETL (Extract, Transform, Load) service used to prepare and transform data for analytics. While Glue can create tables and crawlers to catalog data in S3, it doesn't provide data visualization capabilities. Glue is used to discover, catalog, and transform data, but you still need a visualization tool like QuickSight to create reports and dashboards. Glue alone doesn't meet the requirement for a reporting solution with data visualization.\n\n**Why option D is incorrect:**\nThis option is identical to option C - creating AWS Glue tables and crawlers. As explained above, Glue doesn't provide visualization capabilities. It's a data cataloging and ETL service, not a business intelligence or visualization tool. You need QuickSight or another BI tool on top of Glue to provide the reporting and visualization functionality required.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "A company is implementing a new business application. The application runs on two Amazon \nEC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs \nto ensure that the EC2 instances can access the S3 bucket. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n77 \n \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that grants access to the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM policy that grants access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM group that grants access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user that grants access to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nIAM roles are the recommended and secure way to grant EC2 instances access to AWS services like S3. When you attach an IAM role to an EC2 instance, the instance automatically receives temporary security credentials that allow it to access the S3 bucket. These credentials are automatically rotated and never need to be stored on the instance. Roles eliminate the need to manage access keys and provide better security than storing credentials on the instance. The role can be attached to the instance when it's launched or at any time, and the permissions are automatically applied.\n\n**Why option B is incorrect:**\nAn IAM policy alone cannot grant access to EC2 instances. Policies are documents that define permissions, but they must be attached to an IAM identity (user, group, or role) to be effective. You cannot directly attach a policy to an EC2 instance. To grant EC2 instances access to S3, you need to create a role, attach the policy to that role, and then associate the role with the EC2 instance.\n\n**Why option C is incorrect:**\nIAM groups are collections of IAM users, not EC2 instances. Groups are used to manage permissions for multiple users at once, but they cannot be associated with EC2 instances. EC2 instances cannot be members of IAM groups. To grant EC2 instances access to S3, you must use IAM roles, not groups.\n\n**Why option D is incorrect:**\nWhile you could create an IAM user and store access keys on the EC2 instance, this is not a recommended practice for several reasons. First, it requires managing and securely storing credentials on the instance, which is a security risk. Second, access keys don't automatically rotate and must be manually updated. Third, if the instance is compromised, the credentials could be exposed. IAM roles are the secure, AWS-recommended approach that eliminates these security concerns.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "An application development team is designing a microservice that will convert large images to \nsmaller, compressed images. When a user uploads an image through the web interface, the \nmicroservice should store the image in an Amazon S3 bucket, process and compress the image \nwith an AWS Lambda function, and store the image in its compressed form in a different S3 \nbucket. \n \nA solutions architect needs to design a solution that uses durable, stateless components to \nprocess the images automatically. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the Lambda function to monitor the S3 bucket for new uploads.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nCreating an Amazon SQS queue provides a durable, stateless message queue that can store image processing requests. When images are uploaded to S3, events can be published to SQS, which will durably store the messages even if the processing Lambda function is temporarily unavailable. SQS ensures messages are not lost and provides at-least-once delivery, making it a durable component. The queue itself is stateless and managed by AWS, requiring no infrastructure management.\n\n**Why option B is incorrect:**\nConfiguring the Lambda function to use SQS as an event source is part of the solution, but this option alone doesn't create the durable queue component needed. You first need to create the SQS queue, then configure the Lambda function to consume from it. This option describes the integration step, not the creation of the durable component itself.\n\n**Why option C is incorrect:**\nWhile Lambda can be triggered directly by S3 events, this creates a tight coupling and doesn't provide the durability and decoupling that SQS offers. If the Lambda function fails or is throttled, S3 events might be lost. Additionally, S3 event notifications have limitations on retry behavior. Using SQS as an intermediary provides better durability, retry logic, and decoupling between the upload and processing steps.\n\n**Why option D is incorrect:**\nLaunching an EC2 instance to monitor SQS introduces stateful infrastructure that requires ongoing management, patching, and monitoring. This contradicts the requirement for stateless components and increases operational overhead. EC2 instances are not stateless - they have persistent state, require management, and add unnecessary complexity when Lambda can directly consume from SQS without any EC2 instances.\n\n**Why option E is incorrect:**\nWhile EventBridge (CloudWatch Events) can monitor S3 buckets and trigger Lambda functions, it doesn't provide the same level of durability and message persistence that SQS offers. EventBridge is an event router, not a message queue. If the Lambda function fails, EventBridge events might not be retried as reliably as SQS messages. SQS provides better durability guarantees and is more suitable for ensuring no messages are lost.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company has a three-tier web application that is deployed on AWS. The web servers are \ndeployed in a public subnet in a VPC. The application servers and database servers are deployed \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n78 \nin private subnets in the same VPC. The company has deployed a third-party virtual firewall \nappliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP \ninterface that can accept IP packets. \nA solutions architect needs to Integrate the web application with the appliance to inspect all traffic \nto the application before the traffic teaches the web server. \n \nWhich solution will moot these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a Network Load Balancer the public subnet of the application's VPC to route the traffic to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Application Load Balancer in the public subnet of the application's VPC to route the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a transit gateway in the inspection VPC.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a Gateway Load Balancer in the inspection VPC.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nGateway Load Balancer (GWLB) is specifically designed for deploying, scaling, and managing third-party virtual network appliances like firewalls. It operates at Layer 3 (network layer) and uses AWS Hyperplane technology to distribute traffic across multiple appliance instances. GWLB endpoints can be deployed in the application VPC, and all traffic destined for the web servers is automatically routed through the inspection VPC where the firewall appliance is deployed. This provides inline traffic inspection with minimal operational overhead, as GWLB handles load distribution, health checks, and automatic scaling of the appliance fleet. The solution centralizes the firewall in the inspection VPC while seamlessly routing traffic from the application VPC through it.\n\n**Why option A is incorrect:**\nA Network Load Balancer (NLB) in the application VPC cannot route traffic to a firewall appliance in a different VPC (inspection VPC) without additional networking components. NLBs operate within a single VPC and are designed to distribute traffic to targets within that VPC. To route traffic to the inspection VPC, you would need Transit Gateway or VPC peering, which adds complexity. Gateway Load Balancer is specifically designed for this cross-VPC appliance inspection use case.\n\n**Why option B is incorrect:**\nAn Application Load Balancer (ALB) operates at Layer 7 (application layer) and is designed for HTTP/HTTPS traffic distribution to application servers. It cannot route traffic to a Layer 3 firewall appliance in a different VPC. ALBs also operate within a single VPC and don't have the cross-VPC capabilities that Gateway Load Balancer provides. Additionally, ALBs add unnecessary overhead for simple traffic inspection use cases.\n\n**Why option C is incorrect:**\nDeploying a Transit Gateway in the inspection VPC alone doesn't provide the traffic routing and load balancing needed for the firewall appliance. Transit Gateway is a network transit hub that connects VPCs, but it doesn't integrate with third-party appliances or provide the inline inspection routing that Gateway Load Balancer provides. You would still need additional components to route traffic through the firewall appliance, increasing operational complexity.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company wants to improve its ability to clone large amounts of production data into a test \nenvironment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon \nElastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the \nproduction environment. The software that accesses this data requires consistently high I/O \nperformance. \n \nA solutions architect needs to minimize the time that is required to clone the production data into \nthe test environment. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Take EBS snapshots of the production EBS volumes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the production EBS volumes to use the EBS Multi-Attach feature.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Take EBS snapshots of the production EBS volumes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Take EBS snapshots of the production EBS volumes.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nEBS Fast Snapshot Restore (FSR) enables you to create volumes from snapshots that are fully initialized at creation time, eliminating the performance penalty typically associated with restoring from snapshots. When you enable FSR on a snapshot, volumes created from that snapshot immediately deliver their full provisioned IOPS performance without the initial latency of lazy loading data blocks. This minimizes the time required to clone production data into the test environment while ensuring the cloned volumes have consistently high I/O performance from the start. The cloned volumes are independent copies, so modifications won't affect production.\n\n**Why option A is incorrect:**\nWhile taking EBS snapshots is the first step in cloning data, standard snapshots don't provide fast snapshot restore capabilities. Volumes created from standard snapshots experience lazy loading, where data blocks are loaded on first access, causing inconsistent I/O performance initially. This doesn't meet the requirement for consistently high I/O performance from the start. You need to enable Fast Snapshot Restore on the snapshot to achieve immediate full performance.\n\n**Why option B is incorrect:**\nEBS Multi-Attach allows a single EBS volume to be attached to multiple EC2 instances simultaneously, but it doesn't create independent copies of the data. If you modify data on one instance, those changes are immediately visible to all other instances attached to the same volume. This violates the requirement that modifications to cloned data must not affect the production environment. Multi-Attach is for shared storage scenarios, not for creating independent test environments.\n\n**Why option C is incorrect:**\nThis option is identical to option A - taking EBS snapshots without Fast Snapshot Restore. Standard snapshots create volumes that experience lazy loading, where the first access to each data block incurs latency as it's loaded from S3. This doesn't provide consistently high I/O performance immediately, which is required for the test environment. Fast Snapshot Restore must be enabled to achieve the desired performance characteristics.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will \nfeature exactly one product on sale for a period of 24 hours. The company wants to be able to \nhandle millions of requests each hour with millisecond latency during peak hours. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 to host the full website in different S3 buckets.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the full application to run in containers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon S3 bucket to host the website's static content.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nUsing Amazon S3 to host the website's static content combined with Amazon CloudFront CDN provides a highly scalable, low-latency solution for serving static web content. CloudFront caches content at edge locations worldwide, delivering content to users with millisecond latency. S3 can handle unlimited storage and requests, automatically scaling to millions of requests per hour. For dynamic content (like the daily deal), you can use serverless components like API Gateway, Lambda, and DynamoDB, which also scale automatically. This architecture provides minimal operational overhead as all components are fully managed by AWS and scale automatically without manual intervention.\n\n**Why option A is incorrect:**\nHosting the full website in different S3 buckets doesn't provide a complete solution. While S3 can host static content, a one-deal-a-day website likely needs dynamic functionality to update the deal each day, handle user interactions, and process transactions. S3 alone cannot handle dynamic content generation or server-side logic. You need additional services like Lambda, API Gateway, and DynamoDB to handle the dynamic aspects of the application.\n\n**Why option B is incorrect:**\nDeploying the full website on EC2 instances with Auto Scaling groups requires significant operational overhead. You need to manage the EC2 instances, configure Auto Scaling policies, handle patching and security updates, monitor instance health, and ensure the application can scale to handle millions of requests. While EC2 can handle the load, it requires ongoing management and doesn't provide the millisecond latency that CloudFront edge caching offers. This approach has higher operational overhead than a serverless architecture.\n\n**Why option C is incorrect:**\nMigrating to containers (ECS/EKS) still requires managing the underlying infrastructure, container orchestration, scaling policies, and load balancing. While containers provide portability and scalability, they don't eliminate operational overhead. You still need to manage the container platform, handle container images, configure networking, and ensure high availability. A serverless architecture with S3, CloudFront, Lambda, and DynamoDB provides better scalability and lower operational overhead for this use case.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A solutions architect is using Amazon S3 to design the storage architecture of a new digital media \napplication. The media files must be resilient to the loss of an Availability Zone Some files are \naccessed frequently while other files are rarely accessed in an unpredictable pattern. The \nsolutions architect must minimize the costs of storing and retrieving the media files. \nWhich storage option meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "S3 Standard",
        "correct": false
      },
      {
        "id": 1,
        "text": "S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 2,
        "text": "S3 Standard-Infrequent Access {S3 Standard-IA)",
        "correct": false
      },
      {
        "id": 3,
        "text": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option B is correct:**\nS3 Intelligent-Tiering is specifically designed for data with unknown or unpredictable access patterns. It automatically moves objects between four access tiers (Frequent Access, Infrequent Access, Archive Instant Access, and Archive Access) based on changing access patterns. This optimizes costs automatically - frequently accessed files stay in the frequent access tier, while rarely accessed files move to lower-cost tiers. Intelligent-Tiering monitors access patterns and moves objects between tiers without performance impact or operational overhead. It provides the same durability as S3 Standard (99.999999999% across multiple Availability Zones) while minimizing costs for unpredictable access patterns.\n\n**Why option A is incorrect:**\nS3 Standard is designed for frequently accessed data and provides the highest performance, but it's also the most expensive storage class. Since the scenario mentions that some files are rarely accessed in an unpredictable pattern, using S3 Standard for all files would result in paying premium storage costs for infrequently accessed data. S3 Standard doesn't automatically optimize costs based on access patterns like Intelligent-Tiering does.\n\n**Why option C is incorrect:**\nS3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. However, Standard-IA requires you to know in advance that data will be infrequently accessed. Since the access pattern is unpredictable (some files accessed frequently, others rarely), you can't predict which files should be in Standard-IA. Additionally, Standard-IA has a minimum storage duration charge and retrieval fees, which may not be cost-effective if access patterns change. Intelligent-Tiering automatically handles this optimization.\n\n**Why option D is incorrect:**\nS3 One Zone-IA stores data in a single Availability Zone, which means it's not resilient to the loss of an Availability Zone. The requirement explicitly states that files must be resilient to the loss of an Availability Zone. One Zone-IA provides 99.5% availability within a single AZ, but if that AZ fails, the data becomes unavailable. This doesn't meet the resilience requirement, even though it might be cost-effective for infrequently accessed data.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A company has an on-premises MySQL database used by the global sales team with infrequent \naccess patterns. \nThe sales team requires the database to have minimal downtime. \nA database administrator wants to migrate this database to AWS without selecting a particular \ninstance type in anticipation of more users in the future. \nWhich service should a solution architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Aurora MySQL",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Aurora Serverless for MySQL",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Redshift Spectrum",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon RDS for MySQL",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option B is correct:**\nAurora Serverless automatically scales database capacity up and down based on application demand, eliminating the need to select and manage instance types. It automatically adjusts compute capacity in response to workload changes, scaling from as low as 0.5 ACUs (Aurora Capacity Units) to as high as 128 ACUs. This is perfect for databases with infrequent access patterns, as it can scale down during idle periods and scale up when users connect. Aurora Serverless provides high availability with automatic failover across multiple Availability Zones, meeting the minimal downtime requirement. Since it's serverless, there's no need to choose instance types - capacity is managed automatically.\n\n**Why option A is incorrect:**\nAurora MySQL requires you to select a specific instance type (db.r5.large, db.r5.xlarge, etc.) when creating the cluster. Instance types determine the compute capacity, memory, and network performance. If you anticipate more users in the future, you would need to manually scale up to a larger instance type, which requires downtime or a maintenance window. Aurora Serverless eliminates this need by automatically scaling capacity based on demand.\n\n**Why option C is incorrect:**\nAmazon Redshift Spectrum is a feature that allows Redshift to query data directly from S3, not a database service for migrating MySQL databases. Redshift is a data warehousing service designed for analytical workloads, not transactional databases like MySQL. It uses a columnar storage format optimized for analytics, not row-based transactional operations. Redshift Spectrum cannot host a MySQL database or serve as a replacement for MySQL workloads.\n\n**Why option D is incorrect:**\nAmazon RDS for MySQL requires you to select a specific instance type when creating the database instance. Like Aurora MySQL, you must choose from predefined instance types (db.t3.micro, db.m5.large, etc.) based on your expected workload. If you need more capacity in the future, you must manually modify the instance to a larger type, which may require downtime. RDS doesn't automatically scale compute capacity like Aurora Serverless does.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company is building an application on Amazon EC2 instances that generates temporary \ntransactional data.  \nThe application requires access to data storage that can provide configurable and consistent \nIOPS. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Provision an EC2 instance with a Throughput Optimized HDD (st1) root volume and a Cold",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an EC2 instance with a Throughput Optimized HDD (st1) volume that will serve as the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an EC2 instance with a General Purpose SSD (gp2) root volume and Provisioned",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision an EC2 instance with a General Purpose SSD (gp2) root volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nProvisioned IOPS SSD (io1/io2) volumes allow you to specify the exact IOPS (Input/Output Operations Per Second) you need, providing configurable and consistent performance. These volumes are designed for I/O-intensive workloads that require low latency and consistent performance. The root volume must be an SSD volume (gp2, gp3, io1, or io2) as EC2 instances require SSD for the root volume. By using a General Purpose SSD (gp2) for the root volume and Provisioned IOPS SSD for the data volume, you get the required configurable IOPS for transactional data while maintaining a cost-effective root volume.\n\n**Why option A is incorrect:**\nThroughput Optimized HDD (st1) volumes are designed for large, sequential workloads like big data, data warehouses, and log processing. They don't support configurable IOPS - st1 volumes have a baseline throughput that scales with volume size, but you cannot specify IOPS. Additionally, st1 volumes cannot be used as root volumes - EC2 instances require SSD volumes for the root volume. Cold HDD (sc1) volumes are even less suitable, designed for less frequently accessed data with the lowest cost.\n\n**Why option B is incorrect:**\nUsing st1 (Throughput Optimized HDD) as a data volume doesn't provide configurable IOPS. st1 volumes are optimized for throughput (MB/s) rather than IOPS, and they're designed for sequential workloads, not the random I/O patterns typical of transactional databases. st1 volumes cannot be configured with specific IOPS values - they provide baseline throughput that scales with volume size, which doesn't meet the requirement for configurable and consistent IOPS.\n\n**Why option D is incorrect:**\nGeneral Purpose SSD (gp2) volumes provide baseline IOPS of 3 IOPS per GB, with the ability to burst up to 3,000 IOPS. However, gp2 volumes don't allow you to provision specific IOPS values like Provisioned IOPS volumes do. While gp2 can provide good performance, it doesn't offer the configurable, consistent IOPS that Provisioned IOPS volumes (io1/io2) provide. For workloads requiring configurable and consistent IOPS, Provisioned IOPS volumes are the appropriate choice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is hosting 60 TB of production-level data in an Amazon S3 bucket. A solution \narchitect needs to bring that data on premises for quarterly audit requirements. This export of \ndata must be encrypted while in transit. The company has low network bandwidth in place \nbetween AWS and its on-premises data center. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Migration Hub with 90-day replication windows for data transfer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS Storage Gateway volume gateway on AWS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon Elastic File System (Amazon EFS), with lifecycle policies enabled, on AWS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Snowball device in the on-premises data center after completing an export job",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nAWS Snowball is a petabyte-scale data transport solution designed for transferring large amounts of data when network bandwidth is limited. You create an export job in AWS, which prepares a Snowball device with up to 80 TB capacity. The device is shipped to your data center, where you connect it to your network and transfer the 60 TB of data. Snowball uses 256-bit encryption and supports encryption in transit. Once the data is transferred, the device is shipped back to AWS, where the data is imported into your specified S3 bucket. This approach is ideal for low-bandwidth scenarios where transferring 60 TB over the network would take weeks or months.\n\n**Why option A is incorrect:**\nAWS Migration Hub is a service that provides a single location to track the progress of application migrations across multiple AWS and partner migration tools. It's not designed for data transfer or export. Migration Hub helps coordinate migration projects but doesn't actually transfer data. It's a project management and tracking tool, not a data transport solution. For exporting 60 TB of data, you need a data transfer service like Snowball.\n\n**Why option B is incorrect:**\nAWS Storage Gateway Volume Gateway provides a hybrid cloud storage solution that presents an iSCSI interface to on-premises applications, caching frequently accessed data locally while storing the full dataset in S3. However, Volume Gateway is designed for ongoing hybrid storage operations, not for one-time data export. It doesn't provide a mechanism to export 60 TB of data from S3 to on-premises in a single operation. Volume Gateway is for accessing cloud storage from on-premises, not for bulk data export.\n\n**Why option C is incorrect:**\nAmazon EFS is a fully managed elastic file system for Linux workloads, designed for use with EC2 instances and Lambda functions. EFS doesn't provide data export capabilities to on-premises data centers. While lifecycle policies can move data between EFS storage classes, EFS is not designed for exporting large amounts of data to on-premises. Additionally, EFS is a file system service, not a data transport solution. For exporting 60 TB from S3 to on-premises, you need a data transfer service like Snowball.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect is designing the cloud architecture for a company that needs to host \nhundreds of machine learning models for its users. During startup, the models need to load up to \n10 GB of data from Amazon S3 into memory, but they do not need disk access. Most of the \nmodels are used sporadically, but the users expect all of them to be highly available and \naccessible with low latency. \n \nWhich solution meets the requirements and is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Deploy models as AWS Lambda functions behind an Amazon API Gateway for each model.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy models as AWS Lambda functions behind a single Amazon API Gateway with path-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind a single",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nDeploying models as AWS Lambda functions behind a single API Gateway with path-based routing is the most cost-effective solution. Lambda automatically scales from zero to handle sporadic usage patterns, so you only pay when models are invoked. During idle periods, there are no charges for compute resources. Lambda functions can load the 10 GB of data from S3 into memory (/tmp directory) when invoked, and the data persists in memory during the function's execution. A single API Gateway can route requests to different Lambda functions based on the URL path, eliminating the need for multiple API Gateways. Lambda provides high availability automatically and low latency when functions are warm. This serverless approach minimizes costs for sporadically used models.\n\n**Why option A is incorrect:**\nDeploying a separate API Gateway for each of hundreds of machine learning models creates unnecessary complexity and cost. Each API Gateway incurs monthly charges and request fees. Managing hundreds of API Gateways increases operational overhead significantly. Additionally, Lambda functions have a 10 GB limit for the /tmp directory, which matches the requirement, but having separate API Gateways doesn't provide any benefit over a single API Gateway with path-based routing.\n\n**Why option B is incorrect:**\nAmazon ECS services require running container instances continuously or using Fargate, which incurs charges even when models aren't being used. For sporadically used models, keeping ECS tasks running 24/7 wastes money. While ECS can scale, it doesn't scale to zero like Lambda does. ECS also requires more operational overhead for container management, image building, and service configuration. The cost of maintaining hundreds of ECS services for sporadically used models would be significantly higher than Lambda.\n\n**Why option D is incorrect:**\nSimilar to option B, deploying models as ECS services behind a single API Gateway still requires running container instances continuously. ECS doesn't scale to zero, so you pay for compute resources even when models aren't being used. For hundreds of sporadically used models, this approach would be much more expensive than Lambda, which only charges for actual invocations. ECS also requires more management overhead for container orchestration compared to serverless Lambda functions.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 21,
    "text": "A company is developing an ecommerce application that will consist of a load-balanced front end, \na container-based application, and a relational database. A solutions architect needs to create a \nhighly available solution that operates with as little manual intervention as possible. \n \nWhich solutions meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon RDS DB instance in Multi-AZ mode.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nAmazon RDS Multi-AZ deployment provides high availability for relational databases by maintaining a synchronous standby replica in a different Availability Zone. If the primary database fails, RDS automatically fails over to the standby replica with minimal downtime (typically 60-120 seconds). Multi-AZ deployments are fully managed by AWS, requiring minimal manual intervention. The database automatically handles failover, backups, and maintenance operations, meeting the requirement for high availability with little manual intervention.\n\n**Why option B is incorrect:**\nCreating an RDS instance with manual replicas in another Availability Zone describes Multi-AZ deployment, but the phrasing suggests manually creating and managing replicas, which adds operational overhead. Multi-AZ deployment is an automated feature where AWS manages the standby replica automatically. The correct answer (option A) specifically mentions Multi-AZ mode, which is the managed, automated solution that requires minimal manual intervention.\n\n**Why option C is incorrect:**\nCreating an EC2 instance-based Docker cluster requires significant manual intervention. You must provision, configure, and manage EC2 instances, install Docker, set up container orchestration, configure networking, handle patching and security updates, and manage scaling. This approach requires ongoing operational overhead and doesn't meet the requirement for minimal manual intervention. ECS Fargate (option D) provides the same container functionality without EC2 management.\n\n**Why option D is incorrect:**\nWhile ECS with Fargate launch type is a correct choice for container-based applications with minimal manual intervention (Fargate manages the infrastructure), the question asks for two solutions. Option A (RDS Multi-AZ) addresses the relational database requirement, and ECS Fargate would address the container-based application requirement. However, since only option A is marked as correct in the current data, this might be a data issue, or the question may be focusing on the database solution specifically.\n\n**Why option E is incorrect:**\nECS with EC2 launch type requires managing the underlying EC2 instances, including provisioning, patching, scaling, and monitoring. This adds significant operational overhead compared to Fargate. While ECS provides container orchestration, using EC2 as the launch type means you're responsible for the EC2 infrastructure, which doesn't meet the requirement for minimal manual intervention.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company has an ecommerce application that stores data in an on-premises SQL database. The \ncompany has decided to migrate this database to AWS. However, as part of the migration, the \ncompany wants to find a way to attain sub-millisecond responses to common read requests. \n \nA solutions architect knows that the increase in speed is paramount and that a small percentage \nof stale data returned in the database reads is acceptable. \n \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Build Amazon RDS read replicas.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Build the database as a larger instance type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Build a database cache using Amazon ElastiCache.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Build a database cache using Amazon Elasticsearch Service (Amazon ES).",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nAmazon ElastiCache provides an in-memory caching layer that can deliver sub-millisecond response times for frequently accessed data. By caching common read requests in ElastiCache (using Redis or Memcached), the application can serve cached data directly from memory without querying the database. Since the scenario states that a small percentage of stale data is acceptable, caching is perfect - cached data may be slightly stale, but the performance gain (sub-millisecond responses) outweighs this trade-off. ElastiCache automatically handles cache management, scaling, and high availability, requiring minimal operational overhead.\n\n**Why option A is incorrect:**\nRDS read replicas can improve read performance by distributing read traffic across multiple database instances, but they don't provide sub-millisecond response times. Read replicas still require database queries, which typically take several milliseconds even with optimized queries. Read replicas help with read scaling but don't achieve the sub-millisecond latency that in-memory caching provides. Additionally, read replicas may have replication lag, which could result in stale data, but the primary benefit is read scaling, not ultra-low latency.\n\n**Why option B is incorrect:**\nBuilding the database as a larger instance type can improve performance by providing more CPU, memory, and network capacity, but it still won't achieve sub-millisecond response times for database queries. Even the largest RDS instances have query latency measured in milliseconds, not sub-milliseconds. Larger instances help with overall throughput and reduce query time, but they don't eliminate the fundamental latency of database operations. In-memory caching is required to achieve sub-millisecond response times.\n\n**Why option D is incorrect:**\nAmazon Elasticsearch Service (now OpenSearch Service) is designed for search and analytics workloads, not for caching database queries. While it can provide fast search capabilities, it's not a drop-in replacement for database caching. Elasticsearch/OpenSearch requires data to be indexed in a specific format and is optimized for full-text search, not for caching relational database queries. ElastiCache is specifically designed for caching database queries and provides better performance for this use case.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company is designing an application where users upload small files into Amazon S3.  \nAfter a user uploads a file, the file requires one-time simple processing to transform the data and \nsave the data in JSON format for later analysis. \n \nEach file must be processed as quickly as possible after it is uploaded. Demand will vary.  \nOn some days, users will upload a high number of files. On other days, users will upload a few \nfiles or no files. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EMR to read text files from Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nConfiguring S3 to send event notifications directly to an SQS queue, which then triggers a Lambda function, provides a serverless, event-driven architecture with minimal operational overhead. When a file is uploaded to S3, an event notification is sent to the SQS queue. Lambda automatically polls the SQS queue and processes messages as they arrive, transforming the data and saving it as JSON. This solution scales automatically - Lambda handles high volumes of uploads by processing multiple files concurrently, and scales down to zero when there are no files. SQS provides durability and ensures no events are lost. All components are fully managed by AWS, requiring no infrastructure management.\n\n**Why option A is incorrect:**\nAmazon EMR is a big data processing service designed for large-scale data processing jobs, not for processing individual small files as they're uploaded. EMR requires cluster provisioning, configuration, and management, which adds significant operational overhead. EMR is designed for batch processing of large datasets, not real-time processing of individual files. The overhead of managing EMR clusters doesn't meet the requirement for least operational overhead, and EMR doesn't automatically scale based on file uploads.\n\n**Why option B is incorrect:**\nWhile S3 can send event notifications to SQS, this option likely describes an incomplete solution that doesn't include the Lambda function component needed to process the files. SQS alone cannot transform data or save it as JSON - you need a compute service like Lambda to perform the processing. The complete solution requires S3 -> SQS -> Lambda, not just S3 -> SQS.\n\n**Why option D is incorrect:**\nWhile EventBridge (CloudWatch Events) can monitor S3 buckets and trigger Lambda functions, it's designed as an event router rather than a message queue. EventBridge doesn't provide the same durability guarantees and message persistence that SQS offers. For file processing workflows where you want to ensure no files are missed and provide better retry logic, SQS is more appropriate. Additionally, S3 can send events directly to SQS without needing EventBridge as an intermediary, simplifying the architecture.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "An application allows users at a company's headquarters to access product data. The product \ndata is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an \napplication performance slowdown and wants to separate read traffic from write traffic. \nA solutions architect needs to optimize the application's performance quickly. \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Change the existing database to a Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the existing database to a Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create read replicas for the database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create read replicas for the database.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nCreating read replicas for the RDS MySQL database allows you to separate read traffic from write traffic by directing read queries to the read replicas while writes go to the primary instance. This improves performance by distributing the read workload across multiple database instances. Read replicas can be created quickly (typically within minutes) and can be in the same or different Availability Zones. The application can be quickly modified to direct read queries to the read replica endpoint, providing immediate performance optimization. Read replicas are fully managed by AWS and require minimal operational overhead.\n\n**Why option A is incorrect:**\nMulti-AZ deployment provides high availability and automatic failover, but it doesn't separate read traffic from write traffic. In a Multi-AZ deployment, the standby replica is used only for failover and cannot serve read traffic. All read and write traffic still goes to the primary instance. Multi-AZ improves availability but doesn't address the performance issue of separating read and write traffic, which is what the operations team identified as the problem.\n\n**Why option B is incorrect:**\nThis option is identical to option A - Multi-AZ deployment doesn't separate read and write traffic. The standby replica in Multi-AZ is synchronous and used only for failover, not for serving read queries. To separate read traffic, you need read replicas, which are asynchronous replicas specifically designed to serve read queries.\n\n**Why option C is incorrect:**\nThis option correctly identifies read replicas as the solution, but the phrasing might suggest a different implementation approach. However, since option D is marked as correct and also mentions read replicas, the difference might be in the specific configuration or the question's focus on the quickest optimization method. Both options describe read replicas, but option D likely represents the most direct and quickest approach to implement.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "An Amazon EC2 administrator created the following policy associated with an IAM group \ncontaining several users. \n \n \n \n \nWhat is the effect of this policy?",
    "options": [
      {
        "id": 0,
        "text": "Users can terminate an EC2 instance in any AWS Region except us-east-1.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Users can terminate an EC2 instance with the IP address 10 100 100 1 in the us-east-1 Region",
        "correct": false
      },
      {
        "id": 2,
        "text": "Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is",
        "correct": true
      },
      {
        "id": 3,
        "text": "Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nThe IAM policy uses a Deny statement that blocks all EC2 actions in all regions except us-east-1, and then an Allow statement that permits EC2 termination actions in us-east-1, but only when the user's source IP address is in the 10.100.100.0/24 range. The IP address 10.100.100.254 falls within this CIDR block (10.100.100.0/24), so a user connecting from this IP address can terminate EC2 instances in the us-east-1 region. The policy effectively restricts EC2 actions to us-east-1 only and further restricts termination actions to specific source IP addresses.\n\n**Why option A is incorrect:**\nThe policy doesn't prevent termination in us-east-1 - it actually allows termination in us-east-1 for users with the correct source IP. The Deny statement blocks EC2 actions in all regions except us-east-1, meaning us-east-1 is the only region where EC2 actions are permitted (subject to the IP restriction for termination). Users cannot terminate instances in regions other than us-east-1, but they can terminate in us-east-1 if their source IP matches.\n\n**Why option B is incorrect:**\nThe policy doesn't restrict termination based on the EC2 instance's IP address (10.100.100.1). The IP address condition in IAM policies applies to the source IP address of the user making the API call, not the IP address of the EC2 instance being terminated. The condition checks where the API request is coming from (the user's network), not the target instance's IP address.\n\n**Why option D is incorrect:**\nThis option states the opposite of what the policy allows. Users CAN terminate instances in us-east-1 when their source IP is 10.100.100.254, because this IP falls within the allowed CIDR block (10.100.100.0/24). The policy explicitly allows termination in us-east-1 for source IPs in this range, so termination is permitted, not prevented.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company has a large Microsoft SharePoint deployment running on-premises that requires \nMicrosoft Windows shared file storage. The company wants to migrate this workload to the AWS \nCloud and is considering various storage options. The storage solution must be highly available \nand integrated with Active Directory for access control. \n \nWhich solution will satisfy these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EFS storage and set the Active Directory domain for authentication",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an SMB Me share on an AWS Storage Gateway tile gateway in two Availability Zones",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server file system on AWS and set the Active",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nAmazon FSx for Windows File Server is a fully managed file storage service specifically designed for Windows workloads like SharePoint. It provides native SMB protocol support, which is required for Windows shared file storage. FSx integrates seamlessly with Microsoft Active Directory for authentication and authorization, allowing you to use existing AD credentials and groups. FSx provides high availability through Multi-AZ deployments, automatically replicating data across multiple Availability Zones and providing automatic failover. It's designed to be a drop-in replacement for on-premises Windows file servers, making it ideal for migrating SharePoint deployments.\n\n**Why option A is incorrect:**\nAmazon EFS uses the NFS protocol, not SMB, so it's not compatible with Microsoft SharePoint or Windows shared file storage. SharePoint requires SMB protocol for file shares. Additionally, while EFS can integrate with Active Directory through AWS Directory Service, it's primarily designed for Linux workloads. EFS doesn't provide the native Windows file server functionality that SharePoint requires, and the NFS protocol is not compatible with Windows file sharing requirements.\n\n**Why option B is incorrect:**\nAWS Storage Gateway File Gateway can present an SMB share, but it's designed as a hybrid solution that caches data locally while storing it in S3. File Gateway is typically deployed on-premises or in a hybrid configuration, not as a primary storage solution for cloud-native workloads. While it can work, FSx for Windows File Server is specifically designed for this use case and provides better integration with Active Directory and Windows applications. Additionally, the requirement is for a cloud migration, not a hybrid solution.\n\n**Why option C is incorrect:**\nAmazon S3 is an object storage service, not a file system. While there are tools that can mount S3 as a volume on Windows Server, this approach has significant limitations. S3 doesn't provide native SMB protocol support, doesn't integrate well with Active Directory for fine-grained access control, and doesn't provide the file system semantics that SharePoint requires. S3 is designed for object storage, not for shared file storage with Windows applications. FSx for Windows File Server is the appropriate managed service for this requirement.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "An image-processing company has a web application that users use to upload images. The \napplication uploads the images into an Amazon S3 bucket. The company has set up S3 event \nnotifications to publish the object creation events to an Amazon Simple Queue Service (Amazon \nSQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function \nthat processes the images and sends the results to users through email. \n \nUsers report that they are receiving multiple email messages for every uploaded image. A \nsolutions architect determines that SQS messages are invoking the Lambda function more than \nonce, resulting in multiple email messages. \n \nWhat should the solutions architect do to resolve this issue with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the SQS standard queue to an SQS FIFO queue.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the Lambda function to delete each message from the SQS queue immediately after the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nThe visibility timeout is the period during which SQS hides a message from other consumers after it's been received. If the Lambda function takes longer than the visibility timeout to process a message, the message becomes visible again and can be processed by another Lambda invocation, causing duplicate processing. By increasing the visibility timeout to a value greater than the Lambda function's execution time plus any retry overhead, you ensure that the message remains hidden long enough for processing to complete. This prevents duplicate processing with minimal operational overhead - it's just a configuration change to the SQS queue.\n\n**Why option A is incorrect:**\nLong polling reduces the number of empty responses when polling SQS, which can reduce costs and improve efficiency, but it doesn't prevent duplicate message processing. Long polling affects how Lambda polls for messages, not how messages are handled after they're received. The duplicate processing issue is caused by the visibility timeout being too short, not by the polling method.\n\n**Why option B is incorrect:**\nChanging from a standard queue to a FIFO queue would prevent duplicates by ensuring exactly-once processing, but FIFO queues have limitations: they support up to 3,000 messages per second with batching, or 300 messages per second without batching. Standard queues support nearly unlimited throughput. If the application needs high throughput, FIFO queues might not be suitable. Additionally, FIFO queues require message groups and deduplication IDs, which adds complexity. Increasing visibility timeout is simpler and doesn't require changing the queue type.\n\n**Why option D is incorrect:**\nModifying the Lambda function to delete messages immediately after processing would require code changes and doesn't solve the root cause. The issue is that if the Lambda function fails or times out before completing processing, the message should remain in the queue for retry. Deleting messages immediately would cause message loss if processing fails. Additionally, Lambda automatically deletes messages from SQS after successful processing, so manual deletion isn't necessary. The proper solution is to configure the visibility timeout correctly.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A company is implementing a shared storage solution for a media application that is hosted in the \nAWS Cloud.  \nThe company needs the ability to use SMB clients to access data. The solution must he fully \nmanaged. \nWhich AWS solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Storage Gateway volume gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway tape gateway.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 Windows instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server tile system.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nAmazon FSx for Windows File Server is a fully managed file storage service that provides native SMB protocol support, which is required for SMB clients. It's designed specifically for Windows-based applications and workloads that need shared file storage. FSx is fully managed by AWS, handling provisioning, patching, backups, and high availability automatically. It integrates with Microsoft Active Directory for authentication and provides Multi-AZ deployments for high availability. FSx for Windows File Server is the appropriate solution for applications that need SMB protocol access to shared storage.\n\n**Why option A is incorrect:**\nAWS Storage Gateway Volume Gateway is a hybrid cloud storage service that presents iSCSI volumes to on-premises applications. It doesn't provide SMB protocol support - it uses iSCSI for block storage. Volume Gateway is designed for backup and disaster recovery scenarios where on-premises applications need to store data in the cloud, not for cloud-native applications that need SMB file shares. Additionally, Volume Gateway typically runs on-premises or in a hybrid configuration, not as a fully cloud-managed solution.\n\n**Why option B is incorrect:**\nAWS Storage Gateway Tape Gateway is designed for backup and archival scenarios, presenting a virtual tape library interface to backup applications. It doesn't provide SMB protocol support or file system access. Tape Gateway is for long-term backup storage, not for active file sharing with SMB clients. It's not suitable for a media application that needs real-time file access.\n\n**Why option C is incorrect:**\nCreating an EC2 Windows instance and configuring it as a file server would provide SMB support, but it's not a fully managed solution. You would be responsible for managing the EC2 instance, installing and configuring Windows Server, setting up the file server, managing patches and updates, configuring backups, and ensuring high availability. This requires significant operational overhead, which doesn't meet the requirement for a fully managed solution. FSx for Windows File Server provides the same functionality without the management burden.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company's containerized application runs on an Amazon EC2 instance. The application needs \nto download security certificates before it can communicate with other business applications. The \ncompany wants a highly secure solution to encrypt and decrypt the certificates in near real time. \nThe solution also needs to store data in highly available storage after the data is encrypted. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create AWS Secrets Manager secrets for encrypted certificates.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function that uses the Python cryptography library to receive and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nAWS KMS customer managed keys provide encryption and decryption capabilities for data at rest and in transit. The application can use KMS to encrypt certificates before storing them and decrypt them when needed. KMS integrates seamlessly with other AWS services like S3, which can store the encrypted certificates with high availability across multiple Availability Zones. KMS provides near real-time encryption/decryption with low latency. The solution is fully managed by AWS, requiring minimal operational overhead - you create the key and grant the application permission to use it. KMS automatically handles key rotation, backups, and high availability.\n\n**Why option A is incorrect:**\nAWS Secrets Manager is designed for storing secrets like database credentials, API keys, and passwords, not for storing and managing security certificates that need to be downloaded and used by applications. Secrets Manager can store certificates, but it's optimized for secrets that are retrieved via API calls, not for certificates that need to be downloaded and used by applications in near real-time. Additionally, Secrets Manager has size limitations and cost considerations that may not be suitable for certificate storage at scale.\n\n**Why option B is incorrect:**\nCreating a Lambda function with a Python cryptography library adds unnecessary complexity and operational overhead. You would need to manage the Lambda function, handle errors, ensure it's invoked correctly, and manage the encryption keys yourself. This approach doesn't leverage AWS managed services and requires ongoing maintenance. Additionally, Lambda functions have execution time limits and cold start latency, which may not meet the near real-time requirement. Using KMS directly from the application is simpler and more efficient.\n\n**Why option D is incorrect:**\nThis option is identical to option C (KMS customer managed key), but the question likely distinguishes between different implementation approaches or the option text may be truncated differently. However, KMS customer managed keys are the correct solution for encrypting/decrypting certificates with minimal operational overhead and high availability storage integration.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A solutions architect is designing a VPC with public and private subnets. The VPC and subnets \nuse IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three \nAvailability Zones (AZs) for high availability. An internet gateway is used to provide internet \naccess for the public subnets. The private subnets require access to the internet to allow Amazon \nEC2 instances to download software updates. \n \nWhat should the solutions architect do to enable Internet access for the private subnets?",
    "options": [
      {
        "id": 0,
        "text": "Create three NAT gateways, one for each public subnet in each AZ.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create three NAT instances, one for each private subnet in each AZ.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a second internet gateway on one of the private subnets.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an egress-only internet gateway on one of the public subnets.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nNAT gateways are AWS-managed services that allow instances in private subnets to initiate outbound connections to the internet while preventing inbound connections from the internet. To achieve high availability across three Availability Zones, you need one NAT gateway in each AZ's public subnet. Each private subnet's route table should route internet-bound traffic (0.0.0.0/0) to the NAT gateway in its corresponding AZ. This ensures that if one AZ fails, the other AZs can still provide internet access. NAT gateways are highly available within each AZ and automatically scale to handle the traffic load.\n\n**Why option B is incorrect:**\nNAT instances are EC2 instances that you manage yourself, requiring ongoing maintenance, patching, and monitoring. While you could deploy NAT instances in private subnets, NAT gateways should be deployed in public subnets (not private subnets) because they need direct access to the internet gateway. More importantly, NAT instances require manual management and don't provide the same level of availability and automatic scaling that managed NAT gateways provide. NAT instances are a legacy approach that adds operational overhead.\n\n**Why option C is incorrect:**\nInternet gateways cannot be attached to private subnets - they must be attached to the VPC and route tables determine which subnets can use them. Even if you could attach an internet gateway to a private subnet, this would expose the private instances directly to the internet, defeating the purpose of having private subnets. Private subnets are designed to have no direct internet connectivity for security. NAT gateways provide the secure outbound-only internet access that private subnets need.\n\n**Why option D is incorrect:**\nEgress-only internet gateways are used for IPv6 traffic, not IPv4. The scenario specifies that the VPC uses IPv4 CIDR blocks. For IPv4 private subnets, you need NAT gateways (or NAT instances) to provide outbound internet access. Egress-only internet gateways serve the same purpose for IPv6 that NAT gateways serve for IPv4, but they cannot be used for IPv4 traffic.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP \nserver that stores its data on an NFS-based file system. The server holds 200 GB of data that \nneeds to be transferred. The server must be hosted on an Amazon EC2 instance that uses an \nAmazon Elastic File System (Amazon EFS) file system. \nWhich combination of steps should a solutions architect take to automate this task? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Launch the EC2 instance into the same Availability Zone as the EFS file system.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Install an AWS DataSync agent in the on-premises data center.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance tor",
        "correct": false
      },
      {
        "id": 3,
        "text": "Manually use an operating system copy command to push the data to the EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nLaunching the EC2 instance in the same Availability Zone as the EFS file system minimizes latency and data transfer costs. When the EC2 instance and EFS are in the same AZ, data transfers between them don't incur cross-AZ data transfer charges. This is important for the initial data migration and ongoing operations. EFS is accessible across all AZs in a region, but placing the instance in the same AZ as EFS provides optimal performance.\n\n**Why option B is incorrect:**\nInstalling an AWS DataSync agent in the on-premises data center is actually a correct step for automating the data transfer. DataSync is designed to automate and accelerate data transfers between on-premises storage and AWS services. The agent would be installed on-premises to read from the NFS file system and transfer data to EFS. However, since this is a \"choose two\" question and only option A is marked as correct in the current data, there may be a data issue, or the question may focus on the EC2/EFS configuration rather than the transfer method.\n\n**Why option C is incorrect:**\nCreating a secondary EBS volume on the EC2 instance doesn't help with the migration task. The requirement is to transfer data from the on-premises NFS file system to EFS, not to store it on EBS. EBS volumes are block storage attached to EC2 instances, while EFS is the network file system that the application will use. Adding an EBS volume doesn't automate the transfer process or help migrate data to EFS.\n\n**Why option D is incorrect:**\nManually using operating system copy commands doesn't meet the requirement to automate the task. Manual copying requires ongoing intervention, is error-prone, and doesn't provide the automation, error handling, and progress tracking that AWS DataSync provides. Manual copying also doesn't scale well and requires the administrator to monitor and manage the transfer process.\n\n**Why option E is incorrect:**\nWhile AWS DataSync can create location configurations, this option alone doesn't complete the automation setup. You need both the on-premises agent (option B) and the location configuration to fully automate the transfer. However, the question asks for a combination of steps, and this might be part of the solution, but it needs to be combined with other steps like installing the DataSync agent.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "A company has an AWS Glue extract. transform, and load (ETL) job that runs every day at the \nsame time. The job processes XML data that is in an Amazon S3 bucket. New data is added to \nthe S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data \nduring each run. \nWhat should the solutions architect do to prevent AWS Glue from reprocessing old data?",
    "options": [
      {
        "id": 0,
        "text": "Edit the job to use job bookmarks.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Edit the job to delete data after the data is processed",
        "correct": false
      },
      {
        "id": 2,
        "text": "Edit the job by setting the NumberOfWorkers field to 1.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a FindMatches machine learning (ML) transform.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nAWS Glue job bookmarks track which data has already been processed by maintaining state information from previous job runs. When job bookmarks are enabled, Glue remembers the last processed files or partitions and only processes new data that has been added since the last run. This prevents reprocessing of old XML data files and ensures that only new data added daily to the S3 bucket is processed, significantly reducing processing time and costs. Job bookmarks work automatically once enabled and require no additional configuration.\n\n**Why option B is incorrect:**\nDeleting data after processing would remove the source data from S3, which may not be desirable if the data needs to be retained for compliance, auditing, or other purposes. Additionally, deleting data doesn't prevent Glue from reprocessing it - if the data is deleted, there's nothing to reprocess, but this approach loses the historical data. Job bookmarks provide a better solution by tracking what's been processed without requiring data deletion.\n\n**Why option C is incorrect:**\nSetting the NumberOfWorkers field to 1 reduces the parallelism of the Glue job, which may slow down processing but doesn't prevent reprocessing of old data. Reducing workers affects how fast data is processed, not which data is processed. The job will still process all data in the S3 bucket regardless of the number of workers. Job bookmarks are the correct solution to prevent reprocessing.\n\n**Why option D is incorrect:**\nFindMatches is an ML transform in AWS Glue designed for data deduplication and finding similar records, not for preventing reprocessing of data in ETL jobs. FindMatches uses machine learning to identify duplicate or similar records within a dataset, which is useful for data quality but doesn't solve the problem of preventing Glue from reprocessing files that were already processed in previous runs. Job bookmarks are specifically designed for this use case.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A solutions architect must design a highly available infrastructure for a website. The website is \npowered by Windows web servers that run on Amazon EC2 instances. The solutions architect \nmust implement a solution that can mitigate a large-scale DDoS attack that originates from \nthousands of IP addresses. Downtime is not acceptable for the website. \nWhich actions should the solutions architect take to protect the website from such an attack? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Shield Advanced to stop the DDoS attack.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure Amazon GuardDuty to automatically block the attackers.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the website to use Amazon CloudFront for both static and dynamic content.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Lambda function to automatically add attacker IP addresses to VPC network",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nAWS Shield Advanced provides comprehensive DDoS protection for applications running on AWS, including protection against large-scale attacks originating from thousands of IP addresses. Shield Advanced automatically detects and mitigates DDoS attacks at the network and transport layers, and when combined with AWS WAF, it can also protect against application-layer attacks. Shield Advanced provides 24/7 DDoS response team support and cost protection for scaling during attacks, ensuring the website remains available even during large-scale DDoS attacks.\n\n**Why option B is incorrect:**\nAmazon GuardDuty is a threat detection service that uses machine learning to identify potential security threats by analyzing CloudTrail logs, VPC Flow Logs, and DNS logs. However, GuardDuty is a monitoring and detection service, not a DDoS mitigation service. It can identify suspicious activity but doesn't automatically block attackers or mitigate DDoS attacks. GuardDuty provides alerts and findings, but you would need additional services like Shield or WAF to actually block attacks.\n\n**Why option C is incorrect:**\nWhile CloudFront can help distribute traffic and provide some DDoS protection through its edge locations, using CloudFront alone doesn't provide comprehensive DDoS protection against large-scale attacks. CloudFront is a CDN that caches content and distributes traffic, but for Windows web servers running on EC2, you need Shield Advanced for dedicated DDoS protection. CloudFront can be part of a defense-in-depth strategy but isn't sufficient on its own for mitigating large-scale DDoS attacks.\n\n**Why option D is incorrect:**\nUsing a Lambda function to add attacker IP addresses to VPC network ACLs is reactive and doesn't scale well for attacks from thousands of IP addresses. By the time Lambda identifies and blocks individual IPs, the attack may have already caused damage. Additionally, network ACLs have limits on the number of rules, making it impractical to block thousands of IPs. This approach also requires writing and maintaining custom code. Shield Advanced provides automatic, scalable DDoS mitigation without requiring custom code.\n\n**Why option E is incorrect:**\nEC2 Spot Instances are designed for cost optimization, not DDoS protection. While Auto Scaling can help handle increased load, Spot Instances can be interrupted, which would cause downtime during an attack - contradicting the requirement that downtime is not acceptable. Additionally, scaling up instances doesn't mitigate the DDoS attack itself; it just provides more capacity, which may not be sufficient against a large-scale attack. Shield Advanced provides dedicated DDoS mitigation at the network level before traffic reaches your instances.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company is preparing to deploy a new serverless workload.  \nA solutions architect must use the principle of least privilege to configure permissions that will be \nused to run an AWS Lambda function.  \nAn Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Add an execution role to the function with lambda:InvokeFunction as the action and * as the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an execution role to the function with lambda:InvokeFunction as the action and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add a resource-based policy to the function with lambda:'* as the action and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add a resource-based policy to the function with lambda:InvokeFunction as the action and",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nA resource-based policy attached to the Lambda function allows you to grant specific permissions to invoke the function to specific principals (in this case, EventBridge service). By specifying `lambda:InvokeFunction` as the action and `events.amazonaws.com` as the principal, you grant only the minimum permission needed - the ability for EventBridge to invoke this specific function. This follows the principle of least privilege by granting only the necessary permission to the specific service that needs it, without granting broader permissions or using wildcards.\n\n**Why option A is incorrect:**\nAdding an execution role with `lambda:InvokeFunction` and `*` as the resource grants the function permission to invoke any Lambda function, which violates the principle of least privilege. Execution roles define what the Lambda function itself can do (like access S3 or DynamoDB), not who can invoke it. Additionally, using `*` as the resource grants permissions to all Lambda functions, which is overly permissive. Resource-based policies control who can invoke the function, not what the function can do.\n\n**Why option B is incorrect:**\nSimilar to option A, execution roles define what the Lambda function can do, not who can invoke it. The execution role is used by the Lambda service to assume permissions on behalf of the function when it runs. To allow EventBridge to invoke the function, you need a resource-based policy on the function, not an execution role. Execution roles and resource-based policies serve different purposes in Lambda security.\n\n**Why option C is incorrect:**\nUsing `lambda:*` as the action grants all Lambda permissions, not just `InvokeFunction`. This violates the principle of least privilege by granting more permissions than necessary. The principle of least privilege requires granting only the minimum permissions needed - in this case, only `lambda:InvokeFunction` should be granted, not all Lambda permissions. Additionally, the resource should be the specific function ARN, not a wildcard, to further restrict permissions.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A company has an image processing workload running on Amazon Elastic Container Service \n(Amazon ECS) in two private subnets. Each private subnet uses a NAT instance for internet \naccess. All images are stored in Amazon S3 buckets. \nThe company is concerned about the data transfer costs between Amazon ECS and Amazon S3. \n \nWhat should a solutions architect do to reduce costs?",
    "options": [
      {
        "id": 0,
        "text": "Configure a NAT gateway to replace the NAT instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a gateway endpoint for traffic destined to Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an interface endpoint for traffic destined to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudFront for the S3 bucket storing the images.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option B is correct:**\nA gateway VPC endpoint for S3 provides private connectivity between resources in the VPC and S3 without requiring internet access through NAT instances or NAT gateways. This eliminates data transfer costs through NAT devices, as traffic to S3 stays within the AWS network. Gateway endpoints are free - there are no hourly charges or data processing charges. By routing S3 traffic through the gateway endpoint instead of NAT instances, the company eliminates NAT data transfer costs, which is the primary cost concern. Gateway endpoints automatically scale and require no management.\n\n**Why option A is incorrect:**\nReplacing NAT instances with NAT gateways improves availability and reduces management overhead, but it doesn't reduce data transfer costs. NAT gateways still charge for data processing and data transfer, so the cost concern remains. Gateway VPC endpoints eliminate NAT costs entirely for S3 traffic, making them the cost-effective solution. NAT gateways are better than NAT instances for availability, but gateway endpoints are better for cost reduction.\n\n**Why option C is incorrect:**\nS3 does not support interface endpoints (PrivateLink). Interface endpoints are only available for services that integrate with AWS PrivateLink, and S3 is not on that list. S3 uses gateway endpoints, which operate differently - they're not ENIs in your VPC but rather route table entries that direct S3 traffic to AWS's network. Interface endpoints are for services like API Gateway, KMS, and other PrivateLink-enabled services, not S3.\n\n**Why option D is incorrect:**\nConfiguring CloudFront for the S3 bucket creates a CDN that caches content at edge locations, which can reduce data transfer costs for content delivery to end users. However, CloudFront doesn't help with data transfer costs between ECS tasks and S3 within AWS. The cost concern is about ECS accessing S3, not end users accessing content. CloudFront is for content delivery, not for reducing costs of ECS-to-S3 data transfers. Gateway endpoints are the solution for reducing VPC-to-S3 transfer costs.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company is moving Its on-premises Oracle database to Amazon Aurora PostgreSQL.  \nThe database has several applications that write to the same tables. \nThe applications need to be migrated one by one with a month in between each migration \nManagement has expressed concerns that the database has a high number of reads and writes. \nThe data must be kept in sync across both databases throughout tie migration. \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync tor the initial migration.",
        "correct": false
      },
      {
        "id": 1,
        "text": "UseAVVS DataSync for the initial migration.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Schema Conversion led with AWS DataBase Migration Service (AWS DMS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nAWS Database Migration Service (DMS) with Change Data Capture (CDC) can continuously replicate changes from the source Oracle database to the target Aurora PostgreSQL database, keeping both databases in sync throughout the migration period. The AWS Schema Conversion Tool (SCT) converts the database schema from Oracle to PostgreSQL format. DMS supports ongoing replication, so as applications are migrated one by one over several months, the databases remain synchronized. DMS handles the high number of reads and writes efficiently and can scale to handle the workload. This solution allows for a gradual migration while maintaining data consistency.\n\n**Why option A is incorrect:**\nAWS DataSync is designed for one-time or scheduled data transfers between on-premises storage systems and AWS storage services like S3 or EFS. It's not designed for continuous database replication or keeping databases in sync. DataSync doesn't support Change Data Capture (CDC) or real-time replication between databases. For keeping an Oracle database and Aurora PostgreSQL in sync during a gradual migration, you need DMS, not DataSync.\n\n**Why option B is incorrect:**\nThis option is identical to option A - DataSync is not suitable for continuous database replication. DataSync is for file-based data transfers, not for database replication with CDC capabilities. The requirement to keep databases in sync throughout a multi-month migration requires continuous replication, which DMS provides but DataSync does not.\n\n**Why option D is incorrect:**\nThis option appears identical to option C (Schema Conversion Tool with DMS), but there may be a subtle difference in the implementation details or the question may be testing understanding of the specific DMS configuration needed. However, DMS with CDC is the correct solution for keeping databases synchronized during a gradual migration, regardless of the specific configuration details.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A company wants to migrate a high performance computing (HPC) application and data from on-\npremises to the AWS Cloud. The company uses tiered storage on premises with hot high-\nperformance parallel storage to support the application during periodic runs of the application, \nand more economical cold storage to hold the data when the application is not actively running. \nWhich combination of solutions should a solutions architect recommend to support the storage \nneeds of the application? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 for cold data storage",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon EFS for cold data storage",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 for high-performance parallel storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon FSx for clustre tor high-performance parallel storage",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon FSx for Windows for high-performance parallel storage",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nAmazon S3 is ideal for cold data storage due to its low cost, high durability, and virtually unlimited scalability. S3 offers multiple storage classes (Standard-IA, Glacier, Glacier Deep Archive) that can reduce costs for infrequently accessed data. S3 provides 99.999999999% durability and can store data indefinitely at a fraction of the cost of high-performance storage. For HPC applications where data is only accessed when the application runs periodically, S3 provides economical cold storage that can be accessed when needed.\n\n**Why option B is incorrect:**\nAmazon EFS is designed for shared file storage with low-latency access, not for cold storage. EFS is optimized for frequently accessed data and provides consistent, low-latency performance, which comes at a higher cost than S3. EFS doesn't have the same cost-effective storage classes for infrequently accessed data. For cold storage that's only accessed periodically, S3 is more cost-effective than EFS.\n\n**Why option C is incorrect:**\nS3 is not designed for high-performance parallel storage. While S3 can handle high throughput, it doesn't provide the low-latency, parallel file system performance that HPC applications require during active runs. S3 has higher latency than file systems and isn't optimized for the parallel I/O patterns typical of HPC workloads. FSx for Lustre is designed for high-performance parallel storage.\n\n**Why option D is incorrect:**\nAmazon FSx for Lustre (note: the option text has a typo \"clustre\" but refers to Lustre) is designed for high-performance parallel storage and is ideal for HPC workloads. However, this option is marked as incorrect, likely because the question asks for \"choose two\" and this should be combined with S3 for cold storage. FSx for Lustre provides the high-performance parallel file system needed during application runs, but you also need S3 for economical cold storage when the application isn't running.\n\n**Why option E is incorrect:**\nAmazon FSx for Windows File Server is designed for Windows-based file shares using SMB protocol, not for high-performance parallel storage. It's optimized for Windows workloads like SharePoint and file servers, not for HPC applications that need parallel file system performance. FSx for Lustre is the appropriate choice for HPC high-performance parallel storage needs.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "A company is experiencing growth as demand for its product has increased. The company's \nexisting purchasing application is slow when traffic spikes. The application is a monolithic three \ntier application that uses synchronous transactions and sometimes sees bottlenecks in the \napplication tier. A solutions architect needs to design a solution that can meet required application \nresponse times while accounting for traffic volume spikes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Vertically scale the application instance using a larger Amazon EC2 instance size.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Scale the application's persistence layer horizontally by introducing Oracle RAC on AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Scale the web and application tiers horizontally using Auto Scaling groups and an Application",
        "correct": true
      },
      {
        "id": 3,
        "text": "Decouple the application and data tiers using Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option C is correct:**\nHorizontal scaling using Auto Scaling groups with an Application Load Balancer allows the application to automatically add or remove instances based on demand, handling traffic spikes by distributing load across multiple instances. The ALB distributes incoming requests across healthy instances in the Auto Scaling group, preventing any single instance from becoming a bottleneck. When traffic spikes occur, Auto Scaling automatically launches additional instances to handle the increased load, then scales down when traffic decreases. This approach maintains response times during traffic volume spikes by adding capacity rather than trying to make a single instance faster.\n\n**Why option A is incorrect:**\nVertically scaling (increasing instance size) provides more CPU, memory, and network capacity to a single instance, but it has limitations. There's a maximum instance size, and vertical scaling requires downtime or instance replacement. More importantly, a single larger instance can still become a bottleneck during traffic spikes, and vertical scaling doesn't provide the same level of fault tolerance and load distribution that horizontal scaling provides. Horizontal scaling is more effective for handling unpredictable traffic spikes.\n\n**Why option B is incorrect:**\nScaling the persistence layer (database) horizontally with Oracle RAC addresses database bottlenecks but doesn't solve the application tier bottlenecks mentioned in the scenario. The problem states that bottlenecks occur in the application tier, not the database tier. While database scaling might help overall performance, it doesn't address the specific issue of application tier bottlenecks during traffic spikes. Additionally, Oracle RAC on AWS is complex and expensive compared to managed database services.\n\n**Why option D is incorrect:**\nDecoupling using SQS would require changing the application architecture from synchronous to asynchronous transactions. The scenario explicitly states the application uses synchronous transactions, and changing this would require significant application code changes. Decoupling might help long-term, but it doesn't address the immediate need to handle traffic spikes while maintaining current response times. Horizontal scaling with Auto Scaling and ALB can be implemented without changing the application architecture.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A solutions architect needs to ensure that all Amazon Elastic Block Store (Amazon EBS) volumes \nrestored from unencrypted EBS snapshots are encrypted. \n \nWhat should the solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Enable EBS encryption by default for the AWS Region",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable EBS encryption by default for the specific volumes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new volume and specify the symmetric customer master key (CMK) to use for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new volume and specify the asymmetric customer master key (CMK) to use for",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nEnabling EBS encryption by default for the AWS Region ensures that all new EBS volumes created in that region are automatically encrypted, including volumes restored from snapshots. When you restore an unencrypted snapshot with encryption by default enabled, the restored volume will be encrypted even though the source snapshot was unencrypted. This is a region-level setting that applies to all volumes created in that region, ensuring consistent encryption without requiring manual configuration for each volume. This meets the requirement to ensure all volumes restored from unencrypted snapshots are encrypted.\n\n**Why option B is incorrect:**\nEBS encryption by default cannot be enabled for specific volumes - it's a region-level setting that applies to all volumes in the region. You cannot selectively enable encryption by default for only certain volumes. The setting is either enabled for the entire region or not enabled at all. To ensure all restored volumes are encrypted, you must enable encryption by default at the region level.\n\n**Why option C is incorrect:**\nManually creating volumes and specifying a CMK for encryption requires manual intervention for each volume restoration, which doesn't scale and is error-prone. This approach doesn't ensure that ALL volumes restored from unencrypted snapshots are encrypted - it only encrypts volumes that are manually created with encryption specified. If someone forgets to specify encryption or uses a different method to restore volumes, they might remain unencrypted. Encryption by default ensures automatic encryption without manual steps.\n\n**Why option D is incorrect:**\nEBS encryption uses symmetric keys (AES-256), not asymmetric keys. AWS KMS supports both symmetric and asymmetric CMKs, but EBS volume encryption specifically requires symmetric CMKs. Asymmetric keys are used for different purposes like signing and encryption/decryption where the public and private keys serve different roles. EBS encryption uses symmetric encryption where the same key encrypts and decrypts data, providing the performance needed for block storage encryption.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company is storing backup files by using Amazon S3 Standard storage. The files are accessed \nfrequently for 1 month. However, the files are not accessed after 1 month. The company must \nkeep the files indefinitely. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n92 \n \nWhich storage solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Intelligent-Tiering to automatically migrate objects.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option B is correct:**\nS3 Glacier Deep Archive is the lowest-cost storage class in S3, designed for long-term archival and data that is rarely accessed. Since the files are not accessed after 1 month and must be kept indefinitely, Glacier Deep Archive provides the most cost-effective solution. S3 Lifecycle policies can automatically transition objects from S3 Standard to Glacier Deep Archive after 30 days (or after 1 month as specified), reducing storage costs by up to 98% compared to S3 Standard. Glacier Deep Archive is designed for data that is accessed once or twice per year and provides the same durability as S3 Standard.\n\n**Why option A is incorrect:**\nS3 Intelligent-Tiering automatically moves objects between access tiers based on changing access patterns, but it's designed for data with unknown or unpredictable access patterns. Since the scenario clearly states that files are accessed frequently for 1 month and then not accessed afterward, the access pattern is predictable. Intelligent-Tiering has a monthly monitoring and automation fee per object, which can make it more expensive than a simple Lifecycle policy for data with predictable access patterns. For this use case with a clear access pattern, a Lifecycle policy is more cost-effective.\n\n**Why option C is incorrect:**\nS3 Standard-IA (Infrequent Access) is designed for data that is accessed less frequently but requires rapid access when needed. However, Standard-IA is still more expensive than Glacier Deep Archive for long-term archival storage. Since the files are not accessed after 1 month and must be kept indefinitely, Standard-IA doesn't provide the same cost savings as Glacier Deep Archive. Standard-IA is suitable for data accessed monthly or quarterly, but for data accessed rarely or never, Glacier Deep Archive is more cost-effective.\n\n**Why option D is incorrect:**\nS3 One Zone-IA stores data in a single Availability Zone, which provides lower durability (99.5% availability) compared to Standard-IA or Glacier Deep Archive (99.999999999% durability). While One Zone-IA is cheaper than Standard-IA, it's still more expensive than Glacier Deep Archive for long-term storage. More importantly, One Zone-IA doesn't meet the durability requirements for backup files that must be kept indefinitely - if the AZ fails, the data could be lost. Glacier Deep Archive provides better durability at lower cost.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team \nnotices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions \narchitect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-\ndepth analysis to identify the root cause of the vertical scaling. \nHow should the solutions architect generate the information with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option B is correct:**\nAWS Cost Explorer provides granular filtering capabilities that allow you to filter costs by service, instance type, time period, and other dimensions. You can create custom graphs comparing costs across different time periods (like the last 2 months) and filter specifically by EC2 instance types to identify which instances were scaled vertically and when. Cost Explorer can show cost trends over time, making it easy to identify when instance types changed and correlate that with cost increases. The tool requires minimal operational overhead - it's a managed service with a web interface that provides immediate insights without requiring report generation or data processing.\n\n**Why option A is incorrect:**\nAWS Budgets is designed for setting cost and usage budgets and receiving alerts when thresholds are exceeded. While Budgets can track costs by instance type, it's primarily focused on budget management and alerts, not on detailed cost analysis and comparison. Budgets doesn't provide the same level of granular filtering and graph creation capabilities that Cost Explorer offers. For performing in-depth analysis and creating comparison graphs, Cost Explorer is more suitable.\n\n**Why option C is incorrect:**\nThe AWS Billing and Cost Management dashboard provides high-level cost overviews and summaries, but it doesn't offer the same granular filtering and detailed analysis capabilities as Cost Explorer. The dashboard shows aggregate costs but may not provide the level of detail needed to identify specific instance type changes and perform root cause analysis. Cost Explorer's filtering features are specifically designed for this type of detailed cost investigation.\n\n**Why option D is incorrect:**\nAWS Cost and Usage Reports generate detailed CSV files that are delivered to S3, but these require additional processing and analysis outside of AWS. You would need to download the reports, import them into a spreadsheet or analysis tool, and manually create graphs and comparisons. This approach has significant operational overhead compared to using Cost Explorer, which provides built-in visualization and analysis capabilities without requiring additional tools or manual data processing.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company is designing an application. The application uses an AWS Lambda function to receive \ninformation through Amazon API Gateway and to store the information in an Amazon Aurora \nPostgreSQL database. \nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly \nto handle the high volumes of data that the company needs to load into the database. A solutions \narchitect must recommend a new design to improve scalability and minimize the configuration \neffort. \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n93",
    "options": [
      {
        "id": 0,
        "text": "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the platform from Aurora to Amazon DynamoDB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up two Lambda functions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up two Lambda functions. Configure one function to receive the information.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option D is correct:**\nSetting up two Lambda functions with SQS decouples the API Gateway from the database operations, improving scalability and reducing the need for Lambda quota increases. The first Lambda function receives information from API Gateway and places it in an SQS queue, then returns immediately. The second Lambda function processes messages from the queue and writes to Aurora PostgreSQL at its own pace. This architecture allows the API Gateway-facing Lambda to handle high volumes without being bottlenecked by database write operations. SQS provides buffering and automatic retry, and the second Lambda can scale independently based on queue depth. This decoupled design improves scalability with minimal configuration effort.\n\n**Why option A is incorrect:**\nRefactoring from Lambda to Apache Tomcat on EC2 requires significant code changes, infrastructure management, and configuration effort. You would need to provision and manage EC2 instances, configure Auto Scaling, handle patching and security updates, and manage the application server. This approach increases operational overhead significantly and doesn't minimize configuration effort. Additionally, EC2-based solutions don't automatically scale like Lambda and may not handle traffic spikes as effectively.\n\n**Why option B is incorrect:**\nChanging from Aurora PostgreSQL to DynamoDB requires significant application changes, as DynamoDB is a NoSQL database with a completely different data model and API compared to PostgreSQL. This would require rewriting database queries, changing the data model, and potentially losing relational database features. The migration effort would be substantial, and DynamoDB may not be suitable if the application relies on SQL features or complex queries. This doesn't minimize configuration effort.\n\n**Why option C is incorrect:**\nSimply setting up two Lambda functions without decoupling doesn't solve the scalability issue. If both functions are directly connected (one calling the other synchronously), you still have the same bottleneck problem. The key is using SQS to decouple the functions, allowing asynchronous processing and independent scaling. Without SQS, the second function would still need to process requests synchronously, maintaining the same scalability constraints.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do \nnot have unauthorized configuration changes. \n \nWhat should a solutions architect do to accomplish this goal?",
    "options": [
      {
        "id": 0,
        "text": "Turn on AWS Config with the appropriate rules.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Turn on AWS Trusted Advisor with the appropriate checks.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on Amazon Inspector with the appropriate assessment template.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon S3 server access logging.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nAWS Config continuously monitors and records configuration changes to AWS resources, including S3 buckets. When you enable AWS Config with appropriate rules (like checking for public access, encryption settings, or bucket policies), it can detect unauthorized configuration changes and send notifications. Config maintains a complete configuration history, allowing you to see what changed, when it changed, and who made the change. Config rules can automatically evaluate configurations against desired settings and alert you to non-compliant changes, helping ensure S3 buckets don't have unauthorized configuration modifications.\n\n**Why option B is incorrect:**\nAWS Trusted Advisor provides best practice recommendations and checks for cost optimization, security, fault tolerance, and performance, but it doesn't monitor configuration changes in real-time. Trusted Advisor runs checks periodically and provides recommendations, but it doesn't track configuration history or send alerts when unauthorized changes occur. Trusted Advisor is more suited for identifying optimization opportunities rather than monitoring for unauthorized configuration changes.\n\n**Why option C is incorrect:**\nAmazon Inspector is a security assessment service that analyzes EC2 instances and container images for vulnerabilities and deviations from security best practices. It doesn't monitor S3 bucket configurations or track configuration changes. Inspector focuses on runtime security assessments of compute resources, not on monitoring resource configurations or detecting unauthorized changes to S3 bucket settings.\n\n**Why option D is incorrect:**\nAmazon S3 server access logging records access requests to S3 buckets (who accessed what objects and when), but it doesn't track configuration changes to the buckets themselves. Server access logging is for auditing object access, not for monitoring bucket configuration changes like bucket policies, encryption settings, or public access settings. To monitor configuration changes, you need AWS Config, not server access logging.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 44,
    "text": "A company is launching a new application and will display application metrics on an Amazon \nCloudWatch dashboard. The company's product manager needs to access this dashboard \nperiodically. The product manager does not have an AWS account. A solution architect must \nprovide access to the product manager by following the principle of least privilege. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Share the dashboard from the CloudWatch console.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM user specifically for the product manager.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM user for the company's employees.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a bastion server in a public subnet.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nCloudWatch dashboards can be shared with specific email addresses without requiring the recipients to have AWS accounts. When you share a dashboard, you provide email addresses, and AWS sends invitation links. Each recipient creates their own password to access the dashboard, following the principle of least privilege by granting access only to view the specific dashboard, not the entire AWS account. This approach requires minimal operational overhead - just sharing the dashboard with the product manager's email address. The shared dashboard is read-only, so the product manager can view metrics but cannot make changes.\n\n**Why option B is incorrect:**\nCreating an IAM user for the product manager grants them AWS account access, which violates the principle of least privilege. An IAM user would have permissions beyond just viewing the CloudWatch dashboard - they could potentially access other AWS services depending on the policies attached. Additionally, creating IAM users adds operational overhead for user management, password policies, and access key rotation. Sharing a dashboard is simpler and more secure, as it only grants access to view that specific dashboard.\n\n**Why option C is incorrect:**\nCreating an IAM user for all company employees is overly broad and doesn't follow the principle of least privilege. This would grant AWS account access to multiple people when only the product manager needs dashboard access. Additionally, this approach doesn't address the requirement that the product manager doesn't have an AWS account - you would still need to create an account for them. Sharing a dashboard is more appropriate for granting limited, specific access.\n\n**Why option D is incorrect:**\nDeploying a bastion server in a public subnet is designed for secure remote access to private resources, not for sharing CloudWatch dashboards. A bastion server would require managing EC2 instances, security groups, and network configuration, adding significant operational overhead. This approach doesn't provide a way for the product manager to access the CloudWatch dashboard and is completely unrelated to the requirement. Dashboard sharing is the appropriate solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company is migrating applications to AWS. The applications are deployed in different accounts. \nThe company manages the accounts centrally by using AWS Organizations. The company's \nsecurity team needs a single sign-on (SSO) solution across all the company's accounts.  \nThe company must continue managing the users and groups in its on-premises self-managed \nMicrosoft Active Directory. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Directory Service.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an identity provider (IdP) on premises.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option B is correct:**\nAWS SSO (now AWS IAM Identity Center) can be integrated with on-premises Active Directory through AWS Managed Microsoft AD. You enable AWS SSO, then configure it to use AWS Managed Microsoft AD, which can establish a trust relationship with your on-premises Active Directory. This allows users and groups managed in on-premises AD to authenticate to AWS SSO and access AWS accounts managed by AWS Organizations. Users continue to be managed in on-premises AD, and AWS SSO provides single sign-on access across all AWS accounts in the organization. This solution maintains centralized user management in on-premises AD while providing SSO across AWS accounts.\n\n**Why option A is incorrect:**\nThis option appears identical to option B, but there may be a subtle difference in the configuration approach. However, AWS SSO integrated with AWS Managed Microsoft AD (which trusts on-premises AD) is the correct solution for providing SSO across AWS accounts while maintaining user management in on-premises Active Directory. The key is establishing the trust relationship between AWS Managed Microsoft AD and on-premises AD.\n\n**Why option C is incorrect:**\nAWS Directory Service alone doesn't provide SSO across multiple AWS accounts. While AWS Managed Microsoft AD can be used for authentication, you need AWS SSO (IAM Identity Center) to provide single sign-on access across accounts in an AWS Organizations setup. Directory Service provides the directory infrastructure, but SSO provides the cross-account access management. Both are needed, but SSO is the component that enables access across accounts.\n\n**Why option D is incorrect:**\nDeploying an identity provider (IdP) on-premises would require significant infrastructure management and doesn't integrate as seamlessly with AWS SSO and AWS Organizations. While you could set up SAML federation with an on-premises IdP, AWS SSO with AWS Managed Microsoft AD provides a more integrated solution that works natively with AWS Organizations. The managed approach reduces operational overhead compared to deploying and managing your own IdP infrastructure.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. \nThe service consists of Amazon EC2 instances that run in an Auto Scaling group. The company \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n95 \nhas deployments across multiple AWS Regions. \n \nThe company needs to route users to the Region with the lowest latency. The company also \nneeds automated failover between Regions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a Network Load Balancer (NLB) and an associated target group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an Application Load Balancer (ALB) and an associated target group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a Network Load Balancer (NLB) and an associated target group.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Application Load Balancer (ALB) and an associated target group.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option A is correct:**\nAWS Global Accelerator with Network Load Balancers provides intelligent traffic routing to the AWS Region with the lowest latency based on real-time network conditions. Global Accelerator uses the AWS global network to route traffic to the optimal endpoint, automatically routing users to the closest healthy region. It provides automated failover between regions - if one region becomes unhealthy, traffic automatically routes to the next best region. Global Accelerator supports both TCP and UDP protocols, making it suitable for VoIP services that use UDP. The solution provides static IP addresses and deterministic routing, which is important for UDP-based applications.\n\n**Why option B is incorrect:**\nApplication Load Balancers operate at Layer 7 (HTTP/HTTPS) and are designed for HTTP-based applications. ALBs don't support UDP protocol, which is required for VoIP services. ALBs also don't provide cross-region routing or automated failover between regions - they operate within a single region. For UDP-based VoIP services that need cross-region routing and failover, you need Global Accelerator with Network Load Balancers.\n\n**Why option C is incorrect:**\nThis option appears identical to option A (NLB), but there may be a subtle difference. However, a Network Load Balancer alone doesn't provide cross-region routing or automated failover between regions - NLBs operate within a single region. To route users to the region with lowest latency and provide automated failover between regions, you need Global Accelerator in front of the NLBs. Global Accelerator is the component that provides the intelligent routing and cross-region failover.\n\n**Why option D is incorrect:**\nThis option appears identical to option B (ALB). As explained above, ALBs don't support UDP protocol and don't provide cross-region routing. For UDP-based VoIP services requiring cross-region routing and automated failover, Global Accelerator with Network Load Balancers is the correct solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS \nfor MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a \nmonth and is the only process that uses the database. The team wants to reduce the cost of \nrunning the tests without reducing the compute and memory attributes of the DB instance. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Stop the DB instance when tests are completed.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Auto Scaling policy with the DB instance to automatically scale when tests are",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a snapshot when tests are completed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the DB instance to a low-capacity instance when tests are completed.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nIt's a DB instance, not an EC2 instance. If the DB instance is stopped, you are still paying for the storage.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 48,
    "text": "A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n96 \nAmazon RDS DB instances and Amazon Redshift clusters are configured with tags. The \ncompany wants to minimize the effort of configuring and operating this check. \n \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Config rules to define and detect resources that are not properly tagged.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Cost Explorer to display resources that are not properly tagged.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write API calls to check all resources for proper tag allocation.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write API calls to check all resources for proper tag allocation.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nhttps://docs.aws.amazon.com/config/latest/developerguide/tagging.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A development team needs to host a website that will be accessed by other teams. The website \ncontents consist of HTML, CSS, client-side JavaScript, and images. \nWhich method is the MOST cost-effective for hosting the website?",
    "options": [
      {
        "id": 0,
        "text": "Containerize the website and host it in AWS Fargate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket and host the website there",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy a web server on an Amazon EC2 instance to host the website.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Application Loa d Balancer with an AWS Lambda target that uses the Express js",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nIn Static Websites, Web pages are returned by the server which are prebuilt. They use simple languages such as HTML, CSS, or JavaScript. There is no processing of content on the server (according to the user) in Static Websites. Web pages are returned by the server with no change therefore, static Websites are fast. There is no interaction with databases. Also, they are less costly as the host does not need to support server-side processing with different languages. ============ In Dynamic Websites, Web pages are returned by the server which are processed during runtime means they are not prebuilt web pages but they are built during runtime according to the user's demand. These use server-side scripting languages such as PHP, Node.js, ASP.NET and many more supported by the server. So, they are slower than static websites but updates and interaction with databases are possible.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 50,
    "text": "A company runs an online marketplace web application on AWS. The application serves \nhundreds of thousands of users during peak hours. The company needs a scalable, near-real-\ntime solution to share the details of millions of financial transactions with several other internal \napplications Transactions also need to be processed to remove sensitive data before being \nstored in a document database for low-latency retrieval. \n \nWhat should a solutions architect recommend to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n97",
    "options": [
      {
        "id": 0,
        "text": "Store the transactions data into Amazon DynamoDB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Stream the transactions data into Amazon Kinesis Data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Stream the transactions data into Amazon Kinesis Data Streams.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the batched transactions data in Amazon S3 as files.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThe destination of your Kinesis Data Firehose delivery stream. Kinesis Data Firehose can send data records to various destinations, including Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, and any HTTP endpoint that is owned by you or any of your third-party service providers. The following are the supported destinations: * Amazon OpenSearch Service * Amazon S3 * Datadog * Dynatrace * Honeycomb * HTTP Endpoint * Logic Monitor * MongoDB Cloud * New Relic * Splunk * Sumo Logic https://docs.aws.amazon.com/firehose/latest/dev/create-name.html https://aws.amazon.com/kinesis/data-streams/ Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and \nsecurity, the company must track configuration changes on its AWS resources and record a \nhistory of API calls made to these resources. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCloudTrail - Track user activity and API call history. Config - Assess, audits, and evaluates the configuration and relationships of tag resources.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company is preparing to launch a public-facing web application in the AWS Cloud. The \narchitecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer \n(ELB). A third-party service is used for the DNS. The company's solutions architect must \nrecommend a solution to detect and protect against large-scale DDoS attacks. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable Amazon GuardDuty on the account.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon Inspector on the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable AWS Shield and assign Amazon Route 53 to it.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Shield Advanced and assign the ELB to it.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nAWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global Accelerator standard accelerators. https://aws.amazon.com/shield/faqs/ https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/elastic-load- balancing-bp6.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company is building an application in the AWS Cloud. The application will store data in Amazon \nS3 buckets in two AWS Regions. The company must use an AWS Key Management Service \n(AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data \nin both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the \nkey must be stored in each of the two Regions. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket in each Region.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a customer managed multi-Region KMS key.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a customer managed KMS key and an S3 bucket in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a customer managed KMS key and an S3 bucket in each Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nKMS Multi-region keys are required. https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data \nsources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 \ninstance is rebooted, the data in-flight is lost. \n \nThe company's data science team wants to query ingested data near-real time. \n \nWhich solution provides near-real-time data querying that is scalable with minimal data loss?",
    "options": [
      {
        "id": 0,
        "text": "Publish data to Amazon Kinesis Data Streams.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store ingested data in an EC2 instance store.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nKinesis data streams consists of shards. The more througput is needed, the more shards you add, the less throughput, the more shards you remove, so it's scalable. Each shard can handle up to 1MB/s of writes. However Kinesis data streams stores ingested data for only 1 to 7 days so there is a chance of data loss. Additionally, Kinesis data analytics and kinesis data streams are both for real-time ingestion and analytics. Firehouse on the other hand is also scalable and processes data in near real time as per the requirement. It also transfers data into Redshift which is a data warehouse so data won't be lost. Redshift also has a SQL interface for performing queries for data analytics.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company is developing a mobile game that streams score updates to a backend processor and \nthen posts results on a leaderboard. \nA solutions architect needs to design a solution that can handle large traffic spikes, process the \nmobile game updates in order of receipt, and store the processed updates in a highly available \ndatabase. The company also wants to minimize the management overhead required to maintain \nthe solution. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Push score updates to Amazon Kinesis Data Streams.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Push score updates to Amazon Kinesis Data Streams.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nKeywords to focus on would be highly available database - DynamoDB would be a better choice for leaderboard.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "An ecommerce website is deploying its web application as Amazon Elastic Container Service \n(Amazon ECS) container instance behind an Application Load Balancer (ALB). During periods of \nhigh activity, the website slows down and availability is reduced. A solutions architect uses \nAmazon CloudWatch alarms to receive notifications whenever there is an availability issues so \nthey can scale out resource Company management wants a solution that automatically responds \nto such events. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when there are timeouts on the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when the ALB CPU utilization is too",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when the service's CPU utilization is too",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when the ALB target group CPU",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nMatch deployed capacity to the incoming application load, using scaling policies for both the ECS service and the Auto Scaling group in which the ECS cluster runs. Scaling up cluster instances and service tasks when needed and safely scaling them down when demand subsides, keeps you out of the capacity guessing game. This provides you high availability with lowered costs in the long run. https://aws.amazon.com/blogs/compute/automatic-scaling-with-amazon-ecs/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 57,
    "text": "A company has no existing file share services. A new project requires access to file storage that \nis mountable as a drive for on-premises desktops. The file server must authenticate users to an \nActive Directory domain before they are able to access the storage. \nWhich service will allow Active Directory users to mount storage as a drive on their desktops? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n101",
    "options": [
      {
        "id": 0,
        "text": "AWS S3 Glacier",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS DataSync",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball Edge",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nBefore you create an SMB file share, make sure that you configure SMB security settings for your file gateway. You also configure either Microsoft Active Directory (AD) or guest access for authentication. https://docs.aws.amazon.com/storagegateway/latest/userguide/CreatingAnSMBFileShare.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "Management has decided to deploy all AWS VPCs with IPv6 enabled. After sometime, a \nsolutions architect tries to launch a new instance and receives an error stating that there is no \nenough IP address space available in the subnet. \n \nWhat should the solutions architect do to fix this?",
    "options": [
      {
        "id": 0,
        "text": "Check to make sure that only IPv6 was used during the VPC creation",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IPv4 subnet with a larger range, and then launch the instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new IPv6-only subnet with a larger range, and then launch the instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Disable the IPv4 subnet and migrate all instances to IPv6 only.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nhttps://cloudonaut.io/getting-started-with-ipv6-on-aws/ First of all, there is no IPv6-only VPC on AWS. A VPC is always IPv4 enabled, but you can optionally enable IPv6 (dual-stack).\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company operates an ecommerce website on Amazon EC2 instances behind an Application \nLoad Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues \nrelated to a high request rate from illegitimate external systems with changing IP addresses. The \nsecurity team is worried about potential DDoS attacks against the website. The company must \nblock the illegitimate incoming requests in a way that has a minimal impact on legitimate users. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon Inspector and associate it with the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy rules to the network ACLs associated with the ALB to block the incoming traffic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nRate limit For a rate-based rule, enter the maximum number of requests to allow in any five-minute period from an IP address that matches the rule's conditions. The rate limit must be at least 100. You can specify a rate limit alone, or a rate limit and conditions. If you specify only a rate limit, Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A media company is evaluating the possibility of moving its systems to the AWS Cloud. The \ncompany needs at least 10 TB of storage with the maximum possible I/O performance for video \nprocessing, 300 TB of very durable storage for storing media content, and 900 TB of storage to \nmeet requirements for archival media that is not in use anymore. \nWhich set of services should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EBS for maximum performance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon EBS for maximum performance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon EC2 instance store for maximum performance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 Instance store for maximum performance.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nMax instance store possible at this time is 30TB for NVMe which has the higher I/O compared to EBS. is4gen.8xlarge 4 x 7,500 GB (30 TB) NVMe SSD https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store- volumes\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A company wants to run applications in containers in the AWS Cloud. These applications are \nstateless and can tolerate disruptions within the underlying infrastructure. The company needs a \nsolution that minimizes cost and operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nRunning your Kubernetes and containerized workloads on Amazon EC2 Spot Instances is a great way to save costs. ... AWS makes it easy to run Kubernetes with Amazon Elastic Kubernetes Service (EKS) a managed Kubernetes service to run production-grade workloads on AWS. To cost optimize these workloads, run them on Spot Instances. https://aws.amazon.com/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/ Get Latest & Actual SAA-C03 Exam's\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A company is running a multi-tier web application on premises. The web application is \ncontainerized and runs on a number of Linux hosts connected to a PostgreSQL database that \ncontains user records. The operational overhead of maintaining the infrastructure and capacity \nplanning is limiting the company's growth. A solutions architect must improve the application's \ninfrastructure. \n \nWhich combination of actions should the solutions architect take to accomplish this? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Migrate the PostgreSQL database to Amazon Aurora",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the web application to be hosted on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an Amazon CloudFront distribution for the web application content.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon ElastiCache between the web application and the PostgreSQL database.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nA - Aurora supports PostgreSQL. E - The existing WebApp already run in containers On-Prem and is logical to migrate to a cloud container svc like serverless Fargate on ECS.\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "An application runs on Amazon EC2 instances across multiple Availability Zones. The instances \nrun in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application \nperforms best when the CPU utilization of the EC2 instances is at or near 40%.  \nWhat should a solutions architect do to maintain the desired performance across all instances in \nthe group?",
    "options": [
      {
        "id": 0,
        "text": "Use a simple scaling policy to dynamically scale the Auto Scaling group",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a target tracking policy to dynamically scale the Auto Scaling group",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an AWS Lambda function to update the desired Auto Scaling group capacity.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use scheduled scaling actions to scale up and scale down the Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nWith a target tracking scaling policy, you can increase or decrease the current capacity of the group based on a target value for a specific metric. This policy will help resolve the over- provisioning of your resources. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern. https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target- tracking.html\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 64,
    "text": "A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. \nThe company wants to serve all the files through an Amazon CloudFront distribution. The \ncompany does not want the files to be accessible through direct navigation to the S3 URL. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n104 \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Write individual policies for each S3 bucket to grant read permission for only CloudFront access.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM user. Grant the user read permission to objects in the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nCreate a CloudFront origin access identity (OAI) https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/\n\n**Why other options are incorrect:**\nThe other options do not meet the requirements specified in the scenario.",
    "domain": "Design Secure Architectures"
  }
]