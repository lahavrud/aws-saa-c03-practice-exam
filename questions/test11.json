[
  {
    "id": 0,
    "text": "A company wants to run its critical applications in containers to meet requirements tor scalability \nand availability The company prefers to focus on maintenance of the critical applications. The \ncompany does not want to be responsible for provisioning and managing the underlying \ninfrastructure that runs the containerized workload. \nWhat should a solutions architect do to meet those requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon EC2 Instances, and Install Docker on the Instances",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution directly addresses the requirement of minimizing infrastructure management. AWS Fargate is a serverless compute engine for containers that works with Amazon ECS and Amazon EKS. With Fargate, you don't need to provision, configure, or manage servers. AWS manages the underlying infrastructure, allowing the company to focus solely on deploying and managing their containerized applications. This also inherently provides scalability and availability as Fargate automatically scales resources based on application needs.\n\n**Why option 0 is incorrect:**\nThis approach requires the company to manage the EC2 instances, including patching, scaling, and ensuring high availability. This contradicts the requirement of minimizing infrastructure management. While Docker provides containerization, it doesn't abstract away the underlying infrastructure management.\n\n**Why option 1 is incorrect:**\nWhile Amazon ECS simplifies container orchestration compared to managing Docker directly on EC2, using Amazon EC2 worker nodes still requires the company to manage the EC2 instances. This includes tasks like patching, scaling the EC2 instances, and ensuring their availability. This contradicts the requirement of minimizing infrastructure management. The company wants to avoid managing the underlying infrastructure, which EC2 instances necessitate.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 1,
    "text": "A company hosts more than 300 global websites and applications. The company requires a \nplatform to analyze more than 30 TB of clickstream data each day. What should a solutions \narchitect do to transmit and process the clickstream data?",
    "options": [
      {
        "id": 0,
        "text": "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Cache the data to Amazon CloudFron.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Collect the data from Amazon Kinesis Data Streams.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by providing a scalable and managed service for real-time data streaming. Amazon Kinesis Data Streams is designed to handle high-velocity, high-volume data streams, making it suitable for ingesting clickstream data from numerous sources. It allows for real-time processing and analysis of the data as it arrives, which is crucial for clickstream analysis. It also integrates well with other AWS services for further processing and storage.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because AWS Data Pipeline is primarily designed for batch processing and moving data between different AWS services. While it can archive data to S3, it's not the optimal solution for real-time or near real-time ingestion and processing of a high-volume, high-velocity data stream like clickstream data. It also doesn't address the initial data collection from the websites and applications.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because managing an Auto Scaling group of EC2 instances for data processing would require significant operational overhead, including managing scaling, patching, and fault tolerance. It also doesn't address the initial data collection from the websites and applications. Furthermore, it's less cost-effective and less scalable than using a managed service like Kinesis Data Streams for this purpose.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because Amazon CloudFront is a content delivery network (CDN) primarily used for caching and distributing static and dynamic web content to reduce latency and improve website performance. It's not designed for collecting or processing clickstream data. While CloudFront logs can provide some clickstream information, they are not the primary source for detailed clickstream analysis and do not address the 30TB/day data volume.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 2,
    "text": "A company is running a multi-tier ecommerce web application in the AWS Cloud. \nThe web application is running on Amazon EC2 instances. \nThe database tier Is on a provisioned Amazon Aurora MySQL DB cluster with a writer and a \nreader in a Multi-AZ environment. \nThe new requirement for the database tier is to serve the application to achieve continuous write \navailability through an Instance failover. \nWhat should a solutions architect do to meet this new requirement?",
    "options": [
      {
        "id": 0,
        "text": "Add a new AWS Region to the DB cluster for multiple writes",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add a new reader In the same Availability Zone as the writer.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the database tier to an Aurora multi-master cluster.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Migrate the database tier to an Aurora DB cluster with parallel query enabled.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by enabling multiple writer instances. Aurora Multi-Master allows for multiple instances to accept write operations concurrently. In the event of a failure of one writer instance, the other writer instances can continue to process write requests, ensuring continuous write availability. This eliminates the downtime associated with a traditional failover process where a reader is promoted to a writer.\n\n**Why option 0 is incorrect:**\nAdding a new AWS Region does not directly address the need for continuous write availability during an instance failover. While cross-region replication can provide disaster recovery capabilities, it doesn't eliminate the downtime associated with failing over to the secondary region. Furthermore, it introduces significant complexity and latency for write operations that need to be replicated across regions. This option is more suitable for disaster recovery and not for immediate failover for write operations.\n\n**Why option 1 is incorrect:**\nAdding a new reader in the same Availability Zone as the writer does not improve write availability. Readers are read-only replicas and cannot accept write operations. In the event of a writer instance failure, the reader instance would still need to be promoted to a writer, which involves a failover process and a period of downtime. This option only improves read scalability and availability, not write availability.\n\n**Why option 3 is incorrect:**\nEnabling parallel query in Aurora is designed to speed up complex analytical queries by distributing the workload across multiple nodes. It does not provide continuous write availability or address the requirement of maintaining write operations during an instance failover. Parallel query is a performance optimization feature for read operations, not a solution for high write availability.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 3,
    "text": "A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 \nhour before the desired Amazon EC2 capacity is reached. The peak capacity is the â€˜same every \nnight and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-\neffective solution that will allow for the desired EC2 capacity to be reached quickly and allow the \nAuto Scaling group to scale down after the batch jobs are complete. \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Increase the minimum capacity for the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Increase the maximum capacity for the Auto Scaling group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure scheduled scaling to scale up to the desired compute level.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Change the scaling policy to add more EC2 instances during each scaling operation.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution directly addresses the problem by leveraging the predictable nature of the workload. Scheduled scaling allows you to configure the Auto Scaling group to scale up to the desired compute level at 1 AM every night. This eliminates the gradual scaling process and ensures that the required capacity is available immediately when the batch job starts. After the batch job completes, the Auto Scaling group can scale down based on its scaling policies, optimizing costs. This is more cost-effective than maintaining a higher minimum capacity.\n\n**Why option 0 is incorrect:**\nIncreasing the minimum capacity for the Auto Scaling group would ensure that the desired capacity is always available, but it would also incur unnecessary costs during the hours when the batch job is not running. This is not a cost-effective solution as it maintains a higher baseline capacity than required.\n\n**Why option 1 is incorrect:**\nIncreasing the maximum capacity for the Auto Scaling group only defines the upper limit of instances that can be launched. It doesn't address the slow scaling issue. The Auto Scaling group will still take time to reach the desired capacity, even if the maximum capacity is increased. It doesn't guarantee the desired capacity will be available at 1 AM.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 4,
    "text": "A company runs an application in the AWS Cloud and uses Amazon DynamoDB as the database. \nThe company deploys Amazon EC2 instances to a private network to process data from the \ndatabase. \nThe company uses two NAT instances to provide connectivity to DynamoDB. \nThe company wants to retire the NAT instances. \nA solutions architect must implement a solution that provides connectivity to DynamoDB and that \ndoes not require ongoing management. \nWhat is the MOST cost-effective solution that meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create a gateway VPC endpoint to provide connectivity to DynamoDB",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure a managed NAT gateway to provide connectivity to DynamoDB",
        "correct": false
      },
      {
        "id": 2,
        "text": "Establish an AWS Direct Connect connection between the private network and DynamoDB",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS PrivateLink endpoint service between the private network and DynamoDB",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because a gateway VPC endpoint for DynamoDB provides connectivity to DynamoDB without routing traffic through the internet or using NAT gateways. It is a highly available, scalable, and cost-effective solution. It also eliminates the need for ongoing management of NAT instances, directly addressing the problem statement. Gateway endpoints are free to use; you only pay for the DynamoDB usage itself. This makes it the most cost-effective option compared to other solutions that involve data transfer charges or more complex infrastructure.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while a managed NAT gateway would provide connectivity to DynamoDB and reduce management overhead compared to NAT instances, it incurs data processing and hourly charges. Therefore, it is not the *most* cost-effective solution compared to a gateway VPC endpoint, which is free to use.\n\n**Why option 2 is incorrect:**\nThis is incorrect because establishing an AWS Direct Connect connection is significantly more expensive than other options. Direct Connect is generally used for hybrid cloud scenarios requiring high bandwidth and low latency to on-premises resources, not just for connecting to DynamoDB. It also involves significant setup and recurring costs, making it unsuitable for this scenario.\n\n**Why option 3 is incorrect:**\nThis is incorrect because AWS PrivateLink is designed for providing private connectivity to services hosted by other AWS accounts or third parties. While it can provide connectivity to DynamoDB, it's more complex and expensive than a gateway VPC endpoint for this specific use case. PrivateLink involves creating Network Load Balancers and requires more configuration and management overhead, making it less cost-effective and less simple than a gateway endpoint.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 5,
    "text": "A company has an on-premises business application that generates hundreds of files each day. \nThese files are stored on an SMB file share and require a low-latency connection to the \napplication servers. \nA new company policy states all application-generated files must be copied to AWS. \nThere is already a VPN connection to AWS. \nThe application development team does not have time to make the necessary code modifications \nto move the application to AWS. \nWhich service should a solutions architect recommend to allow the application to copy files to \nAWS?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Elastic File System (Amazon EFS)",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon FSx for Windows File Server",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement of copying files to AWS without code modifications by using AWS Storage Gateway in File Gateway mode. File Gateway provides a local cache for frequently accessed files, ensuring low latency for the application. It supports the SMB protocol, allowing the application to continue writing files to the local file share provided by the gateway. The gateway then asynchronously uploads the files to Amazon S3 in AWS. This approach minimizes changes to the application while fulfilling the requirement of copying files to AWS.\n\n**Why option 0 is incorrect:**\nWhile Amazon EFS is a fully managed NFS file system suitable for Linux-based workloads in AWS, it doesn't directly address the need to integrate with an existing on-premises SMB file share without application modifications. The application would need to be reconfigured to use NFS instead of SMB, which violates the requirement of minimal changes.\n\n**Why option 1 is incorrect:**\nAmazon FSx for Windows File Server provides a fully managed Windows file server in AWS. While it supports SMB, it doesn't directly address the need to integrate with an existing on-premises SMB file share without application modifications. Migrating the entire file share to FSx would likely require significant application changes and doesn't leverage the existing on-premises infrastructure effectively. Also, it doesn't provide a mechanism to easily copy files from on-premises to AWS without application changes.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 6,
    "text": "A company has an automobile sales website that stores its listings in an database on Amazon \nRDS. \nWhen an automobile is sold, the listing needs to be removed from the website and the data must \nbe sent to multiple target systems. \nWhich design should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to",
        "correct": false
      },
      {
        "id": 2,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon",
        "correct": false
      },
      {
        "id": 3,
        "text": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because it leverages a database trigger to invoke a Lambda function upon an update event (automobile sale). The Lambda function can then handle the logic of removing the listing from the website and sending the data to multiple target systems. This approach provides a scalable and flexible solution for reacting to database changes and propagating them to other systems. It also keeps the database logic clean by offloading the post-update processing to a separate service.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not specify the action that the Lambda function will perform or how it will send the data to multiple target systems. It is incomplete and does not provide a full solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because RDS event notifications are primarily designed for operational events (e.g., database instance failover, backup completion) rather than data changes. While it's technically possible to use them for data changes, it's not the intended use case and would require more complex configuration and logic compared to using a database trigger and Lambda function. Also, sending directly to SQS would require additional processing to extract the relevant data and send it to the target systems.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because RDS event notifications are primarily designed for operational events (e.g., database instance failover, backup completion) rather than data changes. While it's technically possible to use them for data changes, it's not the intended use case and would require more complex configuration and logic compared to using a database trigger and Lambda function. Also, sending directly to SNS would require additional processing to extract the relevant data and send it to the target systems. SNS is better suited for fan-out notifications, but in this case, we need to process the data before sending it to the target systems.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 7,
    "text": "A company is developing a video conversion application hosted on AWS. \nThe application will be available in two tiers: a free tier and a paid tier. \nUsers in the paid tier will have their videos converted first and then the tree tier users will have \ntheir videos converted. \nWhich solution meets these requirements and is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "One FIFO queue for the paid tier and one standard queue for the free tier",
        "correct": false
      },
      {
        "id": 1,
        "text": "A single FIFO Amazon Simple Queue Service (Amazon SQS) queue for all file types",
        "correct": false
      },
      {
        "id": 2,
        "text": "A single standard Amazon Simple Queue Service (Amazon SQS) queue for all file types",
        "correct": false
      },
      {
        "id": 3,
        "text": "Two standard Amazon Simple Queue Service (Amazon SQS) queues with one for the paid tier",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by using two separate standard SQS queues. One queue is dedicated to paid tier users, and the other is for free tier users. The application can be designed to process messages from the paid tier queue first, ensuring that paid users' videos are converted before free tier users' videos. While standard queues do not guarantee strict FIFO, the probability of paid tier messages being processed before free tier messages is high if the paid tier queue is consistently polled before the free tier queue. This approach is generally more cost-effective than using FIFO queues because standard queues offer higher throughput and lower cost per message.\n\n**Why option 0 is incorrect:**\nUsing one FIFO queue for the paid tier and one standard queue for the free tier is not the most cost-effective solution. While the FIFO queue guarantees order for paid users, it introduces unnecessary complexity and cost compared to using two standard queues. The standard queue for free tier users would not guarantee order, but order is not a requirement for the free tier. The cost of FIFO queues is higher than standard queues.\n\n**Why option 1 is incorrect:**\nUsing a single FIFO queue for all file types would guarantee the order in which messages are processed, but it doesn't allow for prioritizing paid tier users. All messages would be processed in the order they were received, regardless of the user tier. This does not meet the requirement of prioritizing paid tier users. Furthermore, FIFO queues are more expensive than standard queues, making this a less cost-effective solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 8,
    "text": "A company runs an ecommerce application on Amazon EC2 instances behind an Application \nLoad Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple \nAvailability Zones. The Auto Scaling group scales based on CPU utilization metrics. The \necommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a \nlarge EC2 instance. \n \nThe database's performance degrades quickly as application load increases. The application \nhandles more read requests than write transactions. The company wants a solution that will \nautomatically scale the database to meet the demand of unpredictable read workloads while \nmaintaining high availability. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon Redshift with a single node for leader and compute functionality.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Amazon RDS with a Single-AZ deployment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use Amazon Aurora with a Multi-AZ deployment.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use Amazon ElastiCache for Memcached with EC2 Spot Instances.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by providing automatic read scaling through Aurora Read Replicas. Aurora is compatible with MySQL and PostgreSQL, making migration relatively straightforward. A Multi-AZ deployment ensures high availability by automatically failing over to a standby instance in another Availability Zone in case of a failure. Aurora's architecture is designed for high performance and scalability, making it suitable for read-heavy workloads.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Amazon Redshift is a data warehouse service designed for analytical workloads (OLAP), not transactional workloads (OLTP) like those of an ecommerce application. While Redshift can handle large datasets, it's not optimized for the frequent, small read/write operations typical of an ecommerce database. Also, a single-node Redshift cluster does not provide high availability.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because while Amazon RDS can host MySQL, a Single-AZ deployment does not provide high availability. If the instance in the single Availability Zone fails, the application will experience downtime until the instance is recovered or a new instance is provisioned. The question explicitly requires a solution that maintains high availability.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 9,
    "text": "A company recently migrated to AWS and wants to implement a solution to protect the traffic that \nflows in and out of the production VPC. The company had an inspection server in its on-premises \ndata center. The inspection server performed specific operations such as traffic flow inspection \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n76 \nand traffic filtering. The company wants to have the same functionalities in the AWS Cloud. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution directly addresses the requirement of traffic inspection and filtering. AWS Network Firewall is a managed service that allows you to create rules to inspect and filter network traffic entering and exiting your VPC. It provides features like stateful inspection, intrusion prevention, and web filtering, making it suitable for replicating the functionalities of the on-premises inspection server.\n\n**Why option 0 is incorrect:**\nAmazon GuardDuty is a threat detection service that monitors your AWS environment for malicious activity and unauthorized behavior. While it provides security insights, it doesn't offer the traffic filtering capabilities required by the question. It primarily focuses on identifying threats, not actively blocking or filtering traffic based on custom rules.\n\n**Why option 1 is incorrect:**\nTraffic Mirroring allows you to copy network traffic from EC2 instances, network interfaces, or load balancers and send it to other destinations for inspection and analysis. While it can be used for traffic inspection, it doesn't inherently provide traffic filtering capabilities. You would need to set up a separate system to analyze the mirrored traffic and then take action to filter the original traffic, making it a more complex and less direct solution than AWS Network Firewall. Also, mirroring alone doesn't block or filter traffic; it only copies it.\n\n**Why option 3 is incorrect:**\nAWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across multiple AWS accounts and VPCs. While it can be used to manage AWS Network Firewall rules, it doesn't directly provide the traffic inspection and filtering capabilities itself. It relies on services like AWS Network Firewall to enforce the rules. Therefore, it's not the primary service for implementing traffic inspection and filtering within a single VPC.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 10,
    "text": "A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon \nRDS for PostgreSQL. The company needs a reporting solution that provides data visualization \nand includes all the data sources within the data lake. Only the company's management team \nshould have full access to all the visualizations. The rest of the company should have only limited \naccess. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an analysis in Amazon QuickSight.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an analysis in Amazon OuickSighl.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an AWS Glue table and crawler for the data in Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an AWS Glue table and crawler for the data in Amazon S3.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirements by providing a data visualization tool (QuickSight) that can connect to both Amazon S3 and Amazon RDS for PostgreSQL. QuickSight also offers robust user management and access control features, allowing the company to grant full access to the management team and limited access to the rest of the company through features like row-level security or different dashboards with varying levels of detail.\n\n**Why option 0 is incorrect:**\nThis option is essentially identical to option 1 and therefore also correct. The slight typo in the option does not change the validity of the solution.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nCreating AWS Glue tables and crawlers is a necessary step for cataloging data in S3 and making it accessible for querying, but it doesn't directly provide data visualization or access control. Glue is primarily used for data discovery and ETL, not for reporting and visualization. While Glue can prepare the data for a visualization tool, it doesn't fulfill the entire requirement of providing a reporting solution with different access levels.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 11,
    "text": "A company is implementing a new business application. The application runs on two Amazon \nEC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs \nto ensure that the EC2 instances can access the S3 bucket. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n77 \n \nWhat should the solutions architect do to meet this requirement?",
    "options": [
      {
        "id": 0,
        "text": "Create an IAM role that grants access to the S3 bucket.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM policy that grants access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM group that grants access to the S3 bucket.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an IAM user that grants access to the S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because IAM roles are the recommended way to grant permissions to EC2 instances. By creating an IAM role with the necessary permissions to access the S3 bucket and then attaching that role to the EC2 instances, the instances can securely access the bucket without needing to manage long-term credentials. The role provides temporary credentials that are automatically rotated, enhancing security.\n\n**Why option 1 is incorrect:**\nThis is incorrect because an IAM policy alone does not grant access. A policy must be attached to an IAM principal (user, group, or role) to be effective. While creating a policy is a necessary step, it's not sufficient on its own to solve the problem. The policy needs to be associated with the EC2 instances.\n\n**Why option 2 is incorrect:**\nThis is incorrect because IAM groups are designed to manage permissions for *users*, not EC2 instances. While you could create an IAM user for each EC2 instance and then add those users to a group, this is a less efficient and less secure approach than using IAM roles. It also creates unnecessary overhead in managing individual user accounts for each instance.\n\n**Why option 3 is incorrect:**\nThis is incorrect because creating an IAM user for each EC2 instance is not a best practice. It's more difficult to manage individual user credentials and rotate them securely. IAM roles are specifically designed for granting permissions to AWS services like EC2, providing temporary credentials and simplifying management.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 12,
    "text": "An application development team is designing a microservice that will convert large images to \nsmaller, compressed images. When a user uploads an image through the web interface, the \nmicroservice should store the image in an Amazon S3 bucket, process and compress the image \nwith an AWS Lambda function, and store the image in its compressed form in a different S3 \nbucket. \n \nA solutions architect needs to design a solution that uses durable, stateless components to \nprocess the images automatically. \n \nWhich combination of actions will meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the Lambda function to monitor the S3 bucket for new uploads.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      },
      {
        "id": 4,
        "text": "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because an SQS queue provides a durable and reliable mechanism for decoupling the S3 upload event from the Lambda function invocation. When an image is uploaded to S3, an event can be sent to the SQS queue. The Lambda function can then be triggered by the SQS queue, processing messages (representing image uploads) at its own pace. This decoupling ensures that if the Lambda function is temporarily unavailable or encounters an error, the upload event is not lost and can be retried later, thus providing durability. It also allows the system to handle bursts of uploads without overwhelming the Lambda function.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while the Lambda function will eventually process the messages in the SQS queue, this option alone doesn't establish the initial trigger for adding messages to the queue when an image is uploaded to S3. The Lambda function needs a trigger to start processing the queue, and this option doesn't define that trigger.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Lambda functions are not designed to continuously monitor S3 buckets. While Lambda can be triggered by S3 events, directly monitoring an S3 bucket for new uploads is not a typical or efficient use case for Lambda. S3 event notifications are the preferred method.\n\n**Why option 3 is incorrect:**\nThis is incorrect because launching an EC2 instance to monitor an SQS queue introduces unnecessary complexity and cost. Lambda functions are designed to be triggered by SQS queues directly, eliminating the need for a separate EC2 instance. This option also violates the stateless requirement.\n\n**Why option 4 is incorrect:**\nThis is incorrect because while EventBridge can monitor S3 events, it's not the best choice for ensuring durable processing in this scenario. EventBridge is suitable for routing events, but SQS provides a built-in retry mechanism and message buffering, making it more suitable for handling image processing tasks that might fail or experience temporary outages. Using EventBridge directly to trigger Lambda without SQS would mean that if the Lambda function fails, the event is lost unless additional error handling is implemented in EventBridge.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 13,
    "text": "A company has a three-tier web application that is deployed on AWS. The web servers are \ndeployed in a public subnet in a VPC. The application servers and database servers are deployed \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n78 \nin private subnets in the same VPC. The company has deployed a third-party virtual firewall \nappliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP \ninterface that can accept IP packets. \nA solutions architect needs to Integrate the web application with the appliance to inspect all traffic \nto the application before the traffic teaches the web server. \n \nWhich solution will moot these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create a Network Load Balancer the public subnet of the application's VPC to route the traffic to",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Application Load Balancer in the public subnet of the application's VPC to route the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a transit gateway in the inspection VPC.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a Gateway Load Balancer in the inspection VPC.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by deploying a Gateway Load Balancer (GWLB) in the inspection VPC. The GWLB is specifically designed to handle virtual appliances like firewalls. It provides a single entry point for traffic and distributes it across the firewall instances. The GWLB integrates seamlessly with VPC routing, allowing you to easily route traffic from the application VPC to the inspection VPC and back. This approach minimizes operational overhead because the GWLB handles the scaling and health checks of the firewall appliances, and it simplifies the routing configuration.\n\n**Why option 0 is incorrect:**\nWhile a Network Load Balancer (NLB) can distribute traffic, it's not the ideal solution for integrating with a virtual firewall appliance. NLBs operate at Layer 4 and lack the features needed to efficiently manage and route traffic through the firewall. It would require more complex routing configurations and might not provide the same level of scalability and health checking as a GWLB. Also, NLBs are typically used for distributing traffic to backend instances, not for routing traffic through a separate VPC for inspection.\n\n**Why option 1 is incorrect:**\nAn Application Load Balancer (ALB) is designed for HTTP/HTTPS traffic and operates at Layer 7. While it can distribute traffic based on content, it's not the appropriate choice for integrating with a virtual firewall appliance that needs to inspect all traffic, regardless of the protocol. ALBs are not designed to forward traffic to another VPC for inspection and then back to the web servers. Using an ALB would require significant configuration and might not be feasible for all types of traffic. Furthermore, it adds unnecessary complexity compared to a GWLB.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 14,
    "text": "A company wants to improve its ability to clone large amounts of production data into a test \nenvironment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon \nElastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the \nproduction environment. The software that accesses this data requires consistently high I/O \nperformance. \n \nA solutions architect needs to minimize the time that is required to clone the production data into \nthe test environment. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Take EBS snapshots of the production EBS volumes.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure the production EBS volumes to use the EBS Multi-Attach feature.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Take EBS snapshots of the production EBS volumes.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Take EBS snapshots of the production EBS volumes.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by creating point-in-time copies of the production EBS volumes. These snapshots can then be used to create new EBS volumes for the test environment. Since the new volumes are created from snapshots, modifications to the data in the test environment will not affect the original production volumes. EBS snapshots are an efficient way to clone data, and the resulting volumes can be provisioned with the necessary IOPS to meet the high I/O performance requirement.\n\n**Why option 0 is incorrect:**\nThis option is essentially the same as option 3, and is therefore not distinct enough to be considered a separate option. Taking EBS snapshots is the correct approach, but this option doesn't offer any unique value compared to option 3.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because EBS Multi-Attach allows multiple EC2 instances to attach to a single EBS volume. While this might be useful in some scenarios, it does not address the requirement of cloning data into a separate test environment. Using Multi-Attach would not create a separate, isolated copy of the data, and any modifications made by the test environment would directly affect the production data, violating a core requirement.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 15,
    "text": "An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will \nfeature exactly one product on sale for a period of 24 hours. The company wants to be able to \nhandle millions of requests each hour with millisecond latency during peak hours. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Use Amazon S3 to host the full website in different S3 buckets.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across",
        "correct": false
      },
      {
        "id": 2,
        "text": "Migrate the full application to run in containers.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an Amazon S3 bucket to host the website's static content.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by leveraging Amazon S3 for static content hosting. S3 is highly scalable, durable, and cost-effective for serving static assets. By offloading the static content to S3, the application servers (if any) only need to handle the dynamic parts of the website, reducing their load and operational complexity. S3 integrates seamlessly with Amazon CloudFront, a content delivery network (CDN), which can further improve latency by caching content closer to users globally. This combination provides high performance and scalability with minimal operational overhead, as S3 and CloudFront are managed services.\n\n**Why option 0 is incorrect:**\nWhile S3 can host static websites, hosting the *full* website, including dynamic content, in different S3 buckets is not a practical or scalable solution for a dynamic ecommerce site. S3 is designed for static content, and serving dynamic content directly from S3 would require complex workarounds and would not provide the necessary performance or functionality for an ecommerce application. It also increases operational overhead due to the complexity of managing the dynamic content updates.\n\n**Why option 1 is incorrect:**\nDeploying the full website on Amazon EC2 instances in Auto Scaling groups can provide scalability, but it also introduces significant operational overhead. Managing EC2 instances, including patching, scaling, and monitoring, requires more effort than using managed services like S3 and CloudFront. While Auto Scaling helps with scaling, it doesn't eliminate the operational burden of managing the underlying infrastructure. Furthermore, EC2 instances are not as cost-effective for serving static content as S3 and CloudFront.\n\n**Why option 2 is incorrect:**\nMigrating the full application to run in containers (e.g., using ECS or EKS) can provide benefits like portability and scalability, but it also increases operational complexity compared to using S3 for static content. Containerization introduces the overhead of managing container orchestration platforms, building and deploying container images, and monitoring container health. While containers can be a good solution for the dynamic parts of the application, they are not the most efficient way to serve static content, especially when S3 and CloudFront are available.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 16,
    "text": "A solutions architect is using Amazon S3 to design the storage architecture of a new digital media \napplication. The media files must be resilient to the loss of an Availability Zone Some files are \naccessed frequently while other files are rarely accessed in an unpredictable pattern. The \nsolutions architect must minimize the costs of storing and retrieving the media files. \nWhich storage option meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "S3 Standard",
        "correct": false
      },
      {
        "id": 1,
        "text": "S3 Intelligent-Tiering",
        "correct": true
      },
      {
        "id": 2,
        "text": "S3 Standard-Infrequent Access {S3 Standard-IA)",
        "correct": false
      },
      {
        "id": 3,
        "text": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis option automatically moves data between frequent, infrequent, and archive access tiers based on changing access patterns. This addresses the requirement of minimizing costs for both frequently and rarely accessed files without manual intervention. Additionally, it stores data redundantly across multiple Availability Zones, ensuring resilience to AZ loss.\n\n**Why option 0 is incorrect:**\nThis option stores data redundantly across multiple Availability Zones, providing resilience. However, it does not automatically optimize storage costs based on access patterns. All data is stored at the same cost, regardless of how frequently it is accessed, making it less cost-effective for infrequently accessed files.\n\n**Why option 2 is incorrect:**\nThis option is designed for data that is infrequently accessed but requires rapid retrieval when needed. While it provides cost savings compared to S3 Standard for infrequently accessed data, it doesn't automatically adapt to changing access patterns. If frequently accessed files are stored in S3 Standard-IA, costs will be higher than necessary. It also requires manual tiering, which is not ideal given the unpredictable access pattern.\n\n**Why option 3 is incorrect:**\nThis option stores data in a single Availability Zone, making it the least expensive option but also the least resilient. It does not meet the requirement of being resilient to the loss of an Availability Zone. While it's suitable for infrequently accessed data, the lack of redundancy makes it unsuitable for this scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 17,
    "text": "A company has an on-premises MySQL database used by the global sales team with infrequent \naccess patterns. \nThe sales team requires the database to have minimal downtime. \nA database administrator wants to migrate this database to AWS without selecting a particular \ninstance type in anticipation of more users in the future. \nWhich service should a solution architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Amazon Aurora MySQL",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon Aurora Serverless for MySQL",
        "correct": true
      },
      {
        "id": 2,
        "text": "Amazon Redshift Spectrum",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon RDS for MySQL",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis is correct because Aurora Serverless v1 for MySQL automatically scales database capacity based on application needs and shuts down when not in use. This addresses the infrequent access pattern and the need to avoid pre-selecting an instance type. It also provides high availability and minimal downtime during scaling operations. While Aurora Serverless v2 is generally preferred, the question doesn't specify features only available in v2, and v1 is still a valid option.\n\n**Why option 0 is incorrect:**\nThis is incorrect because while Amazon Aurora MySQL provides high performance and availability, it requires selecting an instance type upfront. This doesn't address the requirement of avoiding instance type selection in anticipation of future growth and infrequent access patterns. It also doesn't automatically pause when not in use, leading to higher costs during periods of inactivity.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis is incorrect because while Amazon RDS for MySQL is a managed database service, it requires selecting an instance type upfront. This doesn't address the requirement of avoiding instance type selection in anticipation of future growth and infrequent access patterns. It also doesn't automatically pause when not in use, leading to higher costs during periods of inactivity. While RDS offers various instance sizes, the question specifically mentions avoiding instance type selection.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 18,
    "text": "A company is building an application on Amazon EC2 instances that generates temporary \ntransactional data.  \nThe application requires access to data storage that can provide configurable and consistent \nIOPS. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Provision an EC2 instance with a Throughput Optimized HDD (st1) root volume and a Cold",
        "correct": false
      },
      {
        "id": 1,
        "text": "Provision an EC2 instance with a Throughput Optimized HDD (st1) volume that will serve as the",
        "correct": false
      },
      {
        "id": 2,
        "text": "Provision an EC2 instance with a General Purpose SSD (gp2) root volume and Provisioned",
        "correct": true
      },
      {
        "id": 3,
        "text": "Provision an EC2 instance with a General Purpose SSD (gp2) root volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by utilizing a Provisioned IOPS SSD (io1 or io2) volume. Provisioned IOPS volumes are specifically designed for I/O-intensive workloads that require consistent and predictable performance. By provisioning a specific number of IOPS, the application is guaranteed a certain level of performance, which is crucial for transactional data. The General Purpose SSD (gp2) root volume is suitable for the operating system and application code, while the Provisioned IOPS SSD volume handles the transactional data storage needs.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because Throughput Optimized HDD (st1) volumes are designed for frequently accessed, throughput-intensive workloads with large block sizes, such as big data, data warehouses, and log processing. They do not provide the consistent IOPS required for transactional data. Also, using it as a root volume is not optimal.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because Throughput Optimized HDD (st1) volumes are designed for frequently accessed, throughput-intensive workloads with large block sizes, such as big data, data warehouses, and log processing. They do not provide the consistent IOPS required for transactional data. While it is not a root volume, it is still not the right choice for the use case.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because General Purpose SSD (gp2) volumes provide a balance of price and performance for a wide variety of workloads. While they can handle some transactional workloads, they do not offer the ability to configure and guarantee consistent IOPS. For applications with specific IOPS requirements, Provisioned IOPS SSD volumes are a better choice.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 19,
    "text": "A company is hosting 60 TB of production-level data in an Amazon S3 bucket. A solution \narchitect needs to bring that data on premises for quarterly audit requirements. This export of \ndata must be encrypted while in transit. The company has low network bandwidth in place \nbetween AWS and its on-premises data center. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy AWS Migration Hub with 90-day replication windows for data transfer.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy an AWS Storage Gateway volume gateway on AWS.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy Amazon Elastic File System (Amazon EFS), with lifecycle policies enabled, on AWS.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an AWS Snowball device in the on-premises data center after completing an export job",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by physically shipping the data using AWS Snowball. Snowball is designed for transferring large amounts of data in and out of AWS when network bandwidth is limited. The data is encrypted in transit and at rest within the Snowball device, meeting the security requirement. Export jobs directly support transferring data from S3 to Snowball.\n\n**Why option 0 is incorrect:**\nAWS Migration Hub primarily focuses on tracking application migrations and doesn't directly facilitate large-scale data transfer. Replication windows are not a core feature of Migration Hub. Even if it could be used for data transfer, 90 days is an unnecessarily long time for a quarterly audit requirement, and it doesn't directly address the low bandwidth constraint.\n\n**Why option 1 is incorrect:**\nAWS Storage Gateway volume gateway requires continuous network connectivity to synchronize data between on-premises and AWS. Given the low network bandwidth, transferring 60 TB of data using Storage Gateway would be extremely slow and impractical. It is designed for active data use, not for a one-time data export for audit purposes. While it does offer encryption, the network constraint makes it unsuitable.\n\n**Why option 2 is incorrect:**\nAmazon EFS is a network file system designed for use with EC2 instances. It is not intended for exporting large amounts of data to on-premises locations for audit purposes. While lifecycle policies can manage data movement within EFS, they don't facilitate the required data export. Furthermore, EFS is not designed for low-bandwidth environments.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 20,
    "text": "A solutions architect is designing the cloud architecture for a company that needs to host \nhundreds of machine learning models for its users. During startup, the models need to load up to \n10 GB of data from Amazon S3 into memory, but they do not need disk access. Most of the \nmodels are used sporadically, but the users expect all of them to be highly available and \naccessible with low latency. \n \nWhich solution meets the requirements and is MOST cost-effective?",
    "options": [
      {
        "id": 0,
        "text": "Deploy models as AWS Lambda functions behind an Amazon API Gateway for each model.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind an",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy models as AWS Lambda functions behind a single Amazon API Gateway with path-",
        "correct": true
      },
      {
        "id": 3,
        "text": "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind a single",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by leveraging AWS Lambda's ability to scale to zero when the models are not in use, making it cost-effective for sporadic usage. The recent increase in Lambda's memory limit allows loading the 10GB of data into memory. Using a single API Gateway with path-based routing allows managing all models through a single entry point, simplifying management and reducing costs compared to having a separate API Gateway for each model. This approach provides high availability and low latency due to Lambda's inherent scaling capabilities and API Gateway's caching and routing features.\n\n**Why option 0 is incorrect:**\nWhile Lambda can handle the memory requirements and sporadic usage, deploying a separate API Gateway for each model would significantly increase costs and management overhead. The question specifically asks for the MOST cost-effective solution, and managing hundreds of API Gateways is not cost-effective.\n\n**Why option 1 is incorrect:**\nECS requires instances to be running even when the models are not in use, leading to higher costs compared to Lambda, which can scale to zero. While ECS can provide high availability and low latency, it is not the most cost-effective solution for sporadic usage patterns. Using a separate ECS service for each model would also increase management overhead. Furthermore, managing hundreds of ECS services and load balancers would be complex and expensive.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 21,
    "text": "A company is developing an ecommerce application that will consist of a load-balanced front end, \na container-based application, and a relational database. A solutions architect needs to create a \nhighly available solution that operates with as little manual intervention as possible. \n \nWhich solutions meet these requirements? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Create an Amazon RDS DB instance in Multi-AZ mode.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type",
        "correct": false
      },
      {
        "id": 4,
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because creating an Amazon RDS DB instance in Multi-AZ mode provides high availability by synchronously replicating data to a standby instance in a different Availability Zone. In case of a failure of the primary instance, RDS automatically fails over to the standby instance, minimizing downtime and requiring no manual intervention. This directly addresses the high availability and minimal manual intervention requirements for the database component.\n\n**Why option 1 is incorrect:**\nWhile creating an Amazon RDS DB instance with read replicas in another Availability Zone improves read performance and availability, it does not provide automatic failover for the primary database instance. Promoting a read replica to a standalone instance requires manual intervention, which violates the 'as little manual intervention as possible' requirement. Read replicas are primarily for scaling read operations, not for high availability of the primary write instance.\n\n**Why option 2 is incorrect:**\nCreating an Amazon EC2 instance-based Docker cluster requires significant manual effort for management, scaling, and patching of the EC2 instances. While it can provide high availability with proper configuration, it does not minimize manual intervention as much as a managed container service like ECS. This option also requires more operational overhead.\n\n**Why option 3 is incorrect:**\nCreating an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type is a good solution for the containerized application. Fargate removes the need to manage the underlying EC2 instances, reducing operational overhead and manual intervention. ECS with Fargate automatically handles scaling and availability of the containers, making it a highly available and managed solution. However, only one answer can be chosen for the container component.\n\n**Why option 4 is incorrect:**\nCreating an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type provides more control over the underlying infrastructure but requires more manual intervention for managing the EC2 instances. This includes patching, scaling, and ensuring high availability of the instances themselves. While it can be configured for high availability, it does not minimize manual intervention as much as using Fargate.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 22,
    "text": "A company has an ecommerce application that stores data in an on-premises SQL database. The \ncompany has decided to migrate this database to AWS. However, as part of the migration, the \ncompany wants to find a way to attain sub-millisecond responses to common read requests. \n \nA solutions architect knows that the increase in speed is paramount and that a small percentage \nof stale data returned in the database reads is acceptable. \n \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Build Amazon RDS read replicas.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Build the database as a larger instance type.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Build a database cache using Amazon ElastiCache.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Build a database cache using Amazon Elasticsearch Service (Amazon ES).",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by providing a caching layer in front of the database. Amazon ElastiCache is designed for in-memory caching, which can deliver sub-millisecond read performance. Since the company is willing to tolerate a small percentage of stale data, ElastiCache's eventual consistency model is acceptable. ElastiCache sits in front of the database and serves the most frequently accessed data directly from memory, significantly reducing the load on the database and improving read latency.\n\n**Why option 0 is incorrect:**\nWhile read replicas can improve read performance by distributing read traffic across multiple database instances, they typically do not provide sub-millisecond response times. Read replicas still involve database operations, which are slower than in-memory caching. Also, read replicas are primarily for scaling read capacity and increasing availability, not necessarily for achieving the extreme low latency required in this scenario.\n\n**Why option 1 is incorrect:**\nIncreasing the instance size might improve database performance to some extent, but it's unlikely to achieve sub-millisecond response times consistently. A larger instance will have more memory and processing power, but it still involves disk I/O and database operations, which are inherently slower than in-memory caching. This option also doesn't address the requirement of tolerating stale data, which is a key indicator that caching is the preferred solution.\n\n**Why option 3 is incorrect:**\nAmazon Elasticsearch Service (Amazon ES) is designed for search and analytics workloads, not for caching database queries. While it can provide fast search results, it's not optimized for caching structured data from a SQL database and delivering sub-millisecond responses for common read requests. ElastiCache is a more appropriate choice for caching database queries due to its in-memory nature and integration with database systems.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 23,
    "text": "A company is designing an application where users upload small files into Amazon S3.  \nAfter a user uploads a file, the file requires one-time simple processing to transform the data and \nsave the data in JSON format for later analysis. \n \nEach file must be processed as quickly as possible after it is uploaded. Demand will vary.  \nOn some days, users will upload a high number of files. On other days, users will upload a few \nfiles or no files. \n \nWhich solution meets these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EMR to read text files from Amazon S3.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service",
        "correct": true
      },
      {
        "id": 3,
        "text": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution effectively addresses the requirements by leveraging S3 event notifications to trigger a processing workflow. S3 can be configured to send a message to SQS whenever a new file is uploaded. SQS acts as a buffer, decoupling the S3 upload process from the processing logic. A separate service (e.g., AWS Lambda) can then consume messages from SQS and process the files. This approach provides scalability to handle varying demand, as SQS can queue messages during peak periods. It also minimizes operational overhead because SQS and Lambda are managed services, reducing the need for manual server management and scaling.\n\n**Why option 0 is incorrect:**\nUsing Amazon EMR for this task is overkill. EMR is designed for large-scale data processing and analytics, typically involving large datasets and complex transformations. For small files and simple transformations, EMR introduces unnecessary complexity and operational overhead. Configuring and managing an EMR cluster requires significant effort, and the startup time for EMR jobs can be substantial, which doesn't align with the requirement for immediate processing.\n\n**Why option 1 is incorrect:**\nWhile this option mentions S3 event notification and SQS, it is incomplete. It doesn't specify what consumes the messages from SQS to perform the actual file processing. SQS by itself only queues the messages; it doesn't execute any code. Therefore, this option is insufficient to meet the requirement of transforming the data and saving it in JSON format.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 24,
    "text": "An application allows users at a company's headquarters to access product data. The product \ndata is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an \napplication performance slowdown and wants to separate read traffic from write traffic. \nA solutions architect needs to optimize the application's performance quickly. \nWhat should the solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Change the existing database to a Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the existing database to a Multi-AZ deployment.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create read replicas for the database.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create read replicas for the database.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by creating read replicas, which are copies of the primary database. Read replicas allow the application to direct read traffic to these replicas, offloading the primary database and improving its performance for write operations. This separation of read and write traffic is a common strategy for optimizing database performance in read-heavy applications.\n\n**Why option 0 is incorrect:**\nChanging the existing database to a Multi-AZ deployment primarily addresses high availability and fault tolerance, not read scalability. While Multi-AZ provides a standby instance in a different Availability Zone, it doesn't automatically distribute read traffic. The standby instance is only used in case of a failover of the primary instance. Therefore, it does not directly address the performance slowdown caused by mixed read and write traffic.\n\n**Why option 1 is incorrect:**\nChanging the existing database to a Multi-AZ deployment primarily addresses high availability and fault tolerance, not read scalability. While Multi-AZ provides a standby instance in a different Availability Zone, it doesn't automatically distribute read traffic. The standby instance is only used in case of a failover of the primary instance. Therefore, it does not directly address the performance slowdown caused by mixed read and write traffic.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 25,
    "text": "An Amazon EC2 administrator created the following policy associated with an IAM group \ncontaining several users. \n \n \n \n \nWhat is the effect of this policy?",
    "options": [
      {
        "id": 0,
        "text": "Users can terminate an EC2 instance in any AWS Region except us-east-1.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Users can terminate an EC2 instance with the IP address 10 100 100 1 in the us-east-1 Region",
        "correct": false
      },
      {
        "id": 2,
        "text": "Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is",
        "correct": true
      },
      {
        "id": 3,
        "text": "Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is correct because the `Deny` effect is applied when the source IP address is *not* within the specified CIDR block (10.100.100.0/24). Therefore, users can terminate EC2 instances in the us-east-1 Region only when their source IP address is within the 10.100.100.0/24 range. If the source IP is outside this range, the `Deny` statement will prevent the termination.\n\n**Why option 0 is incorrect:**\nThis is incorrect because the policy explicitly mentions the `us-east-1` region in the `Resource` section. The `Deny` effect is conditional and only applies based on the source IP address within the us-east-1 region, not all regions except us-east-1.\n\n**Why option 1 is incorrect:**\nThis is incorrect because the policy denies termination when the source IP is *not* 10.100.100.0/24. It does not allow termination of instances with the IP address 10.100.100.1. The policy focuses on the source IP of the *user* making the request, not the IP address of the EC2 instance being terminated.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 26,
    "text": "A company has a large Microsoft SharePoint deployment running on-premises that requires \nMicrosoft Windows shared file storage. The company wants to migrate this workload to the AWS \nCloud and is considering various storage options. The storage solution must be highly available \nand integrated with Active Directory for access control. \n \nWhich solution will satisfy these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Configure Amazon EFS storage and set the Active Directory domain for authentication",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an SMB Me share on an AWS Storage Gateway tile gateway in two Availability Zones",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server file system on AWS and set the Active",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution directly addresses the requirements by providing a fully managed, native Windows file system in AWS. Amazon FSx for Windows File Server is designed for Windows-based applications and supports SMB protocol, which is essential for SharePoint. It integrates seamlessly with Active Directory for authentication and authorization, ensuring existing user access controls are maintained. FSx for Windows File Server also offers high availability options, such as multi-AZ deployments, to meet the availability requirement.\n\n**Why option 0 is incorrect:**\nAmazon EFS is a network file system designed for Linux-based workloads and does not natively support the SMB protocol required for Windows shared file storage used by SharePoint. While EFS can integrate with Active Directory for authentication, it's not the ideal solution for a Windows-centric workload like SharePoint.\n\n**Why option 1 is incorrect:**\nWhile AWS Storage Gateway can provide SMB file shares, using a file gateway in two Availability Zones does not inherently guarantee high availability. The file gateway itself is a single point of failure. Furthermore, Storage Gateway is typically used for hybrid cloud scenarios where some data resides on-premises, and it's not the most efficient or performant solution for a full migration to AWS. It also adds complexity compared to a fully managed service like FSx for Windows File Server.\n\n**Why option 2 is incorrect:**\nAmazon S3 is an object storage service, not a file system. While it's possible to mount an S3 bucket as a volume on a Windows Server using third-party tools, this approach is not a native Windows file system experience and can introduce performance and compatibility issues. It also doesn't directly integrate with Active Directory for access control in the same way as FSx for Windows File Server.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 27,
    "text": "An image-processing company has a web application that users use to upload images. The \napplication uploads the images into an Amazon S3 bucket. The company has set up S3 event \nnotifications to publish the object creation events to an Amazon Simple Queue Service (Amazon \nSQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function \nthat processes the images and sends the results to users through email. \n \nUsers report that they are receiving multiple email messages for every uploaded image. A \nsolutions architect determines that SQS messages are invoking the Lambda function more than \nonce, resulting in multiple email messages. \n \nWhat should the solutions architect do to resolve this issue with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the SQS standard queue to an SQS FIFO queue.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the Lambda function to delete each message from the SQS queue immediately after the",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct solution because it addresses the root cause of the problem: the Lambda function may be taking longer to process the message than the current visibility timeout of the SQS queue. If the visibility timeout expires before the Lambda function completes, SQS will make the message available again, potentially triggering another invocation of the Lambda function. Increasing the visibility timeout to a value greater than the maximum execution time of the Lambda function ensures that the message remains invisible to other consumers (including Lambda) until the function has finished processing it, thus preventing duplicate processing. This approach has minimal operational overhead as it only involves changing a configuration setting in SQS.\n\n**Why option 0 is incorrect:**\nSetting up long polling in SQS will reduce the number of empty responses when the queue is polled, potentially reducing costs and improving efficiency. However, it does not prevent duplicate processing of messages if the visibility timeout is too short. The Lambda function could still be invoked multiple times for the same message if the initial invocation takes longer than the visibility timeout, regardless of whether long polling is enabled. Therefore, this option does not directly address the problem of duplicate emails.\n\n**Why option 1 is incorrect:**\nChanging the SQS standard queue to an SQS FIFO queue would prevent duplicate processing, as FIFO queues guarantee exactly-once processing. However, this option has significant operational overhead. It requires changes to the application code to include message group IDs, and it might also require changes to the Lambda function's concurrency settings. Furthermore, switching from a standard queue to a FIFO queue is not always straightforward and may involve downtime or data migration. The question specifically asks for the solution with the LEAST operational overhead, making this option less desirable than option 2.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 28,
    "text": "A company is implementing a shared storage solution for a media application that is hosted in the \nAWS Cloud.  \nThe company needs the ability to use SMB clients to access data. The solution must he fully \nmanaged. \nWhich AWS solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Create an AWS Storage Gateway volume gateway.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Storage Gateway tape gateway.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an Amazon EC2 Windows instance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an Amazon FSx for Windows File Server tile system.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is the correct answer because Amazon FSx for Windows File Server provides a fully managed Windows file system in the AWS Cloud. It natively supports the SMB protocol, allowing Windows-based applications and users to access shared file storage without the need to manage underlying infrastructure or configure complex networking. It fulfills both the SMB access and fully managed requirements.\n\n**Why option 0 is incorrect:**\nThis is incorrect because AWS Storage Gateway volume gateway requires managing an EC2 instance or on-premises hardware to host the gateway. While it can provide SMB access, it is not a fully managed solution. The customer is responsible for managing the underlying infrastructure of the gateway.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Storage Gateway tape gateway is designed for virtual tape storage and backup, not for general-purpose shared file storage accessible via SMB. It's primarily used for archiving data to AWS, not for active file sharing.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 29,
    "text": "A company's containerized application runs on an Amazon EC2 instance. The application needs \nto download security certificates before it can communicate with other business applications. The \ncompany wants a highly secure solution to encrypt and decrypt the certificates in near real time. \nThe solution also needs to store data in highly available storage after the data is encrypted. \n \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create AWS Secrets Manager secrets for encrypted certificates.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an AWS Lambda function that uses the Python cryptography library to receive and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Create an AWS Key Management Service (AWS KMS) customer managed key.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by leveraging AWS KMS for encryption and decryption. AWS KMS provides a managed, highly available, and secure service for managing encryption keys. Using a customer managed key gives the company control over the key lifecycle. The EC2 instance can use the AWS SDK to call KMS to encrypt and decrypt the certificates in near real-time. The encrypted data can then be stored in a highly available storage service like Amazon S3 or EBS. This approach minimizes operational overhead because AWS manages the underlying infrastructure and security of the KMS service.\n\n**Why option 0 is incorrect:**\nWhile AWS Secrets Manager can store secrets, it's primarily designed for managing and rotating credentials, not for general-purpose encryption and decryption of large files like certificates. Using Secrets Manager for this purpose would likely involve storing the entire certificate as a secret, which might exceed size limits and could be less efficient than using KMS directly. Furthermore, while Secrets Manager provides encryption at rest, it doesn't directly facilitate the near real-time encryption/decryption required by the application. It also doesn't directly address the requirement of storing the data in highly available storage after encryption; that would need to be handled separately.\n\n**Why option 1 is incorrect:**\nWhile using a Lambda function with the Python cryptography library could achieve encryption and decryption, it introduces significant operational overhead. The company would need to manage the Lambda function, including its code, dependencies, scaling, and security. This approach also requires managing the encryption keys, which could be complex and error-prone. Furthermore, the Lambda function would need to be invoked for each encryption/decryption operation, which could add latency and complexity. It also doesn't directly address the requirement of storing the data in highly available storage after encryption; that would need to be handled separately.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 30,
    "text": "A solutions architect is designing a VPC with public and private subnets. The VPC and subnets \nuse IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three \nAvailability Zones (AZs) for high availability. An internet gateway is used to provide internet \naccess for the public subnets. The private subnets require access to the internet to allow Amazon \nEC2 instances to download software updates. \n \nWhat should the solutions architect do to enable Internet access for the private subnets?",
    "options": [
      {
        "id": 0,
        "text": "Create three NAT gateways, one for each public subnet in each AZ.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create three NAT instances, one for each private subnet in each AZ.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a second internet gateway on one of the private subnets.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an egress-only internet gateway on one of the public subnets.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the correct solution because NAT gateways allow instances in private subnets to initiate outbound traffic to the internet while preventing the internet from initiating inbound connections to those instances. Placing a NAT gateway in each public subnet ensures high availability, as instances in the corresponding private subnet can use the NAT gateway in their AZ for internet access. This approach aligns with AWS best practices for providing internet access to private subnets while maintaining security and high availability.\n\n**Why option 1 is incorrect:**\nUsing NAT instances is a valid approach, but it requires more management overhead than NAT gateways. NAT instances need to be patched, scaled, and managed for high availability. While possible, NAT gateways are the preferred and recommended solution by AWS due to their managed nature and inherent high availability when deployed correctly. The question implies a need for a highly available and managed solution, making NAT instances less suitable.\n\n**Why option 2 is incorrect:**\nInternet Gateways are designed to be attached to VPCs and provide a route for traffic to and from the internet. Attaching an Internet Gateway to a private subnet would effectively make it a public subnet, defeating the purpose of having private subnets. This would expose the instances in the private subnet directly to the internet, which is undesirable for security reasons.\n\n**Why option 3 is incorrect:**\nEgress-Only Internet Gateways are designed for IPv6 traffic only. They allow instances in a VPC to initiate outbound connections over IPv6 to the internet but prevent the internet from initiating inbound connections. The question explicitly states that the VPC and subnets use IPv4 CIDR blocks, making an Egress-Only Internet Gateway unsuitable for this scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 31,
    "text": "A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP \nserver that stores its data on an NFS-based file system. The server holds 200 GB of data that \nneeds to be transferred. The server must be hosted on an Amazon EC2 instance that uses an \nAmazon Elastic File System (Amazon EFS) file system. \nWhich combination of steps should a solutions architect take to automate this task? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Launch the EC2 instance into the same Availability Zone as the EFS file system.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Install an AWS DataSync agent in the on-premises data center.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance tor",
        "correct": false
      },
      {
        "id": 3,
        "text": "Manually use an operating system copy command to push the data to the EC2 instance.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because EFS file systems are regional resources, but access to them is provided via mount targets within Availability Zones. To ensure the EC2 instance can access the EFS file system, it must be launched into the same Availability Zone as one of the EFS mount targets. This allows the EC2 instance to mount and utilize the EFS file system for storing the SFTP server's data.\n\n**Why option 1 is incorrect:**\nThis is incorrect because while AWS DataSync is a suitable tool for migrating data, it is not needed in this scenario. The on-premises SFTP server is being migrated to an EC2 instance in AWS. DataSync is typically used for transferring data between on-premises storage and AWS storage services, or between different AWS storage services. Since the SFTP server is being migrated to AWS, the data can be directly copied to the EFS volume once the EC2 instance is running.\n\n**Why option 2 is incorrect:**\nThis is incorrect because the requirement specifies that the data should be stored on an Amazon EFS file system, not an EBS volume. Creating an additional EBS volume would not fulfill the stated requirement.\n\n**Why option 3 is incorrect:**\nThis is incorrect because manually copying the data using operating system commands is not an automated solution. The question specifically asks for steps to *automate* the migration. Manual copying is prone to errors, time-consuming, and doesn't scale well.\n\n**Why option 4 is incorrect:**\nThis is incorrect because AWS DataSync is not needed in this scenario. The on-premises SFTP server is being migrated to an EC2 instance in AWS. DataSync is typically used for transferring data between on-premises storage and AWS storage services, or between different AWS storage services. Since the SFTP server is being migrated to AWS, the data can be directly copied to the EFS volume once the EC2 instance is running.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 32,
    "text": "A company has an AWS Glue extract. transform, and load (ETL) job that runs every day at the \nsame time. The job processes XML data that is in an Amazon S3 bucket. New data is added to \nthe S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data \nduring each run. \nWhat should the solutions architect do to prevent AWS Glue from reprocessing old data?",
    "options": [
      {
        "id": 0,
        "text": "Edit the job to use job bookmarks.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Edit the job to delete data after the data is processed",
        "correct": false
      },
      {
        "id": 2,
        "text": "Edit the job by setting the NumberOfWorkers field to 1.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use a FindMatches machine learning (ML) transform.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Glue job bookmarks are designed to track the state information of a job run. By enabling job bookmarks, Glue can remember which data has already been processed in previous runs. When the job runs again, it will only process the new data that has been added to the S3 bucket since the last run. This prevents reprocessing of old data, improving efficiency and reducing processing time and costs.\n\n**Why option 1 is incorrect:**\nThis is incorrect because deleting data after processing, while preventing reprocessing, is not a practical solution. The data might be needed for historical analysis or other purposes. Deleting data would also make the ETL job destructive, rather than simply processing new data. It's generally better to preserve the data in its original location and only process the new additions.\n\n**Why option 2 is incorrect:**\nThis is incorrect because setting the NumberOfWorkers field to 1 only affects the parallelism of the job and does not prevent reprocessing of old data. Reducing the number of workers might even increase the overall processing time, as the job will be executed sequentially instead of in parallel. The number of workers is related to the compute resources allocated to the job, not the data being processed.\n\n**Why option 3 is incorrect:**\nThis is incorrect because FindMatches is a machine learning transform in AWS Glue used for de-duplication and record linkage. It's not relevant to the problem of preventing reprocessing of old data. FindMatches is used to identify records that refer to the same entity, which is a different use case than tracking processed data.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 33,
    "text": "A solutions architect must design a highly available infrastructure for a website. The website is \npowered by Windows web servers that run on Amazon EC2 instances. The solutions architect \nmust implement a solution that can mitigate a large-scale DDoS attack that originates from \nthousands of IP addresses. Downtime is not acceptable for the website. \nWhich actions should the solutions architect take to protect the website from such an attack? \n(Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Shield Advanced to stop the DDoS attack.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Configure Amazon GuardDuty to automatically block the attackers.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Configure the website to use Amazon CloudFront for both static and dynamic content.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use an AWS Lambda function to automatically add attacker IP addresses to VPC network",
        "correct": false
      },
      {
        "id": 4,
        "text": "Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Shield Advanced provides enhanced DDoS protection capabilities, including proactive monitoring and mitigation of sophisticated attacks. It offers features like DDoS cost protection, 24/7 access to the AWS DDoS Response Team (DRT), and advanced real-time metrics and reporting. Shield Advanced is specifically designed to protect against large and complex DDoS attacks, making it suitable for the scenario described.\n\n**Why option 1 is incorrect:**\nWhile Amazon GuardDuty detects malicious activity and unauthorized behavior, it doesn't directly block attackers in the way required to mitigate a large-scale DDoS attack. GuardDuty primarily focuses on threat detection and alerting, rather than immediate mitigation. It identifies potential threats, but it requires additional configuration and integration with other services to automatically block attackers.\n\n**Why option 2 is incorrect:**\nWhile CloudFront can help cache static content and absorb some DDoS traffic, it's not a complete solution for mitigating large-scale DDoS attacks, especially those targeting dynamic content or the origin server directly. CloudFront primarily protects against volumetric attacks targeting the edge locations, but sophisticated attacks can still overwhelm the origin. It's a good practice to use CloudFront, but it's not sufficient on its own to meet the question's requirements.\n\n**Why option 3 is incorrect:**\nUsing a Lambda function to automatically add attacker IP addresses to VPC network ACLs or security groups is not a scalable or reliable solution for mitigating large-scale DDoS attacks. Network ACLs and security groups have limitations on the number of rules, and updating them frequently can introduce latency and impact performance. Furthermore, identifying and blocking thousands of IP addresses in real-time using Lambda and network ACLs is complex and prone to errors. This approach is not recommended for mitigating large-scale DDoS attacks.\n\n**Why option 4 is incorrect:**\nUsing EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy addresses scaling the application based on load, but it does not directly mitigate a DDoS attack. While scaling can help handle increased traffic, it doesn't prevent the attack from reaching the application and potentially causing performance issues or downtime. Spot Instances are also subject to interruption, which could exacerbate the problem during an attack. This option focuses on scaling, not security.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 34,
    "text": "A company is preparing to deploy a new serverless workload.  \nA solutions architect must use the principle of least privilege to configure permissions that will be \nused to run an AWS Lambda function.  \nAn Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Add an execution role to the function with lambda:InvokeFunction as the action and * as the",
        "correct": false
      },
      {
        "id": 1,
        "text": "Add an execution role to the function with lambda:InvokeFunction as the action and",
        "correct": false
      },
      {
        "id": 2,
        "text": "Add a resource-based policy to the function with lambda:'* as the action and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Add a resource-based policy to the function with lambda:InvokeFunction as the action and",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by granting EventBridge permission to invoke the Lambda function using a resource-based policy. The action `lambda:InvokeFunction` is the precise permission needed for EventBridge to trigger the Lambda function. The resource should be the ARN of the Lambda function, and the principal should be the ARN of the EventBridge rule or the EventBridge service principal. This adheres to the principle of least privilege because it only allows EventBridge to invoke the function and nothing else.\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it uses an execution role, which is for the Lambda function to access other AWS services, not for EventBridge to invoke the Lambda function. Furthermore, using `*` as the resource violates the principle of least privilege, as it would allow the function to be invoked by any principal.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it uses an execution role, which is for the Lambda function to access other AWS services, not for EventBridge to invoke the Lambda function. While specifying the resource ARN is better than `*`, it's still the wrong type of policy.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 35,
    "text": "A company has an image processing workload running on Amazon Elastic Container Service \n(Amazon ECS) in two private subnets. Each private subnet uses a NAT instance for internet \naccess. All images are stored in Amazon S3 buckets. \nThe company is concerned about the data transfer costs between Amazon ECS and Amazon S3. \n \nWhat should a solutions architect do to reduce costs?",
    "options": [
      {
        "id": 0,
        "text": "Configure a NAT gateway to replace the NAT instances.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Configure a gateway endpoint for traffic destined to Amazon S3.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Configure an interface endpoint for traffic destined to Amazon S3.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure Amazon CloudFront for the S3 bucket storing the images.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing a direct, private connection to Amazon S3 from within the VPC. Gateway endpoints for S3 are free to use and do not incur data transfer charges for traffic originating from within the VPC and destined for S3. This eliminates the need for traffic to traverse the NAT instance, thereby reducing data transfer costs.\n\n**Why option 0 is incorrect:**\nWhile replacing NAT instances with NAT gateways is a good practice for improved availability and scalability, it doesn't directly address the data transfer cost issue between ECS and S3. Traffic would still flow through the NAT gateway, incurring data transfer charges. The core problem is the traffic leaving the VPC to reach S3.\n\n**Why option 2 is incorrect:**\nInterface endpoints (powered by PrivateLink) provide private connectivity to AWS services, but they incur hourly charges and data processing charges. While they provide a private connection, they are more expensive than gateway endpoints for S3. The question asks for a cost-effective solution, and gateway endpoints are free for S3.\n\n**Why option 3 is incorrect:**\nAmazon CloudFront is a content delivery network (CDN) used to cache content closer to users for faster access and reduced latency. While it can reduce costs for users accessing the images, it doesn't directly address the data transfer costs between the ECS tasks and the S3 buckets within the same AWS region. The ECS tasks still need to retrieve the images from S3 initially, incurring the same data transfer costs.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 36,
    "text": "A company is moving Its on-premises Oracle database to Amazon Aurora PostgreSQL.  \nThe database has several applications that write to the same tables. \nThe applications need to be migrated one by one with a month in between each migration \nManagement has expressed concerns that the database has a high number of reads and writes. \nThe data must be kept in sync across both databases throughout tie migration. \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS DataSync tor the initial migration.",
        "correct": false
      },
      {
        "id": 1,
        "text": "UseAVVS DataSync for the initial migration.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Use the AWS Schema Conversion led with AWS DataBase Migration Service (AWS DMS)",
        "correct": true
      },
      {
        "id": 3,
        "text": "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement of schema conversion using AWS Schema Conversion Tool (SCT) and continuous data replication using AWS Database Migration Service (DMS). SCT helps convert the database schema from Oracle to PostgreSQL. DMS then handles the initial data load and ongoing replication to keep the databases synchronized during the migration period. DMS supports heterogeneous database migrations and can handle a high volume of reads and writes, making it suitable for this scenario.\n\n**Why option 0 is incorrect:**\nAWS DataSync is designed for moving large amounts of data between on-premises storage and AWS storage services. It is not suitable for database schema conversion or continuous data replication required for a database migration with ongoing synchronization.\n\n**Why option 1 is incorrect:**\nAWS DataSync is designed for moving large amounts of data between on-premises storage and AWS storage services. It is not suitable for database schema conversion or continuous data replication required for a database migration with ongoing synchronization.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 37,
    "text": "A company wants to migrate a high performance computing (HPC) application and data from on-\npremises to the AWS Cloud. The company uses tiered storage on premises with hot high-\nperformance parallel storage to support the application during periodic runs of the application, \nand more economical cold storage to hold the data when the application is not actively running. \nWhich combination of solutions should a solutions architect recommend to support the storage \nneeds of the application? (Choose two.)",
    "options": [
      {
        "id": 0,
        "text": "Amazon S3 for cold data storage",
        "correct": true
      },
      {
        "id": 1,
        "text": "Amazon EFS for cold data storage",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon S3 for high-performance parallel storage",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon FSx for clustre tor high-performance parallel storage",
        "correct": false
      },
      {
        "id": 4,
        "text": "Amazon FSx for Windows for high-performance parallel storage",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Amazon S3 is a highly scalable, durable, and cost-effective object storage service, making it ideal for storing infrequently accessed or 'cold' data. It offers various storage classes, including Glacier and Glacier Deep Archive, which are specifically designed for long-term archival and cost optimization.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon EFS is a network file system designed for shared file storage, not primarily for cost-effective archival of cold data. While EFS has lifecycle management to move files to Infrequent Access storage class, it's still more expensive than S3 Glacier for long-term archival.\n\n**Why option 2 is incorrect:**\nThis is incorrect because while S3 can provide high throughput, it's not designed for the low-latency, high-performance parallel file system access required by HPC applications during active runs. S3 is object storage, not a file system.\n\n**Why option 3 is incorrect:**\nThis is correct because Amazon FSx for Lustre is a fully managed, high-performance file system optimized for compute-intensive workloads like HPC. It provides the parallel access needed for the application during active runs. It is designed for high-throughput and low-latency access to data.\n\n**Why option 4 is incorrect:**\nThis is incorrect because Amazon FSx for Windows File Server is designed for Windows-based applications and workloads. While it provides a fully managed Windows file server, it is not optimized for the high-performance parallel processing needs of an HPC application. Lustre is a better fit for HPC workloads.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 38,
    "text": "A company is experiencing growth as demand for its product has increased. The company's \nexisting purchasing application is slow when traffic spikes. The application is a monolithic three \ntier application that uses synchronous transactions and sometimes sees bottlenecks in the \napplication tier. A solutions architect needs to design a solution that can meet required application \nresponse times while accounting for traffic volume spikes. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Vertically scale the application instance using a larger Amazon EC2 instance size.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Scale the application's persistence layer horizontally by introducing Oracle RAC on AWS",
        "correct": false
      },
      {
        "id": 2,
        "text": "Scale the web and application tiers horizontally using Auto Scaling groups and an Application",
        "correct": true
      },
      {
        "id": 3,
        "text": "Decouple the application and data tiers using Amazon Simple Queue Service (Amazon SQS)",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by horizontally scaling the web and application tiers. Auto Scaling groups automatically adjust the number of instances based on demand, effectively handling traffic spikes. Using an Application Load Balancer distributes traffic across multiple instances, preventing any single instance from becoming a bottleneck. This approach improves both performance and scalability, which are the core requirements of the problem.\n\n**Why option 0 is incorrect:**\nVertically scaling the application instance by using a larger EC2 instance might provide a temporary performance boost, but it does not address the underlying scalability issue. Vertical scaling has limitations, and eventually, a single instance will become a bottleneck again, especially during significant traffic spikes. It also doesn't provide high availability.\n\n**Why option 1 is incorrect:**\nWhile scaling the persistence layer horizontally can improve database performance, introducing Oracle RAC on AWS is complex and expensive. The bottleneck is identified in the application tier, not the database tier. Addressing the application tier bottleneck directly is more efficient and cost-effective. Also, Oracle RAC is not always the best solution for scaling a database and might introduce more complexity than necessary.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 39,
    "text": "A solutions architect needs to ensure that all Amazon Elastic Block Store (Amazon EBS) volumes \nrestored from unencrypted EBS snapshots are encrypted. \n \nWhat should the solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Enable EBS encryption by default for the AWS Region",
        "correct": true
      },
      {
        "id": 1,
        "text": "Enable EBS encryption by default for the specific volumes",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a new volume and specify the symmetric customer master key (CMK) to use for",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a new volume and specify the asymmetric customer master key (CMK) to use for",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the most efficient and comprehensive solution. Enabling EBS encryption by default at the AWS Region level ensures that any new EBS volume created in that region, including those restored from unencrypted snapshots, will be automatically encrypted. This eliminates the need for manual encryption steps for each volume and provides a consistent encryption policy across the region.\n\n**Why option 1 is incorrect:**\nEnabling EBS encryption by default for specific volumes is not a viable solution. EBS encryption is not enabled or disabled on a per-volume default basis. Default encryption is a regional setting. This option is not a valid configuration.\n\n**Why option 2 is incorrect:**\nCreating a new volume and specifying a symmetric CMK is a valid way to encrypt a volume, but it doesn't address the core requirement of automatically encrypting volumes restored from *unencrypted snapshots*. This approach would require manual intervention for each volume restoration, which is not scalable or efficient.\n\n**Why option 3 is incorrect:**\nCreating a new volume and specifying an asymmetric CMK is a valid way to encrypt a volume, but similar to option 2, it doesn't address the core requirement of automatically encrypting volumes restored from *unencrypted snapshots*. This approach would require manual intervention for each volume restoration, which is not scalable or efficient. Asymmetric CMKs are also less commonly used for EBS encryption due to performance considerations.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 40,
    "text": "A company is storing backup files by using Amazon S3 Standard storage. The files are accessed \nfrequently for 1 month. However, the files are not accessed after 1 month. The company must \nkeep the files indefinitely. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n92 \n \nWhich storage solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Configure S3 Intelligent-Tiering to automatically migrate objects.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by transitioning the data from S3 Standard, which is more expensive for long-term storage, to S3 Glacier Deep Archive after one month. S3 Glacier Deep Archive is the cheapest storage option for infrequently accessed data that needs to be retained for long periods. This directly addresses the cost-effectiveness requirement while ensuring the data is kept indefinitely.\n\n**Why option 0 is incorrect:**\nWhile S3 Intelligent-Tiering automatically moves data between frequent and infrequent access tiers based on access patterns, it's designed for data with unpredictable access patterns. Since the access pattern is known (frequent for 1 month, then infrequent), using Intelligent-Tiering would be less cost-effective than directly transitioning to Glacier Deep Archive after 1 month. Intelligent-Tiering also has monitoring costs that would add to the overall expense.\n\n**Why option 2 is incorrect:**\nTransitioning to S3 Standard-IA would be less cost-effective than Glacier Deep Archive for data that is rarely accessed. S3 Standard-IA is designed for infrequently accessed data but still requires faster retrieval times than Glacier Deep Archive. Since the files are not accessed after 1 month, the lower storage cost of Glacier Deep Archive makes it a better choice.\n\n**Why option 3 is incorrect:**\nS3 One Zone-IA stores data in a single availability zone, making it less resilient than other S3 storage classes. While it's cheaper than S3 Standard-IA, it's still more expensive than Glacier Deep Archive. More importantly, it's not ideal for backup data where durability is a key concern. Also, the question states the files must be kept indefinitely, implying a need for high durability, which One Zone-IA doesn't guarantee as strongly as other options.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 41,
    "text": "A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team \nnotices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions \narchitect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-\ndepth analysis to identify the root cause of the vertical scaling. \nHow should the solutions architect generate the information with the LEAST operational \noverhead?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nCost Explorer allows for granular filtering and grouping of costs, including by instance type, usage type, and region. This enables a detailed analysis of EC2 costs over time, making it easy to identify the specific instances that have experienced unwanted vertical scaling and the associated cost increases. Cost Explorer provides built-in visualizations and reporting capabilities, minimizing the operational overhead of creating custom reports or dashboards.\n\n**Why option 0 is incorrect:**\nAWS Budgets are primarily used for setting cost thresholds and receiving alerts when costs exceed those thresholds. While Budgets can provide a high-level overview of EC2 costs, they do not offer the granular filtering and analysis capabilities needed to identify the root cause of unwanted vertical scaling. Creating a budget report would not provide the in-depth analysis required, increasing operational overhead.\n\n**Why option 2 is incorrect:**\nThe AWS Billing and Cost Management dashboard provides a general overview of costs but lacks the granular filtering and analysis capabilities of Cost Explorer. While it can show overall EC2 costs, it's not designed for in-depth analysis of specific instance types or usage patterns to identify the root cause of vertical scaling. This would require more manual effort to derive the necessary insights.\n\n**Why option 3 is incorrect:**\nAWS Cost and Usage Reports (CUR) provide the most detailed cost data, but they require significant effort to set up, configure, and analyze. The reports are delivered to an Amazon S3 bucket and need to be processed using tools like Athena or QuickSight to extract meaningful insights. This approach has the highest operational overhead compared to using Cost Explorer, which provides built-in analysis and visualization features.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 42,
    "text": "A company is designing an application. The application uses an AWS Lambda function to receive \ninformation through Amazon API Gateway and to store the information in an Amazon Aurora \nPostgreSQL database. \nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly \nto handle the high volumes of data that the company needs to load into the database. A solutions \narchitect must recommend a new design to improve scalability and minimize the configuration \neffort. \nWhich solution will meet these requirements? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n93",
    "options": [
      {
        "id": 0,
        "text": "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2",
        "correct": false
      },
      {
        "id": 1,
        "text": "Change the platform from Aurora to Amazon DynamoDB.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up two Lambda functions.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up two Lambda functions. Configure one function to receive the information.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by decoupling the API Gateway endpoint from the database write operation. The first Lambda function receives the information from API Gateway. This function can then place the data into a queue service like SQS. The second Lambda function is triggered by the queue and handles writing the data to the Aurora PostgreSQL database. This asynchronous approach allows the API Gateway endpoint to respond quickly without waiting for the database write to complete, improving scalability. SQS also provides buffering and retries, making the system more resilient. This approach also minimizes configuration effort as it leverages existing managed services (Lambda, API Gateway, SQS, Aurora) without requiring significant infrastructure changes.\n\n**Why option 0 is incorrect:**\nRefactoring the Lambda function to Apache Tomcat code running on Amazon EC2 introduces significant complexity and configuration overhead. It requires managing EC2 instances, configuring Tomcat, and handling scaling and availability. This contradicts the requirement to minimize configuration effort. Also, moving to EC2 doesn't inherently solve the database write bottleneck; it just shifts the execution environment.\n\n**Why option 1 is incorrect:**\nChanging the platform from Aurora to DynamoDB might improve write performance, but it requires a significant application rewrite and a change in the data model. Aurora PostgreSQL is a relational database, while DynamoDB is a NoSQL database. This change would likely involve substantial configuration and development effort, violating the requirement to minimize configuration effort. Furthermore, the question doesn't indicate that Aurora is the bottleneck, but rather the Lambda function's ability to handle the volume of writes.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 43,
    "text": "A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do \nnot have unauthorized configuration changes. \n \nWhat should a solutions architect do to accomplish this goal?",
    "options": [
      {
        "id": 0,
        "text": "Turn on AWS Config with the appropriate rules.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Turn on AWS Trusted Advisor with the appropriate checks.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Turn on Amazon Inspector with the appropriate assessment template.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Turn on Amazon S3 server access logging.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Config allows you to define rules that specify the desired configuration for your AWS resources, including S3 buckets. When a resource deviates from the desired configuration, AWS Config flags the non-compliance. This enables you to track unauthorized configuration changes and take corrective action.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS Trusted Advisor provides best practice recommendations across cost optimization, security, fault tolerance, service limits, and performance. While it can identify some security misconfigurations related to S3, it doesn't provide continuous monitoring and alerting for all configuration changes like AWS Config does. It's more of a point-in-time assessment rather than ongoing configuration tracking.\n\n**Why option 2 is incorrect:**\nThis is incorrect because Amazon Inspector is a vulnerability management service that automates security assessments. It helps improve the security and compliance of applications deployed on AWS. It doesn't monitor configuration changes to S3 buckets. Inspector focuses on finding vulnerabilities in EC2 instances and container images, not configuration drifts.\n\n**Why option 3 is incorrect:**\nThis is incorrect because Amazon S3 server access logging provides detailed records for the requests that are made to an S3 bucket. While useful for auditing and security analysis, it doesn't directly detect unauthorized configuration changes. It only captures access patterns, not the configuration settings themselves. Analyzing logs to detect configuration changes would require significant manual effort and custom scripting.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 44,
    "text": "A company is launching a new application and will display application metrics on an Amazon \nCloudWatch dashboard. The company's product manager needs to access this dashboard \nperiodically. The product manager does not have an AWS account. A solution architect must \nprovide access to the product manager by following the principle of least privilege. \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Share the dashboard from the CloudWatch console.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Create an IAM user specifically for the product manager.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create an IAM user for the company's employees.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy a bastion server in a public subnet.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because the CloudWatch console allows sharing dashboards with users who do not have AWS accounts. This feature generates a shareable link that can be accessed without AWS credentials, providing a simple and secure way to grant access to the dashboard. Sharing the dashboard directly avoids creating unnecessary IAM users and aligns with the principle of least privilege by granting access only to the specific resource needed.\n\n**Why option 1 is incorrect:**\nThis is incorrect because creating an IAM user for the product manager would require them to have AWS credentials, which contradicts the requirement that they do not have an AWS account. It also violates the principle of least privilege, as an IAM user could potentially be granted more permissions than necessary to simply view a CloudWatch dashboard.\n\n**Why option 2 is incorrect:**\nThis is incorrect because creating a single IAM user for all company employees is a poor security practice. It violates the principle of least privilege, as each employee would have the same level of access, regardless of their actual needs. It also creates a single point of failure and makes auditing more difficult.\n\n**Why option 3 is incorrect:**\nThis is incorrect because deploying a bastion server is not relevant to providing access to a CloudWatch dashboard. Bastion servers are typically used to provide secure access to instances in private subnets, which is not the scenario described in the question. It adds unnecessary complexity and cost to the solution.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 45,
    "text": "A company is migrating applications to AWS. The applications are deployed in different accounts. \nThe company manages the accounts centrally by using AWS Organizations. The company's \nsecurity team needs a single sign-on (SSO) solution across all the company's accounts.  \nThe company must continue managing the users and groups in its on-premises self-managed \nMicrosoft Active Directory. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Directory Service.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an identity provider (IdP) on premises.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution directly addresses the requirements by enabling AWS SSO and configuring it to connect to the company's on-premises Microsoft Active Directory. AWS SSO allows users to authenticate using their existing Active Directory credentials and access multiple AWS accounts and applications with a single sign-on. The AWS SSO console provides a centralized management interface for configuring and managing SSO access across the AWS Organization.\n\n**Why option 0 is incorrect:**\nThis option is essentially the same as option 1, but it doesn't provide enough detail about the necessary configuration. While enabling AWS SSO is the first step, it's crucial to configure it to integrate with the on-premises Active Directory to meet the requirement of managing users and groups in the existing directory. Without specifying the integration with Active Directory, the SSO solution wouldn't leverage the existing user base.\n\n**Why option 2 is incorrect:**\nAWS Directory Service provides managed directory services, including AWS Managed Microsoft AD, AD Connector, and Simple AD. While AD Connector can connect to an on-premises Active Directory, it doesn't directly provide an SSO solution across multiple AWS accounts. It primarily focuses on providing directory services within AWS, not SSO. Deploying AWS Managed Microsoft AD would require migrating users and groups from the on-premises Active Directory, which contradicts the requirement of continuing to manage users and groups in the existing on-premises directory.\n\n**Why option 3 is incorrect:**\nDeploying an identity provider (IdP) on premises is a viable solution for SSO, but it would require additional configuration and management overhead. While it could integrate with AWS, using AWS SSO directly simplifies the process and leverages AWS's managed SSO service. AWS SSO is designed to integrate seamlessly with AWS Organizations and provides a centralized management interface for SSO across multiple accounts. Deploying a separate IdP would introduce additional complexity and management responsibilities.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 46,
    "text": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. \nThe service consists of Amazon EC2 instances that run in an Auto Scaling group. The company \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n95 \nhas deployments across multiple AWS Regions. \n \nThe company needs to route users to the Region with the lowest latency. The company also \nneeds automated failover between Regions. \n \nWhich solution will meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Deploy a Network Load Balancer (NLB) and an associated target group.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Deploy an Application Load Balancer (ALB) and an associated target group.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Deploy a Network Load Balancer (NLB) and an associated target group.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy an Application Load Balancer (ALB) and an associated target group.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because Network Load Balancers (NLBs) are designed to handle UDP traffic, which is essential for the VoIP service. NLBs can be deployed across multiple AWS Regions using AWS Global Accelerator or Route 53 latency-based routing to direct users to the Region with the lowest latency. NLBs also provide automated failover between Regions in case of an outage. The associated target group contains the EC2 instances running the VoIP service.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Application Load Balancers (ALBs) do not support UDP traffic. ALBs are designed for HTTP and HTTPS traffic, making them unsuitable for a VoIP service that relies on UDP.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 47,
    "text": "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS \nfor MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a \nmonth and is the only process that uses the database. The team wants to reduce the cost of \nrunning the tests without reducing the compute and memory attributes of the DB instance. \n \nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "id": 0,
        "text": "Stop the DB instance when tests are completed.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use an Auto Scaling policy with the DB instance to automatically scale when tests are",
        "correct": false
      },
      {
        "id": 2,
        "text": "Create a snapshot when tests are completed.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Modify the DB instance to a low-capacity instance when tests are completed.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirement by allowing the team to delete the DB instance after the tests are completed and recreate it from the snapshot when needed. Since RDS charges for the time the instance is running, deleting it when not in use significantly reduces costs. Recreating from a snapshot restores the database to its previous state, preserving the data and configuration needed for the next test run. This is more cost-effective than keeping the instance running or scaling it down, as it eliminates the cost of the instance entirely for the majority of the month.\n\n**Why option 0 is incorrect:**\nStopping the DB instance when tests are completed will reduce costs, but it doesn't address the need to preserve the data and configuration for the next test run. When the instance is stopped, the data is still stored and charged for. While cheaper than running, it is not as cheap as deleting the instance and recreating from a snapshot. Also, stopping and starting an RDS instance can take a significant amount of time, adding to the overall testing time.\n\n**Why option 1 is incorrect:**\nUsing Auto Scaling with an RDS instance is not a standard practice. RDS Auto Scaling typically refers to read replicas, which are not relevant in this scenario as the question specifies it's the only process using the database. Scaling the primary instance up and down is not a feature of RDS Auto Scaling. Even if it were possible, the overhead of scaling operations and the complexity of configuring the scaling policies would likely outweigh the cost savings, especially given the short duration of the testing period and the fact that the instance is idle otherwise. Furthermore, the question specifies that the compute and memory attributes should not be reduced during the test.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 48,
    "text": "A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n96 \nAmazon RDS DB instances and Amazon Redshift clusters are configured with tags. The \ncompany wants to minimize the effort of configuring and operating this check. \n \nWhat should a solutions architect do to accomplish this?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS Config rules to define and detect resources that are not properly tagged.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Use Cost Explorer to display resources that are not properly tagged.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write API calls to check all resources for proper tag allocation.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Write API calls to check all resources for proper tag allocation.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because AWS Config allows you to define rules that check the configuration of your AWS resources. You can create a rule that checks for the presence of specific tags on EC2 instances, RDS DB instances, and Redshift clusters. AWS Config automatically evaluates your resources against these rules and provides a report of non-compliant resources. This minimizes the effort of configuring and operating the check, as it's an automated, managed service.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Cost Explorer is primarily used for analyzing and visualizing AWS costs. While Cost Explorer can use tags to filter and group costs, it doesn't inherently detect or report on resources that are not properly tagged. It requires existing tags to function effectively for cost allocation.\n\n**Why option 2 is incorrect:**\nThis is incorrect because writing API calls to check for proper tag allocation would require significant development and operational overhead. It would involve writing, deploying, and maintaining custom code to iterate through all resources and verify their tags. This approach is less efficient and scalable than using a managed service like AWS Config.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 49,
    "text": "A development team needs to host a website that will be accessed by other teams. The website \ncontents consist of HTML, CSS, client-side JavaScript, and images. \nWhich method is the MOST cost-effective for hosting the website?",
    "options": [
      {
        "id": 0,
        "text": "Containerize the website and host it in AWS Fargate.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an Amazon S3 bucket and host the website there",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy a web server on an Amazon EC2 instance to host the website.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Configure an Application Loa d Balancer with an AWS Lambda target that uses the Express js",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nHosting a static website on Amazon S3 is highly cost-effective because S3 offers low storage costs and charges only for the data stored and the data transferred out. It eliminates the need to manage servers, operating systems, or web server software, reducing operational overhead and associated costs. Furthermore, S3's pay-as-you-go pricing model aligns well with the usage patterns of a website accessed by other teams, making it a cost-optimized solution for static content delivery.\n\n**Why option 0 is incorrect:**\nContainerizing the website and hosting it in AWS Fargate, while a viable solution, involves running containers, which incurs costs for compute resources (vCPU and memory) even if the website has low traffic. This is generally more expensive than hosting static content directly on S3, especially for a simple website with static assets. Fargate is better suited for dynamic applications or microservices.\n\n**Why option 2 is incorrect:**\nDeploying a web server on an Amazon EC2 instance to host the website requires managing the EC2 instance, including patching, security updates, and web server configuration. This incurs costs for the EC2 instance itself, as well as the operational overhead of managing the server. This is significantly more expensive than hosting static content on S3, which requires minimal management.\n\n**Why option 3 is incorrect:**\nConfiguring an Application Load Balancer (ALB) with an AWS Lambda target using Express.js is an overly complex and expensive solution for hosting static website content. Lambda is designed for event-driven, serverless compute, and using it to serve static assets adds unnecessary overhead and cost. The ALB also adds to the cost. This approach is more suitable for dynamic content generation or API endpoints, not for serving static files.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 50,
    "text": "A company runs an online marketplace web application on AWS. The application serves \nhundreds of thousands of users during peak hours. The company needs a scalable, near-real-\ntime solution to share the details of millions of financial transactions with several other internal \napplications Transactions also need to be processed to remove sensitive data before being \nstored in a document database for low-latency retrieval. \n \nWhat should a solutions architect recommend to meet these requirements? \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n97",
    "options": [
      {
        "id": 0,
        "text": "Store the transactions data into Amazon DynamoDB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Stream the transactions data into Amazon Kinesis Data.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Stream the transactions data into Amazon Kinesis Data Streams.",
        "correct": true
      },
      {
        "id": 3,
        "text": "Store the batched transactions data in Amazon S3 as files.",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis solution addresses the requirements by using Kinesis Data Streams for near real-time ingestion of the transaction data. Kinesis Data Streams is designed for high-throughput, scalable streaming data ingestion. It allows for custom processing of the data, which can be used to remove sensitive information before storing it in a document database. The processed data can then be shared with other internal applications. Kinesis Data Streams provides the necessary infrastructure to handle the volume of transactions and the near real-time processing requirement.\n\n**Why option 0 is incorrect:**\nStoring the raw transactions directly into DynamoDB does not address the requirement of removing sensitive data before storage. While DynamoDB offers low-latency retrieval, it doesn't provide built-in data transformation capabilities. Furthermore, directly storing millions of transactions into DynamoDB without any processing or streaming mechanism might impact performance and scalability.\n\n**Why option 1 is incorrect:**\nWhile 'Stream the transactions data into Amazon Kinesis Data' sounds plausible, it's not specific enough. Kinesis Data encompasses several services, including Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics. Kinesis Data Streams is the most appropriate choice for this scenario because it allows for custom processing of the data before storage, which is crucial for removing sensitive information. Kinesis Data Firehose is primarily used for loading data into data lakes and data warehouses, and Kinesis Data Analytics is used for real-time analytics. The question requires data transformation, which is best achieved with Kinesis Data Streams and custom processing logic.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 51,
    "text": "A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and \nsecurity, the company must track configuration changes on its AWS resources and record a \nhistory of API calls made to these resources. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution accurately addresses the requirements. AWS Config continuously monitors and records the configuration of AWS resources, allowing for tracking of configuration changes over time. AWS CloudTrail records API calls made to AWS services, providing a history of actions taken on resources, which is crucial for auditing and security analysis.\n\n**Why option 0 is incorrect:**\nThis option reverses the roles of CloudTrail and Config. CloudTrail is primarily for recording API calls, not configuration changes. Config is designed for tracking configuration changes and compliance.\n\n**Why option 2 is incorrect:**\nAmazon CloudWatch is primarily a monitoring service for metrics and logs. While it can be used to monitor API calls through custom metrics and logs, it's not its primary function, and it doesn't provide the same level of auditing and compliance features as CloudTrail for recording API calls. Config is correctly identified for tracking configuration changes.\n\n**Why option 3 is incorrect:**\nAmazon CloudWatch is primarily a monitoring service for metrics and logs. While it can be used to monitor API calls through custom metrics and logs, it's not its primary function, and it doesn't provide the same level of auditing and compliance features as CloudTrail for recording API calls. CloudTrail is correctly identified for recording API calls, but it is not the primary service for tracking configuration changes.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 52,
    "text": "A company is preparing to launch a public-facing web application in the AWS Cloud. The \narchitecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer \n(ELB). A third-party service is used for the DNS. The company's solutions architect must \nrecommend a solution to detect and protect against large-scale DDoS attacks. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Enable Amazon GuardDuty on the account.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Enable Amazon Inspector on the EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Enable AWS Shield and assign Amazon Route 53 to it.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Enable AWS Shield Advanced and assign the ELB to it.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirement by providing comprehensive DDoS protection for the web application. AWS Shield Advanced offers enhanced detection and mitigation capabilities compared to AWS Shield Standard, which is automatically enabled for all AWS customers. Assigning the ELB to AWS Shield Advanced ensures that the load balancer, which is the entry point for web traffic, is protected against DDoS attacks. Shield Advanced also provides 24x7 access to the AWS DDoS Response Team (DRT) for assistance during attacks. Since the application is public-facing and requires protection against large-scale DDoS attacks, Shield Advanced is the appropriate choice.\n\n**Why option 0 is incorrect:**\nThis is incorrect because Amazon GuardDuty is a threat detection service that monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. While GuardDuty can detect some anomalies that might indicate a DDoS attack, it doesn't provide direct mitigation capabilities against DDoS attacks. It primarily focuses on identifying threats within your AWS environment, not preventing external attacks from reaching your application.\n\n**Why option 1 is incorrect:**\nThis is incorrect because Amazon Inspector is a vulnerability management service that automatically assesses EC2 instances and container images for software vulnerabilities and unintended network exposure. While identifying vulnerabilities is important for overall security, Inspector doesn't directly protect against DDoS attacks. It focuses on finding weaknesses within the EC2 instances themselves, not preventing external attacks from overwhelming the application.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 53,
    "text": "A company is building an application in the AWS Cloud. The application will store data in Amazon \nS3 buckets in two AWS Regions. The company must use an AWS Key Management Service \n(AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data \nin both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the \nkey must be stored in each of the two Regions. \nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "id": 0,
        "text": "Create an S3 bucket in each Region.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a customer managed multi-Region KMS key.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a customer managed KMS key and an S3 bucket in each Region.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create a customer managed KMS key and an S3 bucket in each Region.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by creating a multi-Region KMS key. Multi-Region keys are designed specifically for scenarios where data needs to be encrypted and decrypted across multiple AWS Regions using the same key material. AWS KMS handles the replication and synchronization of the key material between the specified Regions, minimizing operational overhead compared to managing separate keys and their replication manually. This also ensures that the data and the key are stored in each of the two Regions, as the key material is replicated by KMS.\n\n**Why option 0 is incorrect:**\nThis option only addresses the S3 bucket creation requirement but doesn't address the encryption or key management aspects. It's incomplete and doesn't provide any solution for the core requirements of the question.\n\n**Why option 2 is incorrect:**\nWhile this option creates the necessary resources (KMS key and S3 buckets), it doesn't specify how the same key will be used across both regions. Using a standard customer-managed KMS key would require manual key replication or cross-region KMS key access, which increases operational overhead. It does not inherently guarantee that the *same* key is used in both regions, and would require additional configuration and management.\n\n**Why option 3 is incorrect:**\nThis option is identical to option 2 and suffers from the same flaws. It creates the resources but doesn't address the key replication and management complexities for cross-region encryption using the *same* key, leading to higher operational overhead.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 54,
    "text": "A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data \nsources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 \ninstance is rebooted, the data in-flight is lost. \n \nThe company's data science team wants to query ingested data near-real time. \n \nWhich solution provides near-real-time data querying that is scalable with minimal data loss?",
    "options": [
      {
        "id": 0,
        "text": "Publish data to Amazon Kinesis Data Streams.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Store ingested data in an EC2 instance store.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by using Kinesis Data Firehose to ingest data and deliver it to Amazon Redshift. Kinesis Data Firehose provides a reliable and scalable way to ingest streaming data. By using Amazon Redshift as the destination, the data science team can query the ingested data in near-real-time using SQL. Kinesis Data Firehose also handles data buffering and retry mechanisms, minimizing data loss during EC2 instance reboots. The data is persisted to Redshift, which provides durability and availability. The 1 MB/s ingestion rate is well within the capabilities of Kinesis Data Firehose.\n\n**Why option 0 is incorrect:**\nWhile Kinesis Data Streams can handle the ingestion rate and provide near-real-time data processing, it doesn't directly provide a querying capability. Kinesis Data Streams requires a consumer application to process the data, and the data science team would still need a separate data store and querying mechanism. This adds complexity and doesn't directly address the requirement of near-real-time querying. It also does not inherently minimize data loss during EC2 reboots without additional configuration and implementation.\n\n**Why option 2 is incorrect:**\nEC2 instance store provides temporary block-level storage for EC2 instances. Data stored in the instance store is ephemeral and is lost when the instance is stopped, terminated, or fails. This option directly contradicts the requirement of minimizing data loss during EC2 instance reboots.\n\n**Why option 3 is incorrect:**\nWhile EBS volumes provide persistent block storage for EC2 instances, they do not inherently provide a near-real-time querying capability. Storing the data in an EBS volume would require a separate application or database to be set up on the EC2 instance to process and query the data. This adds complexity and doesn't directly address the requirement of near-real-time querying. Furthermore, while EBS provides persistence, the data still needs to be written to the EBS volume, and any data in-flight during a reboot could be lost unless additional mechanisms are implemented.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 55,
    "text": "A company is developing a mobile game that streams score updates to a backend processor and \nthen posts results on a leaderboard. \nA solutions architect needs to design a solution that can handle large traffic spikes, process the \nmobile game updates in order of receipt, and store the processed updates in a highly available \ndatabase. The company also wants to minimize the management overhead required to maintain \nthe solution. \n \nWhat should the solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Push score updates to Amazon Kinesis Data Streams.",
        "correct": true
      },
      {
        "id": 1,
        "text": "Push score updates to Amazon Kinesis Data Streams.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue.",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is the best solution because Kinesis Data Streams is designed for ingesting and processing high-volume, real-time streaming data. It provides the necessary scalability to handle large traffic spikes from the mobile game. Importantly, Kinesis Data Streams guarantees record order within a shard, which fulfills the requirement to process updates in the order of receipt. Furthermore, Kinesis Data Streams is a managed service, reducing the management overhead compared to setting up and maintaining a custom solution.\n\n**Why option 1 is incorrect:**\nWhile Kinesis Data Streams can handle the traffic volume, this option is incorrect because it doesn't specify how the data will be processed or stored. Simply pushing data to Kinesis is not a complete solution. A Kinesis Data Stream needs to be consumed by a Kinesis Data Analytics application or a custom consumer to process the data and store it in a database. The original option is better because it implies that the data will be processed and stored.\n\n**Why option 2 is incorrect:**\nAmazon SNS is a publish/subscribe messaging service that is suitable for broadcasting messages to multiple subscribers. However, it does not inherently guarantee message order, making it unsuitable for the requirement to process updates in the order of receipt. Also, SNS is not designed for high-throughput data streaming like Kinesis Data Streams. Furthermore, SNS does not directly store the data; it only delivers messages to subscribers.\n\n**Why option 3 is incorrect:**\nAmazon SQS is a queuing service that can decouple components and handle asynchronous processing. While SQS FIFO queues can guarantee message order, they typically have lower throughput compared to Kinesis Data Streams. For a mobile game with potentially very high traffic, Kinesis Data Streams is a better choice for handling the volume and guaranteeing order. Also, SQS requires more management overhead compared to Kinesis Data Streams for this specific use case.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 56,
    "text": "An ecommerce website is deploying its web application as Amazon Elastic Container Service \n(Amazon ECS) container instance behind an Application Load Balancer (ALB). During periods of \nhigh activity, the website slows down and availability is reduced. A solutions architect uses \nAmazon CloudWatch alarms to receive notifications whenever there is an availability issues so \nthey can scale out resource Company management wants a solution that automatically responds \nto such events. \n \nWhich solution meets these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when there are timeouts on the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when the ALB CPU utilization is too",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when the service's CPU utilization is too",
        "correct": true
      },
      {
        "id": 3,
        "text": "Set up AWS Auto Scaling to scale out the ECS service when the ALB target group CPU",
        "correct": false
      }
    ],
    "correctAnswers": [
      2
    ],
    "explanation": "**Why option 2 is correct:**\nThis is the correct solution because it directly addresses the problem of high CPU utilization within the ECS service itself. When the service's CPU utilization is high, it indicates that the existing tasks are overloaded and cannot handle the incoming requests efficiently. Scaling out the ECS service based on its CPU utilization ensures that new tasks are launched to distribute the load, thereby improving performance and availability. This approach directly tackles the bottleneck within the ECS service, leading to a more responsive and stable application.\n\n**Why option 0 is incorrect:**\nScaling based on ALB timeouts is reactive and indicates that the application is already failing to respond in a timely manner. While timeouts are a symptom of a problem, scaling based on them means the system is already experiencing performance degradation. Scaling based on resource utilization is a more proactive approach.\n\n**Why option 1 is incorrect:**\nScaling based on ALB CPU utilization is not the most effective approach. The ALB's CPU utilization might be high due to various factors, not necessarily related to the ECS service's capacity. The ALB could be handling a large volume of requests, but the ECS service might still have sufficient resources to handle them. Therefore, scaling based on the ECS service's CPU utilization is more accurate and relevant to the application's performance.\n\n**Why option 3 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 57,
    "text": "A company has no existing file share services. A new project requires access to file storage that \nis mountable as a drive for on-premises desktops. The file server must authenticate users to an \nActive Directory domain before they are able to access the storage. \nWhich service will allow Active Directory users to mount storage as a drive on their desktops? \n \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n101",
    "options": [
      {
        "id": 0,
        "text": "AWS S3 Glacier",
        "correct": false
      },
      {
        "id": 1,
        "text": "AWS DataSync",
        "correct": false
      },
      {
        "id": 2,
        "text": "AWS Snowball Edge",
        "correct": false
      },
      {
        "id": 3,
        "text": "AWS Storage Gateway",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis is correct because AWS Storage Gateway's File Gateway allows on-premises applications to store files as objects in Amazon S3 while maintaining local, low-latency access to frequently used data. It supports standard file protocols like NFS and SMB, enabling users to mount the file share as a drive on their desktops. File Gateway can also integrate with Active Directory for user authentication and authorization, fulfilling all the requirements of the scenario.\n\n**Why option 0 is incorrect:**\nThis is incorrect because AWS S3 Glacier is an archive storage service for infrequently accessed data. It is not designed for mounting as a drive and does not support Active Directory integration for authentication.\n\n**Why option 1 is incorrect:**\nThis is incorrect because AWS DataSync is a data transfer service used to move data between on-premises storage and AWS storage services. It does not provide a mountable file share or Active Directory integration for user authentication.\n\n**Why option 2 is incorrect:**\nThis is incorrect because AWS Snowball Edge is a physical device used to transfer large amounts of data into and out of AWS. While it can provide local storage and compute, it's primarily designed for data migration and edge computing scenarios, not for providing a continuously available, mountable file share with Active Directory integration.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 58,
    "text": "Management has decided to deploy all AWS VPCs with IPv6 enabled. After sometime, a \nsolutions architect tries to launch a new instance and receives an error stating that there is no \nenough IP address space available in the subnet. \n \nWhat should the solutions architect do to fix this?",
    "options": [
      {
        "id": 0,
        "text": "Check to make sure that only IPv6 was used during the VPC creation",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create a new IPv4 subnet with a larger range, and then launch the instance",
        "correct": true
      },
      {
        "id": 2,
        "text": "Create a new IPv6-only subnet with a larger range, and then launch the instance",
        "correct": false
      },
      {
        "id": 3,
        "text": "Disable the IPv4 subnet and migrate all instances to IPv6 only.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the problem directly. Even with IPv6 enabled, AWS requires an IPv4 CIDR block for the VPC and its subnets. The error message indicates that the existing IPv4 subnet has run out of addresses. Creating a new IPv4 subnet with a larger CIDR block provides more IPv4 addresses, allowing the instance to be launched. This approach doesn't disrupt existing IPv6 functionality and resolves the immediate issue.\n\n**Why option 0 is incorrect:**\nChecking if only IPv6 was used during VPC creation is irrelevant. AWS VPCs, by default, require an IPv4 CIDR block, even when IPv6 is enabled. The problem is not about whether IPv6 was used, but about the exhaustion of IPv4 addresses in the existing subnet. This option doesn't address the root cause of the problem.\n\n**Why option 2 is incorrect:**\nCreating a new IPv6-only subnet doesn't solve the problem. While the VPC has IPv6 enabled, AWS still requires an IPv4 CIDR block for the VPC and its subnets. The error message indicates that the existing IPv4 subnet has run out of addresses, so creating an IPv6-only subnet will not resolve the IPv4 address exhaustion issue. The instance still needs an IPv4 address to be launched, even if it primarily uses IPv6.\n\n**Why option 3 is incorrect:**\nDisabling the IPv4 subnet and migrating all instances to IPv6 only is not a practical or immediately feasible solution. While migrating to IPv6-only might be a long-term goal, it's a complex undertaking that requires significant planning and effort. Moreover, many AWS services and applications still rely on IPv4. Disabling the IPv4 subnet without proper migration would likely break existing applications and services. The immediate problem is the inability to launch a new instance, and this option doesn't provide a quick or easy solution.",
    "domain": "Design Resilient Architectures"
  },
  {
    "id": 59,
    "text": "A company operates an ecommerce website on Amazon EC2 instances behind an Application \nLoad Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues \nrelated to a high request rate from illegitimate external systems with changing IP addresses. The \nsecurity team is worried about potential DDoS attacks against the website. The company must \nblock the illegitimate incoming requests in a way that has a minimal impact on legitimate users. \n \nWhat should a solutions architect recommend?",
    "options": [
      {
        "id": 0,
        "text": "Deploy Amazon Inspector and associate it with the ALB.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.",
        "correct": true
      },
      {
        "id": 2,
        "text": "Deploy rules to the network ACLs associated with the ALB to block the incoming traffic.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirement by providing a web application firewall (WAF) that can be associated with the Application Load Balancer (ALB). AWS WAF allows you to define rules to filter traffic based on various criteria, including IP addresses, HTTP headers, and request body. Configuring a rate-limiting rule allows you to limit the number of requests from a specific IP address within a defined time period. This effectively mitigates DDoS attacks and blocks illegitimate requests while minimizing the impact on legitimate users by only limiting requests from offending IP addresses.\n\n**Why option 0 is incorrect:**\nAmazon Inspector is a vulnerability management service that automates security assessments. It helps identify security vulnerabilities and deviations from security best practices within your AWS environment. While important for overall security, it doesn't directly address the immediate need to block illegitimate incoming requests and mitigate DDoS attacks. It won't block traffic or provide rate limiting.\n\n**Why option 2 is incorrect:**\nNetwork ACLs (NACLs) provide stateless packet filtering at the subnet level. While NACLs can block traffic based on IP addresses, they are not well-suited for blocking traffic from a large number of rapidly changing IP addresses. Managing a large number of NACL rules can be complex and error-prone. Also, NACLs are stateless, meaning they don't track connections, making rate limiting difficult. This option would also likely impact legitimate users due to the difficulty of maintaining accurate and granular blocking rules.\n\n**Why option 3 is incorrect:**\nAmazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and unauthorized behavior. While GuardDuty can detect potential DDoS attacks, it doesn't directly block or mitigate them. It primarily provides alerts and findings, requiring you to take separate action to block the traffic. GuardDuty does not have a built-in rate-limiting protection feature.",
    "domain": "Design Secure Architectures"
  },
  {
    "id": 60,
    "text": "A media company is evaluating the possibility of moving its systems to the AWS Cloud. The \ncompany needs at least 10 TB of storage with the maximum possible I/O performance for video \nprocessing, 300 TB of very durable storage for storing media content, and 900 TB of storage to \nmeet requirements for archival media that is not in use anymore. \nWhich set of services should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Amazon EBS for maximum performance.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Amazon EBS for maximum performance.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Amazon EC2 instance store for maximum performance.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Amazon EC2 Instance store for maximum performance.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\nThis solution addresses the requirements by using Amazon EC2 instance store for maximum performance, Amazon S3 for very durable storage, and Amazon S3 Glacier for archival media. EC2 instance store provides the highest possible I/O performance because it's physically attached to the host server. Amazon S3 offers high durability and scalability for storing media content. Amazon S3 Glacier is designed for long-term archival storage at a very low cost.\n\n**Why option 0 is incorrect:**\nThis option only mentions Amazon EBS for maximum performance, but it doesn't address the other storage requirements for durable media content and archival media. While EBS can provide high performance with provisioned IOPS, it's not the most cost-effective or suitable solution for all three storage tiers. Also, EBS is not the absolute highest performance storage option available on AWS.\n\n**Why option 1 is incorrect:**\nThis option only mentions Amazon EBS for maximum performance, but it doesn't address the other storage requirements for durable media content and archival media. While EBS can provide high performance with provisioned IOPS, it's not the most cost-effective or suitable solution for all three storage tiers. Also, EBS is not the absolute highest performance storage option available on AWS.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 61,
    "text": "A company wants to run applications in containers in the AWS Cloud. These applications are \nstateless and can tolerate disruptions within the underlying infrastructure. The company needs a \nsolution that minimizes cost and operational overhead. \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution addresses the requirements by leveraging Spot Instances for cost savings and Amazon EKS managed nodes to reduce operational overhead. Spot Instances offer significant discounts compared to On-Demand Instances, making them ideal for cost-sensitive applications. EKS managed nodes abstract away the complexities of managing the underlying infrastructure for the Kubernetes cluster, further reducing operational burden. The application's stateless nature and tolerance for disruptions align well with the characteristics of Spot Instances, as interruptions can be handled gracefully without significant impact.\n\n**Why option 0 is incorrect:**\nWhile using Spot Instances in an EC2 Auto Scaling group can reduce costs, it requires more manual configuration and management compared to using a managed Kubernetes service like EKS. Managing the container orchestration and scaling within the EC2 Auto Scaling group adds operational overhead that the question aims to minimize. It doesn't provide the container orchestration benefits of EKS.\n\n**Why option 2 is incorrect:**\nOn-Demand Instances are more expensive than Spot Instances. Using them would not meet the requirement of minimizing cost. While they offer more reliability, the application is tolerant of disruptions, making the higher cost of On-Demand Instances unnecessary.\n\n**Why option 3 is incorrect:**\nOn-Demand Instances are more expensive than Spot Instances. Using them would not meet the requirement of minimizing cost. While EKS managed nodes reduce operational overhead, the cost factor makes this option less suitable than using Spot Instances.",
    "domain": "Design Cost-Optimized Architectures"
  },
  {
    "id": 62,
    "text": "A company is running a multi-tier web application on premises. The web application is \ncontainerized and runs on a number of Linux hosts connected to a PostgreSQL database that \ncontains user records. The operational overhead of maintaining the infrastructure and capacity \nplanning is limiting the company's growth. A solutions architect must improve the application's \ninfrastructure. \n \nWhich combination of actions should the solutions architect take to accomplish this? (Choose \ntwo.)",
    "options": [
      {
        "id": 0,
        "text": "Migrate the PostgreSQL database to Amazon Aurora",
        "correct": true
      },
      {
        "id": 1,
        "text": "Migrate the web application to be hosted on Amazon EC2 instances.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Set up an Amazon CloudFront distribution for the web application content.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Set up Amazon ElastiCache between the web application and the PostgreSQL database.",
        "correct": false
      },
      {
        "id": 4,
        "text": "Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container",
        "correct": false
      }
    ],
    "correctAnswers": [
      0
    ],
    "explanation": "**Why option 0 is correct:**\nThis is correct because migrating the PostgreSQL database to Amazon Aurora significantly reduces operational overhead. Aurora handles tasks like patching, backups, and replication automatically, freeing up the company's resources. Aurora also simplifies capacity planning by allowing the company to easily scale the database up or down as needed, without the need for manual provisioning and configuration of hardware.\n\n**Why option 1 is incorrect:**\nWhile migrating the web application to EC2 instances would move the application to the cloud, it doesn't inherently reduce operational overhead related to the web application layer. The company would still be responsible for managing the EC2 instances, including patching, scaling, and ensuring high availability. This doesn't directly address the core problem of reducing operational burden.\n\n**Why option 2 is incorrect:**\nSetting up an Amazon CloudFront distribution would improve the performance and availability of the web application by caching static content closer to users. However, it does not directly address the operational overhead and capacity planning issues related to the underlying infrastructure. CloudFront is primarily focused on content delivery, not infrastructure management.\n\n**Why option 3 is incorrect:**\nSetting up Amazon ElastiCache between the web application and the PostgreSQL database would improve the application's performance by caching frequently accessed data. However, it does not directly address the operational overhead and capacity planning issues related to the database infrastructure. While ElastiCache can reduce the load on the database, it doesn't eliminate the need for database management and scaling.\n\n**Why option 4 is incorrect:**\nMigrating the web application to AWS Fargate with Amazon Elastic Container Service (ECS) is a good choice to reduce operational overhead for the web application layer. Fargate eliminates the need to manage the underlying EC2 instances, allowing the company to focus on the application itself. Capacity planning is also simplified, as Fargate automatically scales the application based on demand. However, the question asks for *two* actions, and migrating the database is also essential.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 63,
    "text": "An application runs on Amazon EC2 instances across multiple Availability Zones. The instances \nrun in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application \nperforms best when the CPU utilization of the EC2 instances is at or near 40%.  \nWhat should a solutions architect do to maintain the desired performance across all instances in \nthe group?",
    "options": [
      {
        "id": 0,
        "text": "Use a simple scaling policy to dynamically scale the Auto Scaling group",
        "correct": false
      },
      {
        "id": 1,
        "text": "Use a target tracking policy to dynamically scale the Auto Scaling group",
        "correct": true
      },
      {
        "id": 2,
        "text": "Use an AWS Lambda function to update the desired Auto Scaling group capacity.",
        "correct": false
      },
      {
        "id": 3,
        "text": "Use scheduled scaling actions to scale up and scale down the Auto Scaling group",
        "correct": false
      }
    ],
    "correctAnswers": [
      1
    ],
    "explanation": "**Why option 1 is correct:**\nThis solution is appropriate because target tracking scaling policies are designed to maintain a specific metric at a target value. In this case, the target metric is CPU utilization, and the target value is 40%. The Auto Scaling group will automatically adjust the number of instances to keep the average CPU utilization of the group as close as possible to the specified target. This is the most effective way to maintain the desired performance across all instances.\n\n**Why option 0 is incorrect:**\nSimple scaling policies scale based on a threshold being breached. While they can be used for CPU utilization, they require defining specific thresholds and scaling adjustments (e.g., add 2 instances when CPU exceeds 80%). They don't automatically maintain a target value like 40%. This requires more manual configuration and is less adaptive than target tracking.\n\n**Why option 2 is incorrect:**\nUsing a Lambda function to update the desired capacity is a valid approach, but it introduces unnecessary complexity. It requires writing and maintaining custom code to monitor CPU utilization and adjust the Auto Scaling group's desired capacity. Target tracking policies provide a built-in, managed solution for this specific use case, making it a simpler and more efficient option.\n\n**Why option 3 is incorrect:**\nScheduled scaling actions scale the Auto Scaling group based on a predefined schedule. This approach is suitable for predictable traffic patterns, but it's not effective for maintaining a specific CPU utilization target. CPU utilization can fluctuate based on various factors, and a scheduled approach won't dynamically adapt to these changes. The application requires dynamic scaling based on real-time CPU utilization, not a fixed schedule.",
    "domain": "Design High-Performing Architectures"
  },
  {
    "id": 64,
    "text": "A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. \nThe company wants to serve all the files through an Amazon CloudFront distribution. The \ncompany does not want the files to be accessible through direct navigation to the S3 URL. \n\n \n                                                                                \nGet Latest & Actual SAA-C03 Exam's Question and Answers from Passleader.                                 \nhttps://www.passleader.com  \n104 \n \nWhat should a solutions architect do to meet these requirements?",
    "options": [
      {
        "id": 0,
        "text": "Write individual policies for each S3 bucket to grant read permission for only CloudFront access.",
        "correct": false
      },
      {
        "id": 1,
        "text": "Create an IAM user. Grant the user read permission to objects in the S3 bucket.",
        "correct": false
      },
      {
        "id": 2,
        "text": "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and",
        "correct": false
      },
      {
        "id": 3,
        "text": "Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution.",
        "correct": true
      }
    ],
    "correctAnswers": [
      3
    ],
    "explanation": "**Why option 3 is correct:**\n{\n    \"analysis\": \"The question describes a scenario where a company wants to use S3 for file storage and CloudFront for content delivery, but they want to prevent users from directly accessing the S3 bucket via its URL. This is a common requirement for securing content and ensuring that it's served through CloudFront's caching and security features. The core of the problem is restricting direct access to S3 while allowing CloudFront to access the objects.\",\n    \"correct_explanations\": {\n\n**Why option 0 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 1 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.\n\n**Why option 2 is incorrect:**\nThis option is incorrect because it does not meet the specific requirements outlined in the scenario.",
    "domain": "Design Secure Architectures"
  }
]